{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We create an instance of a SimpleSkyjoEnv environment, call reset() to initialize the game and list the available agents (players):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rlskyjo.environment.simple_skyjo_env_v2' from '/home/michi/skybo_rl/rlskyjo/environment/simple_skyjo_env_v2.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from rlskyjo.environment import simple_skyjo_env_v2\n",
    "from importlib import reload\n",
    "reload(simple_skyjo_env_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "skyjo_env_cfg = {\"num_players\": 3}\n",
    "env = simple_skyjo_env_v2.env(**skyjo_env_cfg)\n",
    "env.reset()\n",
    "\n",
    "env.agents, env.agent_selection\n",
    "\n",
    "def sample_place():\n",
    "    return np.random.randint(0,11)\n",
    "        \n",
    "def sample_draw():\n",
    "    return np.random.randint(12,13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 draw\n",
      "player_1 place\n"
     ]
    }
   ],
   "source": [
    "print(env.agent_selection, \"draw\")\n",
    "env.step(sample_draw())\n",
    "print(env.agent_selection, \"place\")\n",
    "env.step(\n",
    "    sample_place()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_admissible_policy(observation, action_mask):\n",
    "    \"\"\"picks randomly an admissible action from the action mask\"\"\"\n",
    "    return np.random.choice(\n",
    "        np.arange(len(action_mask)),\n",
    "        p= action_mask/np.sum(action_mask)\n",
    "    )\n",
    "\n",
    "assert 1 not in [random_admissible_policy(None,[1,0,1]) for _ in range(1000)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training fct: {'observations': array([15,  5, 15, 15, 15, 15, 15, 15, 15, 15, 15, 12, -2, 10, -2, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t11\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t5\tx\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t-2\tx]  \n",
      "\n",
      "sampled action player_1: 12\n",
      "training fct: {'observations': array([15,  5, 15, 15, 15, 15, 15, 15, 15, 15, 15, 12, -2, 10, -2,  2],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 2 \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t11\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t5\tx\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t-2\tx]  \n",
      "\n",
      "sampled action player_1: 2\n",
      "training fct: {'observations': array([15,  9, 15, 15, 15, 15, 15, 15, 15, 15, 15,  3, -2, 10, 11, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t5\tx\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t-2\tx]  \n",
      "\n",
      "sampled action player_2: 12\n",
      "training fct: {'observations': array([15,  9, 15, 15, 15, 15, 15, 15, 15, 15, 15,  3, -2, 10, 11,  8],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 8 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t5\tx\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t-2\tx]  \n",
      "\n",
      "sampled action player_2: 10\n",
      "training fct: {'observations': array([15, 12, 15, 15, 15, 15, 15, 15, 15, 15, 15,  5,  7, 10, -2, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t5\tx\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t8\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([15, 12, 15, 15, 15, 15, 15, 15, 15, 15, 15,  5,  7, 10, 11, -2],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: -2 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t5\tx\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t8\tx]  \n",
      "\n",
      "sampled action player_0: 8\n",
      "training fct: {'observations': array([15,  5, 15, 15, 15, 15, 15, 15, 11, 15, 15, 12,  7,  9,  5, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [-2\tx\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t5\tx\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t8\tx]  \n",
      "\n",
      "sampled action player_1: 12\n",
      "training fct: {'observations': array([15,  5, 15, 15, 15, 15, 15, 15, 11, 15, 15, 12,  7,  9,  5,  6],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 6 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [-2\tx\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t5\tx\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t8\tx]  \n",
      "\n",
      "sampled action player_1: 9\n",
      "training fct: {'observations': array([15,  9, 15, 15, 15, 15, 15, 15,  7, 15, 15,  3,  8,  9,  5, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [-2\tx\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t6\tx\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t8\tx]  \n",
      "\n",
      "sampled action player_2: 12\n",
      "training fct: {'observations': array([15,  9, 15, 15, 15, 15, 15, 15,  7, 15, 15,  3,  8,  9,  5,  4],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 4 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [-2\tx\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t6\tx\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t8\tx]  \n",
      "\n",
      "sampled action player_2: 10\n",
      "training fct: {'observations': array([15, 12, 15, 15, 15, 15, 15, 15, -2, 15, 15,  5,  4,  9,  8, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [-2\tx\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t6\tx\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([15, 12, 15, 15, 15, 15, 15, 15, -2, 15, 15,  5,  4,  9,  5,  8],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 8 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [-2\tx\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t6\tx\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_0: 9\n",
      "training fct: {'observations': array([15,  5, 15, 15, 15, 15, 15, 15, 11,  6, 15, 12,  4,  8,  0, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [-2\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t6\tx\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([15,  5, 15, 15, 15, 15, 15, 15, 11,  6, 15, 12,  4,  8,  5,  0],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 0 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [-2\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t6\tx\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_1: 10\n",
      "training fct: {'observations': array([15,  9, 15, 15, 15, 15, 15, 15,  7,  9, 15,  3,  4,  8,  0, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [-2\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t6\t0\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_2: 12\n",
      "training fct: {'observations': array([15,  9, 15, 15, 15, 15, 15, 15,  7,  9, 15,  3,  4,  8,  0, -1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: -1 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [-2\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t6\t0\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_2: 1\n",
      "training fct: {'observations': array([15, 12, 15, 15, 15, 15, 15, 15, -2,  8, 15,  5,  3,  8,  9, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [-2\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t6\t0\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([15, 12, 15, 15, 15, 15, 15, 15, -2,  8, 15,  5,  3,  8,  0,  9],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 9 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [-2\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t6\t0\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_0: 5\n",
      "training fct: {'observations': array([15,  5, 15, 15, 15,  3, 15, 15, 11,  6, 15, 12,  3,  7,  4, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [-2\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t6\t0\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_1: 12\n",
      "training fct: {'observations': array([15,  5, 15, 15, 15,  3, 15, 15, 11,  6, 15, 12,  3,  7,  4,  2],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 2 \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [-2\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t6\t0\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_1: 7\n",
      "training fct: {'observations': array([15, -1, 15, 15, 15,  4, 15, 15,  7,  9, 15,  3,  3,  7,  3, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 3 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [-2\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t0\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_2: 12\n",
      "training fct: {'observations': array([15, -1, 15, 15, 15,  4, 15, 15,  7,  9, 15,  3,  3,  7,  3, 10],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 10 \n",
      "discard pile top: 3 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [-2\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t0\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_2: 2\n",
      "training fct: {'observations': array([15, 12, 15, 15, 15,  9, 15, 15, -2,  8, 15,  5, 10,  7,  0, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [-2\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t0\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_0: 12\n",
      "training fct: {'observations': array([15, 12, 15, 15, 15,  9, 15, 15, -2,  8, 15,  5, 10,  7,  0,  1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 1 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [-2\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t0\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_0: 8\n",
      "training fct: {'observations': array([15,  5, 15, 15, 15,  3, 15, 15, 11,  6, 15, 12, 10,  7, -2, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [1\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t0\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_1: 12\n",
      "training fct: {'observations': array([15,  5, 15, 15, 15,  3, 15, 15, 11,  6, 15, 12, 10,  7, -2, 11],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 11 \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [1\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t0\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_1: 10\n",
      "training fct: {'observations': array([15, -1, 15, 15, 15,  4, 15, 15,  7,  9, 15,  3, 13,  7,  0, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [1\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t11\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_2: 13\n",
      "training fct: {'observations': array([15, -1, 15, 15, 15,  4, 15, 15,  7,  9, 15,  3, 13,  7, -2,  0],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 0 \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [1\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t11\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_2: 6\n",
      "training fct: {'observations': array([15, 12, 15, 15, 15,  9, 15, 15,  1,  8, 15,  5, 13,  7,  8, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [1\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t11\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([15, 12, 15, 15, 15,  9, 15, 15,  1,  8, 15,  5, 13,  7, -2,  8],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 8 \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [1\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t11\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_0: 9\n",
      "training fct: {'observations': array([15,  5, 15, 15, 15,  3, 15, 15, 11,  6, 15, 12, 13,  7,  8, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [1\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t11\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([15,  5, 15, 15, 15,  3, 15, 15, 11,  6, 15, 12, 13,  7, -2,  8],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 8 \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [1\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t11\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_1: 10\n",
      "training fct: {'observations': array([15, -1, 15, 15, 15,  4, 15, 15,  7,  9, 15,  3, 13,  7, 11, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [1\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_2: 13\n",
      "training fct: {'observations': array([15, -1, 15, 15, 15,  4, 15, 15,  7,  9, 15,  3, 13,  7, -2, 11],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 11 \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [1\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [x\tx\t4\tx]  \n",
      "\n",
      "sampled action player_2: 8\n",
      "training fct: {'observations': array([15, 12, 15, 15, 15,  9, 15, 15,  1,  8, 15,  5, 18,  7,  7, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 7 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [1\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [11\tx\t4\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([15, 12, 15, 15, 15,  9, 15, 15,  1,  8, 15,  5, 18,  7, -2,  7],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 7 \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [1\t8\tx\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [11\tx\t4\tx]  \n",
      "\n",
      "sampled action player_0: 10\n",
      "training fct: {'observations': array([15,  5, 15, 15, 15,  3, 15, 15, 11,  6,  8, 12, 18,  6,  5, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [11\tx\t4\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([15,  5, 15, 15, 15,  3, 15, 15, 11,  6,  8, 12, 18,  6, -2,  5],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 5 \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t2\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [11\tx\t4\tx]  \n",
      "\n",
      "sampled action player_1: 2\n",
      "training fct: {'observations': array([15, -1, 15, 15, 15,  4, 15, 15, 11,  9,  4,  3, 21,  6,  2, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [11\tx\t4\tx]  \n",
      "\n",
      "sampled action player_2: 13\n",
      "training fct: {'observations': array([15, -1, 15, 15, 15,  4, 15, 15, 11,  9,  4,  3, 21,  6, -2,  2],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 2 \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [11\tx\t4\tx]  \n",
      "\n",
      "sampled action player_2: 8\n",
      "training fct: {'observations': array([15, 12, 15, 15, 15,  9, 15, 15,  1,  8,  7,  5, 15,  6, 11, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [2\tx\t4\tx]  \n",
      "\n",
      "sampled action player_0: 12\n",
      "training fct: {'observations': array([15, 12, 15, 15, 15,  9, 15, 15,  1,  8,  7,  5, 15,  6, 11, -2],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: -2 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\tx]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [2\tx\t4\tx]  \n",
      "\n",
      "sampled action player_0: 7\n",
      "training fct: {'observations': array([15,  5, 15, 15, 15,  3, 15,  2, 11,  6,  8, 12, 15,  5,  1, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [2\tx\t4\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([15,  5, 15, 15, 15,  3, 15,  2, 11,  6,  8, 12, 15,  5, 11,  1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 1 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [x\tx\tx\t2]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [2\tx\t4\tx]  \n",
      "\n",
      "sampled action player_1: 5\n",
      "training fct: {'observations': array([15, -1, 15, 15, 15,  4, 15,  5,  2,  9,  4,  3, 15,  5,  3, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 3 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [x\t1\tx\t2]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [2\tx\t4\tx]  \n",
      "\n",
      "sampled action player_2: 13\n",
      "training fct: {'observations': array([15, -1, 15, 15, 15,  4, 15,  5,  2,  9,  4,  3, 15,  5, 11,  3],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 3 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [x\t1\tx\t2]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [2\tx\t4\tx]  \n",
      "\n",
      "sampled action player_2: 9\n",
      "training fct: {'observations': array([15, 12, 15, 15, 15,  9, 15, -2,  1,  8,  7,  5, 18,  5,  9, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [x\t1\tx\t2]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [2\t3\t4\tx]  \n",
      "\n",
      "sampled action player_0: 12\n",
      "training fct: {'observations': array([15, 12, 15, 15, 15,  9, 15, -2,  1,  8,  7,  5, 18,  5,  9,  1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 1 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [x\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [x\t1\tx\t2]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [2\t3\t4\tx]  \n",
      "\n",
      "sampled action player_0: 4\n",
      "training fct: {'observations': array([15,  5, 15, 15,  5,  1, 15,  2, 11,  6,  8, 12, 18,  4,  9, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [1\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [x\t1\tx\t2]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [2\t3\t4\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([15,  5, 15, 15,  5,  1, 15,  2, 11,  6,  8, 12, 18,  4,  9,  9],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 9 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [1\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [x\t1\tx\t2]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [2\t3\t4\tx]  \n",
      "\n",
      "sampled action player_1: 7\n",
      "training fct: {'observations': array([15, -1, 15, 15,  6,  4, 15,  5,  2,  3,  4,  3, 18,  4,  2, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [1\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [x\t1\tx\t9]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [2\t3\t4\tx]  \n",
      "\n",
      "sampled action player_2: 13\n",
      "training fct: {'observations': array([15, -1, 15, 15,  6,  4, 15,  5,  2,  3,  4,  3, 18,  4,  9,  2],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 2 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [1\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [x\t1\tx\t9]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [2\t3\t4\tx]  \n",
      "\n",
      "sampled action player_2: 8\n",
      "training fct: {'observations': array([15, 12, 15, 15,  1,  9, 15, -2,  1,  8,  7,  5, 18,  4,  2, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [1\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [x\t1\tx\t9]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [2\t3\t4\tx]  \n",
      "\n",
      "sampled action player_0: 12\n",
      "training fct: {'observations': array([15, 12, 15, 15,  1,  9, 15, -2,  1,  8,  7,  5, 18,  4,  2,  9],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 9 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [x\t12\tx\tx]\n",
      " [1\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [x\t1\tx\t9]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [2\t3\t4\tx]  \n",
      "\n",
      "sampled action player_0: 2\n",
      "training fct: {'observations': array([15,  5,  5, 15,  5,  1, 15,  9, 11,  6,  8, 12, 18,  3,  0, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [x\t12\t9\tx]\n",
      " [1\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [x\t1\tx\t9]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [2\t3\t4\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([15,  5,  5, 15,  5,  1, 15,  9, 11,  6,  8, 12, 18,  3,  2,  0],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 0 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [x\t12\t9\tx]\n",
      " [1\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [x\t1\tx\t9]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [2\t3\t4\tx]  \n",
      "\n",
      "sampled action player_1: 4\n",
      "training fct: {'observations': array([15, -1, 10, 15,  6,  4, 15,  5,  2,  3,  4,  3, 18,  3,  5, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [x\t12\t9\tx]\n",
      " [1\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [2\t3\t4\tx]  \n",
      "\n",
      "sampled action player_2: 12\n",
      "training fct: {'observations': array([15, -1, 10, 15,  6,  4, 15,  5,  2,  3,  4,  3, 18,  3,  5,  8],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 8 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [x\t12\t9\tx]\n",
      " [1\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\tx]\n",
      " [2\t3\t4\tx]  \n",
      "\n",
      "sampled action player_2: 7\n",
      "training fct: {'observations': array([15, 12,  9, 15,  1,  9, 15, -2,  1,  8,  7,  5, 26,  3,  5, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [x\t12\t9\tx]\n",
      " [1\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t8]\n",
      " [2\t3\t4\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([15, 12,  9, 15,  1,  9, 15, -2,  1,  8,  7,  5, 26,  3,  5,  5],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 5 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [x\t12\t9\tx]\n",
      " [1\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t8]\n",
      " [2\t3\t4\tx]  \n",
      "\n",
      "sampled action player_0: 1\n",
      "training fct: {'observations': array([15,  5,  5, 15,  0,  1, 15,  9, 11,  6,  8, 12, 26,  3, 12, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      " [x\t5\t9\tx]\n",
      " [1\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t8]\n",
      " [2\t3\t4\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([15,  5,  5, 15,  0,  1, 15,  9, 11,  6,  8, 12, 26,  3,  5, 12],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 12 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [x\t5\t9\tx]\n",
      " [1\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t8]\n",
      " [2\t3\t4\tx]  \n",
      "\n",
      "sampled action player_1: 0\n",
      "training fct: {'observations': array([15, -1, 10, 15,  6,  4, 15,  8,  2,  3,  4,  3, 26,  3,  0, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [x\t5\t9\tx]\n",
      " [1\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\tx\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t8]\n",
      " [2\t3\t4\tx]  \n",
      "\n",
      "sampled action player_2: 12\n",
      "training fct: {'observations': array([15, -1, 10, 15,  6,  4, 15,  8,  2,  3,  4,  3, 26,  3,  0,  9],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 9 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [x\t5\t9\tx]\n",
      " [1\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\tx\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t8]\n",
      " [2\t3\t4\tx]  \n",
      "\n",
      "sampled action player_2: 9\n",
      "training fct: {'observations': array([15,  5,  9, 15,  1,  9, 15, -2,  1,  8,  7,  5, 32,  3,  3, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 3 \n",
      "======= Player 0 ========== \n",
      " [x\t5\t9\tx]\n",
      " [1\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\tx\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t8]\n",
      " [2\t9\t4\tx]  \n",
      "\n",
      "sampled action player_0: 12\n",
      "training fct: {'observations': array([15,  5,  9, 15,  1,  9, 15, -2,  1,  8,  7,  5, 32,  3,  3,  6],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 6 \n",
      "discard pile top: 3 \n",
      "======= Player 0 ========== \n",
      " [x\t5\t9\tx]\n",
      " [1\t9\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\tx\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t8]\n",
      " [2\t9\t4\tx]  \n",
      "\n",
      "sampled action player_0: 5\n",
      "training fct: {'observations': array([15,  5,  5, 15,  0,  1, 15,  9, 11,  6,  8, 12, 32,  3,  9, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [x\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\tx\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t8]\n",
      " [2\t9\t4\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([15,  5,  5, 15,  0,  1, 15,  9, 11,  6,  8, 12, 32,  3,  3,  9],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 9 \n",
      "discard pile top: 3 \n",
      "======= Player 0 ========== \n",
      " [x\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\tx\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [x\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t8]\n",
      " [2\t9\t4\tx]  \n",
      "\n",
      "sampled action player_1: 8\n",
      "training fct: {'observations': array([15, -1, 10, 15,  6,  4, 15,  8,  2,  9,  4,  3, 32,  3, 11, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\tx\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [9\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t8]\n",
      " [2\t9\t4\tx]  \n",
      "\n",
      "sampled action player_2: 12\n",
      "training fct: {'observations': array([15, -1, 10, 15,  6,  4, 15,  8,  2,  9,  4,  3, 32,  3, 11,  3],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 3 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\tx\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [9\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t8]\n",
      " [2\t9\t4\tx]  \n",
      "\n",
      "sampled action player_2: 7\n",
      "training fct: {'observations': array([15,  5,  9, 15,  1,  6, 15, -2,  1,  8,  7,  5, 27,  3,  8, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [x\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\tx\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [9\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t3]\n",
      " [2\t9\t4\tx]  \n",
      "\n",
      "sampled action player_0: 12\n",
      "training fct: {'observations': array([15,  5,  9, 15,  1,  6, 15, -2,  1,  8,  7,  5, 27,  3,  8,  0],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 0 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [x\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\tx\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [9\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t3]\n",
      " [2\t9\t4\tx]  \n",
      "\n",
      "sampled action player_0: 8\n",
      "training fct: {'observations': array([15,  5,  5, 15,  0,  1, 15,  9,  9,  6,  8, 12, 27,  3,  1, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [x\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\tx\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [9\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t3]\n",
      " [2\t9\t4\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([15,  5,  5, 15,  0,  1, 15,  9,  9,  6,  8, 12, 27,  3,  8,  1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 1 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [x\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\tx\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [9\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t3]\n",
      " [2\t9\t4\tx]  \n",
      "\n",
      "sampled action player_1: 1\n",
      "training fct: {'observations': array([15, -1, 10, 15,  6,  4, 15,  3,  2,  9,  4,  3, 27,  3,  5, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [x\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [9\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t3]\n",
      " [2\t9\t4\tx]  \n",
      "\n",
      "sampled action player_2: 12\n",
      "training fct: {'observations': array([15, -1, 10, 15,  6,  4, 15,  3,  2,  9,  4,  3, 27,  3,  5, -1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: -1 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [x\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [9\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t3]\n",
      " [2\t9\t4\tx]  \n",
      "\n",
      "sampled action player_2: 10\n",
      "training fct: {'observations': array([15,  5,  9, 15,  1,  6, 15, -2,  0,  8,  7,  5, 22,  3,  4, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      " [x\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [9\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t3]\n",
      " [2\t9\t-1\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([15,  5,  9, 15,  1,  6, 15, -2,  0,  8,  7,  5, 22,  3,  5,  4],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 4 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [x\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [9\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t3]\n",
      " [2\t9\t-1\tx]  \n",
      "\n",
      "sampled action player_0: 0\n",
      "training fct: {'observations': array([12,  1,  5, 15,  0,  1, 15,  9,  9,  6,  8, 12, 22,  2,  2, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [4\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [9\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t3]\n",
      " [2\t9\t-1\tx]  \n",
      "\n",
      "sampled action player_1: 12\n",
      "training fct: {'observations': array([12,  1,  5, 15,  0,  1, 15,  9,  9,  6,  8, 12, 22,  2,  2,  6],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 6 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [4\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [9\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t3]\n",
      " [2\t9\t-1\tx]  \n",
      "\n",
      "sampled action player_1: 8\n",
      "training fct: {'observations': array([11, -1, 10, 15,  6,  4, 15,  3,  2,  9, -1,  3, 22,  2,  9, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [4\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t3]\n",
      " [2\t9\t-1\tx]  \n",
      "\n",
      "sampled action player_2: 13\n",
      "training fct: {'observations': array([11, -1, 10, 15,  6,  4, 15,  3,  2,  9, -1,  3, 22,  2,  2,  9],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 9 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [4\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\tx]\n",
      " [x\tx\t0\t3]\n",
      " [2\t9\t-1\tx]  \n",
      "\n",
      "sampled action player_2: 3\n",
      "training fct: {'observations': array([ 4,  5,  9, 15,  1,  6, 15, -2,  0,  8,  7,  5, 31,  2,  1, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [4\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [x\tx\t0\t3]\n",
      " [2\t9\t-1\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([ 4,  5,  9, 15,  1,  6, 15, -2,  0,  8,  7,  5, 31,  2,  2,  1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 1 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [4\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [x\tx\t0\t3]\n",
      " [2\t9\t-1\tx]  \n",
      "\n",
      "sampled action player_0: 4\n",
      "training fct: {'observations': array([12,  1,  5, 15,  0,  1, 15,  9,  6,  6,  8, 12, 31,  2,  1, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [4\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [x\tx\t0\t3]\n",
      " [2\t9\t-1\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([12,  1,  5, 15,  0,  1, 15,  9,  6,  6,  8, 12, 31,  2,  2,  1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 1 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [4\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t6\t8\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [x\tx\t0\t3]\n",
      " [2\t9\t-1\tx]  \n",
      "\n",
      "sampled action player_1: 10\n",
      "training fct: {'observations': array([11, -1, 10, 15,  6,  4, 15,  3,  2,  9, -1,  3, 31,  2,  8, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [4\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t6\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [x\tx\t0\t3]\n",
      " [2\t9\t-1\tx]  \n",
      "\n",
      "sampled action player_2: 12\n",
      "training fct: {'observations': array([11, -1, 10, 15,  6,  4, 15,  3,  2,  9, -1,  3, 31,  2,  8,  1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 1 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [4\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t6\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [x\tx\t0\t3]\n",
      " [2\t9\t-1\tx]  \n",
      "\n",
      "sampled action player_2: 4\n",
      "training fct: {'observations': array([ 4,  5,  9, 15,  1,  6, 15, -2,  0,  8,  7,  5, 32,  2,  6, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [4\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t6\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [1\tx\t0\t3]\n",
      " [2\t9\t-1\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([ 4,  5,  9, 15,  1,  6, 15, -2,  0,  8,  7,  5, 32,  2,  8,  6],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 6 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [4\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t6\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [1\tx\t0\t3]\n",
      " [2\t9\t-1\tx]  \n",
      "\n",
      "sampled action player_0: 0\n",
      "training fct: {'observations': array([12,  1,  5, 15,  0,  1, 15,  9,  6,  6,  1, 12, 32,  2,  4, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t6\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [1\tx\t0\t3]\n",
      " [2\t9\t-1\tx]  \n",
      "\n",
      "sampled action player_1: 12\n",
      "training fct: {'observations': array([12,  1,  5, 15,  0,  1, 15,  9,  6,  6,  1, 12, 32,  2,  4,  2],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 2 \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t6\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [1\tx\t0\t3]\n",
      " [2\t9\t-1\tx]  \n",
      "\n",
      "sampled action player_1: 9\n",
      "training fct: {'observations': array([11, -1, 10, 15,  1,  4, 15,  3,  2,  9, -1,  3, 32,  2,  6, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t2\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [1\tx\t0\t3]\n",
      " [2\t9\t-1\tx]  \n",
      "\n",
      "sampled action player_2: 12\n",
      "training fct: {'observations': array([11, -1, 10, 15,  1,  4, 15,  3,  2,  9, -1,  3, 32,  2,  6,  5],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 5 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t2\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [1\tx\t0\t3]\n",
      " [2\t9\t-1\tx]  \n",
      "\n",
      "sampled action player_2: 10\n",
      "training fct: {'observations': array([ 6,  5,  9, 15,  1,  6, 15, -2,  0,  8,  7,  5, 37,  2, -1, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t2\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [1\tx\t0\t3]\n",
      " [2\t9\t5\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([ 6,  5,  9, 15,  1,  6, 15, -2,  0,  8,  7,  5, 37,  2,  6, -1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: -1 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [0\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t2\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [1\tx\t0\t3]\n",
      " [2\t9\t5\tx]  \n",
      "\n",
      "sampled action player_0: 8\n",
      "training fct: {'observations': array([12,  1,  5, 15,  0,  1, 15,  9,  6,  2,  1, 12, 37,  2,  0, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t2\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [1\tx\t0\t3]\n",
      " [2\t9\t5\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([12,  1,  5, 15,  0,  1, 15,  9,  6,  2,  1, 12, 37,  2,  6,  0],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 0 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t2\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [1\tx\t0\t3]\n",
      " [2\t9\t5\tx]  \n",
      "\n",
      "sampled action player_1: 9\n",
      "training fct: {'observations': array([11, -1, 10, 15,  1,  4, 15,  3,  2,  9,  5,  3, 35,  2,  2, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [1\tx\t0\t3]\n",
      " [2\t9\t5\tx]  \n",
      "\n",
      "sampled action player_2: 12\n",
      "training fct: {'observations': array([11, -1, 10, 15,  1,  4, 15,  3,  2,  9,  5,  3, 35,  2,  2,  4],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 4 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [1\tx\t0\t3]\n",
      " [2\t9\t5\tx]  \n",
      "\n",
      "sampled action player_2: 5\n",
      "training fct: {'observations': array([ 6,  5,  9, 15,  1,  6, 15, -2, -1,  8,  7,  5, 35,  2,  4, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [1\t4\t0\t3]\n",
      " [2\t9\t5\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([ 6,  5,  9, 15,  1,  6, 15, -2, -1,  8,  7,  5, 35,  2,  2,  4],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 4 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [1\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [1\t4\t0\t3]\n",
      " [2\t9\t5\tx]  \n",
      "\n",
      "sampled action player_0: 4\n",
      "training fct: {'observations': array([12,  1,  5, 15,  0,  1, 15,  9,  6,  0,  1, 12, 35,  2,  1, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [1\t4\t0\t3]\n",
      " [2\t9\t5\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([12,  1,  5, 15,  0,  1, 15,  9,  6,  0,  1, 12, 35,  2,  2,  1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 1 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [1\t4\t0\t3]\n",
      " [2\t9\t5\tx]  \n",
      "\n",
      "sampled action player_1: 1\n",
      "training fct: {'observations': array([11, -1, 10, 15,  1,  4, 15,  3,  2,  9,  5,  3, 35,  2,  1, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [1\t4\t0\t3]\n",
      " [2\t9\t5\tx]  \n",
      "\n",
      "sampled action player_2: 13\n",
      "training fct: {'observations': array([11, -1, 10, 15,  1,  4, 15,  3,  2,  9,  5,  3, 35,  2,  2,  1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 1 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t10\t9]\n",
      " [1\t4\t0\t3]\n",
      " [2\t9\t5\tx]  \n",
      "\n",
      "sampled action player_2: 2\n",
      "training fct: {'observations': array([ 6,  5,  9, 15,  4,  6, 15, -2, -1,  8,  7,  5, 33,  2, 10, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [1\t4\t0\t3]\n",
      " [2\t9\t5\tx]  \n",
      "\n",
      "sampled action player_0: 12\n",
      "training fct: {'observations': array([ 6,  5,  9, 15,  4,  6, 15, -2, -1,  8,  7,  5, 33,  2, 10,  5],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 5 \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [1\t4\t0\t3]\n",
      " [2\t9\t5\tx]  \n",
      "\n",
      "sampled action player_0: 1\n",
      "training fct: {'observations': array([12,  1,  5, 15,  0,  1, 15,  9,  6,  0,  1, 12, 33,  2,  5, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [1\t4\t0\t3]\n",
      " [2\t9\t5\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([12,  1,  5, 15,  0,  1, 15,  9,  6,  0,  1, 12, 33,  2, 10,  5],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 5 \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [1\t4\t0\t3]\n",
      " [2\t9\t5\tx]  \n",
      "\n",
      "sampled action player_1: 2\n",
      "training fct: {'observations': array([11, -1,  1, 15,  1,  4, 15,  3,  2,  9,  5,  3, 33,  2,  5, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [1\t4\t0\t3]\n",
      " [2\t9\t5\tx]  \n",
      "\n",
      "sampled action player_2: 13\n",
      "training fct: {'observations': array([11, -1,  1, 15,  1,  4, 15,  3,  2,  9,  5,  3, 33,  2, 10,  5],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 5 \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [1\t4\t0\t3]\n",
      " [2\t9\t5\tx]  \n",
      "\n",
      "sampled action player_2: 8\n",
      "training fct: {'observations': array([ 6,  5,  9, 15,  4,  6, 15, -2, -1,  8,  7,  5, 35,  2,  2, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [1\t4\t0\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_0: 12\n",
      "training fct: {'observations': array([ 6,  5,  9, 15,  4,  6, 15, -2, -1,  8,  7,  5, 35,  2,  2,  6],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 6 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [1\t4\t0\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_0: 1\n",
      "training fct: {'observations': array([12,  1,  5, 15,  0,  1, 15,  9,  6,  0,  1, 12, 35,  2,  5, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [6\t6\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [1\t4\t0\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([12,  1,  5, 15,  0,  1, 15,  9,  6,  0,  1, 12, 35,  2,  2,  5],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 5 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [6\t6\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [0\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [1\t4\t0\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_1: 4\n",
      "training fct: {'observations': array([11, -1,  1, 15,  1,  4, 15,  3,  5,  9,  5,  3, 36,  2,  0, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t6\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [5\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [1\t4\t0\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_2: 12\n",
      "training fct: {'observations': array([11, -1,  1, 15,  1,  4, 15,  3,  5,  9,  5,  3, 36,  2,  0,  4],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 4 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t6\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [5\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [1\t4\t0\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_2: 4\n",
      "training fct: {'observations': array([ 6,  6,  9, 15,  4,  6, 15, -2, -1,  8,  7,  5, 39,  2,  1, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [6\t6\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [5\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [4\t4\t0\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([ 6,  6,  9, 15,  4,  6, 15, -2, -1,  8,  7,  5, 39,  2,  0,  1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 1 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t6\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t5]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [5\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [4\t4\t0\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_0: 11\n",
      "training fct: {'observations': array([12,  1,  5, 15,  5,  1, 15,  9,  6,  0,  1, 12, 39,  2,  5, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [6\t6\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [5\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [4\t4\t0\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_1: 12\n",
      "training fct: {'observations': array([12,  1,  5, 15,  5,  1, 15,  9,  6,  0,  1, 12, 39,  2,  5,  7],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 7 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [6\t6\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\tx]\n",
      " [5\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [4\t4\t0\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_1: 3\n",
      "training fct: {'observations': array([11, -1,  1, 15,  4,  4, 15,  3,  5,  9,  5,  3, 39,  2, 11, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [6\t6\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [5\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [4\t4\t0\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_2: 12\n",
      "training fct: {'observations': array([11, -1,  1, 15,  4,  4, 15,  3,  5,  9,  5,  3, 39,  2, 11, -2],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: -2 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [6\t6\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [5\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [4\t4\t0\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_2: 6\n",
      "training fct: {'observations': array([ 6,  6,  9, 15,  4,  6, 15, -2, -1,  8,  7,  1, 37,  2,  0, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t6\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [5\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([ 6,  6,  9, 15,  4,  6, 15, -2, -1,  8,  7,  1, 37,  2, 11,  0],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 0 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [6\t6\t9\tx]\n",
      " [4\t6\tx\t-2]\n",
      " [-1\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [5\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_0: 7\n",
      "training fct: {'observations': array([12,  1,  5, 15,  5,  1, 15,  9,  6,  0,  1, 12, 37,  2, -2, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [6\t6\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [-1\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [5\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([12,  1,  5, 15,  5,  1, 15,  9,  6,  0,  1, 12, 37,  2, 11, -2],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: -2 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [6\t6\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [-1\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [5\t1\tx\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_1: 6\n",
      "training fct: {'observations': array([11, -1,  1, 15,  4,  4, 15,  3,  5,  9,  5,  3, 37,  1, -1, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      " [6\t6\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [-1\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [5\t1\t-2\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_2: 13\n",
      "training fct: {'observations': array([11, -1,  1, 15,  4,  4, 15,  3,  5,  9,  5,  3, 37,  1, 11, -1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: -1 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [6\t6\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [-1\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [5\t1\t-2\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t9]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_2: 3\n",
      "training fct: {'observations': array([ 6,  6,  9, 15,  4,  6, 15,  0, -1,  8,  7,  1, 27,  1,  9, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [6\t6\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [-1\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [5\t1\t-2\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_0: 12\n",
      "training fct: {'observations': array([ 6,  6,  9, 15,  4,  6, 15,  0, -1,  8,  7,  1, 27,  1,  9, 10],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 10 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [6\t6\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [-1\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [5\t1\t-2\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_0: 1\n",
      "training fct: {'observations': array([12,  1,  5, 15,  5,  1, 15,  9,  6,  0,  1, 12, 27,  1,  6, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [6\t10\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [-1\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [5\t1\t-2\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([12,  1,  5, 15,  5,  1, 15,  9,  6,  0,  1, 12, 27,  1,  9,  6],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 6 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [6\t10\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [-1\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [5\t1\t-2\t9]\n",
      " [6\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_1: 10\n",
      "training fct: {'observations': array([11, -1,  1, 15,  4,  4, 15,  3,  5,  9,  5,  3, 27,  1,  1, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [6\t10\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [-1\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [5\t1\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_2: 12\n",
      "training fct: {'observations': array([11, -1,  1, 15,  4,  4, 15,  3,  5,  9,  5,  3, 27,  1,  1,  9],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 9 \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [6\t10\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [-1\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [5\t1\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t5\tx]  \n",
      "\n",
      "sampled action player_2: 10\n",
      "training fct: {'observations': array([ 6, 10,  9, 15,  4,  6, 15,  0, -1,  8,  7,  1, 31,  1,  5, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [6\t10\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [-1\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [5\t1\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_0: 12\n",
      "training fct: {'observations': array([ 6, 10,  9, 15,  4,  6, 15,  0, -1,  8,  7,  1, 31,  1,  5,  8],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 8 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [6\t10\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [-1\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [5\t1\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_0: 8\n",
      "training fct: {'observations': array([12,  1,  5, 15,  5,  1, 15,  9,  6,  0,  6, 12, 31,  1, -1, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      " [6\t10\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [8\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [5\t1\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_1: 12\n",
      "training fct: {'observations': array([12,  1,  5, 15,  5,  1, 15,  9,  6,  0,  6, 12, 31,  1, -1,  1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 1 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      " [6\t10\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [8\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [5\t1\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_1: 4\n",
      "training fct: {'observations': array([11, -1,  1, 15,  4,  4, 15,  3,  5,  9,  9,  3, 31,  1,  5, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [6\t10\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [8\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [1\t1\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_2: 13\n",
      "training fct: {'observations': array([11, -1,  1, 15,  4,  4, 15,  3,  5,  9,  9,  3, 31,  1, -1,  5],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 5 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      " [6\t10\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [8\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [1\t1\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_2: 2\n",
      "training fct: {'observations': array([ 6, 10,  9, 15,  4,  6, 15,  0,  8,  8,  7,  1, 35,  1,  1, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [6\t10\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [8\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [1\t1\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t5\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([ 6, 10,  9, 15,  4,  6, 15,  0,  8,  8,  7,  1, 35,  1, -1,  1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 1 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      " [6\t10\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [8\t8\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [1\t1\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t5\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_0: 9\n",
      "training fct: {'observations': array([12,  1,  5, 15,  1,  1, 15,  9,  6,  0,  6, 12, 35,  1,  8, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [6\t10\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [8\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [1\t1\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t5\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([12,  1,  5, 15,  1,  1, 15,  9,  6,  0,  6, 12, 35,  1, -1,  8],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 8 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      " [6\t10\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [8\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [1\t1\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t5\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_1: 5\n",
      "training fct: {'observations': array([11, -1,  5, 15,  4,  4, 15,  3,  5,  9,  9,  3, 35,  1,  1, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [6\t10\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [8\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [1\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t5\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_2: 13\n",
      "training fct: {'observations': array([11, -1,  5, 15,  4,  4, 15,  3,  5,  9,  9,  3, 35,  1, -1,  1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 1 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      " [6\t10\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [8\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [1\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t5\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_2: 2\n",
      "training fct: {'observations': array([ 6, 10,  9, 15,  4,  6, 15,  0,  8,  1,  7,  1, 31,  1,  5, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [6\t10\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [8\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [1\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([ 6, 10,  9, 15,  4,  6, 15,  0,  8,  1,  7,  1, 31,  1, -1,  5],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 5 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      " [6\t10\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [8\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [1\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_0: 1\n",
      "training fct: {'observations': array([12,  1,  5, 15,  1,  8, 15,  9,  6,  0,  6, 12, 31,  1, 10, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [8\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [1\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([12,  1,  5, 15,  1,  8, 15,  9,  6,  0,  6, 12, 31,  1, -1, 10],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 10 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [8\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t7]\n",
      " [1\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_1: 3\n",
      "training fct: {'observations': array([11, -1,  1, 15,  4,  4, 15,  3,  5,  9,  9,  3, 31,  1,  7, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 7 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [8\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t10]\n",
      " [1\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_2: 13\n",
      "training fct: {'observations': array([11, -1,  1, 15,  4,  4, 15,  3,  5,  9,  9,  3, 31,  1, -1,  7],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 7 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [8\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t10]\n",
      " [1\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t4\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_2: 5\n",
      "training fct: {'observations': array([ 6,  5,  9, 15,  4,  6, 15,  0,  8,  1,  7,  1, 34,  1,  4, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [8\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t10]\n",
      " [1\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([ 6,  5,  9, 15,  4,  6, 15,  0,  8,  1,  7,  1, 34,  1, -1,  4],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 4 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\tx]\n",
      " [4\t6\tx\t0]\n",
      " [8\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t10]\n",
      " [1\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_0: 3\n",
      "training fct: {'observations': array([12,  1,  5, 10,  1,  8, 15,  9,  6,  0,  6, 12, 34,  1,  0, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t6\tx\t0]\n",
      " [8\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t10]\n",
      " [1\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([12,  1,  5, 10,  1,  8, 15,  9,  6,  0,  6, 12, 34,  1, -1,  0],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 0 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t6\tx\t0]\n",
      " [8\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t10]\n",
      " [1\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_1: 4\n",
      "training fct: {'observations': array([11, -1,  1, -1,  4,  7, 15,  3,  5,  9,  9,  3, 34,  1,  1, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t6\tx\t0]\n",
      " [8\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t10]\n",
      " [0\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_2: 13\n",
      "training fct: {'observations': array([11, -1,  1, -1,  4,  7, 15,  3,  5,  9,  9,  3, 34,  1, -1,  1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 1 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t6\tx\t0]\n",
      " [8\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t10]\n",
      " [0\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t9\tx]  \n",
      "\n",
      "sampled action player_2: 10\n",
      "training fct: {'observations': array([ 6,  5,  9,  4,  4,  6, 15,  0,  8,  1,  7,  1, 26,  1,  9, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t6\tx\t0]\n",
      " [8\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t10]\n",
      " [0\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t1\tx]  \n",
      "\n",
      "sampled action player_0: 12\n",
      "training fct: {'observations': array([ 6,  5,  9,  4,  4,  6, 15,  0,  8,  1,  7,  1, 26,  1,  9,  4],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 4 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t6\tx\t0]\n",
      " [8\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t10]\n",
      " [0\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t1\tx]  \n",
      "\n",
      "sampled action player_0: 8\n",
      "training fct: {'observations': array([12,  1,  5, 10,  0,  8, 15,  9,  6,  0,  6, 12, 26,  1,  8, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t6\tx\t0]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t10]\n",
      " [0\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t1\tx]  \n",
      "\n",
      "sampled action player_1: 12\n",
      "training fct: {'observations': array([12,  1,  5, 10,  0,  8, 15,  9,  6,  0,  6, 12, 26,  1,  8,  7],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 7 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t6\tx\t0]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t5\t10]\n",
      " [0\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t1\tx]  \n",
      "\n",
      "sampled action player_1: 2\n",
      "training fct: {'observations': array([11, -1,  1, -1,  4,  7, 15,  3,  5,  9,  1,  3, 26,  1,  5, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t6\tx\t0]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [0\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t1\tx]  \n",
      "\n",
      "sampled action player_2: 13\n",
      "training fct: {'observations': array([11, -1,  1, -1,  4,  7, 15,  3,  5,  9,  1,  3, 26,  1,  8,  5],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 5 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t6\tx\t0]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [0\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t-1]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t1\tx]  \n",
      "\n",
      "sampled action player_2: 3\n",
      "training fct: {'observations': array([ 6,  5,  9,  4,  4,  6, 15,  0,  4,  1,  7,  1, 32,  1, -1, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t6\tx\t0]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [0\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t5]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t1\tx]  \n",
      "\n",
      "sampled action player_0: 12\n",
      "training fct: {'observations': array([ 6,  5,  9,  4,  4,  6, 15,  0,  4,  1,  7,  1, 32,  1, -1, -1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: -1 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t6\tx\t0]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [0\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t5]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t1\tx]  \n",
      "\n",
      "sampled action player_0: 7\n",
      "training fct: {'observations': array([12,  1,  7, 10,  0,  8, 15,  9,  6,  0,  6, 12, 32,  1,  0, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t6\tx\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [0\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t5]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t1\tx]  \n",
      "\n",
      "sampled action player_1: 12\n",
      "training fct: {'observations': array([12,  1,  7, 10,  0,  8, 15,  9,  6,  0,  6, 12, 32,  1,  0, 11],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 11 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t6\tx\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [0\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t5]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t1\tx]  \n",
      "\n",
      "sampled action player_1: 4\n",
      "training fct: {'observations': array([11, -1,  1,  5,  4,  7, 15,  3,  5,  9,  1,  3, 32,  1,  0, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t6\tx\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t5]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t1\tx]  \n",
      "\n",
      "sampled action player_2: 13\n",
      "training fct: {'observations': array([11, -1,  1,  5,  4,  7, 15,  3,  5,  9,  1,  3, 32,  1,  0,  0],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 0 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t6\tx\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t1\t5]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t1\tx]  \n",
      "\n",
      "sampled action player_2: 2\n",
      "training fct: {'observations': array([ 6,  5,  9,  4,  4,  6, 15, -1,  4,  1,  7,  1, 31,  1,  1, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t6\tx\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t1\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([ 6,  5,  9,  4,  4,  6, 15, -1,  4,  1,  7,  1, 31,  1,  0,  1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 1 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t6\tx\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t1\tx]  \n",
      "\n",
      "sampled action player_0: 5\n",
      "training fct: {'observations': array([12,  1,  7, 10, 11,  8, 15,  9,  6,  0,  6, 12, 31,  1,  6, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t1\tx\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t1\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([12,  1,  7, 10, 11,  8, 15,  9,  6,  0,  6, 12, 31,  1,  0,  6],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 6 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t1\tx\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t1\tx]  \n",
      "\n",
      "sampled action player_1: 8\n",
      "training fct: {'observations': array([11, -1,  0,  5,  4,  7, 15,  3,  5,  9,  1,  3, 31,  1,  6, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t1\tx\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t1\tx]  \n",
      "\n",
      "sampled action player_2: 13\n",
      "training fct: {'observations': array([11, -1,  0,  5,  4,  7, 15,  3,  5,  9,  1,  3, 31,  1,  0,  6],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 6 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t1\tx\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t1\tx]  \n",
      "\n",
      "sampled action player_2: 10\n",
      "training fct: {'observations': array([ 6,  5,  9,  4,  4,  1, 15, -1,  4,  1,  7,  1, 36,  1,  1, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t1\tx\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t6\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([ 6,  5,  9,  4,  4,  1, 15, -1,  4,  1,  7,  1, 36,  1,  0,  1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 1 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t1\tx\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t6\tx]  \n",
      "\n",
      "sampled action player_0: 5\n",
      "training fct: {'observations': array([12,  1,  7, 10, 11,  8, 15,  9,  6,  0,  6, 12, 36,  1,  1, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t1\tx\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t6\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([12,  1,  7, 10, 11,  8, 15,  9,  6,  0,  6, 12, 36,  1,  0,  1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 1 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t1\tx\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t6\tx]  \n",
      "\n",
      "sampled action player_1: 1\n",
      "training fct: {'observations': array([11, -1,  0,  5,  4,  7, 15,  3,  5,  9,  6,  3, 36,  1,  1, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t1\tx\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t6\tx]  \n",
      "\n",
      "sampled action player_2: 13\n",
      "training fct: {'observations': array([11, -1,  0,  5,  4,  7, 15,  3,  5,  9,  6,  3, 36,  1,  0,  1],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 1 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t1\tx\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [4\t7\t-2\t3]\n",
      " [5\t9\t6\tx]  \n",
      "\n",
      "sampled action player_2: 4\n",
      "training fct: {'observations': array([ 6,  5,  9,  4,  4,  1, 15, -1,  4,  1,  7,  1, 33,  1,  4, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t1\tx\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [1\t7\t-2\t3]\n",
      " [5\t9\t6\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([ 6,  5,  9,  4,  4,  1, 15, -1,  4,  1,  7,  1, 33,  1,  0,  4],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 4 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t1\tx\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [1\t7\t-2\t3]\n",
      " [5\t9\t6\tx]  \n",
      "\n",
      "sampled action player_0: 6\n",
      "training fct: {'observations': array([12,  1,  7, 10, 11,  8, -2,  9,  6,  0,  6, 12, 33,  0,  8, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t1\t4\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [1\t7\t-2\t3]\n",
      " [5\t9\t6\tx]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([12,  1,  7, 10, 11,  8, -2,  9,  6,  0,  6, 12, 33,  0,  0,  8],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 8 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t1\t4\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [1\t7\t-2\t3]\n",
      " [5\t9\t6\tx]  \n",
      "\n",
      "sampled action player_1: 9\n",
      "training fct: {'observations': array([11, -1,  0,  5,  1,  7, -2,  3,  5,  9,  6,  3, 33,  0,  0, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t1\t4\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t8\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [1\t7\t-2\t3]\n",
      " [5\t9\t6\tx]  \n",
      "\n",
      "sampled action player_2: 12\n",
      "training fct: {'observations': array([11, -1,  0,  5,  1,  7, -2,  3,  5,  9,  6,  3, 33,  0,  0,  2],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 2 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t1\t4\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t8\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [1\t7\t-2\t3]\n",
      " [5\t9\t6\tx]  \n",
      "\n",
      "sampled action player_2: 5\n",
      "training fct: {'observations': array([ 6,  5,  9,  4,  4,  1,  4, -1,  4,  1,  7,  1, 28,  0,  7, 15],\n",
      "      dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 7 \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t1\t4\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t8\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [1\t2\t-2\t3]\n",
      " [5\t9\t6\tx]  \n",
      "\n",
      "sampled action player_0: 13\n",
      "training fct: {'observations': array([ 6,  5,  9,  4,  4,  1,  4, -1,  4,  1,  7,  1, 28,  0,  7, 15],\n",
      "      dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)} -16.66666666666667 True {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 7 \n",
      "======= GAME DONE ======== \n",
      "Results: {0: 90, 1: 88, 2: 42} \n",
      "======= Player 0 ========== \n",
      " [6\t5\t9\t4]\n",
      " [4\t1\t4\t-1]\n",
      " [4\t1\t7\t1]  \n",
      "======= Player 1 ========== \n",
      " [12\t1\t7\t10]\n",
      " [11\t8\t-2\t9]\n",
      " [6\t8\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\t-1\t0\t5]\n",
      " [1\t2\t-2\t3]\n",
      " [5\t9\t6\tx]  \n",
      "\n",
      "done -16.66666666666667\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "i_episode = 1  \n",
    "agent_iters = 0\n",
    "while i_episode <= 1:\n",
    "    for agent in env.agent_iter(max_iter=600):        \n",
    "        # get observation (state) for current agent:\n",
    "        obs, reward, done, info = env.last()\n",
    "        \n",
    "        print(\"training fct:\", obs, reward, done, info)\n",
    "        # perform q-learning with update_Q_value()\n",
    "        # your code here\n",
    "        \n",
    "        env.render()\n",
    "        if agent_iters > 600:\n",
    "            break\n",
    "        \n",
    "        # store current state            \n",
    "        if not done: \n",
    "            # choose action using epsilon_greedy_policy()\n",
    "            # your code here    \n",
    "            observation = obs[\"observations\"]\n",
    "            action_mask = obs[\"action_mask\"]\n",
    "            action = random_admissible_policy(observation, action_mask)\n",
    "        \n",
    "            print(f\"sampled action {agent}: {action}\")\n",
    "            env.step(action)\n",
    "        else: \n",
    "            # agent is done\n",
    "            env.step(None)\n",
    "            print('done', reward)\n",
    "            break\n",
    "    agent_iters = 0\n",
    "    i_episode  += 1\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more envs test with rllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.17.87.73',\n",
       " 'raylet_ip_address': '172.17.87.73',\n",
       " 'redis_address': '172.17.87.73:10779',\n",
       " 'object_store_address': '/tmp/ray/session_2022-01-27_19-56-08_083579_780/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-01-27_19-56-08_083579_780/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-01-27_19-56-08_083579_780',\n",
       " 'metrics_export_port': 62240,\n",
       " 'node_id': 'fd5d09a29e39bf0e73787ce8221f3b694e40ec9719853ef40f468ada'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import init\n",
    "init(local_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "from rlskyjo.environment import simple_skyjo_env_v2\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "import ray.rllib.agents.dqn as dqn\n",
    "from ray.rllib.agents import ppo\n",
    "from copy import deepcopy\n",
    "from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from rlskyjo.models.action_mask_model import TorchMaskedActions, TorchActionMaskModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.agents.dqn import DQNTrainer, DQNTorchPolicy\n",
    "from ray.tune.logger import pretty_print\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "  warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "/home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "  warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env_name  = \"pettingzoo_skyjo\"\n",
    "def env_creator():\n",
    "        env = simple_skyjo_env_v2.env(**skyjo_env_cfg)\n",
    "        return env\n",
    "\n",
    "register_env(env_name,\n",
    "                lambda config: PettingZooEnv(env_creator()))\n",
    "ModelCatalog.register_custom_model(\n",
    "        \"pa_model2\", TorchActionMaskModel\n",
    "    )\n",
    "env = PettingZooEnv(env_creator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observations': array([15, -1, 15, 15, 15, 15, 15, 15,  9, 15, 15, 15,  8, 10,  0, 15],\n",
       "       dtype=int8),\n",
       " 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int8)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.observe(env.env.agent_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with multiagent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-27 19:56:36,861\tWARNING ppo.py:143 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=16 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 250.\n",
      "2022-01-27 19:56:36,862\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-01-27 19:56:36,864\tINFO trainer.py:743 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2022-01-27 19:56:36,902\tWARNING worker.py:502 -- `ray.get_gpu_ids()` will always return the empty list when called from the driver. This is because Ray does not manage GPU allocations to the driver process.\n",
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":actor_name:RolloutWorker\n",
      ":actor_name:RolloutWorker\n"
     ]
    }
   ],
   "source": [
    "custom_config={\n",
    "    \"env\":env_name,\n",
    "    \"model\": {\n",
    "        \"custom_model\": \"pa_model2\",\n",
    "    },\n",
    "    \"framework\": \"torch\",\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "    \"num_gpus\": int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "    \"num_workers\": os.cpu_count(),\n",
    "    \"multiagent\":{\n",
    "            \"policies\": {\n",
    "                name: (None, env.observation_space, env.action_space, {}) for name in env.agents\n",
    "            },\n",
    "            \"policy_mapping_fn\": lambda agent_id: agent_id\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "ppo_config = ppo.DEFAULT_CONFIG.copy()\n",
    "ppo_config.update(custom_config)\n",
    "\n",
    "trainer = ppo.PPOTrainer(config=ppo_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 3968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_19-56-55\n",
      "done: false\n",
      "episode_len_mean: 145.73684210526315\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -2.243819165558211e-15\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 19\n",
      "episodes_total: 19\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5696350447336833\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006987270397140285\n",
      "        policy_loss: -0.14716034355262916\n",
      "        total_loss: 30.90483606974284\n",
      "        vf_explained_var: 0.01261245866616567\n",
      "        vf_loss: 31.05059880574544\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5667730486392974\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005447234967851536\n",
      "        policy_loss: 0.04465158905213078\n",
      "        total_loss: 169.36929793675742\n",
      "        vf_explained_var: 0.003955875635147095\n",
      "        vf_loss: 169.3235590362549\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5642924650510153\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011455004199203494\n",
      "        policy_loss: -0.013621867162485918\n",
      "        total_loss: 144.4597945491473\n",
      "        vf_explained_var: 0.0008191025257110596\n",
      "        vf_loss: 144.47112552165984\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 3968\n",
      "  num_agent_steps_trained: 3968\n",
      "  num_steps_sampled: 4000\n",
      "  num_steps_trained: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.430434782608693\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 26.734782608695646\n",
      "  vram_util_percent0: 0.095703125\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 11.666666666666671\n",
      "  player_1: 36.0\n",
      "  player_2: 38.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: -3.2105263157894743\n",
      "  player_1: -4.684210526315789\n",
      "  player_2: 7.894736842105263\n",
      "policy_reward_min:\n",
      "  player_0: -49.0\n",
      "  player_1: -42.66666666666667\n",
      "  player_2: -33.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.035800531890862734\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09760418686133857\n",
      "  mean_inference_ms: 0.7796406520940293\n",
      "  mean_raw_obs_processing_ms: 0.088973574279414\n",
      "time_since_restore: 10.51049518585205\n",
      "time_this_iter_s: 10.51049518585205\n",
      "time_total_s: 10.51049518585205\n",
      "timers:\n",
      "  learn_throughput: 627.384\n",
      "  learn_time_ms: 6375.681\n",
      "  load_throughput: 6492730.65\n",
      "  load_time_ms: 0.616\n",
      "  sample_throughput: 354.776\n",
      "  sample_time_ms: 11274.712\n",
      "  update_time_ms: 35.084\n",
      "timestamp: 1643309815\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 11968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_19-57-13\n",
      "done: false\n",
      "episode_len_mean: 148.25714285714287\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -1.0150610510858574e-15\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 70\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5474694168567658\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012845474327855587\n",
      "        policy_loss: -0.0365781477962931\n",
      "        total_loss: 175.4171685155233\n",
      "        vf_explained_var: 0.03081464648246765\n",
      "        vf_loss: 175.4511792119344\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.554332702557246\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006798259796987623\n",
      "        policy_loss: -0.03894207010666529\n",
      "        total_loss: 256.2774345721801\n",
      "        vf_explained_var: 0.021408306161562602\n",
      "        vf_loss: 256.3150162068009\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5398937606811522\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011559579114497561\n",
      "        policy_loss: -0.02674608678246538\n",
      "        total_loss: 188.0399420070648\n",
      "        vf_explained_var: 0.004430781006813049\n",
      "        vf_loss: 188.0643788719177\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 11968\n",
      "  num_agent_steps_trained: 11968\n",
      "  num_steps_sampled: 12000\n",
      "  num_steps_trained: 12000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 3\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.966666666666665\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 26.850000000000005\n",
      "  vram_util_percent0: 0.095703125\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 29.66666666666667\n",
      "  player_1: 44.66666666666667\n",
      "  player_2: 45.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 0.05238095238095204\n",
      "  player_1: 0.7380952380952378\n",
      "  player_2: -0.7904761904761908\n",
      "policy_reward_min:\n",
      "  player_0: -49.0\n",
      "  player_1: -52.66666666666667\n",
      "  player_2: -53.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03546800024601688\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09571355388697535\n",
      "  mean_inference_ms: 0.7544719893836577\n",
      "  mean_raw_obs_processing_ms: 0.08858961911827212\n",
      "time_since_restore: 28.321810245513916\n",
      "time_this_iter_s: 9.033188581466675\n",
      "time_total_s: 28.321810245513916\n",
      "timers:\n",
      "  learn_throughput: 716.495\n",
      "  learn_time_ms: 5582.732\n",
      "  load_throughput: 7009002.646\n",
      "  load_time_ms: 0.571\n",
      "  sample_throughput: 398.849\n",
      "  sample_time_ms: 10028.858\n",
      "  update_time_ms: 31.368\n",
      "timestamp: 1643309833\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 12000\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 19968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_19-57-32\n",
      "done: false\n",
      "episode_len_mean: 162.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -7.105427357601002e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 115\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.513096573750178\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012054259748031957\n",
      "        policy_loss: -0.054403660592312615\n",
      "        total_loss: 288.0622464052836\n",
      "        vf_explained_var: -0.014980552196502685\n",
      "        vf_loss: 288.1142412694295\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5477290554841359\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009264392017596492\n",
      "        policy_loss: -0.046084393759568534\n",
      "        total_loss: 187.26662205060322\n",
      "        vf_explained_var: -0.0004374782244364421\n",
      "        vf_loss: 187.31085505962372\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4896948579947153\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012838352282607653\n",
      "        policy_loss: -0.030011661127209665\n",
      "        total_loss: 226.93360627492268\n",
      "        vf_explained_var: 0.012063119411468506\n",
      "        vf_loss: 226.9610485458374\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 19968\n",
      "  num_agent_steps_trained: 19968\n",
      "  num_steps_sampled: 20000\n",
      "  num_steps_trained: 20000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 5\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.658333333333335\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 26.8\n",
      "  vram_util_percent0: 0.095703125\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 43.0\n",
      "  player_1: 51.66666666666667\n",
      "  player_2: 45.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: -1.5833333333333335\n",
      "  player_1: 3.9066666666666663\n",
      "  player_2: -2.3233333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -54.0\n",
      "  player_1: -52.66666666666667\n",
      "  player_2: -55.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03521063360405159\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09491648031361251\n",
      "  mean_inference_ms: 0.7342702767351034\n",
      "  mean_raw_obs_processing_ms: 0.08811744388263097\n",
      "time_since_restore: 47.79675889015198\n",
      "time_this_iter_s: 9.936680316925049\n",
      "time_total_s: 47.79675889015198\n",
      "timers:\n",
      "  learn_throughput: 698.898\n",
      "  learn_time_ms: 5723.296\n",
      "  load_throughput: 6805068.549\n",
      "  load_time_ms: 0.588\n",
      "  sample_throughput: 408.04\n",
      "  sample_time_ms: 9802.964\n",
      "  update_time_ms: 30.168\n",
      "timestamp: 1643309852\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 20000\n",
      "training_iteration: 5\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 27968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_19-57-51\n",
      "done: false\n",
      "episode_len_mean: 168.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -1.7053025658242404e-15\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 164\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4547973974545796\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011402950908698889\n",
      "        policy_loss: -0.06244531733294328\n",
      "        total_loss: 264.6457677459717\n",
      "        vf_explained_var: 0.0023146021366119386\n",
      "        vf_loss: 264.7059308560689\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5004183169205982\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010071693283255172\n",
      "        policy_loss: -0.05550946096268793\n",
      "        total_loss: 376.38349421183267\n",
      "        vf_explained_var: -0.0035921037197113037\n",
      "        vf_loss: 376.43698989868165\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.456279589732488\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011118191273635981\n",
      "        policy_loss: -0.035163006521761415\n",
      "        total_loss: 161.33224758783976\n",
      "        vf_explained_var: -0.12038652122020721\n",
      "        vf_loss: 161.3651869859298\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 27968\n",
      "  num_agent_steps_trained: 27968\n",
      "  num_steps_sampled: 28000\n",
      "  num_steps_trained: 28000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 7\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.87272727272727\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 26.809090909090912\n",
      "  vram_util_percent0: 0.095703125\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 51.0\n",
      "  player_1: 51.66666666666667\n",
      "  player_2: 45.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: -2.5400000000000005\n",
      "  player_1: 2.1399999999999992\n",
      "  player_2: 0.39999999999999963\n",
      "policy_reward_min:\n",
      "  player_0: -54.0\n",
      "  player_1: -55.0\n",
      "  player_2: -55.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03499486993641097\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09421762541000742\n",
      "  mean_inference_ms: 0.7169424338828173\n",
      "  mean_raw_obs_processing_ms: 0.08756610364184052\n",
      "time_since_restore: 66.49210739135742\n",
      "time_this_iter_s: 8.966224431991577\n",
      "time_total_s: 66.49210739135742\n",
      "timers:\n",
      "  learn_throughput: 700.842\n",
      "  learn_time_ms: 5707.424\n",
      "  load_throughput: 6816839.564\n",
      "  load_time_ms: 0.587\n",
      "  sample_throughput: 408.776\n",
      "  sample_time_ms: 9785.3\n",
      "  update_time_ms: 30.296\n",
      "timestamp: 1643309871\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 28000\n",
      "training_iteration: 7\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 35968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_19-58-08\n",
      "done: false\n",
      "episode_len_mean: 174.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -5.684341886080802e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 208\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4315631111462912\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009362371161508539\n",
      "        policy_loss: -0.0698504538461566\n",
      "        total_loss: 181.11594223022462\n",
      "        vf_explained_var: 0.020701862573623657\n",
      "        vf_loss: 181.18392033894858\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4715589765707653\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006020505254843741\n",
      "        policy_loss: -0.04251943884417415\n",
      "        total_loss: 218.58334813435872\n",
      "        vf_explained_var: 0.01773299753665924\n",
      "        vf_loss: 218.62466454823812\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3997580242156982\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006297374362940902\n",
      "        policy_loss: -0.016729125815133254\n",
      "        total_loss: 227.62655772884688\n",
      "        vf_explained_var: -0.05974257449309031\n",
      "        vf_loss: 227.64202719926834\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 35968\n",
      "  num_agent_steps_trained: 35968\n",
      "  num_steps_sampled: 36000\n",
      "  num_steps_trained: 36000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 9\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.699999999999996\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 26.9\n",
      "  vram_util_percent0: 0.095703125\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 51.0\n",
      "  player_1: 48.33333333333333\n",
      "  player_2: 44.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 0.5133333333333332\n",
      "  player_1: -1.8566666666666665\n",
      "  player_2: 1.3433333333333328\n",
      "policy_reward_min:\n",
      "  player_0: -51.33333333333333\n",
      "  player_1: -55.0\n",
      "  player_2: -50.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0348157837181597\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09340308281685694\n",
      "  mean_inference_ms: 0.7085050086765919\n",
      "  mean_raw_obs_processing_ms: 0.08706718598668567\n",
      "time_since_restore: 83.62162613868713\n",
      "time_this_iter_s: 8.524166584014893\n",
      "time_total_s: 83.62162613868713\n",
      "timers:\n",
      "  learn_throughput: 716.451\n",
      "  learn_time_ms: 5583.073\n",
      "  load_throughput: 6919707.804\n",
      "  load_time_ms: 0.578\n",
      "  sample_throughput: 419.721\n",
      "  sample_time_ms: 9530.136\n",
      "  update_time_ms: 30.487\n",
      "timestamp: 1643309888\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 36000\n",
      "training_iteration: 9\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 43968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_19-58-25\n",
      "done: false\n",
      "episode_len_mean: 187.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -2.842170943040401e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 18\n",
      "episodes_total: 243\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.398563704888026\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010558953126489239\n",
      "        policy_loss: -0.04119941125313441\n",
      "        total_loss: 190.05300014813741\n",
      "        vf_explained_var: -0.04498798072338104\n",
      "        vf_loss: 190.0920872147878\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4483741001288095\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008483243355886476\n",
      "        policy_loss: -0.05195841364562512\n",
      "        total_loss: 93.63806125640869\n",
      "        vf_explained_var: 0.046537681619326275\n",
      "        vf_loss: 93.68832297007243\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3398077162106832\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009450186961830696\n",
      "        policy_loss: -0.04800878671308358\n",
      "        total_loss: 201.49018559773762\n",
      "        vf_explained_var: 0.03913711925347646\n",
      "        vf_loss: 201.53630406856536\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 43968\n",
      "  num_agent_steps_trained: 43968\n",
      "  num_steps_sampled: 44000\n",
      "  num_steps_trained: 44000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 11\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.30909090909091\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 26.9\n",
      "  vram_util_percent0: 0.095703125\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 45.0\n",
      "  player_1: 48.33333333333333\n",
      "  player_2: 44.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.0166666666666668\n",
      "  player_1: -1.693333333333334\n",
      "  player_2: 0.6766666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -51.33333333333333\n",
      "  player_1: -55.0\n",
      "  player_2: -52.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.034529697025094286\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09232379333978429\n",
      "  mean_inference_ms: 0.6990301457434464\n",
      "  mean_raw_obs_processing_ms: 0.0857454668339849\n",
      "time_since_restore: 100.46840596199036\n",
      "time_this_iter_s: 8.232948541641235\n",
      "time_total_s: 100.46840596199036\n",
      "timers:\n",
      "  learn_throughput: 741.133\n",
      "  learn_time_ms: 5397.141\n",
      "  load_throughput: 7083477.306\n",
      "  load_time_ms: 0.565\n",
      "  sample_throughput: 436.043\n",
      "  sample_time_ms: 9173.405\n",
      "  update_time_ms: 30.879\n",
      "timestamp: 1643309905\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 44000\n",
      "training_iteration: 11\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 51968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_19-58-41\n",
      "done: false\n",
      "episode_len_mean: 213.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -3.552713678800501e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 16\n",
      "episodes_total: 277\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3353099783261617\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010279048671328232\n",
      "        policy_loss: -0.061955911790331206\n",
      "        total_loss: 67.16710801442464\n",
      "        vf_explained_var: -0.08922195355097452\n",
      "        vf_loss: 67.22700804511706\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4130400685469309\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00879149442775694\n",
      "        policy_loss: -0.056044374406337735\n",
      "        total_loss: 153.27859002431234\n",
      "        vf_explained_var: 0.05856710314750671\n",
      "        vf_loss: 153.33287506421408\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3086662562688192\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011526564805834975\n",
      "        policy_loss: -0.03462828343113263\n",
      "        total_loss: 218.8886694717407\n",
      "        vf_explained_var: 0.00885460595289866\n",
      "        vf_loss: 218.9209925810496\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 51968\n",
      "  num_agent_steps_trained: 51968\n",
      "  num_steps_sampled: 52000\n",
      "  num_steps_trained: 52000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 13\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.259999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 26.9\n",
      "  vram_util_percent0: 0.095703125\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 45.0\n",
      "  player_1: 45.66666666666667\n",
      "  player_2: 38.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.5933333333333337\n",
      "  player_1: -2.486666666666667\n",
      "  player_2: 0.8933333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -42.0\n",
      "  player_1: -52.0\n",
      "  player_2: -59.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.034055596181083544\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.0905729939629352\n",
      "  mean_inference_ms: 0.685928339753237\n",
      "  mean_raw_obs_processing_ms: 0.08401106066073655\n",
      "time_since_restore: 116.65482640266418\n",
      "time_this_iter_s: 8.108402729034424\n",
      "time_total_s: 116.65482640266418\n",
      "timers:\n",
      "  learn_throughput: 750.3\n",
      "  learn_time_ms: 5331.197\n",
      "  load_throughput: 7060226.402\n",
      "  load_time_ms: 0.567\n",
      "  sample_throughput: 448.386\n",
      "  sample_time_ms: 8920.882\n",
      "  update_time_ms: 30.646\n",
      "timestamp: 1643309921\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 52000\n",
      "training_iteration: 13\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 59968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_19-58-59\n",
      "done: false\n",
      "episode_len_mean: 229.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -9.237055564881303e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 17\n",
      "episodes_total: 308\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3497778447469075\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010253264095566313\n",
      "        policy_loss: -0.05245394389455517\n",
      "        total_loss: 151.039022843043\n",
      "        vf_explained_var: 0.01987474819024404\n",
      "        vf_loss: 151.089425350825\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3961260350545248\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008640173305343687\n",
      "        policy_loss: -0.054376229255770646\n",
      "        total_loss: 206.92508651653927\n",
      "        vf_explained_var: -0.03468765695889791\n",
      "        vf_loss: 206.9777331240972\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2708986095587413\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008808728436148765\n",
      "        policy_loss: -0.04432788329819838\n",
      "        total_loss: 44.85401127656301\n",
      "        vf_explained_var: 0.07412302712599436\n",
      "        vf_loss: 44.896577363014224\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 59968\n",
      "  num_agent_steps_trained: 59968\n",
      "  num_steps_sampled: 60000\n",
      "  num_steps_trained: 60000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 15\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.709090909090907\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 26.927272727272726\n",
      "  vram_util_percent0: 0.095703125\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 45.0\n",
      "  player_1: 48.666666666666664\n",
      "  player_2: 38.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.2733333333333335\n",
      "  player_1: -2.773333333333334\n",
      "  player_2: 3.0466666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -42.0\n",
      "  player_1: -52.0\n",
      "  player_2: -59.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03373957975243522\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.08942075072736681\n",
      "  mean_inference_ms: 0.677436381732832\n",
      "  mean_raw_obs_processing_ms: 0.0827465366398354\n",
      "time_since_restore: 133.9206600189209\n",
      "time_this_iter_s: 9.155616760253906\n",
      "time_total_s: 133.9206600189209\n",
      "timers:\n",
      "  learn_throughput: 766.344\n",
      "  learn_time_ms: 5219.586\n",
      "  load_throughput: 7221908.657\n",
      "  load_time_ms: 0.554\n",
      "  sample_throughput: 462.257\n",
      "  sample_time_ms: 8653.197\n",
      "  update_time_ms: 32.005\n",
      "timestamp: 1643309939\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 60000\n",
      "training_iteration: 15\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 67968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_19-59-18\n",
      "done: false\n",
      "episode_len_mean: 247.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -1.0658141036401502e-15\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 10\n",
      "episodes_total: 333\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3277977271874746\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01077426928181353\n",
      "        policy_loss: -0.03740635527608295\n",
      "        total_loss: 105.28934435804685\n",
      "        vf_explained_var: -0.16994604686896006\n",
      "        vf_loss: 105.32459596514701\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3804836956659954\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012154100114203174\n",
      "        policy_loss: -0.07707351579641303\n",
      "        total_loss: 28.243621275027593\n",
      "        vf_explained_var: -0.05711093544960022\n",
      "        vf_loss: 28.318264000813166\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.240596936941147\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007498984455317744\n",
      "        policy_loss: -0.040247351601719855\n",
      "        total_loss: 46.82894140621026\n",
      "        vf_explained_var: -0.3241290440162023\n",
      "        vf_loss: 46.86768944233656\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 67968\n",
      "  num_agent_steps_trained: 67968\n",
      "  num_steps_sampled: 68000\n",
      "  num_steps_trained: 68000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 17\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.618181818181817\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.0\n",
      "  vram_util_percent0: 0.095703125\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 51.0\n",
      "  player_1: 48.666666666666664\n",
      "  player_2: 30.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 0.15999999999999964\n",
      "  player_1: -2.2100000000000004\n",
      "  player_2: 2.05\n",
      "policy_reward_min:\n",
      "  player_0: -41.66666666666667\n",
      "  player_1: -56.0\n",
      "  player_2: -59.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03353788859892229\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.08873778585662782\n",
      "  mean_inference_ms: 0.6714224503698916\n",
      "  mean_raw_obs_processing_ms: 0.08199555763036527\n",
      "time_since_restore: 152.71666932106018\n",
      "time_this_iter_s: 8.555250406265259\n",
      "time_total_s: 152.71666932106018\n",
      "timers:\n",
      "  learn_throughput: 768.098\n",
      "  learn_time_ms: 5207.666\n",
      "  load_throughput: 7312245.467\n",
      "  load_time_ms: 0.547\n",
      "  sample_throughput: 460.546\n",
      "  sample_time_ms: 8685.335\n",
      "  update_time_ms: 32.76\n",
      "timestamp: 1643309958\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 68000\n",
      "training_iteration: 17\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 75968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_19-59-35\n",
      "done: false\n",
      "episode_len_mean: 268.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -2.1316282072803005e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 13\n",
      "episodes_total: 358\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2675343481699626\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008108572564540223\n",
      "        policy_loss: -0.04663052516678969\n",
      "        total_loss: 72.34229719718297\n",
      "        vf_explained_var: -0.2097748061021169\n",
      "        vf_loss: 72.38730646371842\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.334756993452708\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008326646003254912\n",
      "        policy_loss: -0.05766967228613794\n",
      "        total_loss: 138.90256661951543\n",
      "        vf_explained_var: -0.17827489594618479\n",
      "        vf_loss: 138.9585709486405\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2588978159427642\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009964795653074059\n",
      "        policy_loss: -0.04932079682747523\n",
      "        total_loss: 74.4419194396337\n",
      "        vf_explained_var: -0.17806740840276084\n",
      "        vf_loss: 74.48924691756567\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 75968\n",
      "  num_agent_steps_trained: 75968\n",
      "  num_steps_sampled: 76000\n",
      "  num_steps_trained: 76000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 19\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.858333333333334\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.08333333333334\n",
      "  vram_util_percent0: 0.095947265625\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 51.0\n",
      "  player_1: 48.666666666666664\n",
      "  player_2: 51.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.2899999999999999\n",
      "  player_1: -3.03\n",
      "  player_2: 2.74\n",
      "policy_reward_min:\n",
      "  player_0: -41.66666666666667\n",
      "  player_1: -56.0\n",
      "  player_2: -59.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03353735725972807\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.08875599575104434\n",
      "  mean_inference_ms: 0.6711678063922579\n",
      "  mean_raw_obs_processing_ms: 0.08171554700282206\n",
      "time_since_restore: 170.1327452659607\n",
      "time_this_iter_s: 8.758033990859985\n",
      "time_total_s: 170.1327452659607\n",
      "timers:\n",
      "  learn_throughput: 768.584\n",
      "  learn_time_ms: 5204.373\n",
      "  load_throughput: 7347149.551\n",
      "  load_time_ms: 0.544\n",
      "  sample_throughput: 460.513\n",
      "  sample_time_ms: 8685.973\n",
      "  update_time_ms: 32.508\n",
      "timestamp: 1643309975\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 76000\n",
      "training_iteration: 19\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 83970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_19-59-53\n",
      "done: false\n",
      "episode_len_mean: 290.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 0.0\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 7\n",
      "episodes_total: 379\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2517368046442667\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010447236611204669\n",
      "        policy_loss: -0.005977745627363523\n",
      "        total_loss: 43.618702588478726\n",
      "        vf_explained_var: -0.2819854587316513\n",
      "        vf_loss: 43.622591123978296\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3375575983524322\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009525123812909441\n",
      "        policy_loss: -0.0936962683374683\n",
      "        total_loss: 68.1190756992499\n",
      "        vf_explained_var: -0.270471731821696\n",
      "        vf_loss: 68.210866792202\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1951638074715931\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006851939586498095\n",
      "        policy_loss: -0.009626660645008087\n",
      "        total_loss: 50.46451951344808\n",
      "        vf_explained_var: -0.17515161673227947\n",
      "        vf_loss: 50.472775459686915\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 83970\n",
      "  num_agent_steps_trained: 83970\n",
      "  num_steps_sampled: 84000\n",
      "  num_steps_trained: 84000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 21\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.072727272727274\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.1\n",
      "  vram_util_percent0: 0.09923946496212123\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 51.0\n",
      "  player_1: 48.666666666666664\n",
      "  player_2: 51.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.24999999999999986\n",
      "  player_1: -5.53\n",
      "  player_2: 5.780000000000001\n",
      "policy_reward_min:\n",
      "  player_0: -41.66666666666667\n",
      "  player_1: -56.0\n",
      "  player_2: -41.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.033531878819255324\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.08878784281020045\n",
      "  mean_inference_ms: 0.6701897663524499\n",
      "  mean_raw_obs_processing_ms: 0.08151706758142022\n",
      "time_since_restore: 188.3828570842743\n",
      "time_this_iter_s: 9.071163654327393\n",
      "time_total_s: 188.3828570842743\n",
      "timers:\n",
      "  learn_throughput: 759.181\n",
      "  learn_time_ms: 5268.838\n",
      "  load_throughput: 6699363.495\n",
      "  load_time_ms: 0.597\n",
      "  sample_throughput: 454.966\n",
      "  sample_time_ms: 8791.871\n",
      "  update_time_ms: 33.326\n",
      "timestamp: 1643309993\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 84000\n",
      "training_iteration: 21\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 91968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-00-12\n",
      "done: false\n",
      "episode_len_mean: 309.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -7.105427357601002e-17\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 390\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1855962852636972\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011421113413592442\n",
      "        policy_loss: -0.04427077302942053\n",
      "        total_loss: 21.276883940696717\n",
      "        vf_explained_var: -0.13453477541605632\n",
      "        vf_loss: 21.31887030561765\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3164931690692903\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010296916410734411\n",
      "        policy_loss: -0.062330440289030474\n",
      "        total_loss: 25.998340608676276\n",
      "        vf_explained_var: -0.28633642633756\n",
      "        vf_loss: 26.05861168503761\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1836916216214497\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00944486089363636\n",
      "        policy_loss: -0.024584602216879525\n",
      "        total_loss: 5.177935053706169\n",
      "        vf_explained_var: -0.055524838169415794\n",
      "        vf_loss: 5.200630690455437\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 91968\n",
      "  num_agent_steps_trained: 91968\n",
      "  num_steps_sampled: 92000\n",
      "  num_steps_trained: 92000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 23\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.608333333333334\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.100000000000005\n",
      "  vram_util_percent0: 0.09758843315972221\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 51.0\n",
      "  player_1: 48.666666666666664\n",
      "  player_2: 51.0\n",
      "policy_reward_mean:\n",
      "  player_0: -1.383333333333333\n",
      "  player_1: -4.543333333333333\n",
      "  player_2: 5.926666666666668\n",
      "policy_reward_min:\n",
      "  player_0: -45.33333333333333\n",
      "  player_1: -56.0\n",
      "  player_2: -41.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.033575574111048126\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.08897338094871582\n",
      "  mean_inference_ms: 0.6710046872435242\n",
      "  mean_raw_obs_processing_ms: 0.08148102099412409\n",
      "time_since_restore: 207.0564045906067\n",
      "time_this_iter_s: 9.89658498764038\n",
      "time_total_s: 207.0564045906067\n",
      "timers:\n",
      "  learn_throughput: 735.975\n",
      "  learn_time_ms: 5434.968\n",
      "  load_throughput: 6539805.099\n",
      "  load_time_ms: 0.612\n",
      "  sample_throughput: 447.583\n",
      "  sample_time_ms: 8936.888\n",
      "  update_time_ms: 36.054\n",
      "timestamp: 1643310012\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 92000\n",
      "training_iteration: 23\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 99968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-00-35\n",
      "done: false\n",
      "episode_len_mean: 364.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -1.4210854715202004e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 7\n",
      "episodes_total: 405\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1791935988267264\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009592843888929867\n",
      "        policy_loss: -0.0265958850334088\n",
      "        total_loss: 82.8484869201978\n",
      "        vf_explained_var: -0.2715191912651062\n",
      "        vf_loss: 82.87316406011581\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2705560147762298\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009046229386724463\n",
      "        policy_loss: -0.06060427763848566\n",
      "        total_loss: 67.35728323797385\n",
      "        vf_explained_var: -0.23520708998044332\n",
      "        vf_loss: 67.41607876896859\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2098314839601516\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013585249906536773\n",
      "        policy_loss: -0.09478293056910236\n",
      "        total_loss: 6.099188795015216\n",
      "        vf_explained_var: -0.2030040834347407\n",
      "        vf_loss: 6.1912546898921335\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 99968\n",
      "  num_agent_steps_trained: 99968\n",
      "  num_steps_sampled: 100000\n",
      "  num_steps_trained: 100000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 25\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.157142857142855\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.135714285714283\n",
      "  vram_util_percent0: 0.09635416666666667\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 53.0\n",
      "  player_1: 43.66666666666667\n",
      "  player_2: 51.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.0866666666666665\n",
      "  player_1: -5.346666666666668\n",
      "  player_2: 5.433333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -45.33333333333333\n",
      "  player_1: -56.0\n",
      "  player_2: -41.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03376150180950274\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.08957180775443113\n",
      "  mean_inference_ms: 0.6750956739905871\n",
      "  mean_raw_obs_processing_ms: 0.08176211452066724\n",
      "time_since_restore: 229.81588411331177\n",
      "time_this_iter_s: 10.935995101928711\n",
      "time_total_s: 229.81588411331177\n",
      "timers:\n",
      "  learn_throughput: 706.136\n",
      "  learn_time_ms: 5664.629\n",
      "  load_throughput: 6206427.937\n",
      "  load_time_ms: 0.644\n",
      "  sample_throughput: 418.603\n",
      "  sample_time_ms: 9555.587\n",
      "  update_time_ms: 36.122\n",
      "timestamp: 1643310035\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 100000\n",
      "training_iteration: 25\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 107968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-00-55\n",
      "done: false\n",
      "episode_len_mean: 409.12\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 6.394884621840901e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 6\n",
      "episodes_total: 418\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1479515218734742\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00929635714127168\n",
      "        policy_loss: -0.03986527347005904\n",
      "        total_loss: 32.13745682477951\n",
      "        vf_explained_var: -0.18887400686740874\n",
      "        vf_loss: 32.17546308497588\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.215286823908488\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009722483026607734\n",
      "        policy_loss: -0.05031829858819643\n",
      "        total_loss: 56.73286066452662\n",
      "        vf_explained_var: -0.20293944478034973\n",
      "        vf_loss: 56.781234418153765\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0887721498807272\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009169630112720255\n",
      "        policy_loss: -0.034984318409115076\n",
      "        total_loss: 32.21014742215475\n",
      "        vf_explained_var: -0.20586534718672433\n",
      "        vf_loss: 32.24329823235671\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 107968\n",
      "  num_agent_steps_trained: 107968\n",
      "  num_steps_sampled: 108000\n",
      "  num_steps_trained: 108000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 27\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.645454545454545\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.2\n",
      "  vram_util_percent0: 0.09635416666666666\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 53.0\n",
      "  player_1: 50.66666666666667\n",
      "  player_2: 51.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.7400000000000003\n",
      "  player_1: -5.78\n",
      "  player_2: 5.04\n",
      "policy_reward_min:\n",
      "  player_0: -45.33333333333333\n",
      "  player_1: -56.0\n",
      "  player_2: -41.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03389151687346823\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.08994805178150246\n",
      "  mean_inference_ms: 0.6778336375106639\n",
      "  mean_raw_obs_processing_ms: 0.08193780250163728\n",
      "time_since_restore: 249.5473918914795\n",
      "time_this_iter_s: 8.667401790618896\n",
      "time_total_s: 249.5473918914795\n",
      "timers:\n",
      "  learn_throughput: 697.963\n",
      "  learn_time_ms: 5730.967\n",
      "  load_throughput: 6106357.052\n",
      "  load_time_ms: 0.655\n",
      "  sample_throughput: 412.636\n",
      "  sample_time_ms: 9693.78\n",
      "  update_time_ms: 35.023\n",
      "timestamp: 1643310055\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 108000\n",
      "training_iteration: 27\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 115969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-01-13\n",
      "done: false\n",
      "episode_len_mean: 438.84\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 3.552713678800501e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 429\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.102694328625997\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008812785593581793\n",
      "        policy_loss: -0.0409860692700992\n",
      "        total_loss: 18.95108179907004\n",
      "        vf_explained_var: -0.425182360013326\n",
      "        vf_loss: 18.99030518134435\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.182564148902893\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012111494111200986\n",
      "        policy_loss: -0.06046673658614357\n",
      "        total_loss: 3.326161420742671\n",
      "        vf_explained_var: -0.4108354606231054\n",
      "        vf_loss: 3.384205865065257\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1129111162821452\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0073818799134183636\n",
      "        policy_loss: -0.039781501491864525\n",
      "        total_loss: 7.714036451975504\n",
      "        vf_explained_var: -0.24768264094988504\n",
      "        vf_loss: 7.752341593801975\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 115969\n",
      "  num_agent_steps_trained: 115969\n",
      "  num_steps_sampled: 116000\n",
      "  num_steps_trained: 116000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 29\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.058333333333337\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.233333333333334\n",
      "  vram_util_percent0: 0.09635416666666667\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 53.0\n",
      "  player_1: 50.66666666666667\n",
      "  player_2: 51.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.4666666666666668\n",
      "  player_1: -5.6433333333333335\n",
      "  player_2: 5.176666666666668\n",
      "policy_reward_min:\n",
      "  player_0: -45.33333333333333\n",
      "  player_1: -52.66666666666667\n",
      "  player_2: -41.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.034034500989110796\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09038656167158254\n",
      "  mean_inference_ms: 0.6807613022633763\n",
      "  mean_raw_obs_processing_ms: 0.08207502255663904\n",
      "time_since_restore: 267.7197308540344\n",
      "time_this_iter_s: 9.519483804702759\n",
      "time_total_s: 267.7197308540344\n",
      "timers:\n",
      "  learn_throughput: 684.792\n",
      "  learn_time_ms: 5841.187\n",
      "  load_throughput: 6013123.544\n",
      "  load_time_ms: 0.665\n",
      "  sample_throughput: 412.306\n",
      "  sample_time_ms: 9701.531\n",
      "  update_time_ms: 35.042\n",
      "timestamp: 1643310073\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 116000\n",
      "training_iteration: 29\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 123968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-01-31\n",
      "done: false\n",
      "episode_len_mean: 473.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -7.105427357601002e-17\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 437\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.091799501379331\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005936736598329541\n",
      "        policy_loss: -0.14600830322752395\n",
      "        total_loss: 22.255002001027265\n",
      "        vf_explained_var: -0.5048383331298828\n",
      "        vf_loss: 22.399822955528894\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1071074658632278\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005948711669140418\n",
      "        policy_loss: -0.001055079052845637\n",
      "        total_loss: 31.27467188000679\n",
      "        vf_explained_var: -0.05527550498644511\n",
      "        vf_loss: 31.27453681310018\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0919313808282216\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004211331271817471\n",
      "        policy_loss: 0.054595898271848756\n",
      "        total_loss: 40.770273414651555\n",
      "        vf_explained_var: -0.36187593976656596\n",
      "        vf_loss: 40.71483463793993\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 123968\n",
      "  num_agent_steps_trained: 123968\n",
      "  num_steps_sampled: 124000\n",
      "  num_steps_trained: 124000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 31\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.83636363636364\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.3\n",
      "  vram_util_percent0: 0.09635416666666666\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 53.0\n",
      "  player_1: 50.66666666666667\n",
      "  player_2: 47.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 0.20666666666666672\n",
      "  player_1: -3.663333333333333\n",
      "  player_2: 3.456666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -45.33333333333333\n",
      "  player_1: -52.66666666666667\n",
      "  player_2: -56.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03409840021679242\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09056551531361277\n",
      "  mean_inference_ms: 0.6823400864097994\n",
      "  mean_raw_obs_processing_ms: 0.08214837800291414\n",
      "time_since_restore: 285.24706196784973\n",
      "time_this_iter_s: 8.560158252716064\n",
      "time_total_s: 285.24706196784973\n",
      "timers:\n",
      "  learn_throughput: 687.811\n",
      "  learn_time_ms: 5815.549\n",
      "  load_throughput: 6183097.221\n",
      "  load_time_ms: 0.647\n",
      "  sample_throughput: 411.491\n",
      "  sample_time_ms: 9720.736\n",
      "  update_time_ms: 33.099\n",
      "timestamp: 1643310091\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 124000\n",
      "training_iteration: 31\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 131970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-01-48\n",
      "done: false\n",
      "episode_len_mean: 516.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -2.1316282072803005e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 449\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0587548553943633\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0074602484931617676\n",
      "        policy_loss: -0.016426382611195247\n",
      "        total_loss: 27.42820642064015\n",
      "        vf_explained_var: -0.349762603243192\n",
      "        vf_loss: 27.443140697081883\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1275759601593018\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011157004054766731\n",
      "        policy_loss: -0.07641031003246705\n",
      "        total_loss: 16.038165178696314\n",
      "        vf_explained_var: -0.5324061089754104\n",
      "        vf_loss: 16.112344031333922\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0771213428179423\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007022929788630184\n",
      "        policy_loss: -0.04337845359928906\n",
      "        total_loss: 14.507618644038835\n",
      "        vf_explained_var: -0.41956421713034314\n",
      "        vf_loss: 14.550294989347458\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 131970\n",
      "  num_agent_steps_trained: 131970\n",
      "  num_steps_sampled: 132000\n",
      "  num_steps_trained: 132000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 33\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.60909090909091\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.3\n",
      "  vram_util_percent0: 0.09635416666666666\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 53.0\n",
      "  player_1: 50.66666666666667\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.16000000000000023\n",
      "  player_1: -3.74\n",
      "  player_2: 3.9\n",
      "policy_reward_min:\n",
      "  player_0: -45.33333333333333\n",
      "  player_1: -47.0\n",
      "  player_2: -56.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.034186587373846715\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09084222320750428\n",
      "  mean_inference_ms: 0.6837043772380601\n",
      "  mean_raw_obs_processing_ms: 0.0822029971881196\n",
      "time_since_restore: 302.31812047958374\n",
      "time_this_iter_s: 8.49977731704712\n",
      "time_total_s: 302.31812047958374\n",
      "timers:\n",
      "  learn_throughput: 700.458\n",
      "  learn_time_ms: 5710.546\n",
      "  load_throughput: 6353562.069\n",
      "  load_time_ms: 0.63\n",
      "  sample_throughput: 413.888\n",
      "  sample_time_ms: 9664.446\n",
      "  update_time_ms: 30.575\n",
      "timestamp: 1643310108\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 132000\n",
      "training_iteration: 33\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 139970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-02-05\n",
      "done: false\n",
      "episode_len_mean: 603.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -7.105427357601002e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 456\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0933173563083012\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009167160123997747\n",
      "        policy_loss: -0.03469226046154896\n",
      "        total_loss: 58.17489910513163\n",
      "        vf_explained_var: -0.4236759243408839\n",
      "        vf_loss: 58.20775792429845\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1142983094851175\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012279179117786043\n",
      "        policy_loss: -0.061245714245984954\n",
      "        total_loss: 71.07955513834953\n",
      "        vf_explained_var: -0.3351089332501094\n",
      "        vf_loss: 71.1383453989029\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0308868219455083\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008832398213768708\n",
      "        policy_loss: -0.016842869537261624\n",
      "        total_loss: 17.858922018210095\n",
      "        vf_explained_var: -0.4842849133412043\n",
      "        vf_loss: 17.87488160888354\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 139970\n",
      "  num_agent_steps_trained: 139970\n",
      "  num_steps_sampled: 140000\n",
      "  num_steps_trained: 140000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 35\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.645454545454545\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.3\n",
      "  vram_util_percent0: 0.09635416666666666\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 53.0\n",
      "  player_1: 50.66666666666667\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.41333333333333366\n",
      "  player_1: -3.2533333333333343\n",
      "  player_2: 3.666666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -45.33333333333333\n",
      "  player_1: -47.0\n",
      "  player_2: -56.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03423762217624061\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09099496363493037\n",
      "  mean_inference_ms: 0.6847847121636869\n",
      "  mean_raw_obs_processing_ms: 0.08215174854359544\n",
      "time_since_restore: 319.47017455101013\n",
      "time_this_iter_s: 8.584974527359009\n",
      "time_total_s: 319.47017455101013\n",
      "timers:\n",
      "  learn_throughput: 734.481\n",
      "  learn_time_ms: 5446.024\n",
      "  load_throughput: 6734592.164\n",
      "  load_time_ms: 0.594\n",
      "  sample_throughput: 437.845\n",
      "  sample_time_ms: 9135.653\n",
      "  update_time_ms: 29.242\n",
      "timestamp: 1643310125\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 140000\n",
      "training_iteration: 35\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 147968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-02-23\n",
      "done: false\n",
      "episode_len_mean: 648.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -7.815970093361102e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 6\n",
      "episodes_total: 465\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0052195946375528\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008157232275552815\n",
      "        policy_loss: -0.05766311240692933\n",
      "        total_loss: 8.217124935785929\n",
      "        vf_explained_var: -0.20059663116931914\n",
      "        vf_loss: 8.273156524995963\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1195039504766464\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014577441165180668\n",
      "        policy_loss: -0.049642440741881726\n",
      "        total_loss: 33.36185640911261\n",
      "        vf_explained_var: 0.06408856093883514\n",
      "        vf_loss: 33.40858342746893\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1318367024262745\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010056166257208284\n",
      "        policy_loss: -0.06164905940492948\n",
      "        total_loss: 32.79888521432876\n",
      "        vf_explained_var: -0.45415407915910083\n",
      "        vf_loss: 32.85952856640021\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 147968\n",
      "  num_agent_steps_trained: 147968\n",
      "  num_steps_sampled: 148000\n",
      "  num_steps_trained: 148000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 37\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.15\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.399999999999995\n",
      "  vram_util_percent0: 0.09635416666666667\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 53.0\n",
      "  player_1: 50.66666666666667\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.02666666666666707\n",
      "  player_1: -2.1366666666666667\n",
      "  player_2: 2.163333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -45.33333333333333\n",
      "  player_1: -47.0\n",
      "  player_2: -56.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03427580197323829\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09112168541260855\n",
      "  mean_inference_ms: 0.6860335598773384\n",
      "  mean_raw_obs_processing_ms: 0.08211998487962954\n",
      "time_since_restore: 337.4180669784546\n",
      "time_this_iter_s: 9.054187774658203\n",
      "time_total_s: 337.4180669784546\n",
      "timers:\n",
      "  learn_throughput: 749.541\n",
      "  learn_time_ms: 5336.596\n",
      "  load_throughput: 6907330.891\n",
      "  load_time_ms: 0.579\n",
      "  sample_throughput: 453.885\n",
      "  sample_time_ms: 8812.809\n",
      "  update_time_ms: 30.569\n",
      "timestamp: 1643310143\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 148000\n",
      "training_iteration: 37\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 155968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-02-41\n",
      "done: false\n",
      "episode_len_mean: 686.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -8.526512829121202e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 474\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0536556458473205\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008495847287826309\n",
      "        policy_loss: -0.0550607255846262\n",
      "        total_loss: 26.104628881216048\n",
      "        vf_explained_var: -0.7103567409515381\n",
      "        vf_loss: 26.15799010867874\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1214263836542766\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007463979713356821\n",
      "        policy_loss: -0.05636368863595029\n",
      "        total_loss: 38.74325182596842\n",
      "        vf_explained_var: -0.32172217468420666\n",
      "        vf_loss: 38.79812229990959\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1319408450524013\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012957270686207873\n",
      "        policy_loss: -0.04512644285336137\n",
      "        total_loss: 109.34575959856312\n",
      "        vf_explained_var: -0.09210515320301056\n",
      "        vf_loss: 109.38959105342627\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 155968\n",
      "  num_agent_steps_trained: 155968\n",
      "  num_steps_sampled: 156000\n",
      "  num_steps_trained: 156000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 39\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.85\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.424999999999997\n",
      "  vram_util_percent0: 0.09635416666666667\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 53.0\n",
      "  player_1: 50.66666666666667\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.17999999999999985\n",
      "  player_1: -0.4000000000000004\n",
      "  player_2: 0.21999999999999958\n",
      "policy_reward_min:\n",
      "  player_0: -45.33333333333333\n",
      "  player_1: -47.0\n",
      "  player_2: -56.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0343641728389232\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09138347068481835\n",
      "  mean_inference_ms: 0.6882807176610961\n",
      "  mean_raw_obs_processing_ms: 0.08216170706026618\n",
      "time_since_restore: 355.68107891082764\n",
      "time_this_iter_s: 9.11580777168274\n",
      "time_total_s: 355.68107891082764\n",
      "timers:\n",
      "  learn_throughput: 753.032\n",
      "  learn_time_ms: 5311.859\n",
      "  load_throughput: 6960633.946\n",
      "  load_time_ms: 0.575\n",
      "  sample_throughput: 450.604\n",
      "  sample_time_ms: 8876.973\n",
      "  update_time_ms: 31.731\n",
      "timestamp: 1643310161\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 156000\n",
      "training_iteration: 39\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 163968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-03-00\n",
      "done: false\n",
      "episode_len_mean: 718.28\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -3.552713678800501e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 480\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9039575018485387\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005452367688151299\n",
      "        policy_loss: -0.02002045648250108\n",
      "        total_loss: 57.218449769206345\n",
      "        vf_explained_var: -0.38667991201082863\n",
      "        vf_loss: 57.23738028178612\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0021748401721318\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008004042140916384\n",
      "        policy_loss: -0.04360021438449621\n",
      "        total_loss: 52.99266057332357\n",
      "        vf_explained_var: -0.09310840288798014\n",
      "        vf_loss: 53.0346599427859\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1322535302241643\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011174218463935782\n",
      "        policy_loss: -0.09356433748578032\n",
      "        total_loss: 1.3866625115275384\n",
      "        vf_explained_var: -0.49078190406163535\n",
      "        vf_loss: 1.479109429717064\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 163968\n",
      "  num_agent_steps_trained: 163968\n",
      "  num_steps_sampled: 164000\n",
      "  num_steps_trained: 164000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 41\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.25\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.483333333333334\n",
      "  vram_util_percent0: 0.09635416666666667\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 53.0\n",
      "  player_1: 50.66666666666667\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.5633333333333335\n",
      "  player_1: 0.9533333333333335\n",
      "  player_2: -1.5166666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -43.33333333333333\n",
      "  player_1: -47.0\n",
      "  player_2: -56.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03442520362609746\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.0915811290182863\n",
      "  mean_inference_ms: 0.6896357331666491\n",
      "  mean_raw_obs_processing_ms: 0.08219919699112842\n",
      "time_since_restore: 374.7747218608856\n",
      "time_this_iter_s: 9.353429079055786\n",
      "time_total_s: 374.7747218608856\n",
      "timers:\n",
      "  learn_throughput: 736.751\n",
      "  learn_time_ms: 5429.241\n",
      "  load_throughput: 7271047.933\n",
      "  load_time_ms: 0.55\n",
      "  sample_throughput: 447.775\n",
      "  sample_time_ms: 8933.056\n",
      "  update_time_ms: 32.916\n",
      "timestamp: 1643310180\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 164000\n",
      "training_iteration: 41\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 171968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-03-18\n",
      "done: false\n",
      "episode_len_mean: 736.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -7.105427357601002e-17\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 483\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9521557589372\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004036405043185596\n",
      "        policy_loss: -0.015184805691242219\n",
      "        total_loss: 27.019476824821904\n",
      "        vf_explained_var: -0.5906448630491893\n",
      "        vf_loss: 27.03385354101658\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9571611587206522\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011623846247190765\n",
      "        policy_loss: -0.05830254942178726\n",
      "        total_loss: 13.000169860720634\n",
      "        vf_explained_var: -0.25522888978322344\n",
      "        vf_loss: 13.056147600809734\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1076204031705856\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010325005327282876\n",
      "        policy_loss: -0.06212225321990748\n",
      "        total_loss: 5.090891840060552\n",
      "        vf_explained_var: -0.5708960109949112\n",
      "        vf_loss: 5.151981645425161\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 171968\n",
      "  num_agent_steps_trained: 171968\n",
      "  num_steps_sampled: 172000\n",
      "  num_steps_trained: 172000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 43\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.86363636363636\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.5\n",
      "  vram_util_percent0: 0.09635416666666666\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 53.0\n",
      "  player_1: 50.66666666666667\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.6966666666666668\n",
      "  player_1: 1.416666666666667\n",
      "  player_2: -2.1133333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -43.33333333333333\n",
      "  player_1: -47.0\n",
      "  player_2: -56.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03441680753848714\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09155340391653284\n",
      "  mean_inference_ms: 0.6895159562209419\n",
      "  mean_raw_obs_processing_ms: 0.0821187084192918\n",
      "time_since_restore: 392.7194080352783\n",
      "time_this_iter_s: 8.817463397979736\n",
      "time_total_s: 392.7194080352783\n",
      "timers:\n",
      "  learn_throughput: 729.162\n",
      "  learn_time_ms: 5485.753\n",
      "  load_throughput: 7299837.271\n",
      "  load_time_ms: 0.548\n",
      "  sample_throughput: 442.713\n",
      "  sample_time_ms: 9035.207\n",
      "  update_time_ms: 33.012\n",
      "timestamp: 1643310198\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 172000\n",
      "training_iteration: 43\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 179968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-03-36\n",
      "done: false\n",
      "episode_len_mean: 800.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 4.263256414560601e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 490\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9889026621977488\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009449465385162057\n",
      "        policy_loss: -0.04176759282747904\n",
      "        total_loss: 4.43995147275428\n",
      "        vf_explained_var: -0.5724991963307062\n",
      "        vf_loss: 4.480774124960105\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.867073727051417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009255596921672502\n",
      "        policy_loss: -0.05136720751256992\n",
      "        total_loss: 8.766840813954671\n",
      "        vf_explained_var: -0.19560675104459127\n",
      "        vf_loss: 8.816356801191965\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0960019139448802\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013322081597919654\n",
      "        policy_loss: -0.08511342513064543\n",
      "        total_loss: 1.0077304471222064\n",
      "        vf_explained_var: -0.36916662017504376\n",
      "        vf_loss: 1.0915116808811824\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 179968\n",
      "  num_agent_steps_trained: 179968\n",
      "  num_steps_sampled: 180000\n",
      "  num_steps_trained: 180000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 45\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.858333333333334\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.533333333333335\n",
      "  vram_util_percent0: 0.09635416666666667\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 53.0\n",
      "  player_1: 50.66666666666667\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.26\n",
      "  player_1: 3.13\n",
      "  player_2: -3.39\n",
      "policy_reward_min:\n",
      "  player_0: -43.33333333333333\n",
      "  player_1: -47.0\n",
      "  player_2: -56.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03445794780012944\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.0917038712446452\n",
      "  mean_inference_ms: 0.6903884809910376\n",
      "  mean_raw_obs_processing_ms: 0.08212686508061283\n",
      "time_since_restore: 410.476594209671\n",
      "time_this_iter_s: 8.955112218856812\n",
      "time_total_s: 410.476594209671\n",
      "timers:\n",
      "  learn_throughput: 722.351\n",
      "  learn_time_ms: 5537.477\n",
      "  load_throughput: 7287471.115\n",
      "  load_time_ms: 0.549\n",
      "  sample_throughput: 440.056\n",
      "  sample_time_ms: 9089.752\n",
      "  update_time_ms: 33.018\n",
      "timestamp: 1643310216\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 180000\n",
      "training_iteration: 45\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 187968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-03-54\n",
      "done: false\n",
      "episode_len_mean: 832.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 2.1316282072803005e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 498\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9165164919694265\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009218152503599412\n",
      "        policy_loss: -0.05040463480477532\n",
      "        total_loss: 8.502645537952581\n",
      "        vf_explained_var: -0.4720881821711858\n",
      "        vf_loss: 8.552128338416418\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8893478304147721\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008356297671020793\n",
      "        policy_loss: -0.023837109816571077\n",
      "        total_loss: 7.203538023134072\n",
      "        vf_explained_var: -0.5089867480595907\n",
      "        vf_loss: 7.225703840057055\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1366744834184646\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012327911658515708\n",
      "        policy_loss: -0.0400785992667079\n",
      "        total_loss: 1.8312573383251827\n",
      "        vf_explained_var: -0.6777739705642064\n",
      "        vf_loss: 1.8701031476755936\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 187968\n",
      "  num_agent_steps_trained: 187968\n",
      "  num_steps_sampled: 188000\n",
      "  num_steps_trained: 188000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 47\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.808333333333334\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.600000000000005\n",
      "  vram_util_percent0: 0.09635416666666667\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 53.0\n",
      "  player_1: 50.66666666666667\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.22\n",
      "  player_1: 4.249999999999999\n",
      "  player_2: -4.03\n",
      "policy_reward_min:\n",
      "  player_0: -43.33333333333333\n",
      "  player_1: -47.0\n",
      "  player_2: -56.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03441452462165329\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09160281885303792\n",
      "  mean_inference_ms: 0.6896808601238925\n",
      "  mean_raw_obs_processing_ms: 0.08191854713040002\n",
      "time_since_restore: 428.35474467277527\n",
      "time_this_iter_s: 9.179362297058105\n",
      "time_total_s: 428.35474467277527\n",
      "timers:\n",
      "  learn_throughput: 723.361\n",
      "  learn_time_ms: 5529.74\n",
      "  load_throughput: 7049546.62\n",
      "  load_time_ms: 0.567\n",
      "  sample_throughput: 439.554\n",
      "  sample_time_ms: 9100.139\n",
      "  update_time_ms: 31.786\n",
      "timestamp: 1643310234\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 188000\n",
      "training_iteration: 47\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 195968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-04-12\n",
      "done: false\n",
      "episode_len_mean: 852.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 2.1316282072803005e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 502\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9093112605810165\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02019403596726382\n",
      "        policy_loss: -0.027458713948726655\n",
      "        total_loss: 5.14602113087972\n",
      "        vf_explained_var: -0.4935397154092789\n",
      "        vf_loss: 5.171460413758953\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9721572558085124\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009571862995608929\n",
      "        policy_loss: -0.03539306241398056\n",
      "        total_loss: 5.689860086167852\n",
      "        vf_explained_var: -0.5253654583295186\n",
      "        vf_loss: 5.72333871970574\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0862141734361648\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01466772862773117\n",
      "        policy_loss: -0.023159419558942317\n",
      "        total_loss: 0.2759086511369484\n",
      "        vf_explained_var: -0.5619536141554514\n",
      "        vf_loss: 0.2976012953867515\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 195968\n",
      "  num_agent_steps_trained: 195968\n",
      "  num_steps_sampled: 196000\n",
      "  num_steps_trained: 196000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 49\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.700000000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.600000000000005\n",
      "  vram_util_percent0: 0.09635416666666667\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 53.0\n",
      "  player_1: 50.66666666666667\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.2900000000000001\n",
      "  player_1: 3.8199999999999994\n",
      "  player_2: -4.11\n",
      "policy_reward_min:\n",
      "  player_0: -43.33333333333333\n",
      "  player_1: -47.0\n",
      "  player_2: -56.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03438006463341842\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09151364859265644\n",
      "  mean_inference_ms: 0.6890638682780772\n",
      "  mean_raw_obs_processing_ms: 0.0817598393020481\n",
      "time_since_restore: 446.30721044540405\n",
      "time_this_iter_s: 8.97585153579712\n",
      "time_total_s: 446.30721044540405\n",
      "timers:\n",
      "  learn_throughput: 727.513\n",
      "  learn_time_ms: 5498.181\n",
      "  load_throughput: 7079591.527\n",
      "  load_time_ms: 0.565\n",
      "  sample_throughput: 439.581\n",
      "  sample_time_ms: 9099.576\n",
      "  update_time_ms: 30.699\n",
      "timestamp: 1643310252\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 196000\n",
      "training_iteration: 49\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 203968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-04-29\n",
      "done: false\n",
      "episode_len_mean: 856.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 4.263256414560601e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 505\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9493133982022604\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012412920392271189\n",
      "        policy_loss: -0.04093984812498093\n",
      "        total_loss: 8.377352656573057\n",
      "        vf_explained_var: -0.487348792552948\n",
      "        vf_loss: 8.416430499106646\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9699544159571329\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009311107216575087\n",
      "        policy_loss: -0.03990285218111239\n",
      "        total_loss: 9.372827328474571\n",
      "        vf_explained_var: -0.3465822813908259\n",
      "        vf_loss: 9.410867762168248\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1008855831623077\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018088491885003718\n",
      "        policy_loss: -0.04791279258827368\n",
      "        total_loss: 35.31893033546706\n",
      "        vf_explained_var: -0.10108723322550456\n",
      "        vf_loss: 35.36503416873515\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 203968\n",
      "  num_agent_steps_trained: 203968\n",
      "  num_steps_sampled: 204000\n",
      "  num_steps_trained: 204000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 51\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.400000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.681818181818176\n",
      "  vram_util_percent0: 0.09635416666666666\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 49.0\n",
      "  player_1: 50.66666666666667\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.8199999999999994\n",
      "  player_1: 4.6499999999999995\n",
      "  player_2: -3.8300000000000005\n",
      "policy_reward_min:\n",
      "  player_0: -43.33333333333333\n",
      "  player_1: -45.0\n",
      "  player_2: -56.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03436011970452279\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09146018136641426\n",
      "  mean_inference_ms: 0.6889277831884558\n",
      "  mean_raw_obs_processing_ms: 0.08166536704945768\n",
      "time_since_restore: 463.65276646614075\n",
      "time_this_iter_s: 8.515138149261475\n",
      "time_total_s: 463.65276646614075\n",
      "timers:\n",
      "  learn_throughput: 743.812\n",
      "  learn_time_ms: 5377.706\n",
      "  load_throughput: 7172202.462\n",
      "  load_time_ms: 0.558\n",
      "  sample_throughput: 445.856\n",
      "  sample_time_ms: 8971.51\n",
      "  update_time_ms: 29.571\n",
      "timestamp: 1643310269\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 204000\n",
      "training_iteration: 51\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 211968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-04-47\n",
      "done: false\n",
      "episode_len_mean: 887.12\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 0.0\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 508\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9395649820566178\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0119946249613425\n",
      "        policy_loss: -0.019323710451523462\n",
      "        total_loss: 1.2481834064299862\n",
      "        vf_explained_var: -0.423523825208346\n",
      "        vf_loss: 1.2657079231242339\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.92019058684508\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006156767014957343\n",
      "        policy_loss: -0.044775835300485296\n",
      "        total_loss: 34.23229332397381\n",
      "        vf_explained_var: -0.5566983654101689\n",
      "        vf_loss: 34.27583882043759\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0648791952927907\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00871972900651936\n",
      "        policy_loss: -0.03357198509077231\n",
      "        total_loss: 46.99796186263362\n",
      "        vf_explained_var: -0.6529324229558309\n",
      "        vf_loss: 47.03066222886245\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 211968\n",
      "  num_agent_steps_trained: 211968\n",
      "  num_steps_sampled: 212000\n",
      "  num_steps_trained: 212000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 53\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.836363636363636\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.7\n",
      "  vram_util_percent0: 0.09635416666666666\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 49.0\n",
      "  player_1: 50.333333333333336\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.7599999999999999\n",
      "  player_1: 5.13\n",
      "  player_2: -4.37\n",
      "policy_reward_min:\n",
      "  player_0: -43.33333333333333\n",
      "  player_1: -45.0\n",
      "  player_2: -56.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.034323822778254943\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09137267895022676\n",
      "  mean_inference_ms: 0.6879134439149562\n",
      "  mean_raw_obs_processing_ms: 0.08153358784541943\n",
      "time_since_restore: 481.5183973312378\n",
      "time_this_iter_s: 8.770804166793823\n",
      "time_total_s: 481.5183973312378\n",
      "timers:\n",
      "  learn_throughput: 746.771\n",
      "  learn_time_ms: 5356.397\n",
      "  load_throughput: 7178339.894\n",
      "  load_time_ms: 0.557\n",
      "  sample_throughput: 447.334\n",
      "  sample_time_ms: 8941.873\n",
      "  update_time_ms: 29.484\n",
      "timestamp: 1643310287\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 212000\n",
      "training_iteration: 53\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 219970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-05-06\n",
      "done: false\n",
      "episode_len_mean: 944.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 7.105427357601002e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 517\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9162396208445232\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010593437518085315\n",
      "        policy_loss: -0.055238177826007206\n",
      "        total_loss: 22.864873717327914\n",
      "        vf_explained_var: -0.41517083168029784\n",
      "        vf_loss: 22.91852272540331\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9078380457560221\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008685093802632764\n",
      "        policy_loss: -0.02475198679060365\n",
      "        total_loss: 9.57948791521291\n",
      "        vf_explained_var: -0.4477668287356695\n",
      "        vf_loss: 9.602502955148617\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1210230042537053\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006915000684239203\n",
      "        policy_loss: -0.05261268749833107\n",
      "        total_loss: 21.016286084552608\n",
      "        vf_explained_var: -0.24902758618195853\n",
      "        vf_loss: 21.068207206527394\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 219970\n",
      "  num_agent_steps_trained: 219970\n",
      "  num_steps_sampled: 220000\n",
      "  num_steps_trained: 220000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 55\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.33636363636364\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.7\n",
      "  vram_util_percent0: 0.09635416666666666\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 46.33333333333333\n",
      "  player_1: 50.333333333333336\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: -1.906666666666667\n",
      "  player_1: 5.783333333333332\n",
      "  player_2: -3.8766666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -43.33333333333333\n",
      "  player_1: -45.0\n",
      "  player_2: -56.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0342310766720005\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09118325096041838\n",
      "  mean_inference_ms: 0.6860939002581613\n",
      "  mean_raw_obs_processing_ms: 0.08121512062361369\n",
      "time_since_restore: 499.7844319343567\n",
      "time_this_iter_s: 8.664584159851074\n",
      "time_total_s: 499.7844319343567\n",
      "timers:\n",
      "  learn_throughput: 746.813\n",
      "  learn_time_ms: 5356.094\n",
      "  load_throughput: 7105076.017\n",
      "  load_time_ms: 0.563\n",
      "  sample_throughput: 444.398\n",
      "  sample_time_ms: 9000.939\n",
      "  update_time_ms: 30.853\n",
      "timestamp: 1643310306\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 220000\n",
      "training_iteration: 55\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 227968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-05-23\n",
      "done: false\n",
      "episode_len_mean: 988.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 7.815970093361102e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 522\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.867891431649526\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009367585636039925\n",
      "        policy_loss: -0.02851212685306867\n",
      "        total_loss: 6.448134947617849\n",
      "        vf_explained_var: -0.3792349499464035\n",
      "        vf_loss: 6.475241902967294\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9460925197601319\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007560382013458972\n",
      "        policy_loss: -0.030714224462086957\n",
      "        total_loss: 6.366472777053714\n",
      "        vf_explained_var: -0.438254821896553\n",
      "        vf_loss: 6.395674901803335\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0776838713884354\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011513012831625626\n",
      "        policy_loss: -0.07813003874383867\n",
      "        total_loss: 2.8651817038158574\n",
      "        vf_explained_var: -0.11646240433057149\n",
      "        vf_loss: 2.942160463680824\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 227968\n",
      "  num_agent_steps_trained: 227968\n",
      "  num_steps_sampled: 228000\n",
      "  num_steps_trained: 228000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 57\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.5\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.8\n",
      "  vram_util_percent0: 0.09635416666666666\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 46.33333333333333\n",
      "  player_1: 50.333333333333336\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: -2.8733333333333335\n",
      "  player_1: 5.536666666666667\n",
      "  player_2: -2.663333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -43.33333333333333\n",
      "  player_1: -45.0\n",
      "  player_2: -56.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03418193776376635\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09105619982604846\n",
      "  mean_inference_ms: 0.6854340127529261\n",
      "  mean_raw_obs_processing_ms: 0.08103282920622867\n",
      "time_since_restore: 517.1104049682617\n",
      "time_this_iter_s: 8.551268577575684\n",
      "time_total_s: 517.1104049682617\n",
      "timers:\n",
      "  learn_throughput: 752.423\n",
      "  learn_time_ms: 5316.16\n",
      "  load_throughput: 6797348.675\n",
      "  load_time_ms: 0.588\n",
      "  sample_throughput: 447.704\n",
      "  sample_time_ms: 8934.475\n",
      "  update_time_ms: 31.137\n",
      "timestamp: 1643310323\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 228000\n",
      "training_iteration: 57\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 235968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-05-41\n",
      "done: false\n",
      "episode_len_mean: 1083.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 8.526512829121202e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 527\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9174149819215138\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013512131119018705\n",
      "        policy_loss: -0.050494550294242796\n",
      "        total_loss: 44.609684299826625\n",
      "        vf_explained_var: -0.569914309779803\n",
      "        vf_loss: 44.65815235843261\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8426818233728409\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013202445974942141\n",
      "        policy_loss: -0.056706653827180466\n",
      "        total_loss: 17.07307719886303\n",
      "        vf_explained_var: -0.06744046032428741\n",
      "        vf_loss: 17.12714322179556\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0190730742613474\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011773020296780358\n",
      "        policy_loss: -0.04576337760935227\n",
      "        total_loss: 80.06692537059386\n",
      "        vf_explained_var: -0.27217038532098137\n",
      "        vf_loss: 80.11151166414221\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 235968\n",
      "  num_agent_steps_trained: 235968\n",
      "  num_steps_sampled: 236000\n",
      "  num_steps_trained: 236000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 59\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.090909090909093\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.836363636363636\n",
      "  vram_util_percent0: 0.09635416666666666\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 46.33333333333333\n",
      "  player_1: 50.333333333333336\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: -2.5699999999999994\n",
      "  player_1: 6.450000000000001\n",
      "  player_2: -3.88\n",
      "policy_reward_min:\n",
      "  player_0: -43.33333333333333\n",
      "  player_1: -45.0\n",
      "  player_2: -56.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.034150498728625876\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09099971493601262\n",
      "  mean_inference_ms: 0.6849155998269919\n",
      "  mean_raw_obs_processing_ms: 0.08090554061037299\n",
      "time_since_restore: 534.6094355583191\n",
      "time_this_iter_s: 8.532897472381592\n",
      "time_total_s: 534.6094355583191\n",
      "timers:\n",
      "  learn_throughput: 756.717\n",
      "  learn_time_ms: 5285.994\n",
      "  load_throughput: 6836681.337\n",
      "  load_time_ms: 0.585\n",
      "  sample_throughput: 450.025\n",
      "  sample_time_ms: 8888.398\n",
      "  update_time_ms: 30.834\n",
      "timestamp: 1643310341\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 236000\n",
      "training_iteration: 59\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 243968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-05-59\n",
      "done: false\n",
      "episode_len_mean: 1088.12\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 2.842170943040401e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 532\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9578161392609278\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016303175274179012\n",
      "        policy_loss: -0.06361569048836828\n",
      "        total_loss: 1.7018229375655451\n",
      "        vf_explained_var: -0.5468639250596364\n",
      "        vf_loss: 1.7629931490619977\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8632881712913513\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0064673805301401896\n",
      "        policy_loss: -0.030151441637426616\n",
      "        total_loss: 5.764236957790951\n",
      "        vf_explained_var: -0.41916662752628325\n",
      "        vf_loss: 5.793094849536816\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.041361870765686\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013885333413338738\n",
      "        policy_loss: -0.04505803291064998\n",
      "        total_loss: 3.366882487932841\n",
      "        vf_explained_var: -0.43191492060820263\n",
      "        vf_loss: 3.4105519610643387\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 243968\n",
      "  num_agent_steps_trained: 243968\n",
      "  num_steps_sampled: 244000\n",
      "  num_steps_trained: 244000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 61\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.87272727272727\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.9\n",
      "  vram_util_percent0: 0.09635416666666666\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 46.33333333333333\n",
      "  player_1: 50.333333333333336\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: -2.6366666666666667\n",
      "  player_1: 5.423333333333335\n",
      "  player_2: -2.786666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -43.33333333333333\n",
      "  player_1: -45.0\n",
      "  player_2: -56.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03411531820869105\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09094957804942397\n",
      "  mean_inference_ms: 0.6841475973163805\n",
      "  mean_raw_obs_processing_ms: 0.08076947316140594\n",
      "time_since_restore: 552.8909993171692\n",
      "time_this_iter_s: 9.145099878311157\n",
      "time_total_s: 552.8909993171692\n",
      "timers:\n",
      "  learn_throughput: 746.858\n",
      "  learn_time_ms: 5355.768\n",
      "  load_throughput: 6636031.959\n",
      "  load_time_ms: 0.603\n",
      "  sample_throughput: 449.212\n",
      "  sample_time_ms: 8904.482\n",
      "  update_time_ms: 31.634\n",
      "timestamp: 1643310359\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 244000\n",
      "training_iteration: 61\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 251968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-06-16\n",
      "done: false\n",
      "episode_len_mean: 1110.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 4.263256414560601e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 537\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9831426986058553\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013159909990463348\n",
      "        policy_loss: -0.04177796972449869\n",
      "        total_loss: 5.372994348804156\n",
      "        vf_explained_var: -0.7049851709604263\n",
      "        vf_loss: 5.412798298299313\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8244662823279699\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014773341722750664\n",
      "        policy_loss: -0.05580807988842328\n",
      "        total_loss: 4.0334899663925174\n",
      "        vf_explained_var: -0.4764914608001709\n",
      "        vf_loss: 4.086343378225962\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0101490259170531\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017480024360726626\n",
      "        policy_loss: -0.08994125599662463\n",
      "        total_loss: 0.8337205058926096\n",
      "        vf_explained_var: -0.8274190012613932\n",
      "        vf_loss: 0.9219137629866601\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 251968\n",
      "  num_agent_steps_trained: 251968\n",
      "  num_steps_sampled: 252000\n",
      "  num_steps_trained: 252000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 63\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.808333333333334\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 27.899999999999995\n",
      "  vram_util_percent0: 0.09635416666666667\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 46.33333333333333\n",
      "  player_1: 50.333333333333336\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: -2.93\n",
      "  player_1: 5.330000000000001\n",
      "  player_2: -2.400000000000001\n",
      "policy_reward_min:\n",
      "  player_0: -43.33333333333333\n",
      "  player_1: -45.0\n",
      "  player_2: -52.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0340720896352914\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09085523052826032\n",
      "  mean_inference_ms: 0.6833640257280823\n",
      "  mean_raw_obs_processing_ms: 0.08060926369544698\n",
      "time_since_restore: 570.1655070781708\n",
      "time_this_iter_s: 8.745062351226807\n",
      "time_total_s: 570.1655070781708\n",
      "timers:\n",
      "  learn_throughput: 751.429\n",
      "  learn_time_ms: 5323.193\n",
      "  load_throughput: 6640759.975\n",
      "  load_time_ms: 0.602\n",
      "  sample_throughput: 450.077\n",
      "  sample_time_ms: 8887.36\n",
      "  update_time_ms: 31.635\n",
      "timestamp: 1643310376\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 252000\n",
      "training_iteration: 63\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 259968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-06-33\n",
      "done: false\n",
      "episode_len_mean: 1219.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 7.105427357601002e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 543\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9507988739013672\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011525689212236708\n",
      "        policy_loss: -0.044080465411146484\n",
      "        total_loss: 12.557311507277191\n",
      "        vf_explained_var: 0.13571499764919281\n",
      "        vf_loss: 12.599663233657678\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8198532519737879\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007900559804470818\n",
      "        policy_loss: -0.058035681542629995\n",
      "        total_loss: 5.632374187707901\n",
      "        vf_explained_var: -0.038003116448720294\n",
      "        vf_loss: 5.688829674224059\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.00112295349439\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013524270347807033\n",
      "        policy_loss: -0.05977508979383856\n",
      "        total_loss: 3.710813717246056\n",
      "        vf_explained_var: -0.11442154427369436\n",
      "        vf_loss: 3.7692363311847052\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 259968\n",
      "  num_agent_steps_trained: 259968\n",
      "  num_steps_sampled: 260000\n",
      "  num_steps_trained: 260000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 65\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.66363636363636\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 28.0\n",
      "  vram_util_percent0: 0.09635416666666666\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 46.33333333333333\n",
      "  player_1: 50.333333333333336\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: -2.9466666666666668\n",
      "  player_1: 5.033333333333333\n",
      "  player_2: -2.086666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -43.33333333333333\n",
      "  player_1: -43.33333333333333\n",
      "  player_2: -52.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03402805038268494\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09077168020673966\n",
      "  mean_inference_ms: 0.6824585902689687\n",
      "  mean_raw_obs_processing_ms: 0.0804368615691865\n",
      "time_since_restore: 587.1342129707336\n",
      "time_this_iter_s: 8.471981763839722\n",
      "time_total_s: 587.1342129707336\n",
      "timers:\n",
      "  learn_throughput: 759.858\n",
      "  learn_time_ms: 5264.145\n",
      "  load_throughput: 6724335.07\n",
      "  load_time_ms: 0.595\n",
      "  sample_throughput: 456.389\n",
      "  sample_time_ms: 8764.461\n",
      "  update_time_ms: 30.486\n",
      "timestamp: 1643310393\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 260000\n",
      "training_iteration: 65\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 267968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-06-51\n",
      "done: false\n",
      "episode_len_mean: 1286.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 9.237055564881303e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 550\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9394133883714676\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00788693807158173\n",
      "        policy_loss: -0.035923227474559095\n",
      "        total_loss: 25.36618933478991\n",
      "        vf_explained_var: -0.3764786676565806\n",
      "        vf_loss: 25.400929333170254\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8527450031042099\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010405346261741215\n",
      "        policy_loss: -0.04782661287114024\n",
      "        total_loss: 26.450773373643557\n",
      "        vf_explained_var: -0.03001244843006134\n",
      "        vf_loss: 26.496519096891085\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9816928096612294\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008763125293371558\n",
      "        policy_loss: -0.04508900122096141\n",
      "        total_loss: 35.4539174224933\n",
      "        vf_explained_var: -0.29079676230748497\n",
      "        vf_loss: 35.49813068250815\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 267968\n",
      "  num_agent_steps_trained: 267968\n",
      "  num_steps_sampled: 268000\n",
      "  num_steps_trained: 268000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 67\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.325\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 28.0\n",
      "  vram_util_percent0: 0.09635416666666667\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 26.66666666666667\n",
      "  player_1: 50.333333333333336\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: -4.226666666666667\n",
      "  player_1: 7.293333333333334\n",
      "  player_2: -3.066666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -43.33333333333333\n",
      "  player_1: -43.33333333333333\n",
      "  player_2: -52.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.033973774514862165\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09065820527103777\n",
      "  mean_inference_ms: 0.6819483760899702\n",
      "  mean_raw_obs_processing_ms: 0.0801940965093734\n",
      "time_since_restore: 605.1341753005981\n",
      "time_this_iter_s: 8.841985702514648\n",
      "time_total_s: 605.1341753005981\n",
      "timers:\n",
      "  learn_throughput: 752.524\n",
      "  learn_time_ms: 5315.445\n",
      "  load_throughput: 7205160.404\n",
      "  load_time_ms: 0.555\n",
      "  sample_throughput: 453.632\n",
      "  sample_time_ms: 8817.722\n",
      "  update_time_ms: 30.182\n",
      "timestamp: 1643310411\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 268000\n",
      "training_iteration: 67\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 275968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-07-09\n",
      "done: false\n",
      "episode_len_mean: 1250.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 1.0658141036401502e-15\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 555\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9295662075281144\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008811137466637623\n",
      "        policy_loss: -0.036205801861360666\n",
      "        total_loss: 43.278664320906\n",
      "        vf_explained_var: -0.30481100340684253\n",
      "        vf_loss: 43.313548737565675\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8140119192997615\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008934443848479532\n",
      "        policy_loss: -0.060382201833029586\n",
      "        total_loss: 0.5005081674208244\n",
      "        vf_explained_var: -0.3614449340105057\n",
      "        vf_loss: 0.5591034740706284\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9480922679106395\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012343826145724452\n",
      "        policy_loss: -0.05098864920437336\n",
      "        total_loss: 37.79380634009838\n",
      "        vf_explained_var: -0.28459755500157674\n",
      "        vf_loss: 37.84356192310651\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 275968\n",
      "  num_agent_steps_trained: 275968\n",
      "  num_steps_sampled: 276000\n",
      "  num_steps_trained: 276000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 69\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.818181818181817\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 28.0\n",
      "  vram_util_percent0: 0.09635416666666666\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 26.66666666666667\n",
      "  player_1: 50.333333333333336\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: -4.52\n",
      "  player_1: 6.63\n",
      "  player_2: -2.11\n",
      "policy_reward_min:\n",
      "  player_0: -43.33333333333333\n",
      "  player_1: -43.33333333333333\n",
      "  player_2: -52.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03393101447457231\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09057256954604956\n",
      "  mean_inference_ms: 0.6814335929133694\n",
      "  mean_raw_obs_processing_ms: 0.0800907493606403\n",
      "time_since_restore: 622.7008430957794\n",
      "time_this_iter_s: 8.665082931518555\n",
      "time_total_s: 622.7008430957794\n",
      "timers:\n",
      "  learn_throughput: 749.367\n",
      "  learn_time_ms: 5337.839\n",
      "  load_throughput: 7141976.076\n",
      "  load_time_ms: 0.56\n",
      "  sample_throughput: 453.334\n",
      "  sample_time_ms: 8823.511\n",
      "  update_time_ms: 30.273\n",
      "timestamp: 1643310429\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 276000\n",
      "training_iteration: 69\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 283970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-07-27\n",
      "done: false\n",
      "episode_len_mean: 1351.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 7.815970093361102e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 564\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9350030924876531\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01202414673538442\n",
      "        policy_loss: -0.047757912029822665\n",
      "        total_loss: 24.552214883565902\n",
      "        vf_explained_var: -0.11908716241518656\n",
      "        vf_loss: 24.598169610699017\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8722114237149556\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011765155607608904\n",
      "        policy_loss: -0.050550139049688976\n",
      "        total_loss: 20.89228616476059\n",
      "        vf_explained_var: 0.13284887353579203\n",
      "        vf_loss: 20.940483304510515\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0146760672330857\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015103409018168652\n",
      "        policy_loss: -0.058393276346226534\n",
      "        total_loss: 4.521285026570161\n",
      "        vf_explained_var: -0.4157045880953471\n",
      "        vf_loss: 4.578167935212453\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 283970\n",
      "  num_agent_steps_trained: 283970\n",
      "  num_steps_sampled: 284000\n",
      "  num_steps_trained: 284000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 71\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.71818181818182\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 28.081818181818186\n",
      "  vram_util_percent0: 0.09635416666666666\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 26.66666666666667\n",
      "  player_1: 46.66666666666667\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: -6.183333333333334\n",
      "  player_1: 7.956666666666666\n",
      "  player_2: -1.7733333333333337\n",
      "policy_reward_min:\n",
      "  player_0: -56.33333333333333\n",
      "  player_1: -43.33333333333333\n",
      "  player_2: -49.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.033887463729014786\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09053577569143681\n",
      "  mean_inference_ms: 0.6807988793939923\n",
      "  mean_raw_obs_processing_ms: 0.07985513922458866\n",
      "time_since_restore: 640.2800908088684\n",
      "time_this_iter_s: 8.572126388549805\n",
      "time_total_s: 640.2800908088684\n",
      "timers:\n",
      "  learn_throughput: 757.389\n",
      "  learn_time_ms: 5281.302\n",
      "  load_throughput: 7036832.48\n",
      "  load_time_ms: 0.568\n",
      "  sample_throughput: 454.281\n",
      "  sample_time_ms: 8805.115\n",
      "  update_time_ms: 29.486\n",
      "timestamp: 1643310447\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 284000\n",
      "training_iteration: 71\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 291968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-07-44\n",
      "done: false\n",
      "episode_len_mean: 1368.12\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 1.3500311979441904e-15\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 569\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.932493820587794\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008415475326728483\n",
      "        policy_loss: -0.040175901893526315\n",
      "        total_loss: 16.384525505999726\n",
      "        vf_explained_var: -0.08258155008157095\n",
      "        vf_loss: 16.42343919237455\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.84433701445659\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013675780972898793\n",
      "        policy_loss: -0.07733908499280612\n",
      "        total_loss: 2.3879730792840324\n",
      "        vf_explained_var: 0.0626835834980011\n",
      "        vf_loss: 2.462577022711436\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0226228543122609\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008574913345673849\n",
      "        policy_loss: -0.025563007595095163\n",
      "        total_loss: 11.776035476773977\n",
      "        vf_explained_var: -0.16253499289353687\n",
      "        vf_loss: 11.800741025706133\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 291968\n",
      "  num_agent_steps_trained: 291968\n",
      "  num_steps_sampled: 292000\n",
      "  num_steps_trained: 292000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 73\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.58181818181818\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 28.1\n",
      "  vram_util_percent0: 0.09635416666666666\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 27.66666666666667\n",
      "  player_1: 46.66666666666667\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: -6.346666666666668\n",
      "  player_1: 7.623333333333332\n",
      "  player_2: -1.276666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -56.33333333333333\n",
      "  player_1: -43.33333333333333\n",
      "  player_2: -49.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03384395489256895\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09043898365410737\n",
      "  mean_inference_ms: 0.6802730661093868\n",
      "  mean_raw_obs_processing_ms: 0.07972444492449558\n",
      "time_since_restore: 657.7820317745209\n",
      "time_this_iter_s: 8.5490243434906\n",
      "time_total_s: 657.7820317745209\n",
      "timers:\n",
      "  learn_throughput: 751.675\n",
      "  learn_time_ms: 5321.447\n",
      "  load_throughput: 7016819.741\n",
      "  load_time_ms: 0.57\n",
      "  sample_throughput: 455.133\n",
      "  sample_time_ms: 8788.632\n",
      "  update_time_ms: 30.067\n",
      "timestamp: 1643310464\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 292000\n",
      "training_iteration: 73\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 299968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-08-03\n",
      "done: false\n",
      "episode_len_mean: 1389.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 8.526512829121202e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 576\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9923206361134848\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010148777337453794\n",
      "        policy_loss: -0.036800862016777194\n",
      "        total_loss: 17.119777353505295\n",
      "        vf_explained_var: -0.1669530479113261\n",
      "        vf_loss: 17.155055932899316\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8164704358577728\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008731242725164824\n",
      "        policy_loss: -0.07638923509046436\n",
      "        total_loss: 3.797188243667285\n",
      "        vf_explained_var: -0.029845860401789347\n",
      "        vf_loss: 3.871831206480662\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0169898549715677\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012711443187675624\n",
      "        policy_loss: -0.053300587460398674\n",
      "        total_loss: 9.524208810528119\n",
      "        vf_explained_var: -0.33293087720870973\n",
      "        vf_loss: 9.576238266328971\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 299968\n",
      "  num_agent_steps_trained: 299968\n",
      "  num_steps_sampled: 300000\n",
      "  num_steps_trained: 300000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 75\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.775\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 28.150000000000002\n",
      "  vram_util_percent0: 0.09635416666666667\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 27.66666666666667\n",
      "  player_1: 46.66666666666667\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: -6.989999999999998\n",
      "  player_1: 7.440000000000001\n",
      "  player_2: -0.44999999999999946\n",
      "policy_reward_min:\n",
      "  player_0: -56.33333333333333\n",
      "  player_1: -43.33333333333333\n",
      "  player_2: -49.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03377945795813882\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09030513607573669\n",
      "  mean_inference_ms: 0.6788579259520912\n",
      "  mean_raw_obs_processing_ms: 0.07954062986713316\n",
      "time_since_restore: 676.3617107868195\n",
      "time_this_iter_s: 9.178396224975586\n",
      "time_total_s: 676.3617107868195\n",
      "timers:\n",
      "  learn_throughput: 737.631\n",
      "  learn_time_ms: 5422.766\n",
      "  load_throughput: 6871683.801\n",
      "  load_time_ms: 0.582\n",
      "  sample_throughput: 448.017\n",
      "  sample_time_ms: 8928.232\n",
      "  update_time_ms: 30.834\n",
      "timestamp: 1643310483\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 300000\n",
      "training_iteration: 75\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 307968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-08-20\n",
      "done: false\n",
      "episode_len_mean: 1375.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 8.526512829121202e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 578\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9965675654013951\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012947468645053656\n",
      "        policy_loss: -0.049000303965682786\n",
      "        total_loss: 0.45139481739063436\n",
      "        vf_explained_var: -0.29177116374174755\n",
      "        vf_loss: 0.49845300262173015\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8313972053925196\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011167952722050055\n",
      "        policy_loss: -0.1019881642702967\n",
      "        total_loss: 1.7863244443635147\n",
      "        vf_explained_var: -0.17338214536507923\n",
      "        vf_loss: 1.8860790129005909\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9375564455986023\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01611636650046421\n",
      "        policy_loss: -0.06735407470570257\n",
      "        total_loss: 1.4448032179226478\n",
      "        vf_explained_var: -0.41177798211574557\n",
      "        vf_loss: 1.5105456543962161\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 307968\n",
      "  num_agent_steps_trained: 307968\n",
      "  num_steps_sampled: 308000\n",
      "  num_steps_trained: 308000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 77\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.227272727272727\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 28.2\n",
      "  vram_util_percent0: 0.09635416666666666\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 27.66666666666667\n",
      "  player_1: 53.66666666666667\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: -7.070000000000001\n",
      "  player_1: 8.080000000000002\n",
      "  player_2: -1.0099999999999993\n",
      "policy_reward_min:\n",
      "  player_0: -56.33333333333333\n",
      "  player_1: -43.33333333333333\n",
      "  player_2: -55.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03377781180283936\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09030519403575689\n",
      "  mean_inference_ms: 0.6790589641276047\n",
      "  mean_raw_obs_processing_ms: 0.07952045056199891\n",
      "time_since_restore: 693.376987695694\n",
      "time_this_iter_s: 8.558735609054565\n",
      "time_total_s: 693.376987695694\n",
      "timers:\n",
      "  learn_throughput: 745.32\n",
      "  learn_time_ms: 5366.822\n",
      "  load_throughput: 6901647.949\n",
      "  load_time_ms: 0.58\n",
      "  sample_throughput: 451.198\n",
      "  sample_time_ms: 8865.296\n",
      "  update_time_ms: 30.632\n",
      "timestamp: 1643310500\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 308000\n",
      "training_iteration: 77\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 315968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-08-38\n",
      "done: false\n",
      "episode_len_mean: 1389.84\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: 2.842170943040401e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 584\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9824588749806086\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008750165596672256\n",
      "        policy_loss: -0.044232402375588814\n",
      "        total_loss: 10.367341923117637\n",
      "        vf_explained_var: -0.5327392446994782\n",
      "        vf_loss: 10.410261734028657\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8064955159028371\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009452408102568673\n",
      "        policy_loss: -0.03955061500271161\n",
      "        total_loss: 55.21914517064889\n",
      "        vf_explained_var: -0.3324385831753413\n",
      "        vf_loss: 55.25680570969979\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.893314065138499\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006352872674227304\n",
      "        policy_loss: -0.03796967014670372\n",
      "        total_loss: 97.37708243245879\n",
      "        vf_explained_var: -0.16480717559655508\n",
      "        vf_loss: 97.41441870351632\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 315968\n",
      "  num_agent_steps_trained: 315968\n",
      "  num_steps_sampled: 316000\n",
      "  num_steps_trained: 316000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 79\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.16363636363636\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 28.2\n",
      "  vram_util_percent0: 0.09635416666666666\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 27.66666666666667\n",
      "  player_1: 59.33333333333333\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: -6.756666666666665\n",
      "  player_1: 7.993333333333335\n",
      "  player_2: -1.2366666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -56.33333333333333\n",
      "  player_1: -43.33333333333333\n",
      "  player_2: -69.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03373152103656008\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09020464067494176\n",
      "  mean_inference_ms: 0.678491397613108\n",
      "  mean_raw_obs_processing_ms: 0.07932067137217279\n",
      "time_since_restore: 711.2979700565338\n",
      "time_this_iter_s: 8.943951845169067\n",
      "time_total_s: 711.2979700565338\n",
      "timers:\n",
      "  learn_throughput: 740.672\n",
      "  learn_time_ms: 5400.498\n",
      "  load_throughput: 6949101.603\n",
      "  load_time_ms: 0.576\n",
      "  sample_throughput: 451.526\n",
      "  sample_time_ms: 8858.854\n",
      "  update_time_ms: 30.779\n",
      "timestamp: 1643310518\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 316000\n",
      "training_iteration: 79\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 323968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-08-57\n",
      "done: false\n",
      "episode_len_mean: 1391.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -2.842170943040401e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 592\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0002928431828817\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01705167463311227\n",
      "        policy_loss: -0.07146641251941523\n",
      "        total_loss: 0.7437213824192683\n",
      "        vf_explained_var: -0.7596305859088898\n",
      "        vf_loss: 0.8126300432284673\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7046454415718715\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007552637140809869\n",
      "        policy_loss: -0.03843181328071902\n",
      "        total_loss: 24.569799799919128\n",
      "        vf_explained_var: -0.14935686588287353\n",
      "        vf_loss: 24.60672054688136\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9153888197739919\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010163479782931972\n",
      "        policy_loss: -0.05767575861265262\n",
      "        total_loss: 18.64961749692758\n",
      "        vf_explained_var: -0.2718099610010783\n",
      "        vf_loss: 18.70627678513527\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 323968\n",
      "  num_agent_steps_trained: 323968\n",
      "  num_steps_sampled: 324000\n",
      "  num_steps_trained: 324000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 81\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.46923076923077\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 28.2923076923077\n",
      "  vram_util_percent0: 0.0982697315705128\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 27.66666666666667\n",
      "  player_1: 59.33333333333333\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: -6.983333333333333\n",
      "  player_1: 8.676666666666666\n",
      "  player_2: -1.6933333333333338\n",
      "policy_reward_min:\n",
      "  player_0: -56.33333333333333\n",
      "  player_1: -37.66666666666667\n",
      "  player_2: -69.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.033697619114582106\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09016434137521352\n",
      "  mean_inference_ms: 0.6782333863369387\n",
      "  mean_raw_obs_processing_ms: 0.07917244372690267\n",
      "time_since_restore: 730.41636967659\n",
      "time_this_iter_s: 10.03678011894226\n",
      "time_total_s: 730.41636967659\n",
      "timers:\n",
      "  learn_throughput: 726.761\n",
      "  learn_time_ms: 5503.874\n",
      "  load_throughput: 7116528.526\n",
      "  load_time_ms: 0.562\n",
      "  sample_throughput: 446.449\n",
      "  sample_time_ms: 8959.583\n",
      "  update_time_ms: 30.67\n",
      "timestamp: 1643310537\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 324000\n",
      "training_iteration: 81\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 331968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-09-17\n",
      "done: false\n",
      "episode_len_mean: 1412.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -4.263256414560601e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 596\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9762545563777287\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009554640341035944\n",
      "        policy_loss: -0.07727990771954259\n",
      "        total_loss: 4.281148167792708\n",
      "        vf_explained_var: -0.21616824547449748\n",
      "        vf_loss: 4.356994916150967\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7432222211360932\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014529064857245734\n",
      "        policy_loss: -0.014804746359586715\n",
      "        total_loss: 1.5950184029216568\n",
      "        vf_explained_var: -0.27787863731384277\n",
      "        vf_loss: 1.6069173362851144\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8723031812906266\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01379993856227884\n",
      "        policy_loss: -0.0757172408948342\n",
      "        total_loss: 2.9784004268050195\n",
      "        vf_explained_var: -0.2380973768234253\n",
      "        vf_loss: 3.0527376635869343\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 331968\n",
      "  num_agent_steps_trained: 331968\n",
      "  num_steps_sampled: 332000\n",
      "  num_steps_trained: 332000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 83\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.441666666666666\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 28.3\n",
      "  vram_util_percent0: 0.10026041666666669\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 27.66666666666667\n",
      "  player_1: 59.33333333333333\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: -7.72\n",
      "  player_1: 9.079999999999998\n",
      "  player_2: -1.3600000000000005\n",
      "policy_reward_min:\n",
      "  player_0: -56.33333333333333\n",
      "  player_1: -37.66666666666667\n",
      "  player_2: -69.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0336529353114992\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09004631877093626\n",
      "  mean_inference_ms: 0.677436372779765\n",
      "  mean_raw_obs_processing_ms: 0.07904233359504012\n",
      "time_since_restore: 750.2285025119781\n",
      "time_this_iter_s: 9.564212560653687\n",
      "time_total_s: 750.2285025119781\n",
      "timers:\n",
      "  learn_throughput: 703.145\n",
      "  learn_time_ms: 5688.728\n",
      "  load_throughput: 6994003.669\n",
      "  load_time_ms: 0.572\n",
      "  sample_throughput: 436.071\n",
      "  sample_time_ms: 9172.821\n",
      "  update_time_ms: 30.441\n",
      "timestamp: 1643310557\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 332000\n",
      "training_iteration: 83\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 339968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-09-34\n",
      "done: false\n",
      "episode_len_mean: 1422.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -2.842170943040401e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 600\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9143097511927287\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010221739357942473\n",
      "        policy_loss: -0.07867518114546934\n",
      "        total_loss: 1.5762974704088022\n",
      "        vf_explained_var: -0.26181074321269987\n",
      "        vf_loss: 1.65343938027819\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7702389657497406\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010675311592785874\n",
      "        policy_loss: -0.027242374836156765\n",
      "        total_loss: 3.0456594066267524\n",
      "        vf_explained_var: 0.09693934639294942\n",
      "        vf_loss: 3.070766724149386\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9074907555182775\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018911367984910613\n",
      "        policy_loss: -0.04970399659126997\n",
      "        total_loss: 2.0658165683348972\n",
      "        vf_explained_var: 0.2050223288933436\n",
      "        vf_loss: 2.1136294321219125\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 339968\n",
      "  num_agent_steps_trained: 339968\n",
      "  num_steps_sampled: 340000\n",
      "  num_steps_trained: 340000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 85\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.663636363636364\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 28.35454545454546\n",
      "  vram_util_percent0: 0.10009765625\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 27.66666666666667\n",
      "  player_1: 59.33333333333333\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: -7.7733333333333325\n",
      "  player_1: 9.946666666666665\n",
      "  player_2: -2.1733333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -56.33333333333333\n",
      "  player_1: -37.66666666666667\n",
      "  player_2: -69.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03362843339471684\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09000075890447543\n",
      "  mean_inference_ms: 0.6770051611627431\n",
      "  mean_raw_obs_processing_ms: 0.07892564526535119\n",
      "time_since_restore: 767.5877087116241\n",
      "time_this_iter_s: 8.444823503494263\n",
      "time_total_s: 767.5877087116241\n",
      "timers:\n",
      "  learn_throughput: 717.565\n",
      "  learn_time_ms: 5574.406\n",
      "  load_throughput: 6746236.68\n",
      "  load_time_ms: 0.593\n",
      "  sample_throughput: 435.626\n",
      "  sample_time_ms: 9182.179\n",
      "  update_time_ms: 29.488\n",
      "timestamp: 1643310574\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 340000\n",
      "training_iteration: 85\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 347968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-09-53\n",
      "done: false\n",
      "episode_len_mean: 1441.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -4.263256414560601e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 608\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9273572228352229\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011466007143462775\n",
      "        policy_loss: -0.05765323855448514\n",
      "        total_loss: 26.65078268309434\n",
      "        vf_explained_var: -0.31100070794423423\n",
      "        vf_loss: 26.706715896526973\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.691003310183684\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01079456450282254\n",
      "        policy_loss: -0.03639031238853931\n",
      "        total_loss: 53.80797482550144\n",
      "        vf_explained_var: -0.2922101165850957\n",
      "        vf_loss: 53.84220638240377\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9444375791152319\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013567446968894122\n",
      "        policy_loss: -0.056370463576167824\n",
      "        total_loss: 67.45365644594034\n",
      "        vf_explained_var: -0.22859187086423238\n",
      "        vf_loss: 67.50867043773333\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 347968\n",
      "  num_agent_steps_trained: 347968\n",
      "  num_steps_sampled: 348000\n",
      "  num_steps_trained: 348000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 87\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.045454545454547\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 28.4\n",
      "  vram_util_percent0: 0.09715317234848485\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 30.33333333333333\n",
      "  player_1: 59.33333333333333\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: -7.370000000000001\n",
      "  player_1: 9.979999999999999\n",
      "  player_2: -2.6100000000000008\n",
      "policy_reward_min:\n",
      "  player_0: -56.33333333333333\n",
      "  player_1: -37.66666666666667\n",
      "  player_2: -69.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.033619179227288125\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09002050024099331\n",
      "  mean_inference_ms: 0.677081102451841\n",
      "  mean_raw_obs_processing_ms: 0.0788425020860357\n",
      "time_since_restore: 785.7068400382996\n",
      "time_this_iter_s: 9.00543737411499\n",
      "time_total_s: 785.7068400382996\n",
      "timers:\n",
      "  learn_throughput: 706.842\n",
      "  learn_time_ms: 5658.974\n",
      "  load_throughput: 6679891.703\n",
      "  load_time_ms: 0.599\n",
      "  sample_throughput: 433.503\n",
      "  sample_time_ms: 9227.159\n",
      "  update_time_ms: 30.145\n",
      "timestamp: 1643310593\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 348000\n",
      "training_iteration: 87\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 355968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-10-14\n",
      "done: false\n",
      "episode_len_mean: 1459.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -3.1974423109204507e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 611\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8659494137763977\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011358614945080869\n",
      "        policy_loss: -0.0568460918078199\n",
      "        total_loss: 3.1755414834618567\n",
      "        vf_explained_var: -0.48371470232804614\n",
      "        vf_loss: 3.230683799435695\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7680902264515559\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013063926593919556\n",
      "        policy_loss: -0.051735002516458434\n",
      "        total_loss: 10.417594075202942\n",
      "        vf_explained_var: -0.5757190350691478\n",
      "        vf_loss: 10.466716358164946\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8314542166392008\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012807377974919897\n",
      "        policy_loss: -0.03297600137690703\n",
      "        total_loss: 22.110446964502334\n",
      "        vf_explained_var: -0.09686467945575714\n",
      "        vf_loss: 22.142142107387382\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 355968\n",
      "  num_agent_steps_trained: 355968\n",
      "  num_steps_sampled: 356000\n",
      "  num_steps_trained: 356000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 89\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.66923076923077\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 28.399999999999995\n",
      "  vram_util_percent0: 0.10603215144230768\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 30.33333333333333\n",
      "  player_1: 59.33333333333333\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: -7.400000000000001\n",
      "  player_1: 10.52\n",
      "  player_2: -3.1200000000000006\n",
      "policy_reward_min:\n",
      "  player_0: -56.33333333333333\n",
      "  player_1: -37.66666666666667\n",
      "  player_2: -69.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03360114938864503\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.08998160279595516\n",
      "  mean_inference_ms: 0.6766340251710781\n",
      "  mean_raw_obs_processing_ms: 0.07874746658698649\n",
      "time_since_restore: 806.6277794837952\n",
      "time_this_iter_s: 10.799355268478394\n",
      "time_total_s: 806.6277794837952\n",
      "timers:\n",
      "  learn_throughput: 681.505\n",
      "  learn_time_ms: 5869.362\n",
      "  load_throughput: 6637869.832\n",
      "  load_time_ms: 0.603\n",
      "  sample_throughput: 424.778\n",
      "  sample_time_ms: 9416.68\n",
      "  update_time_ms: 32.317\n",
      "timestamp: 1643310614\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 356000\n",
      "training_iteration: 89\n",
      "trial_id: default\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0127 20:10:14.461889269    1819 backup_poller.cc:134]       Run client channel backup poller: {\"created\":\"@1643310614.461853751\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1643310614.461847088\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 363968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-10-36\n",
      "done: false\n",
      "episode_len_mean: 1450.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -5.329070518200751e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 615\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8091247719526291\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01162297041501006\n",
      "        policy_loss: -0.07666398223799964\n",
      "        total_loss: 5.942584645152092\n",
      "        vf_explained_var: -0.1222428168853124\n",
      "        vf_loss: 6.0175051950414975\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6725424802303315\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007156486404986329\n",
      "        policy_loss: -0.05072031959891319\n",
      "        total_loss: 40.86258522172769\n",
      "        vf_explained_var: -0.6322859674692154\n",
      "        vf_loss: 40.9118743361036\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9193497643868128\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010718590974950004\n",
      "        policy_loss: -0.03950701614220937\n",
      "        total_loss: 83.35599838328858\n",
      "        vf_explained_var: -0.5489625616868337\n",
      "        vf_loss: 83.39443347275257\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 363968\n",
      "  num_agent_steps_trained: 363968\n",
      "  num_steps_sampled: 364000\n",
      "  num_steps_trained: 364000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 91\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.384615384615383\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 28.44615384615384\n",
      "  vram_util_percent0: 0.10986328125\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 30.33333333333333\n",
      "  player_1: 59.33333333333333\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: -7.17\n",
      "  player_1: 11.03\n",
      "  player_2: -3.8600000000000008\n",
      "policy_reward_min:\n",
      "  player_0: -56.33333333333333\n",
      "  player_1: -37.66666666666667\n",
      "  player_2: -69.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.033578876994324595\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.0899346923951651\n",
      "  mean_inference_ms: 0.6761235463994574\n",
      "  mean_raw_obs_processing_ms: 0.0786636685857765\n",
      "time_since_restore: 829.3174545764923\n",
      "time_this_iter_s: 10.703737020492554\n",
      "time_total_s: 829.3174545764923\n",
      "timers:\n",
      "  learn_throughput: 660.835\n",
      "  learn_time_ms: 6052.95\n",
      "  load_throughput: 6505066.108\n",
      "  load_time_ms: 0.615\n",
      "  sample_throughput: 405.253\n",
      "  sample_time_ms: 9870.387\n",
      "  update_time_ms: 34.047\n",
      "timestamp: 1643310636\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 364000\n",
      "training_iteration: 91\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 371968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-10-59\n",
      "done: false\n",
      "episode_len_mean: 1533.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -6.750155989720952e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 621\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9345073491334915\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01716406691517477\n",
      "        policy_loss: -0.032844247097770375\n",
      "        total_loss: 13.859072641531627\n",
      "        vf_explained_var: -0.6012335942188899\n",
      "        vf_loss: 13.889342392683028\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6875365618864695\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008205529155529802\n",
      "        policy_loss: -0.039234365795273334\n",
      "        total_loss: 8.740575985809167\n",
      "        vf_explained_var: -0.43579141398270926\n",
      "        vf_loss: 8.778169413109621\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8848759317398072\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015337166438732935\n",
      "        policy_loss: -0.0561933669494465\n",
      "        total_loss: 1.9023886690537135\n",
      "        vf_explained_var: -0.08958878556887309\n",
      "        vf_loss: 1.9570483309030533\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 371968\n",
      "  num_agent_steps_trained: 371968\n",
      "  num_steps_sampled: 372000\n",
      "  num_steps_trained: 372000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 93\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.15\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 28.5\n",
      "  vram_util_percent0: 0.1038411458333333\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 30.33333333333333\n",
      "  player_1: 59.33333333333333\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: -7.0966666666666685\n",
      "  player_1: 12.043333333333335\n",
      "  player_2: -4.946666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -56.33333333333333\n",
      "  player_1: -37.66666666666667\n",
      "  player_2: -69.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.033613809384896116\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09009530199414986\n",
      "  mean_inference_ms: 0.6768823974864422\n",
      "  mean_raw_obs_processing_ms: 0.0787413555127066\n",
      "time_since_restore: 852.0295159816742\n",
      "time_this_iter_s: 11.032413005828857\n",
      "time_total_s: 852.0295159816742\n",
      "timers:\n",
      "  learn_throughput: 649.656\n",
      "  learn_time_ms: 6157.102\n",
      "  load_throughput: 6372627.341\n",
      "  load_time_ms: 0.628\n",
      "  sample_throughput: 393.642\n",
      "  sample_time_ms: 10161.527\n",
      "  update_time_ms: 37.645\n",
      "timestamp: 1643310659\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 372000\n",
      "training_iteration: 93\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 379968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-11-20\n",
      "done: false\n",
      "episode_len_mean: 1498.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -6.750155989720952e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 625\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9456866214672724\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014542301297357578\n",
      "        policy_loss: -0.0492491300838689\n",
      "        total_loss: 2.5342076092834276\n",
      "        vf_explained_var: -0.4886027314265569\n",
      "        vf_loss: 2.581275396347046\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7090837794542313\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009534921854393835\n",
      "        policy_loss: -0.04639195037074387\n",
      "        total_loss: 11.571285130182902\n",
      "        vf_explained_var: -0.23798530419667563\n",
      "        vf_loss: 11.61576999505361\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9320912067095438\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011447466288699009\n",
      "        policy_loss: -0.04138602770011251\n",
      "        total_loss: 19.631806973914305\n",
      "        vf_explained_var: -0.352790044148763\n",
      "        vf_loss: 19.6720485407114\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 379968\n",
      "  num_agent_steps_trained: 379968\n",
      "  num_steps_sampled: 380000\n",
      "  num_steps_trained: 380000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 95\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.118181818181814\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 28.60000000000001\n",
      "  vram_util_percent0: 0.10384114583333331\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 30.33333333333333\n",
      "  player_1: 59.33333333333333\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: -7.186666666666667\n",
      "  player_1: 12.113333333333335\n",
      "  player_2: -4.926666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -56.33333333333333\n",
      "  player_1: -37.66666666666667\n",
      "  player_2: -69.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03360578569630876\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09008990115601238\n",
      "  mean_inference_ms: 0.6765863159657444\n",
      "  mean_raw_obs_processing_ms: 0.0786848542601111\n",
      "time_since_restore: 872.9692962169647\n",
      "time_this_iter_s: 9.008587121963501\n",
      "time_total_s: 872.9692962169647\n",
      "timers:\n",
      "  learn_throughput: 625.59\n",
      "  learn_time_ms: 6393.961\n",
      "  load_throughput: 6503048.955\n",
      "  load_time_ms: 0.615\n",
      "  sample_throughput: 378.477\n",
      "  sample_time_ms: 10568.668\n",
      "  update_time_ms: 38.404\n",
      "timestamp: 1643310680\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 380000\n",
      "training_iteration: 95\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 387968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-11-38\n",
      "done: false\n",
      "episode_len_mean: 1520.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -3.907985046680551e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 629\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9463227583964666\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01288389799495538\n",
      "        policy_loss: -0.057579984329640864\n",
      "        total_loss: 8.667069387471614\n",
      "        vf_explained_var: -0.48004184186458587\n",
      "        vf_loss: 8.722716569205126\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7518761722246806\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006290178246505699\n",
      "        policy_loss: -0.03556157359387726\n",
      "        total_loss: 15.83876648346583\n",
      "        vf_explained_var: -0.16946406046549478\n",
      "        vf_loss: 15.873069989681245\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9432120629151662\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0168371577580668\n",
      "        policy_loss: -0.07743138339060049\n",
      "        total_loss: 2.9037753853201864\n",
      "        vf_explained_var: -0.5426535195112229\n",
      "        vf_loss: 2.9795231159528095\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 387968\n",
      "  num_agent_steps_trained: 387968\n",
      "  num_steps_sampled: 388000\n",
      "  num_steps_trained: 388000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 97\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.745454545454546\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 28.60000000000001\n",
      "  vram_util_percent0: 0.10384114583333331\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 30.33333333333333\n",
      "  player_1: 59.33333333333333\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: -7.873333333333334\n",
      "  player_1: 12.936666666666667\n",
      "  player_2: -5.063333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -56.33333333333333\n",
      "  player_1: -32.0\n",
      "  player_2: -69.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03362148917472584\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09014898961333083\n",
      "  mean_inference_ms: 0.6770650563735284\n",
      "  mean_raw_obs_processing_ms: 0.07870737483807554\n",
      "time_since_restore: 890.322300195694\n",
      "time_this_iter_s: 8.676683187484741\n",
      "time_total_s: 890.322300195694\n",
      "timers:\n",
      "  learn_throughput: 631.929\n",
      "  learn_time_ms: 6329.829\n",
      "  load_throughput: 6335567.388\n",
      "  load_time_ms: 0.631\n",
      "  sample_throughput: 380.259\n",
      "  sample_time_ms: 10519.143\n",
      "  update_time_ms: 37.945\n",
      "timestamp: 1643310698\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 388000\n",
      "training_iteration: 97\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 395968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-27_20-11-54\n",
      "done: false\n",
      "episode_len_mean: 1560.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.4210854715202004e-14\n",
      "episode_reward_mean: -4.618527782440651e-16\n",
      "episode_reward_min: -1.4210854715202004e-14\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 633\n",
      "experiment_id: 8d908192f3c54600a1a791e1afb15a5a\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9082640411456426\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010336279245075275\n",
      "        policy_loss: -0.03808084378639857\n",
      "        total_loss: 52.1037711408486\n",
      "        vf_explained_var: -0.6324331565697988\n",
      "        vf_loss: 52.140301186939084\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7095785970489185\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007644481031954153\n",
      "        policy_loss: -0.047137208357453345\n",
      "        total_loss: 12.493878745834033\n",
      "        vf_explained_var: -0.3923095667362213\n",
      "        vf_loss: 12.539487020770709\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.09999999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8227723437547684\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014357631819839298\n",
      "        policy_loss: -0.0500892771834818\n",
      "        total_loss: 12.980172575910887\n",
      "        vf_explained_var: -0.043579565286636354\n",
      "        vf_loss: 13.02882609128952\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 395968\n",
      "  num_agent_steps_trained: 395968\n",
      "  num_steps_sampled: 396000\n",
      "  num_steps_trained: 396000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 99\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.345454545454547\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 28.60000000000001\n",
      "  vram_util_percent0: 0.10384114583333331\n",
      "pid: 780\n",
      "policy_reward_max:\n",
      "  player_0: 30.33333333333333\n",
      "  player_1: 59.33333333333333\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: -7.4766666666666675\n",
      "  player_1: 12.823333333333332\n",
      "  player_2: -5.346666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -56.33333333333333\n",
      "  player_1: -42.0\n",
      "  player_2: -69.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03362053884082422\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09014054778914041\n",
      "  mean_inference_ms: 0.6772908906924333\n",
      "  mean_raw_obs_processing_ms: 0.07863901879087565\n",
      "time_since_restore: 907.2603147029877\n",
      "time_this_iter_s: 8.434856176376343\n",
      "time_total_s: 907.2603147029877\n",
      "timers:\n",
      "  learn_throughput: 662.053\n",
      "  learn_time_ms: 6041.812\n",
      "  load_throughput: 6397901.079\n",
      "  load_time_ms: 0.625\n",
      "  sample_throughput: 388.735\n",
      "  sample_time_ms: 10289.79\n",
      "  update_time_ms: 36.082\n",
      "timestamp: 1643310714\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 396000\n",
      "training_iteration: 99\n",
      "trial_id: default\n",
      "\n",
      "training done, because max_iters 100 reached\n",
      "Finished training. Running manual test/inference loop.\n"
     ]
    }
   ],
   "source": [
    "# run manual training loop and print results after each iteration\n",
    "max_steps = 1e6\n",
    "max_iters = 100\n",
    "for iters in range(max_iters):\n",
    "    result = trainer.train()\n",
    "    if iters % 2 ==0:\n",
    "        print(pretty_print(result))\n",
    "    # stop training if the target train steps or reward are reached\n",
    "    if result[\"timesteps_total\"] >= max_steps:\n",
    "        print(f\"training done, because max_steps {max_steps} {result['timesteps_total']} reached\")\n",
    "        break\n",
    "else:\n",
    "    print(f\"training done, because max_iters {max_iters} reached\")\n",
    "# manual test loop\n",
    "print(\"Finished training. Running manual test/inference loop.\")\n",
    "# prepare environment with max 10 steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [x\t3\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t7\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\tx\tx]\n",
      " [x\t2\tx\tx]\n",
      " [0\tx\tx\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t-1\tx]\n",
      " [x\tx\tx\tx]\n",
      " [8\tx\tx\tx]  \n",
      "\n",
      "agent  player_0  action  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "  warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "/home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "  warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'int' and 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     logits \u001b[38;5;241m=\u001b[39m action_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_dist_inputs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent \u001b[39m\u001b[38;5;124m\"\u001b[39m, agent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m action \u001b[39m\u001b[38;5;124m\"\u001b[39m, logits\u001b[38;5;241m.\u001b[39margmax())\n\u001b[0;32m---> 13\u001b[0m     next_obs, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43magent\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# observations contain original observations and the action mask\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# print(f\"Obs: {obs}, Action: {action}, done: {done}\")\u001b[39;00m\n\u001b[1;32m     16\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/anaconda3/envs/skybo/lib/python3.9/site-packages/ray/rllib/env/wrappers/pettingzoo_env.py:107\u001b[0m, in \u001b[0;36mPettingZooEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_selection\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     obs_d \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    109\u001b[0m     rew_d \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/order_enforcing.py:62\u001b[0m, in \u001b[0;36mOrderEnforcingWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:94\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_selection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magent_selection\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrewards\n",
      "File \u001b[0;32m~/skybo_rl/rlskyjo/environment/simple_skyjo_env_v2.py:127\u001b[0m, in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_was_done_step(action)\n\u001b[1;32m    126\u001b[0m player_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name_to_player_id(current_agent)\n\u001b[0;32m--> 127\u001b[0m expected_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expected_agentname_and_action()[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_action \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m12\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m action \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m13\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: '<=' not supported between instances of 'int' and 'tuple'"
     ]
    }
   ],
   "source": [
    "env = PettingZooEnv(env_creator())\n",
    "obs = env.reset()\n",
    "done = {'__all__': False}\n",
    "# run one iteration until done\n",
    "while not done[\"__all__\"]:\n",
    "    agent = env.env.agent_selection\n",
    "    obs = env.env.observe(agent)\n",
    "    env.render()\n",
    "    \n",
    "    # get information on action\n",
    "    policy = trainer.get_policy(policy_id=agent)\n",
    "    action_off_policy, _, action_info = policy.compute_single_action(obs, policy_id=agent)\n",
    "    logits = action_info['action_dist_inputs']\n",
    "    action = logits.argmax()\n",
    "    print(\"agent \", agent, \" action \", logits.argmax())\n",
    "    next_obs, reward, done, _ = env.step({agent: action})\n",
    "    # observations contain original observations and the action mask\n",
    "    # print(f\"Obs: {obs}, Action: {action}, done: {done}\")\n",
    "env.render()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = {'observation': np.array([15, 15,  7,  4, 15, -2, 15,  6, 12, 15, 15,  4,  9,  6,  3,  0,  1,\n",
    "        0,  1,  0,  2,  1,  1,  2,  0,  1,  1,  0,  1, -2,  4], dtype=np.int8), 'action_mask': np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=np.int8)}\n",
    "action = trainer.compute_single_action(obs, policy_id=agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try creating with rllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_name = \"DQN\"\n",
    "\n",
    "\n",
    "config = deepcopy(get_agent_class(alg_name)._default_config)\n",
    "\n",
    "register_env(env_name,\n",
    "                lambda config: PettingZooEnv(env_creator()))\n",
    "\n",
    "ModelCatalog.register_custom_model(\n",
    "        \"pa_model\", TorchMaskedActions\n",
    "    )\n",
    "config[\"model\"] = {\n",
    "            \"custom_model\": \"pa_model\",\n",
    "        }\n",
    "config[\"framework\"] = \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_env = PettingZooEnv(env_creator())\n",
    "obs_space = sample_env.observation_space\n",
    "act_space = sample_env.action_space\n",
    "\n",
    "config[\"multiagent\"] = {\n",
    "    \"policies\": {\n",
    "        name: (None, obs_space, act_space, {}) for name in sample_env.agents\n",
    "    },\n",
    "    \"policy_mapping_fn\": lambda agent_id: agent_id\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"num_workers\"] = 0\n",
    "config['env'] = env_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
