{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We create an instance of a SimpleSkyjoEnv environment, call reset() to initialize the game and list the available agents (players):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rlskyjo.environment.simple_skyjo_env_v2' from '/home/michi/skybo_rl/rlskyjo/environment/simple_skyjo_env_v2.py'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from rlskyjo.environment import simple_skyjo_env_v2\n",
    "from importlib import reload\n",
    "reload(simple_skyjo_env_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "skyjo_env_cfg = {\"num_players\": 3}\n",
    "env_pettingzoo = simple_skyjo_env_v2.env(**skyjo_env_cfg)\n",
    "env_pettingzoo.reset()\n",
    "\n",
    "env_pettingzoo.agents, env_pettingzoo.agent_selection\n",
    "\n",
    "def sample_place():\n",
    "    return np.random.randint(0,23)\n",
    "        \n",
    "def sample_draw():\n",
    "    return np.random.randint(24,25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_2 draw\n",
      "player_2 place\n"
     ]
    }
   ],
   "source": [
    "print(env_pettingzoo.agent_selection, \"draw\")\n",
    "env_pettingzoo.step(sample_draw())\n",
    "print(env_pettingzoo.agent_selection, \"place\")\n",
    "env_pettingzoo.step(\n",
    "    sample_place()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_admissible_policy(observation, action_mask):\n",
    "    \"\"\"picks randomly an admissible action from the action mask\"\"\"\n",
    "    return np.random.choice(\n",
    "        np.arange(len(action_mask)),\n",
    "        p= action_mask/np.sum(action_mask)\n",
    "    )\n",
    "\n",
    "assert 1 not in [random_admissible_policy(None,[1,0,1]) for _ in range(1000)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training fct: {'observations': array([15, 15, 15,  8,  3, 15, 15, 15, 15, 15, 15, 15,  3, 10,  0,  0,  2,\n",
      "        0,  0,  1,  0,  1,  0,  0,  1,  1,  0,  1,  0, 11, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [9\tx\tx\tx]\n",
      " [x\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\tx\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\tx\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\t0]\n",
      " [x\tx\t3\tx]  \n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([15, 15, 15,  8,  3, 15, 15, 15, 15, 15, 15, 15,  3, 10,  0,  0,  2,\n",
      "        0,  0,  1,  0,  1,  0,  0,  1,  1,  0,  1,  0, 11, -2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: -2 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [9\tx\tx\tx]\n",
      " [x\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\tx\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\tx\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\t0]\n",
      " [x\tx\t3\tx]  \n",
      "\n",
      "sampled action player_1: 22\n",
      "training fct: {'observations': array([15, 15, 15,  1, 10, 15, 15, 15, 15, 15, 15, 15,  3,  9,  1,  0,  2,\n",
      "        0,  0,  1,  0,  1,  0,  0,  1,  1,  0,  1,  1, -2, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [9\tx\tx\tx]\n",
      " [x\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\tx\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\t0]\n",
      " [x\tx\t3\tx]  \n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([15, 15, 15,  1, 10, 15, 15, 15, 15, 15, 15, 15,  3,  9,  1,  0,  2,\n",
      "        0,  0,  1,  0,  1,  0,  0,  1,  1,  0,  1,  1, -2,  5], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 5 \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [9\tx\tx\tx]\n",
      " [x\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\tx\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\t0]\n",
      " [x\tx\t3\tx]  \n",
      "\n",
      "sampled action player_2: 7\n",
      "training fct: {'observations': array([15, 15, 15,  5,  9, 15, 15, 15, 15, 15, 15, 15,  8,  9,  1,  0,  2,\n",
      "        0,  0,  1,  0,  2,  0,  0,  1,  1,  0,  1,  1,  0, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [9\tx\tx\tx]\n",
      " [x\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\tx\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\t5]\n",
      " [x\tx\t3\tx]  \n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([15, 15, 15,  5,  9, 15, 15, 15, 15, 15, 15, 15,  8,  9,  1,  0,  1,\n",
      "        0,  0,  1,  0,  2,  0,  0,  1,  1,  0,  1,  1, -2,  0], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 0 \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [9\tx\tx\tx]\n",
      " [x\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\tx\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\t5]\n",
      " [x\tx\t3\tx]  \n",
      "\n",
      "sampled action player_0: 20\n",
      "training fct: {'observations': array([15, 15, 15,  8,  3, 15, 15, 15,  1, 15, 15, 15,  8,  9,  1,  0,  2,\n",
      "        1,  0,  1,  0,  2,  0,  0,  1,  1,  0,  1,  1,  0, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [9\tx\tx\tx]\n",
      " [1\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\tx\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\t5]\n",
      " [x\tx\t3\tx]  \n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([15, 15, 15,  8,  3, 15, 15, 15,  1, 15, 15, 15,  8,  9,  1,  0,  2,\n",
      "        1,  0,  1,  0,  2,  0,  0,  1,  1,  0,  1,  1,  0, -2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: -2 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [9\tx\tx\tx]\n",
      " [1\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\tx\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\t5]\n",
      " [x\tx\t3\tx]  \n",
      "\n",
      "sampled action player_1: 12\n",
      "training fct: {'observations': array([15, 15, 15,  1, 10, 15, 15, 15, -1, 15, 15, 15,  8,  8,  2,  0,  2,\n",
      "        1,  0,  1,  0,  2,  0,  0,  1,  2,  0,  1,  1, -2, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [9\tx\tx\tx]\n",
      " [1\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\tx\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\t5]\n",
      " [x\tx\t3\tx]  \n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([15, 15, 15,  1, 10, 15, 15, 15, -1, 15, 15, 15,  8,  8,  2,  0,  2,\n",
      "        1,  0,  1,  0,  2,  0,  0,  1,  2,  0,  1,  1, -2,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 3 \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [9\tx\tx\tx]\n",
      " [1\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\tx\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\t5]\n",
      " [x\tx\t3\tx]  \n",
      "\n",
      "sampled action player_2: 4\n",
      "training fct: {'observations': array([15, 15, 15,  5,  9, 15, 15, 15,  1, 15, 15, 15, 11,  8,  2,  0,  2,\n",
      "        1,  0,  2,  0,  2,  0,  0,  1,  2,  1,  1,  1, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [9\tx\tx\tx]\n",
      " [1\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\tx\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [x\tx\t3\tx]  \n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([15, 15, 15,  5,  9, 15, 15, 15,  1, 15, 15, 15, 11,  8,  2,  0,  2,\n",
      "        1,  0,  2,  0,  2,  0,  0,  1,  2,  0,  1,  1, -2, 10], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 10 \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [9\tx\tx\tx]\n",
      " [1\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\tx\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [x\tx\t3\tx]  \n",
      "\n",
      "sampled action player_0: 4\n",
      "training fct: {'observations': array([15, 15, 15,  8,  3, 15, 15, 15,  1, 15, 15, 15, 11,  8,  2,  0,  2,\n",
      "        1,  0,  2,  0,  2,  0,  0,  1,  2,  1,  1,  1,  9, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [10\tx\tx\tx]\n",
      " [1\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\tx\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [x\tx\t3\tx]  \n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([15, 15, 15,  8,  3, 15, 15, 15,  1, 15, 15, 15, 11,  8,  2,  0,  2,\n",
      "        1,  0,  2,  0,  2,  0,  0,  1,  1,  1,  1,  1, -2,  9], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 9 \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [10\tx\tx\tx]\n",
      " [1\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\tx\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [x\tx\t3\tx]  \n",
      "\n",
      "sampled action player_1: 14\n",
      "training fct: {'observations': array([15, 15, 15,  1,  3, 15, 15, 15, -1, 15, 15, 15, 11,  7,  2,  0,  2,\n",
      "        1,  0,  2,  0,  3,  0,  0,  1,  2,  1,  1,  1,  9, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [10\tx\tx\tx]\n",
      " [1\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [x\tx\t3\tx]  \n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([15, 15, 15,  1,  3, 15, 15, 15, -1, 15, 15, 15, 11,  7,  2,  0,  2,\n",
      "        1,  0,  2,  0,  3,  0,  0,  1,  2,  1,  1,  1,  9,  5], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 5 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [10\tx\tx\tx]\n",
      " [1\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [x\tx\t3\tx]  \n",
      "\n",
      "sampled action player_2: 20\n",
      "training fct: {'observations': array([15, 15, 15,  5, 10, 15, 15, 15,  1, 15, 15, 15, 10,  7,  2,  1,  2,\n",
      "        1,  0,  2,  0,  4,  0,  0,  1,  2,  1,  1,  1,  5, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [10\tx\tx\tx]\n",
      " [1\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\tx\t3\tx]  \n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([15, 15, 15,  5, 10, 15, 15, 15,  1, 15, 15, 15, 10,  7,  2,  1,  2,\n",
      "        1,  0,  2,  0,  4,  0,  0,  1,  2,  1,  1,  1,  5,  6], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 6 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [10\tx\tx\tx]\n",
      " [1\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\tx\t3\tx]  \n",
      "\n",
      "sampled action player_0: 4\n",
      "training fct: {'observations': array([15, 15, 15,  8,  3, 15, 15, 15,  1, 15, 15, 15, 10,  7,  2,  1,  2,\n",
      "        1,  0,  2,  0,  4,  1,  0,  1,  2,  1,  1,  1, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [6\tx\tx\tx]\n",
      " [1\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\tx\t3\tx]  \n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([15, 15, 15,  8,  3, 15, 15, 15,  1, 15, 15, 15, 10,  7,  2,  1,  2,\n",
      "        1,  0,  2,  0,  4,  1,  0,  1,  2,  1,  1,  1, 10, 11], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 11 \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [6\tx\tx\tx]\n",
      " [1\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\tx\tx]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\tx\t3\tx]  \n",
      "\n",
      "sampled action player_1: 19\n",
      "training fct: {'observations': array([15, 15, 15,  1,  3, 15, 15, 15, -1, 15, 15, 15, 10,  6,  2,  1,  2,\n",
      "        1,  0,  2,  0,  5,  1,  0,  1,  2,  1,  2,  1, 11, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [6\tx\tx\tx]\n",
      " [1\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\tx\t5]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\tx\t3\tx]  \n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([15, 15, 15,  1,  3, 15, 15, 15, -1, 15, 15, 15, 10,  6,  2,  1,  2,\n",
      "        1,  0,  2,  0,  5,  1,  0,  1,  2,  1,  2,  1, 11,  5], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 5 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [6\tx\tx\tx]\n",
      " [1\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\tx\t5]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\tx\t3\tx]  \n",
      "\n",
      "sampled action player_2: 11\n",
      "training fct: {'observations': array([15, 15, 15,  5,  6, 15, 15, 15,  1, 15, 15, 15, 12,  6,  2,  1,  2,\n",
      "        1,  0,  2,  0,  6,  1,  0,  1,  2,  1,  3,  1, 11, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [6\tx\tx\tx]\n",
      " [1\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\tx\t5]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\tx\t3\t5]  \n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([15, 15, 15,  5,  6, 15, 15, 15,  1, 15, 15, 15, 12,  6,  2,  1,  2,\n",
      "        1,  0,  2,  0,  6,  1,  0,  1,  2,  1,  2,  1, 11, 11], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 11 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [6\tx\tx\tx]\n",
      " [1\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\tx\t5]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\tx\t3\t5]  \n",
      "\n",
      "sampled action player_0: 8\n",
      "training fct: {'observations': array([15, 15, 15,  8,  3, 15, 15, 15,  1, 15, 15, 15, 15,  6,  2,  1,  2,\n",
      "        1,  0,  2,  0,  6,  1,  0,  1,  2,  1,  3,  1,  1, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [6\tx\tx\tx]\n",
      " [11\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\tx\t5]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\tx\t3\t5]  \n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([15, 15, 15,  8,  3, 15, 15, 15,  1, 15, 15, 15, 15,  6,  2,  1,  2,\n",
      "        0,  0,  2,  0,  6,  1,  0,  1,  2,  1,  3,  1, 11,  1], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 1 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [6\tx\tx\tx]\n",
      " [11\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\tx\t5]\n",
      " [x\t0\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\tx\t3\t5]  \n",
      "\n",
      "sampled action player_1: 10\n",
      "training fct: {'observations': array([15, 15, 15,  1,  3, 15, 15, 15, -1, 15, 15, 15, 15,  6,  2,  1,  2,\n",
      "        1,  0,  2,  0,  6,  1,  0,  1,  2,  1,  3,  1, 12, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [6\tx\tx\tx]\n",
      " [11\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\tx\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\tx\t3\t5]  \n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([15, 15, 15,  1,  3, 15, 15, 15, -1, 15, 15, 15, 15,  6,  2,  1,  2,\n",
      "        1,  0,  2,  0,  6,  1,  0,  1,  2,  1,  3,  0, 11, 12], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 12 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [6\tx\tx\tx]\n",
      " [11\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\tx\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\tx\t3\t5]  \n",
      "\n",
      "sampled action player_2: 9\n",
      "training fct: {'observations': array([15, 15, 15,  5,  6, 15, 15, 15, 11, 15, 15, 15, 22,  6,  2,  1,  2,\n",
      "        1,  0,  2,  0,  6,  2,  0,  1,  2,  1,  3,  1,  6, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [6\tx\tx\tx]\n",
      " [11\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\tx\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([15, 15, 15,  5,  6, 15, 15, 15, 11, 15, 15, 15, 22,  6,  2,  1,  2,\n",
      "        1,  0,  2,  0,  6,  1,  0,  1,  2,  1,  3,  1, 11,  6], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 6 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t5]\n",
      " [6\tx\tx\tx]\n",
      " [11\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\tx\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_0: 3\n",
      "training fct: {'observations': array([15, 15, 15,  8,  3, 15, 15, 15,  1, 15, 15, 15, 23,  6,  2,  1,  2,\n",
      "        1,  0,  2,  0,  6,  2,  0,  1,  2,  1,  3,  1,  5, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\tx\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([15, 15, 15,  8,  3, 15, 15, 15,  1, 15, 15, 15, 23,  6,  2,  1,  2,\n",
      "        1,  0,  2,  0,  6,  2,  0,  1,  2,  1,  3,  1,  5,  4], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 4 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\tx\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_1: 18\n",
      "training fct: {'observations': array([15, 15, 15,  1,  3, 15, 15, 15, -1, 15, 15, 15, 23,  5,  2,  1,  2,\n",
      "        1,  0,  2,  1,  6,  2,  0,  1,  2,  1,  3,  2,  4, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\t12\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([15, 15, 15,  1,  3, 15, 15, 15, -1, 15, 15, 15, 23,  5,  2,  1,  2,\n",
      "        1,  0,  2,  1,  6,  2,  0,  1,  2,  1,  3,  2,  4,  8], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 8 \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\t12\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\tx\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_2: 2\n",
      "training fct: {'observations': array([15, 15, 15,  6,  6, 15, 15, 15, 11, 15, 15, 15, 23,  5,  2,  1,  2,\n",
      "        1,  0,  2,  1,  6,  2,  0,  2,  3,  1,  3,  2,  9, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\t12\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t8\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([15, 15, 15,  6,  6, 15, 15, 15, 11, 15, 15, 15, 23,  5,  2,  1,  2,\n",
      "        1,  0,  2,  1,  6,  2,  0,  2,  2,  1,  3,  2,  4,  9], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 9 \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\tx\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\t12\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t8\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_0: 21\n",
      "training fct: {'observations': array([15, 15, 15,  8,  3, 15, 15, 15,  1,  0, 15, 15, 31,  5,  2,  1,  2,\n",
      "        1,  0,  2,  1,  6,  2,  0,  3,  3,  1,  3,  2,  9, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\t12\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t8\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([15, 15, 15,  8,  3, 15, 15, 15,  1,  0, 15, 15, 31,  5,  2,  1,  2,\n",
      "        1,  0,  2,  1,  6,  2,  0,  3,  2,  1,  3,  2,  4,  9], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 9 \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\t12\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t8\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_1: 6\n",
      "training fct: {'observations': array([15, 15, 15,  1,  3, 15, 15, 15, -1, 12, 15, 15, 31,  5,  2,  1,  2,\n",
      "        1,  0,  2,  1,  6,  2,  0,  3,  3,  1,  3,  2, 12, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\t9\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t8\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([15, 15, 15,  1,  3, 15, 15, 15, -1, 12, 15, 15, 31,  5,  2,  1,  2,\n",
      "        1,  0,  2,  1,  6,  2,  0,  3,  3,  1,  3,  2, 12,  1], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 1 \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\t9\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t8\tx]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_2: 15\n",
      "training fct: {'observations': array([15, 15, 15,  6,  6, 15, 15, 15, 11,  8, 15, 15, 31,  4,  2,  1,  2,\n",
      "        3,  0,  2,  1,  6,  2,  0,  3,  3,  1,  3,  2,  1, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\t9\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([15, 15, 15,  6,  6, 15, 15, 15, 11,  8, 15, 15, 31,  4,  2,  1,  2,\n",
      "        3,  0,  2,  1,  6,  2,  0,  3,  3,  1,  3,  2,  1, 10], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 10 \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\tx\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\t9\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_0: 22\n",
      "training fct: {'observations': array([15, 15, 15,  8,  3, 15, 15, 15,  1,  0,  1, 15, 33,  4,  2,  1,  2,\n",
      "        3,  1,  2,  1,  6,  2,  0,  3,  3,  2,  3,  2, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\t9\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([15, 15, 15,  8,  3, 15, 15, 15,  1,  0,  1, 15, 33,  4,  2,  1,  2,\n",
      "        3,  1,  2,  1,  6,  2,  0,  3,  3,  1,  3,  2,  1, 10], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 10 \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [x\tx\t9\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_1: 16\n",
      "training fct: {'observations': array([15, 15, 15,  1,  3, 15, 15, 15, -1, 12,  3, 15, 33,  4,  2,  1,  2,\n",
      "        3,  1,  3,  1,  6,  2,  0,  3,  3,  2,  3,  2, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([15, 15, 15,  1,  3, 15, 15, 15, -1, 12,  3, 15, 33,  4,  2,  1,  2,\n",
      "        3,  1,  3,  1,  6,  2,  0,  3,  3,  2,  3,  2, 10, -1], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: -1 \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [x\tx\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_2: 0\n",
      "training fct: {'observations': array([15, 15, 15,  6,  6, 15, 15, 15, 11,  8,  2, 15, 33,  3,  2,  2,  2,\n",
      "        3,  1,  3,  1,  6,  3,  0,  3,  3,  2,  3,  2,  6, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\tx\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([15, 15, 15,  6,  6, 15, 15, 15, 11,  8,  2, 15, 33,  3,  2,  2,  2,\n",
      "        3,  1,  3,  1,  6,  3,  0,  3,  3,  2,  3,  2,  6,  6], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 6 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\tx\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_0: 12\n",
      "training fct: {'observations': array([ 9, 15, 15,  8,  3, 15, 15, 15,  1,  0,  1, 15, 35,  3,  2,  2,  2,\n",
      "        3,  1,  3,  1,  6,  4,  0,  3,  4,  2,  3,  2,  6, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [9\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\tx\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([ 9, 15, 15,  8,  3, 15, 15, 15,  1,  0,  1, 15, 35,  3,  2,  2,  2,\n",
      "        3,  1,  3,  1,  6,  3,  0,  3,  4,  2,  3,  2,  6,  6], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 6 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [9\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [x\t0\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\tx\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_1: 10\n",
      "training fct: {'observations': array([-1, 15, 15,  1,  3, 15, 15, 15, -1, 12,  3, 15, 35,  3,  2,  2,  2,\n",
      "        3,  1,  3,  1,  6,  4,  0,  3,  4,  2,  3,  2,  1, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [9\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [x\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\tx\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([-1, 15, 15,  1,  3, 15, 15, 15, -1, 12,  3, 15, 35,  3,  2,  2,  2,\n",
      "        2,  1,  3,  1,  6,  4,  0,  3,  4,  2,  3,  2,  6,  1], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 1 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [9\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [x\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\tx\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_2: 3\n",
      "training fct: {'observations': array([ 9, 15, 15,  6,  6, 15, 15, 15, 11,  8,  2, 15, 35,  3,  2,  2,  2,\n",
      "        3,  1,  3,  1,  6,  4,  0,  3,  4,  2,  3,  2,  1, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [9\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [x\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\tx\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([ 9, 15, 15,  6,  6, 15, 15, 15, 11,  8,  2, 15, 35,  3,  2,  2,  2,\n",
      "        2,  1,  3,  1,  6,  4,  0,  3,  4,  2,  3,  2,  6,  1], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 1 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [9\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [x\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\tx\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_0: 0\n",
      "training fct: {'observations': array([ 9, 15, 15,  8,  3, 15, 15, 15,  1,  0,  6, 15, 34,  3,  2,  2,  2,\n",
      "        3,  1,  3,  1,  6,  4,  0,  3,  4,  2,  3,  2,  9, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [1\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [x\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\tx\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([ 9, 15, 15,  8,  3, 15, 15, 15,  1,  0,  6, 15, 34,  3,  2,  2,  2,\n",
      "        3,  1,  3,  1,  6,  4,  0,  3,  3,  2,  3,  2,  6,  9], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 9 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [1\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [x\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\tx\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_1: 20\n",
      "training fct: {'observations': array([-1, 15, 15,  1,  3, 15, 15, 15, -1, 12,  3, 15, 34,  3,  2,  2,  2,\n",
      "        4,  1,  3,  1,  6,  4,  0,  3,  4,  2,  3,  2,  9, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [1\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\tx\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([-1, 15, 15,  1,  3, 15, 15, 15, -1, 12,  3, 15, 34,  3,  2,  2,  2,\n",
      "        4,  1,  3,  1,  6,  4,  0,  3,  3,  2,  3,  2,  6,  9], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 9 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [1\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\tx\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_2: 13\n",
      "training fct: {'observations': array([ 1, 15, 15,  6,  6, 15, 15, 15, 11,  8,  2, 15, 34,  2,  2,  2,  2,\n",
      "        4,  1,  3,  1,  6,  4,  0,  3,  5,  2,  3,  2,  9, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [1\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([ 1, 15, 15,  6,  6, 15, 15, 15, 11,  8,  2, 15, 34,  2,  2,  2,  2,\n",
      "        4,  1,  3,  1,  6,  4,  0,  3,  5,  2,  3,  2,  9,  7], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 7 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [1\tx\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_0: 1\n",
      "training fct: {'observations': array([ 9,  4, 15,  8,  3, 15, 15, 15,  1,  0,  6, 15, 41,  2,  2,  2,  2,\n",
      "        4,  2,  3,  1,  6,  4,  1,  3,  5,  2,  3,  2,  2, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [1\t7\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([ 9,  4, 15,  8,  3, 15, 15, 15,  1,  0,  6, 15, 41,  2,  2,  2,  2,\n",
      "        4,  1,  3,  1,  6,  4,  1,  3,  5,  2,  3,  2,  9,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 2 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [1\t7\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t0\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_1: 9\n",
      "training fct: {'observations': array([-1,  9, 15,  1,  3, 15, 15, 15, -1, 12,  3, 15, 41,  2,  2,  2,  2,\n",
      "        4,  2,  3,  1,  6,  4,  1,  3,  5,  2,  3,  2,  0, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [1\t7\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([-1,  9, 15,  1,  3, 15, 15, 15, -1, 12,  3, 15, 41,  2,  2,  2,  1,\n",
      "        4,  2,  3,  1,  6,  4,  1,  3,  5,  2,  3,  2,  9,  0], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 0 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [1\t7\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\tx\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_2: 6\n",
      "training fct: {'observations': array([ 1,  7, 15,  6,  6, 15, 15, 15, 11,  8,  2, 15, 41,  1,  2,  2,  2,\n",
      "        5,  2,  3,  1,  6,  4,  1,  3,  5,  2,  3,  2,  1, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [1\t7\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t0\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([ 1,  7, 15,  6,  6, 15, 15, 15, 11,  8,  2, 15, 41,  1,  2,  2,  2,\n",
      "        4,  2,  3,  1,  6,  4,  1,  3,  5,  2,  3,  2,  9,  1], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 1 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [1\t7\tx\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t0\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_0: 2\n",
      "training fct: {'observations': array([ 9,  4,  5,  8,  3, 15, 15, 15,  1,  2,  6, 15, 42,  1,  2,  2,  3,\n",
      "        5,  2,  3,  1,  6,  4,  1,  3,  5,  2,  3,  2,  0, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t1\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t0\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([ 9,  4,  5,  8,  3, 15, 15, 15,  1,  2,  6, 15, 42,  1,  2,  2,  2,\n",
      "        5,  2,  3,  1,  6,  4,  1,  3,  5,  2,  3,  2,  9,  0], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 0 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t1\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\tx\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t0\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([-1,  9,  8,  1,  3, 15, 15, 15, -1, 12,  3, 15, 42,  1,  2,  2,  3,\n",
      "        5,  2,  3,  2,  6,  4,  1,  3,  5,  2,  3,  2,  0, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t1\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t0\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([-1,  9,  8,  1,  3, 15, 15, 15, -1, 12,  3, 15, 42,  1,  2,  2,  3,\n",
      "        5,  2,  3,  2,  6,  4,  1,  3,  5,  2,  3,  2,  0,  9], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 9 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t1\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t0\t5]\n",
      " [-1\t12\t3\t5]  \n",
      "\n",
      "sampled action player_2: 9\n",
      "training fct: {'observations': array([ 1,  7,  1,  6,  6, 15, 15, 15, 11,  8,  2, 15, 41,  1,  2,  2,  3,\n",
      "        5,  2,  3,  2,  6,  4,  1,  3,  6,  2,  3,  2, 12, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t1\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t0\t5]\n",
      " [-1\t9\t3\t5]  \n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([ 1,  7,  1,  6,  6, 15, 15, 15, 11,  8,  2, 15, 41,  1,  2,  2,  3,\n",
      "        5,  2,  3,  2,  6,  4,  1,  3,  6,  2,  3,  1,  0, 12], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 12 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t1\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\tx]  \n",
      "======= Player 1 ========== \n",
      " [9\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t0\t5]\n",
      " [-1\t9\t3\t5]  \n",
      "\n",
      "sampled action player_0: 23\n",
      "training fct: {'observations': array([ 9,  4,  5,  8,  3, 15, 15, 15,  1,  2,  6,  4, 41,  1,  2,  2,  4,\n",
      "        5,  2,  3,  2,  6,  4,  1,  3,  6,  2,  3,  2, 12, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t1\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [9\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t0\t5]\n",
      " [-1\t9\t3\t5]  \n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([ 9,  4,  5,  8,  3, 15, 15, 15,  1,  2,  6,  4, 41,  1,  2,  2,  4,\n",
      "        5,  2,  3,  2,  6,  4,  1,  3,  6,  2,  3,  1,  0, 12], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 12 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t1\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [9\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t6\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t0\t5]\n",
      " [-1\t9\t3\t5]  \n",
      "\n",
      "sampled action player_1: 10\n",
      "training fct: {'observations': array([-1,  9,  8,  1,  3, 15, 15, 15, -1,  9,  3,  5, 41,  1,  2,  2,  4,\n",
      "        5,  2,  3,  2,  6,  4,  1,  3,  6,  2,  3,  2,  6, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t1\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [9\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t0\t5]\n",
      " [-1\t9\t3\t5]  \n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([-1,  9,  8,  1,  3, 15, 15, 15, -1,  9,  3,  5, 41,  1,  2,  2,  4,\n",
      "        5,  2,  3,  2,  6,  3,  1,  3,  6,  2,  3,  2,  0,  6], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 6 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t1\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [9\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t0\t5]\n",
      " [-1\t9\t3\t5]  \n",
      "\n",
      "sampled action player_2: 6\n",
      "training fct: {'observations': array([ 1,  7,  1,  6,  6, 15, 15, 15, 11,  8,  2,  0, 42,  1,  2,  2,  4,\n",
      "        5,  2,  3,  2,  6,  4,  1,  3,  6,  2,  3,  2,  0, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t1\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [9\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t9\t3\t5]  \n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([ 1,  7,  1,  6,  6, 15, 15, 15, 11,  8,  2,  0, 42,  1,  2,  2,  4,\n",
      "        5,  2,  3,  2,  6,  4,  1,  3,  6,  2,  3,  2,  0,  9], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 9 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t1\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [9\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t9\t3\t5]  \n",
      "\n",
      "sampled action player_0: 2\n",
      "training fct: {'observations': array([ 9,  4,  5,  8,  3, 15, 15, 15,  1,  2, 12,  4, 47,  1,  2,  2,  4,\n",
      "        5,  2,  3,  2,  6,  4,  1,  3,  7,  2,  3,  2,  1, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [9\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t9\t3\t5]  \n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([ 9,  4,  5,  8,  3, 15, 15, 15,  1,  2, 12,  4, 47,  1,  2,  2,  4,\n",
      "        4,  2,  3,  2,  6,  4,  1,  3,  7,  2,  3,  2,  0,  1], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 1 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [9\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t9\t3\t5]  \n",
      "\n",
      "sampled action player_1: 0\n",
      "training fct: {'observations': array([-1,  9,  8,  1,  3, 15, 15, 15, -1,  9,  3,  5, 47,  1,  2,  2,  4,\n",
      "        5,  2,  3,  2,  6,  4,  1,  3,  7,  2,  3,  2,  9, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t9\t3\t5]  \n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([-1,  9,  8,  1,  3, 15, 15, 15, -1,  9,  3,  5, 47,  1,  2,  2,  4,\n",
      "        5,  2,  3,  2,  6,  4,  1,  3,  6,  2,  3,  2,  0,  9], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 9 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t9\t3\t5]  \n",
      "\n",
      "sampled action player_2: 1\n",
      "training fct: {'observations': array([ 1,  7,  9,  6,  6, 15, 15, 15, 11,  8,  2,  0, 47,  1,  2,  2,  4,\n",
      "        5,  2,  3,  2,  6,  4,  1,  3,  7,  2,  3,  2,  9, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t9\t3\t5]  \n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([ 1,  7,  9,  6,  6, 15, 15, 15, 11,  8,  2,  0, 47,  1,  2,  2,  4,\n",
      "        5,  2,  3,  2,  6,  4,  1,  3,  7,  2,  3,  2,  9,  8], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 8 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t9\t3\t5]  \n",
      "\n",
      "sampled action player_0: 9\n",
      "training fct: {'observations': array([ 1,  4,  5,  8,  3, 15, 15, 15,  1,  2, 12,  4, 47,  1,  2,  2,  4,\n",
      "        5,  2,  3,  2,  6,  4,  1,  4,  7,  2,  3,  2,  8, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t9\t3\t5]  \n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([ 1,  4,  5,  8,  3, 15, 15, 15,  1,  2, 12,  4, 47,  1,  2,  2,  4,\n",
      "        5,  2,  3,  2,  6,  4,  1,  4,  7,  2,  3,  2,  8, -1], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: -1 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t9\t3\t5]  \n",
      "\n",
      "sampled action player_1: 8\n",
      "training fct: {'observations': array([-1,  9,  8,  1,  3, 15, 15, 15, -1,  9,  3,  5, 47,  1,  2,  3,  4,\n",
      "        5,  2,  3,  2,  6,  4,  1,  4,  7,  2,  3,  2,  1, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [-1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t9\t3\t5]  \n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([-1,  9,  8,  1,  3, 15, 15, 15, -1,  9,  3,  5, 47,  1,  2,  3,  4,\n",
      "        5,  2,  3,  2,  6,  4,  1,  4,  7,  2,  3,  2,  1,  6], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 6 \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [-1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t9\t3\t5]  \n",
      "\n",
      "sampled action player_2: 9\n",
      "training fct: {'observations': array([ 1,  7,  9,  6,  6, 15, 15, 15, 11,  8,  2,  0, 44,  1,  2,  3,  4,\n",
      "        5,  2,  3,  2,  6,  5,  1,  4,  7,  2,  3,  2,  9, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [-1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t5]  \n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([ 1,  7,  9,  6,  6, 15, 15, 15, 11,  8,  2,  0, 44,  1,  2,  3,  4,\n",
      "        5,  2,  3,  2,  6,  5,  1,  4,  6,  2,  3,  2,  1,  9], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 9 \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\tx\tx\tx]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [-1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t5]  \n",
      "\n",
      "sampled action player_0: 7\n",
      "training fct: {'observations': array([ 1,  4,  5,  8,  3, 15, 15,  5, -1,  2, 12,  4, 44,  1,  2,  3,  4,\n",
      "        5,  2,  3,  2,  6,  5,  1,  5,  7,  2,  3,  2,  8, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\tx\tx\t9]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [-1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t5]  \n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([ 1,  4,  5,  8,  3, 15, 15,  5, -1,  2, 12,  4, 44,  1,  2,  3,  4,\n",
      "        5,  2,  3,  2,  6,  5,  1,  4,  7,  2,  3,  2,  1,  8], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 8 \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\tx\tx\t9]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t4\t5\t8]\n",
      " [3\tx\t9\t5]\n",
      " [-1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t5]  \n",
      "\n",
      "sampled action player_1: 4\n",
      "training fct: {'observations': array([-1,  9,  8,  1,  3, 15, 15,  5, -1,  6,  3,  5, 44,  1,  2,  3,  4,\n",
      "        5,  2,  3,  2,  6,  5,  1,  5,  7,  2,  3,  2,  3, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 3 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\tx\tx\t9]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t4\t5\t8]\n",
      " [8\tx\t9\t5]\n",
      " [-1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t5]  \n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([-1,  9,  8,  1,  3, 15, 15,  5, -1,  6,  3,  5, 44,  1,  2,  3,  4,\n",
      "        5,  2,  3,  2,  6,  5,  1,  5,  7,  2,  3,  2,  3, -2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: -2 \n",
      "discard pile top: 3 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\tx\tx\t9]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t4\t5\t8]\n",
      " [8\tx\t9\t5]\n",
      " [-1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t8\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t5]  \n",
      "\n",
      "sampled action player_2: 2\n",
      "training fct: {'observations': array([ 1,  7,  9,  6,  6, 15, 15,  9, 11,  8,  2,  0, 34,  1,  3,  3,  4,\n",
      "        5,  2,  3,  2,  6,  5,  1,  5,  7,  2,  3,  2,  8, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\tx\tx\t9]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t4\t5\t8]\n",
      " [8\tx\t9\t5]\n",
      " [-1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t-2\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t5]  \n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([ 1,  7,  9,  6,  6, 15, 15,  9, 11,  8,  2,  0, 34,  1,  3,  3,  4,\n",
      "        5,  2,  3,  2,  6,  5,  1,  5,  7,  2,  3,  2,  8, 11], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 11 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\tx\tx\t9]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t4\t5\t8]\n",
      " [8\tx\t9\t5]\n",
      " [-1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t-2\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t5]  \n",
      "\n",
      "sampled action player_0: 5\n",
      "training fct: {'observations': array([ 1,  4,  5,  8,  8,  0, 15,  5, -1,  2, 12,  4, 34,  1,  3,  3,  4,\n",
      "        5,  2,  3,  2,  7,  5,  1,  5,  7,  2,  4,  2,  5, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\t11\tx\t9]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t4\t5\t8]\n",
      " [8\tx\t9\t5]\n",
      " [-1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t-2\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t5]  \n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([ 1,  4,  5,  8,  8,  0, 15,  5, -1,  2, 12,  4, 34,  1,  3,  3,  4,\n",
      "        5,  2,  3,  2,  6,  5,  1,  5,  7,  2,  4,  2,  8,  5], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 5 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\t11\tx\t9]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t4\t5\t8]\n",
      " [8\tx\t9\t5]\n",
      " [-1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t-2\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t5]  \n",
      "\n",
      "sampled action player_1: 1\n",
      "training fct: {'observations': array([-1,  9, -2,  1,  3, -1, 15,  5, -1,  6,  3,  5, 34,  1,  3,  3,  4,\n",
      "        5,  2,  3,  2,  7,  5,  1,  5,  7,  2,  4,  2,  4, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\t11\tx\t9]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\tx\t9\t5]\n",
      " [-1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t-2\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t5]  \n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([-1,  9, -2,  1,  3, -1, 15,  5, -1,  6,  3,  5, 34,  1,  3,  3,  4,\n",
      "        5,  2,  3,  2,  7,  5,  1,  5,  7,  2,  4,  2,  4,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 3 \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\t11\tx\t9]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\tx\t9\t5]\n",
      " [-1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [-1\t9\t-2\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t5]  \n",
      "\n",
      "sampled action player_2: 0\n",
      "training fct: {'observations': array([ 1,  7,  9,  6,  6, 11, 15,  9, 11,  8,  2,  0, 38,  1,  3,  3,  4,\n",
      "        5,  2,  4,  2,  7,  5,  1,  5,  7,  2,  4,  2, -1, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\t11\tx\t9]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\tx\t9\t5]\n",
      " [-1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t-2\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t5]  \n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([ 1,  7,  9,  6,  6, 11, 15,  9, 11,  8,  2,  0, 38,  1,  3,  3,  4,\n",
      "        5,  2,  4,  2,  7,  5,  1,  5,  7,  2,  4,  2, -1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 3 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\t11\tx\t9]\n",
      " [11\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\tx\t9\t5]\n",
      " [-1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t-2\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t5]  \n",
      "\n",
      "sampled action player_0: 8\n",
      "training fct: {'observations': array([ 1,  5,  5,  8,  8,  0, 15,  5, -1,  2, 12,  4, 38,  1,  3,  3,  4,\n",
      "        5,  2,  5,  2,  7,  5,  1,  5,  7,  2,  4,  2, 11, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\t11\tx\t9]\n",
      " [3\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\tx\t9\t5]\n",
      " [-1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t-2\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t5]  \n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([ 1,  5,  5,  8,  8,  0, 15,  5, -1,  2, 12,  4, 38,  1,  3,  3,  4,\n",
      "        5,  2,  5,  2,  7,  5,  1,  5,  7,  2,  4,  2, 11,  9], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 9 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\t11\tx\t9]\n",
      " [3\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\tx\t9\t5]\n",
      " [-1\t2\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t-2\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t5]  \n",
      "\n",
      "sampled action player_1: 9\n",
      "training fct: {'observations': array([ 3,  9, -2,  1,  3, -1, 15,  5, -1,  6,  3,  5, 38,  1,  3,  3,  4,\n",
      "        5,  2,  5,  2,  7,  5,  1,  5,  8,  2,  4,  2,  2, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\t11\tx\t9]\n",
      " [3\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\tx\t9\t5]\n",
      " [-1\t9\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t-2\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t5]  \n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([ 3,  9, -2,  1,  3, -1, 15,  5, -1,  6,  3,  5, 38,  1,  3,  3,  4,\n",
      "        5,  1,  5,  2,  7,  5,  1,  5,  8,  2,  4,  2, 11,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 2 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\t11\tx\t9]\n",
      " [3\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\tx\t9\t5]\n",
      " [-1\t9\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t-2\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t5]  \n",
      "\n",
      "sampled action player_2: 11\n",
      "training fct: {'observations': array([ 1,  7,  9,  6,  6, 11, 15,  9,  3,  8,  2,  0, 35,  1,  3,  3,  4,\n",
      "        5,  2,  5,  2,  7,  5,  1,  5,  8,  2,  4,  2,  5, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\t11\tx\t9]\n",
      " [3\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\tx\t9\t5]\n",
      " [-1\t9\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t-2\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t2]  \n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([ 1,  7,  9,  6,  6, 11, 15,  9,  3,  8,  2,  0, 35,  1,  3,  3,  4,\n",
      "        5,  2,  5,  2,  6,  5,  1,  5,  8,  2,  4,  2, 11,  5], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 5 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [6\t11\tx\t9]\n",
      " [3\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\tx\t9\t5]\n",
      " [-1\t9\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t-2\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t2]  \n",
      "\n",
      "sampled action player_0: 4\n",
      "training fct: {'observations': array([ 1,  5,  5,  8,  8,  0, 15,  5, -1,  9, 12,  4, 35,  1,  3,  3,  4,\n",
      "        5,  2,  5,  2,  7,  5,  1,  5,  8,  2,  4,  2,  6, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [5\t11\tx\t9]\n",
      " [3\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\tx\t9\t5]\n",
      " [-1\t9\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t-2\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t2]  \n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([ 1,  5,  5,  8,  8,  0, 15,  5, -1,  9, 12,  4, 35,  1,  3,  3,  4,\n",
      "        5,  2,  5,  2,  7,  5,  1,  5,  8,  2,  4,  2,  6,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 3 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [5\t11\tx\t9]\n",
      " [3\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\tx\t9\t5]\n",
      " [-1\t9\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t-2\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t2]  \n",
      "\n",
      "sampled action player_1: 5\n",
      "training fct: {'observations': array([ 3,  9, -2,  1,  3, -1, 15,  5, -1,  6,  3,  2, 35,  1,  3,  3,  5,\n",
      "        5,  2,  6,  2,  7,  5,  1,  5,  8,  2,  4,  2,  0, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [5\t11\tx\t9]\n",
      " [3\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\t3\t9\t5]\n",
      " [-1\t9\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t-2\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t2]  \n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([ 3,  9, -2,  1,  3, -1, 15,  5, -1,  6,  3,  2, 35,  1,  3,  3,  5,\n",
      "        5,  2,  6,  2,  7,  5,  1,  5,  8,  2,  4,  2,  0, 10], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 10 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [5\t11\tx\t9]\n",
      " [3\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\t3\t9\t5]\n",
      " [-1\t9\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t-2\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t2]  \n",
      "\n",
      "sampled action player_2: 2\n",
      "training fct: {'observations': array([ 1,  7,  9,  6,  5, 11, 15,  9,  3,  8,  2,  0, 47,  1,  3,  3,  5,\n",
      "        5,  2,  6,  2,  7,  5,  1,  5,  8,  3,  4,  2, -2, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [5\t11\tx\t9]\n",
      " [3\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\t3\t9\t5]\n",
      " [-1\t9\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t10\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t2]  \n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([ 1,  7,  9,  6,  5, 11, 15,  9,  3,  8,  2,  0, 47,  1,  3,  3,  5,\n",
      "        5,  2,  6,  2,  7,  5,  1,  5,  8,  3,  4,  2, -2,  7], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 7 \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [5\t11\tx\t9]\n",
      " [3\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\t3\t9\t5]\n",
      " [-1\t9\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t10\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t2]  \n",
      "\n",
      "sampled action player_0: 6\n",
      "training fct: {'observations': array([ 1,  5,  5,  8,  8,  3,  9,  5, -1,  9, 12,  4, 47,  0,  3,  3,  5,\n",
      "        5,  2,  6,  2,  7,  5,  2,  5,  8,  4,  4,  2, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [5\t11\t7\t9]\n",
      " [3\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\t3\t9\t5]\n",
      " [-1\t9\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t10\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t2]  \n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([ 1,  5,  5,  8,  8,  3,  9,  5, -1,  9, 12,  4, 47,  0,  3,  3,  5,\n",
      "        5,  2,  6,  2,  7,  5,  2,  5,  8,  4,  4,  2, 10, -2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: -2 \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [5\t11\t7\t9]\n",
      " [3\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\t3\t9\t5]\n",
      " [-1\t9\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t10\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t2]  \n",
      "\n",
      "sampled action player_1: 6\n",
      "training fct: {'observations': array([ 3,  9, 10,  1,  3, -1,  6,  5, -1,  6,  3,  2, 47,  0,  4,  3,  5,\n",
      "        5,  2,  6,  2,  7,  5,  2,  5,  8,  4,  4,  2,  9, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [5\t11\t7\t9]\n",
      " [3\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\t3\t-2\t5]\n",
      " [-1\t9\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t10\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t2]  \n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([ 3,  9, 10,  1,  3, -1,  6,  5, -1,  6,  3,  2, 47,  0,  4,  3,  5,\n",
      "        5,  2,  6,  2,  7,  5,  2,  5,  8,  4,  4,  2,  9, 10], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 10 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [5\t11\t7\t9]\n",
      " [3\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\t3\t-2\t5]\n",
      " [-1\t9\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t10\t1]\n",
      " [3\tx\t6\t5]\n",
      " [-1\t6\t3\t2]  \n",
      "\n",
      "sampled action player_2: 6\n",
      "training fct: {'observations': array([ 1,  7,  9,  6,  5, 11,  7,  9,  3,  8,  2,  0, 51,  0,  4,  3,  5,\n",
      "        5,  2,  6,  2,  7,  5,  2,  5,  8,  5,  4,  2,  6, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [5\t11\t7\t9]\n",
      " [3\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\t3\t-2\t5]\n",
      " [-1\t9\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t10\t1]\n",
      " [3\tx\t10\t5]\n",
      " [-1\t6\t3\t2]  \n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([ 1,  7,  9,  6,  5, 11,  7,  9,  3,  8,  2,  0, 51,  0,  4,  3,  5,\n",
      "        5,  2,  6,  2,  7,  5,  2,  5,  8,  5,  4,  2,  6, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} -17.733333333333327 True {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 6 \n",
      "======= GAME DONE ======== \n",
      "Results: {0: 81.6, 1: 57.0, 2: 50.0} \n",
      "======= Player 0 ========== \n",
      " [1\t7\t9\t6]\n",
      " [5\t11\t7\t9]\n",
      " [3\t8\t2\t0]  \n",
      "======= Player 1 ========== \n",
      " [1\t5\t5\t8]\n",
      " [8\t3\t-2\t5]\n",
      " [-1\t9\t12\tx]  \n",
      "======= Player 2 ========== \n",
      " [3\t9\t10\t1]\n",
      " [3\tx\t10\t5]\n",
      " [-1\t6\t3\t2]  \n",
      "\n",
      "done -17.733333333333327\n",
      "{'player_1': 6.866666666666667, 'player_2': 13.866666666666667}\n"
     ]
    }
   ],
   "source": [
    "i_episode = 1  \n",
    "while i_episode <= 1:\n",
    "    i_episode  += 1\n",
    "    env_pettingzoo.reset()\n",
    "    for agent in env_pettingzoo.agent_iter(max_iter=600):        \n",
    "        # get observation (state) for current agent:\n",
    "        obs, reward, done, info = env_pettingzoo.last()\n",
    "        \n",
    "        print(\"training fct:\", obs, reward, done, info)\n",
    "        # perform q-learning with update_Q_value()\n",
    "        # your code here\n",
    "        \n",
    "        env_pettingzoo.render()\n",
    "        \n",
    "        # store current state            \n",
    "        if not done: \n",
    "            # choose action using epsilon_greedy_policy()\n",
    "            # your code here    \n",
    "            observation = obs[\"observations\"]\n",
    "            action_mask = obs[\"action_mask\"]\n",
    "            action = random_admissible_policy(observation, action_mask)\n",
    "        \n",
    "            print(f\"sampled action {agent}: {action}\")\n",
    "            env_pettingzoo.step(action)\n",
    "        else: \n",
    "            # agent is done\n",
    "            env_pettingzoo.step(None)\n",
    "            print('done', reward)\n",
    "            break\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(env_pettingzoo._cumulative_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more envs test with rllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.17.87.73',\n",
       " 'raylet_ip_address': '172.17.87.73',\n",
       " 'redis_address': '172.17.87.73:64228',\n",
       " 'object_store_address': '/tmp/ray/session_2022-01-28_00-10-44_149462_2156/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-01-28_00-10-44_149462_2156/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-01-28_00-10-44_149462_2156',\n",
       " 'metrics_export_port': 43368,\n",
       " 'node_id': '8ff9d6fb5900ae0f0508f4e7852164560a041347c64fd4442910b6f3'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import init\n",
    "init(num_cpus=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "from rlskyjo.environment import simple_skyjo_env_v2\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "import ray.rllib.agents.dqn as dqn\n",
    "from ray.rllib.agents import ppo\n",
    "from copy import deepcopy\n",
    "from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from rlskyjo.models.action_mask_model import TorchMaskedActions, TorchActionMaskModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.agents.dqn import DQNTrainer, DQNTorchPolicy\n",
    "from ray.tune.logger import pretty_print\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "  warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "/home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "  warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env_name  = \"pettingzoo_skyjo\"\n",
    "def env_creator():\n",
    "        env = simple_skyjo_env_v2.env(**skyjo_env_cfg)\n",
    "        return env\n",
    "\n",
    "register_env(env_name,\n",
    "                lambda config: PettingZooEnv(env_creator()))\n",
    "ModelCatalog.register_custom_model(\n",
    "        \"pa_model2\", TorchActionMaskModel\n",
    "    )\n",
    "env = PettingZooEnv(env_creator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observations': array([ 3, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 11,  2, 10,  0,  1,  1,\n",
       "         1,  1,  0,  0,  0,  1,  0,  0,  1,  0,  0,  1,  6, 15], dtype=int8),\n",
       " 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1], dtype=int8)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.observe(env.env.agent_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with multiagent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-28 00:10:48,936\tWARNING ppo.py:143 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=16 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 250.\n",
      "2022-01-28 00:10:48,937\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-01-28 00:10:48,938\tINFO trainer.py:743 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7794)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7794)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7794)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7794)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7784)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7784)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7784)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7784)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7788)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7788)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7788)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7788)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7781)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7781)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7781)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7781)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7787)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7787)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7787)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7787)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7795)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7795)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7795)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7795)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7778)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7778)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7778)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7778)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7783)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7783)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7783)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7783)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7782)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7782)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7782)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7782)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7786)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7786)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7786)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7786)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7779)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7779)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7779)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7779)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7780)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7780)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7780)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7780)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7789)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7789)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7789)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7789)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7776)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7776)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7776)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7776)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7785)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7785)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7785)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7785)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7777)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7777)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7777)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7777)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "2022-01-28 00:11:05,432\tINFO trainable.py:124 -- Trainable.setup took 16.497 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    }
   ],
   "source": [
    "custom_config={\n",
    "    \"env\":env_name,\n",
    "    \"model\": {\n",
    "        \"custom_model\": \"pa_model2\",\n",
    "    },\n",
    "    \"framework\": \"torch\",\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "    \"num_gpus\": int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "    \"num_workers\": os.cpu_count(),\n",
    "    \"multiagent\":{\n",
    "            \"policies\": {\n",
    "                name: (None, env.observation_space, env.action_space, {}) for name in env.agents\n",
    "            },\n",
    "            \"policy_mapping_fn\": lambda agent_id: agent_id\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "ppo_config = ppo.DEFAULT_CONFIG.copy()\n",
    "ppo_config.update(custom_config)\n",
    "\n",
    "trainer = ppo.PPOTrainer(config=ppo_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=7785)\u001b[0m 2022-01-28 00:11:05,530\tWARNING deprecation.py:45 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7785)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/numpy/core/_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7785)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7785)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/numpy/core/_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7785)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 3968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-11-13\n",
      "done: false\n",
      "episode_len_mean: 100.875\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000284\n",
      "episode_reward_mean: 3.0000000000000027\n",
      "episode_reward_min: 2.9999999999999716\n",
      "episodes_this_iter: 32\n",
      "episodes_total: 32\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.7334793674945832\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006546088004887527\n",
      "        policy_loss: -0.06398468928101163\n",
      "        total_loss: 124.69429440816243\n",
      "        vf_explained_var: 0.004620983203252157\n",
      "        vf_loss: 124.75696977615357\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.7338204069932301\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014712815219143553\n",
      "        policy_loss: -0.03666329956613481\n",
      "        total_loss: 174.47338802337646\n",
      "        vf_explained_var: 0.006159005165100098\n",
      "        vf_loss: 174.50710808436077\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.7327739846706391\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012151473058521564\n",
      "        policy_loss: -0.018222006869812808\n",
      "        total_loss: 143.30579254786173\n",
      "        vf_explained_var: 0.006795242230097453\n",
      "        vf_loss: 143.32158432006835\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 3968\n",
      "  num_agent_steps_trained: 3968\n",
      "  num_steps_sampled: 4000\n",
      "  num_steps_trained: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.269999999999996\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 53.92999999999999\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 22.333333333333336\n",
      "  player_1: 33.0\n",
      "  player_2: 34.60000000000001\n",
      "policy_reward_mean:\n",
      "  player_0: -1.4708333333333323\n",
      "  player_1: 2.3229166666666674\n",
      "  player_2: 2.147916666666667\n",
      "policy_reward_min:\n",
      "  player_0: -39.86666666666666\n",
      "  player_1: -32.19999999999999\n",
      "  player_2: -35.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11632950657392402\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 7.448144880423983\n",
      "  mean_inference_ms: 1.9692682412516076\n",
      "  mean_raw_obs_processing_ms: 0.2559716126833304\n",
      "time_since_restore: 7.894867181777954\n",
      "time_this_iter_s: 7.894867181777954\n",
      "time_total_s: 7.894867181777954\n",
      "timers:\n",
      "  learn_throughput: 769.429\n",
      "  learn_time_ms: 5198.663\n",
      "  load_throughput: 5628049.648\n",
      "  load_time_ms: 0.711\n",
      "  sample_throughput: 1460.904\n",
      "  sample_time_ms: 2738.031\n",
      "  update_time_ms: 3.837\n",
      "timestamp: 1643325073\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 11969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-11-25\n",
      "done: false\n",
      "episode_len_mean: 104.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000284\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999716\n",
      "episodes_this_iter: 40\n",
      "episodes_total: 106\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.686346339782079\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010094869440605786\n",
      "        policy_loss: -0.07004322651773691\n",
      "        total_loss: 127.71297318776449\n",
      "        vf_explained_var: 0.016863374908765157\n",
      "        vf_loss: 127.78099718729655\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6878636531035105\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007599687489843443\n",
      "        policy_loss: -0.012850477620959281\n",
      "        total_loss: 151.76063599268596\n",
      "        vf_explained_var: 0.033483384450276696\n",
      "        vf_loss: 151.7719670677185\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.690782390832901\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0077563723774336036\n",
      "        policy_loss: -0.06135629542171955\n",
      "        total_loss: 111.60385606447856\n",
      "        vf_explained_var: 0.027514755328496298\n",
      "        vf_loss: 111.66366130193074\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 11969\n",
      "  num_agent_steps_trained: 11969\n",
      "  num_steps_sampled: 12000\n",
      "  num_steps_trained: 12000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 3\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.875\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.05\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.733333333333334\n",
      "  player_1: 34.0\n",
      "  player_2: 34.60000000000001\n",
      "policy_reward_mean:\n",
      "  player_0: 1.268\n",
      "  player_1: 1.114\n",
      "  player_2: 0.6180000000000003\n",
      "policy_reward_min:\n",
      "  player_0: -39.86666666666666\n",
      "  player_1: -37.13333333333334\n",
      "  player_2: -38.86666666666666\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11472563458027711\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 4.345492139927969\n",
      "  mean_inference_ms: 1.9624978140130047\n",
      "  mean_raw_obs_processing_ms: 0.25655019259528433\n",
      "time_since_restore: 20.07686686515808\n",
      "time_this_iter_s: 6.0557777881622314\n",
      "time_total_s: 20.07686686515808\n",
      "timers:\n",
      "  learn_throughput: 758.263\n",
      "  learn_time_ms: 5275.217\n",
      "  load_throughput: 6269512.706\n",
      "  load_time_ms: 0.638\n",
      "  sample_throughput: 806.58\n",
      "  sample_time_ms: 4959.21\n",
      "  update_time_ms: 4.126\n",
      "timestamp: 1643325085\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 12000\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 19968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-11-37\n",
      "done: false\n",
      "episode_len_mean: 109.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000284\n",
      "episode_reward_mean: 2.999999999999998\n",
      "episode_reward_min: 2.9999999999999716\n",
      "episodes_this_iter: 36\n",
      "episodes_total: 178\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6499117894967397\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007539463827694514\n",
      "        policy_loss: -0.0432715719845146\n",
      "        total_loss: 124.47016607920328\n",
      "        vf_explained_var: 0.009117435415585835\n",
      "        vf_loss: 124.51192986806234\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.645322811603546\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008999395490545722\n",
      "        policy_loss: -0.0643458605852599\n",
      "        total_loss: 112.53849906921387\n",
      "        vf_explained_var: 0.0352096160252889\n",
      "        vf_loss: 112.60104521433513\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6703814772764842\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008777307712650699\n",
      "        policy_loss: -0.04862074116244912\n",
      "        total_loss: 117.44041996002197\n",
      "        vf_explained_var: 0.016569079359372456\n",
      "        vf_loss: 117.48728485743204\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 19968\n",
      "  num_agent_steps_trained: 19968\n",
      "  num_steps_sampled: 20000\n",
      "  num_steps_trained: 20000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 5\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.2625\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 38.4\n",
      "  player_1: 35.4\n",
      "  player_2: 34.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: -0.09466666666666726\n",
      "  player_1: 2.6133333333333324\n",
      "  player_2: 0.48133333333333256\n",
      "policy_reward_min:\n",
      "  player_0: -35.4\n",
      "  player_1: -46.599999999999994\n",
      "  player_2: -31.53333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11171773587969298\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.1491087785264202\n",
      "  mean_inference_ms: 1.9487816236309663\n",
      "  mean_raw_obs_processing_ms: 0.25420587764199365\n",
      "time_since_restore: 32.356690645217896\n",
      "time_this_iter_s: 6.115891695022583\n",
      "time_total_s: 32.356690645217896\n",
      "timers:\n",
      "  learn_throughput: 751.404\n",
      "  learn_time_ms: 5323.365\n",
      "  load_throughput: 6395218.419\n",
      "  load_time_ms: 0.625\n",
      "  sample_throughput: 737.001\n",
      "  sample_time_ms: 5427.403\n",
      "  update_time_ms: 4.293\n",
      "timestamp: 1643325097\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 20000\n",
      "training_iteration: 5\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 27970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-11-50\n",
      "done: false\n",
      "episode_len_mean: 105.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000284\n",
      "episode_reward_mean: 2.999999999999998\n",
      "episode_reward_min: 2.9999999999999716\n",
      "episodes_this_iter: 35\n",
      "episodes_total: 255\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6361991186936697\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008304346664491315\n",
      "        policy_loss: -0.018957350483785072\n",
      "        total_loss: 104.51368809064229\n",
      "        vf_explained_var: 0.004114976326624552\n",
      "        vf_loss: 104.53098583221436\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6319476576646168\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008679912715329767\n",
      "        policy_loss: -0.07211456380784512\n",
      "        total_loss: 75.19175444285075\n",
      "        vf_explained_var: 0.07626412332057952\n",
      "        vf_loss: 75.26213302612305\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6388898003101349\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009471032322665754\n",
      "        policy_loss: -0.08677335582440719\n",
      "        total_loss: 80.73141613642375\n",
      "        vf_explained_var: -1.274545987447103e-05\n",
      "        vf_loss: 80.8162943649292\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 27970\n",
      "  num_agent_steps_trained: 27970\n",
      "  num_steps_sampled: 28000\n",
      "  num_steps_trained: 28000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 7\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.825\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 40.26666666666667\n",
      "  player_1: 35.4\n",
      "  player_2: 39.93333333333334\n",
      "policy_reward_mean:\n",
      "  player_0: 0.9726666666666658\n",
      "  player_1: 1.1806666666666659\n",
      "  player_2: 0.8466666666666657\n",
      "policy_reward_min:\n",
      "  player_0: -29.33333333333333\n",
      "  player_1: -37.93333333333333\n",
      "  player_2: -33.4\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10908665703716583\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 1.5133767409279109\n",
      "  mean_inference_ms: 1.938211114852248\n",
      "  mean_raw_obs_processing_ms: 0.2514323626961378\n",
      "time_since_restore: 44.53431701660156\n",
      "time_this_iter_s: 6.1159584522247314\n",
      "time_total_s: 44.53431701660156\n",
      "timers:\n",
      "  learn_throughput: 751.241\n",
      "  learn_time_ms: 5324.521\n",
      "  load_throughput: 6327954.739\n",
      "  load_time_ms: 0.632\n",
      "  sample_throughput: 710.953\n",
      "  sample_time_ms: 5626.251\n",
      "  update_time_ms: 4.128\n",
      "timestamp: 1643325110\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 28000\n",
      "training_iteration: 7\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 35968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-12-02\n",
      "done: false\n",
      "episode_len_mean: 113.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000284\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999716\n",
      "episodes_this_iter: 32\n",
      "episodes_total: 322\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5918861734867096\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00893356987234559\n",
      "        policy_loss: -0.033108335950722295\n",
      "        total_loss: 110.76265005747477\n",
      "        vf_explained_var: -0.0006312509377797445\n",
      "        vf_loss: 110.7939711634318\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5984257964293163\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009648611743979331\n",
      "        policy_loss: -0.05653159244994943\n",
      "        total_loss: 122.81499776204427\n",
      "        vf_explained_var: 0.06881422241528828\n",
      "        vf_loss: 122.86959968566894\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.589170200030009\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008286395621289556\n",
      "        policy_loss: -0.08786511701842149\n",
      "        total_loss: 89.99919175942739\n",
      "        vf_explained_var: -0.03077902853488922\n",
      "        vf_loss: 90.08539956887563\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 35968\n",
      "  num_agent_steps_trained: 35968\n",
      "  num_steps_sampled: 36000\n",
      "  num_steps_trained: 36000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 9\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.0\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.075\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.666666666666664\n",
      "  player_1: 32.0\n",
      "  player_2: 35.86666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 3.4739999999999998\n",
      "  player_1: 0.884\n",
      "  player_2: -1.358\n",
      "policy_reward_min:\n",
      "  player_0: -29.93333333333333\n",
      "  player_1: -31.333333333333336\n",
      "  player_2: -33.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10655575265504297\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 1.220277344273677\n",
      "  mean_inference_ms: 1.9178604220349689\n",
      "  mean_raw_obs_processing_ms: 0.24875249227625104\n",
      "time_since_restore: 56.87899088859558\n",
      "time_this_iter_s: 6.16641640663147\n",
      "time_total_s: 56.87899088859558\n",
      "timers:\n",
      "  learn_throughput: 748.382\n",
      "  learn_time_ms: 5344.866\n",
      "  load_throughput: 6376744.964\n",
      "  load_time_ms: 0.627\n",
      "  sample_throughput: 695.571\n",
      "  sample_time_ms: 5750.669\n",
      "  update_time_ms: 4.071\n",
      "timestamp: 1643325122\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 36000\n",
      "training_iteration: 9\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 43970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-12-14\n",
      "done: false\n",
      "episode_len_mean: 114.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000284\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.9999999999999716\n",
      "episodes_this_iter: 36\n",
      "episodes_total: 394\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.563098931312561\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01075991927903355\n",
      "        policy_loss: -0.05378850138435761\n",
      "        total_loss: 81.38300903638203\n",
      "        vf_explained_var: 0.10756038685639699\n",
      "        vf_loss: 81.43464635531107\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6071120790640514\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009182566965628212\n",
      "        policy_loss: -0.09112434029579163\n",
      "        total_loss: 122.81294542948405\n",
      "        vf_explained_var: -0.0196009620030721\n",
      "        vf_loss: 122.9022327931722\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5514805428187053\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009793837903464676\n",
      "        policy_loss: -0.0069179855442295475\n",
      "        total_loss: 117.4773288345337\n",
      "        vf_explained_var: 0.04902909755706787\n",
      "        vf_loss: 117.48228843688965\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 43970\n",
      "  num_agent_steps_trained: 43970\n",
      "  num_steps_sampled: 44000\n",
      "  num_steps_trained: 44000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 11\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.075\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 29.666666666666664\n",
      "  player_1: 36.199999999999996\n",
      "  player_2: 35.86666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 0.8553333333333338\n",
      "  player_1: 1.3653333333333335\n",
      "  player_2: 0.7793333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -33.73333333333333\n",
      "  player_1: -31.6\n",
      "  player_2: -28.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10525075114344837\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 1.029865792917817\n",
      "  mean_inference_ms: 1.8966180158310024\n",
      "  mean_raw_obs_processing_ms: 0.24725975746155043\n",
      "time_since_restore: 69.19785499572754\n",
      "time_this_iter_s: 6.144651412963867\n",
      "time_total_s: 69.19785499572754\n",
      "timers:\n",
      "  learn_throughput: 744.464\n",
      "  learn_time_ms: 5372.996\n",
      "  load_throughput: 6536492.773\n",
      "  load_time_ms: 0.612\n",
      "  sample_throughput: 651.533\n",
      "  sample_time_ms: 6139.371\n",
      "  update_time_ms: 4.056\n",
      "timestamp: 1643325134\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 44000\n",
      "training_iteration: 11\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 51970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-12-27\n",
      "done: false\n",
      "episode_len_mean: 116.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999716\n",
      "episodes_this_iter: 32\n",
      "episodes_total: 460\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5277244023482004\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01260600951397161\n",
      "        policy_loss: -0.07058340940624476\n",
      "        total_loss: 99.50254425048828\n",
      "        vf_explained_var: 0.06530844767888387\n",
      "        vf_loss: 99.57060618082683\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5716764811674755\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009125215210660826\n",
      "        policy_loss: -0.10253740119437377\n",
      "        total_loss: 118.59835978190104\n",
      "        vf_explained_var: 0.019385356903076172\n",
      "        vf_loss: 118.69907190958659\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5372624615828197\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009149568129225379\n",
      "        policy_loss: -0.012827905803763619\n",
      "        total_loss: 66.95419569651285\n",
      "        vf_explained_var: -0.08096335629622141\n",
      "        vf_loss: 66.96519384066264\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 51970\n",
      "  num_agent_steps_trained: 51970\n",
      "  num_steps_sampled: 52000\n",
      "  num_steps_trained: 52000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 13\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.674999999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.087500000000006\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 38.53333333333333\n",
      "  player_1: 27.46666666666667\n",
      "  player_2: 34.26666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 2.989333333333333\n",
      "  player_1: -1.092666666666667\n",
      "  player_2: 1.1033333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -33.466666666666676\n",
      "  player_1: -37.666666666666664\n",
      "  player_2: -40.06666666666666\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10476196378255775\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.9184597937645347\n",
      "  mean_inference_ms: 1.888974662460743\n",
      "  mean_raw_obs_processing_ms: 0.24682226586417932\n",
      "time_since_restore: 81.63531494140625\n",
      "time_this_iter_s: 6.193282842636108\n",
      "time_total_s: 81.63531494140625\n",
      "timers:\n",
      "  learn_throughput: 740.459\n",
      "  learn_time_ms: 5402.054\n",
      "  load_throughput: 6422884.269\n",
      "  load_time_ms: 0.623\n",
      "  sample_throughput: 647.798\n",
      "  sample_time_ms: 6174.766\n",
      "  update_time_ms: 4.233\n",
      "timestamp: 1643325147\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 52000\n",
      "training_iteration: 13\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 59968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-12-39\n",
      "done: false\n",
      "episode_len_mean: 118.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999716\n",
      "episodes_this_iter: 32\n",
      "episodes_total: 528\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5114528755346934\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011118188302158766\n",
      "        policy_loss: -0.026879808836771796\n",
      "        total_loss: 104.6905355199178\n",
      "        vf_explained_var: 0.16152551054954528\n",
      "        vf_loss: 104.71519186655681\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5263055209318797\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010084473281274261\n",
      "        policy_loss: -0.09589805267751217\n",
      "        total_loss: 109.67125418345134\n",
      "        vf_explained_var: 0.13585246702035267\n",
      "        vf_loss: 109.7651361211141\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5237053835391998\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010433484562680255\n",
      "        policy_loss: -0.07842269172271092\n",
      "        total_loss: 115.03158364613851\n",
      "        vf_explained_var: 0.10928916792074839\n",
      "        vf_loss: 115.10791919708252\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 59968\n",
      "  num_agent_steps_trained: 59968\n",
      "  num_steps_sampled: 60000\n",
      "  num_steps_trained: 60000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 15\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.150000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.199999999999996\n",
      "  player_1: 26.666666666666664\n",
      "  player_2: 25.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 3.2526666666666677\n",
      "  player_1: -1.6073333333333335\n",
      "  player_2: 1.3546666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -26.86666666666666\n",
      "  player_1: -37.666666666666664\n",
      "  player_2: -46.533333333333324\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10467961007840793\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.8311998994245055\n",
      "  mean_inference_ms: 1.8866590126182121\n",
      "  mean_raw_obs_processing_ms: 0.24691576313160365\n",
      "time_since_restore: 93.93537950515747\n",
      "time_this_iter_s: 6.116190671920776\n",
      "time_total_s: 93.93537950515747\n",
      "timers:\n",
      "  learn_throughput: 740.12\n",
      "  learn_time_ms: 5404.531\n",
      "  load_throughput: 6409878.505\n",
      "  load_time_ms: 0.624\n",
      "  sample_throughput: 646.441\n",
      "  sample_time_ms: 6187.726\n",
      "  update_time_ms: 4.208\n",
      "timestamp: 1643325159\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 60000\n",
      "training_iteration: 15\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 67969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-12-52\n",
      "done: false\n",
      "episode_len_mean: 116.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 38\n",
      "episodes_total: 600\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4874718153476716\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009674388602523197\n",
      "        policy_loss: -0.12236172133125364\n",
      "        total_loss: 81.17489995956421\n",
      "        vf_explained_var: 0.050275697509447735\n",
      "        vf_loss: 81.2953264840444\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5215297961235046\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010667320555136636\n",
      "        policy_loss: -0.05037563701781134\n",
      "        total_loss: 72.49280555089315\n",
      "        vf_explained_var: 0.07535386939843496\n",
      "        vf_loss: 72.54104784011841\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4993072060743968\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010210890133882156\n",
      "        policy_loss: -0.032652657522509496\n",
      "        total_loss: 77.40405487060546\n",
      "        vf_explained_var: 0.060623080730438234\n",
      "        vf_loss: 77.43466504414876\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 67969\n",
      "  num_agent_steps_trained: 67969\n",
      "  num_steps_sampled: 68000\n",
      "  num_steps_trained: 68000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 17\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.699999999999996\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.199999999999996\n",
      "  player_1: 23.066666666666663\n",
      "  player_2: 28.79999999999999\n",
      "policy_reward_mean:\n",
      "  player_0: 1.7886666666666668\n",
      "  player_1: -0.9133333333333337\n",
      "  player_2: 2.1246666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -26.733333333333327\n",
      "  player_1: -32.4\n",
      "  player_2: -46.533333333333324\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10388477340168217\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7622277645590319\n",
      "  mean_inference_ms: 1.875164282453178\n",
      "  mean_raw_obs_processing_ms: 0.2468620750940741\n",
      "time_since_restore: 106.33748412132263\n",
      "time_this_iter_s: 6.198893070220947\n",
      "time_total_s: 106.33748412132263\n",
      "timers:\n",
      "  learn_throughput: 736.316\n",
      "  learn_time_ms: 5432.452\n",
      "  load_throughput: 6442368.482\n",
      "  load_time_ms: 0.621\n",
      "  sample_throughput: 645.084\n",
      "  sample_time_ms: 6200.744\n",
      "  update_time_ms: 4.135\n",
      "timestamp: 1643325172\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 68000\n",
      "training_iteration: 17\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 75971\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-13-04\n",
      "done: false\n",
      "episode_len_mean: 114.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999716\n",
      "episodes_this_iter: 33\n",
      "episodes_total: 667\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4724859722455343\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01046046160212427\n",
      "        policy_loss: -0.031763205726941426\n",
      "        total_loss: 91.34200085322063\n",
      "        vf_explained_var: -0.005304662386576334\n",
      "        vf_loss: 91.37167181015015\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4671253935496011\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010221898960238226\n",
      "        policy_loss: -0.09425977898218359\n",
      "        total_loss: 78.48122994740804\n",
      "        vf_explained_var: 0.05499399900436401\n",
      "        vf_loss: 78.57344530105591\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4782843001683552\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01112425698217218\n",
      "        policy_loss: -0.07307602651417255\n",
      "        total_loss: 79.82941521326701\n",
      "        vf_explained_var: 0.0952333551645279\n",
      "        vf_loss: 79.90026643753052\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 75971\n",
      "  num_agent_steps_trained: 75971\n",
      "  num_steps_sampled: 76000\n",
      "  num_steps_trained: 76000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 19\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.625\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 28.333333333333336\n",
      "  player_2: 22.866666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 1.3913333333333335\n",
      "  player_1: -0.6646666666666667\n",
      "  player_2: 2.273333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -32.333333333333336\n",
      "  player_1: -30.666666666666664\n",
      "  player_2: -24.333333333333332\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1032312627411035\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7134767793406587\n",
      "  mean_inference_ms: 1.8706931834178036\n",
      "  mean_raw_obs_processing_ms: 0.24623209323899925\n",
      "time_since_restore: 118.7502543926239\n",
      "time_this_iter_s: 6.2001800537109375\n",
      "time_total_s: 118.7502543926239\n",
      "timers:\n",
      "  learn_throughput: 734.545\n",
      "  learn_time_ms: 5445.551\n",
      "  load_throughput: 6361994.615\n",
      "  load_time_ms: 0.629\n",
      "  sample_throughput: 644.072\n",
      "  sample_time_ms: 6210.489\n",
      "  update_time_ms: 4.218\n",
      "timestamp: 1643325184\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 76000\n",
      "training_iteration: 19\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 83968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-13-17\n",
      "done: false\n",
      "episode_len_mean: 117.62\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999716\n",
      "episodes_this_iter: 35\n",
      "episodes_total: 735\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4331843443711598\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012598948588127335\n",
      "        policy_loss: -0.06300169774641594\n",
      "        total_loss: 88.5226274617513\n",
      "        vf_explained_var: -0.07768062353134156\n",
      "        vf_loss: 88.58310843785604\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4228053414821624\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011927829282528061\n",
      "        policy_loss: -0.09430623004833857\n",
      "        total_loss: 92.28139031092326\n",
      "        vf_explained_var: 0.042563575903574624\n",
      "        vf_loss: 92.37331093470256\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4800889197985332\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010777206981316945\n",
      "        policy_loss: -0.07879200581461192\n",
      "        total_loss: 121.48833346048991\n",
      "        vf_explained_var: -0.05299693504969279\n",
      "        vf_loss: 121.5649713897705\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 83968\n",
      "  num_agent_steps_trained: 83968\n",
      "  num_steps_sampled: 84000\n",
      "  num_steps_trained: 84000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 21\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.575000000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.199999999999996\n",
      "  player_1: 28.333333333333336\n",
      "  player_2: 27.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.7826666666666665\n",
      "  player_1: 0.23466666666666647\n",
      "  player_2: 1.9826666666666661\n",
      "policy_reward_min:\n",
      "  player_0: -36.333333333333336\n",
      "  player_1: -37.4\n",
      "  player_2: -27.800000000000004\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1030834106911105\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.6752019519262881\n",
      "  mean_inference_ms: 1.8688858399971098\n",
      "  mean_raw_obs_processing_ms: 0.24598910447659073\n",
      "time_since_restore: 131.37269639968872\n",
      "time_this_iter_s: 6.190374374389648\n",
      "time_total_s: 131.37269639968872\n",
      "timers:\n",
      "  learn_throughput: 729.951\n",
      "  learn_time_ms: 5479.823\n",
      "  load_throughput: 6311969.902\n",
      "  load_time_ms: 0.634\n",
      "  sample_throughput: 640.913\n",
      "  sample_time_ms: 6241.094\n",
      "  update_time_ms: 4.074\n",
      "timestamp: 1643325197\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 84000\n",
      "training_iteration: 21\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 91970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-13-29\n",
      "done: false\n",
      "episode_len_mean: 121.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 30\n",
      "episodes_total: 795\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3774481924374897\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009684934013418266\n",
      "        policy_loss: -0.06181515065332254\n",
      "        total_loss: 85.7290998395284\n",
      "        vf_explained_var: 0.04703875879446665\n",
      "        vf_loss: 85.78897827148438\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4066765232880911\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011862097080820456\n",
      "        policy_loss: -0.0742905521641175\n",
      "        total_loss: 63.89496512254079\n",
      "        vf_explained_var: 0.1515316488345464\n",
      "        vf_loss: 63.96688296477\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4356767888863882\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009985350598265844\n",
      "        policy_loss: -0.072852168182532\n",
      "        total_loss: 77.125373655955\n",
      "        vf_explained_var: 0.12761967261632284\n",
      "        vf_loss: 77.19622890472412\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 91970\n",
      "  num_agent_steps_trained: 91970\n",
      "  num_steps_sampled: 92000\n",
      "  num_steps_trained: 92000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 23\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.8\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.86666666666667\n",
      "  player_1: 31.666666666666664\n",
      "  player_2: 30.199999999999996\n",
      "policy_reward_mean:\n",
      "  player_0: 1.4093333333333333\n",
      "  player_1: 0.7473333333333332\n",
      "  player_2: 0.843333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -27.0\n",
      "  player_1: -28.866666666666667\n",
      "  player_2: -34.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10293271410841626\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.6461262837032862\n",
      "  mean_inference_ms: 1.866990591101437\n",
      "  mean_raw_obs_processing_ms: 0.2467207844408373\n",
      "time_since_restore: 143.70738172531128\n",
      "time_this_iter_s: 6.1677727699279785\n",
      "time_total_s: 143.70738172531128\n",
      "timers:\n",
      "  learn_throughput: 730.805\n",
      "  learn_time_ms: 5473.413\n",
      "  load_throughput: 6431008.893\n",
      "  load_time_ms: 0.622\n",
      "  sample_throughput: 641.579\n",
      "  sample_time_ms: 6234.62\n",
      "  update_time_ms: 3.745\n",
      "timestamp: 1643325209\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 92000\n",
      "training_iteration: 23\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 99968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-13-42\n",
      "done: false\n",
      "episode_len_mean: 131.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 31\n",
      "episodes_total: 856\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3216416442394257\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010764340283021739\n",
      "        policy_loss: -0.0346060004333655\n",
      "        total_loss: 95.83067240873973\n",
      "        vf_explained_var: 0.04400100588798523\n",
      "        vf_loss: 95.86312599817911\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3656844699382782\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01049137040385934\n",
      "        policy_loss: -0.1252366440774252\n",
      "        total_loss: 64.62165043512981\n",
      "        vf_explained_var: 0.1370951157808304\n",
      "        vf_loss: 64.74478917121887\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4052345538139344\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011051653357728277\n",
      "        policy_loss: -0.07121951095759868\n",
      "        total_loss: 43.791836865743\n",
      "        vf_explained_var: 0.11407635748386383\n",
      "        vf_loss: 43.86084585189819\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 99968\n",
      "  num_agent_steps_trained: 99968\n",
      "  num_steps_sampled: 100000\n",
      "  num_steps_trained: 100000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 25\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.8625\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.86666666666667\n",
      "  player_1: 31.666666666666664\n",
      "  player_2: 30.199999999999996\n",
      "policy_reward_mean:\n",
      "  player_0: -0.03733333333333316\n",
      "  player_1: -0.7773333333333332\n",
      "  player_2: 3.814666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -33.73333333333333\n",
      "  player_1: -28.666666666666664\n",
      "  player_2: -27.133333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10274413342632283\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.6169889301017482\n",
      "  mean_inference_ms: 1.8675315637857828\n",
      "  mean_raw_obs_processing_ms: 0.24668501037336696\n",
      "time_since_restore: 156.0664415359497\n",
      "time_this_iter_s: 6.137138605117798\n",
      "time_total_s: 156.0664415359497\n",
      "timers:\n",
      "  learn_throughput: 729.981\n",
      "  learn_time_ms: 5479.591\n",
      "  load_throughput: 6380139.945\n",
      "  load_time_ms: 0.627\n",
      "  sample_throughput: 641.207\n",
      "  sample_time_ms: 6238.235\n",
      "  update_time_ms: 3.541\n",
      "timestamp: 1643325222\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 100000\n",
      "training_iteration: 25\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 107968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-13-54\n",
      "done: false\n",
      "episode_len_mean: 135.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 915\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3262112629413605\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012542406795579003\n",
      "        policy_loss: -0.07463048121581475\n",
      "        total_loss: 111.26323190689087\n",
      "        vf_explained_var: 0.058674930930137634\n",
      "        vf_loss: 111.33535374323527\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3069707222779592\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010537314609706906\n",
      "        policy_loss: -0.0971827440087994\n",
      "        total_loss: 76.56444810231527\n",
      "        vf_explained_var: 0.03779358963171641\n",
      "        vf_loss: 76.65952350616455\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3773553975423176\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00849507300300805\n",
      "        policy_loss: -0.05199468653649092\n",
      "        total_loss: 172.61957075277965\n",
      "        vf_explained_var: -0.039665992061297096\n",
      "        vf_loss: 172.66986629009247\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 107968\n",
      "  num_agent_steps_trained: 107968\n",
      "  num_steps_sampled: 108000\n",
      "  num_steps_trained: 108000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 27\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.0375\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 44.4\n",
      "  player_1: 31.200000000000003\n",
      "  player_2: 29.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 1.467333333333333\n",
      "  player_1: -1.1306666666666665\n",
      "  player_2: 2.6633333333333336\n",
      "policy_reward_min:\n",
      "  player_0: -33.73333333333333\n",
      "  player_1: -30.333333333333336\n",
      "  player_2: -59.800000000000004\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10269413968883008\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5946294620110345\n",
      "  mean_inference_ms: 1.867666209290145\n",
      "  mean_raw_obs_processing_ms: 0.24589934340047154\n",
      "time_since_restore: 168.41077303886414\n",
      "time_this_iter_s: 6.143361330032349\n",
      "time_total_s: 168.41077303886414\n",
      "timers:\n",
      "  learn_throughput: 730.284\n",
      "  learn_time_ms: 5477.318\n",
      "  load_throughput: 6322197.686\n",
      "  load_time_ms: 0.633\n",
      "  sample_throughput: 641.311\n",
      "  sample_time_ms: 6237.22\n",
      "  update_time_ms: 3.625\n",
      "timestamp: 1643325234\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 108000\n",
      "training_iteration: 27\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 115970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-14-07\n",
      "done: false\n",
      "episode_len_mean: 143.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 30\n",
      "episodes_total: 971\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2871171514193216\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010984709722773307\n",
      "        policy_loss: -0.06183080931815008\n",
      "        total_loss: 57.71083464463552\n",
      "        vf_explained_var: 0.1189343911409378\n",
      "        vf_loss: 57.770468541781106\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3050160721937816\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011637071786029386\n",
      "        policy_loss: -0.03440948319931825\n",
      "        total_loss: 62.48574562390645\n",
      "        vf_explained_var: 0.06876820663611094\n",
      "        vf_loss: 62.51782748540243\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.35118155280749\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010900330065584664\n",
      "        policy_loss: -0.1000528796055975\n",
      "        total_loss: 76.4518493350347\n",
      "        vf_explained_var: 0.05015365242958069\n",
      "        vf_loss: 76.54972214539846\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 115970\n",
      "  num_agent_steps_trained: 115970\n",
      "  num_steps_sampled: 116000\n",
      "  num_steps_trained: 116000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 29\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.049999999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 44.4\n",
      "  player_1: 32.0\n",
      "  player_2: 29.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.5180000000000002\n",
      "  player_1: 0.4879999999999999\n",
      "  player_2: -0.006000000000000085\n",
      "policy_reward_min:\n",
      "  player_0: -28.066666666666663\n",
      "  player_1: -35.93333333333333\n",
      "  player_2: -59.800000000000004\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10227572594011859\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5737787209751\n",
      "  mean_inference_ms: 1.8642085380974935\n",
      "  mean_raw_obs_processing_ms: 0.24474195095136944\n",
      "time_since_restore: 180.7016944885254\n",
      "time_this_iter_s: 6.189841270446777\n",
      "time_total_s: 180.7016944885254\n",
      "timers:\n",
      "  learn_throughput: 732.109\n",
      "  learn_time_ms: 5463.667\n",
      "  load_throughput: 6385239.201\n",
      "  load_time_ms: 0.626\n",
      "  sample_throughput: 642.449\n",
      "  sample_time_ms: 6226.173\n",
      "  update_time_ms: 3.592\n",
      "timestamp: 1643325247\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 116000\n",
      "training_iteration: 29\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 123969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-14-19\n",
      "done: false\n",
      "episode_len_mean: 149.84\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 1020\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1990484285354615\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010498109480226957\n",
      "        policy_loss: -0.06047661075989405\n",
      "        total_loss: 84.39737068335215\n",
      "        vf_explained_var: 0.21890050331751507\n",
      "        vf_loss: 84.45574773629507\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2957304263114928\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011782167746950517\n",
      "        policy_loss: -0.05821806962136179\n",
      "        total_loss: 61.98735425631205\n",
      "        vf_explained_var: 0.19344402452309925\n",
      "        vf_loss: 62.04321580251058\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3181318231423695\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0117682413626729\n",
      "        policy_loss: -0.1028060958472391\n",
      "        total_loss: 54.08360690434774\n",
      "        vf_explained_var: -0.16013230899969735\n",
      "        vf_loss: 54.18405984242757\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 123969\n",
      "  num_agent_steps_trained: 123969\n",
      "  num_steps_sampled: 124000\n",
      "  num_steps_trained: 124000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 31\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.174999999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.333333333333336\n",
      "  player_1: 32.0\n",
      "  player_2: 30.266666666666673\n",
      "policy_reward_mean:\n",
      "  player_0: -0.7766666666666663\n",
      "  player_1: 1.4333333333333333\n",
      "  player_2: 2.3433333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -41.0\n",
      "  player_1: -35.93333333333333\n",
      "  player_2: -30.800000000000004\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10210742017864127\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5567887183671707\n",
      "  mean_inference_ms: 1.862080895612893\n",
      "  mean_raw_obs_processing_ms: 0.24402285447933245\n",
      "time_since_restore: 193.00863599777222\n",
      "time_this_iter_s: 6.163563251495361\n",
      "time_total_s: 193.00863599777222\n",
      "timers:\n",
      "  learn_throughput: 736.417\n",
      "  learn_time_ms: 5431.702\n",
      "  load_throughput: 6378927.037\n",
      "  load_time_ms: 0.627\n",
      "  sample_throughput: 645.336\n",
      "  sample_time_ms: 6198.326\n",
      "  update_time_ms: 3.707\n",
      "timestamp: 1643325259\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 124000\n",
      "training_iteration: 31\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 131969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-14-32\n",
      "done: false\n",
      "episode_len_mean: 149.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 1075\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2573403588930765\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011472556449540813\n",
      "        policy_loss: -0.0742943186375002\n",
      "        total_loss: 69.75014716148377\n",
      "        vf_explained_var: 0.02007952590783437\n",
      "        vf_loss: 69.82214625835418\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.270030565659205\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011212596884300485\n",
      "        policy_loss: -0.0682328711791585\n",
      "        total_loss: 80.69750909805298\n",
      "        vf_explained_var: 0.21226618071397146\n",
      "        vf_loss: 80.76349894841512\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2955690864721934\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012222179069199986\n",
      "        policy_loss: -0.0961099013624092\n",
      "        total_loss: 41.042893184026084\n",
      "        vf_explained_var: 0.10445146242777506\n",
      "        vf_loss: 41.136558567682904\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 131969\n",
      "  num_agent_steps_trained: 131969\n",
      "  num_steps_sampled: 132000\n",
      "  num_steps_trained: 132000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 33\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.6\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 30.133333333333333\n",
      "  player_1: 31.93333333333333\n",
      "  player_2: 42.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 1.386666666666667\n",
      "  player_1: -0.9613333333333333\n",
      "  player_2: 2.5746666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -41.0\n",
      "  player_1: -31.333333333333336\n",
      "  player_2: -30.4\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10175496801457547\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5404808195746333\n",
      "  mean_inference_ms: 1.8575293888767468\n",
      "  mean_raw_obs_processing_ms: 0.24317759527676663\n",
      "time_since_restore: 206.0731644630432\n",
      "time_this_iter_s: 6.895628213882446\n",
      "time_total_s: 206.0731644630432\n",
      "timers:\n",
      "  learn_throughput: 727.015\n",
      "  learn_time_ms: 5501.952\n",
      "  load_throughput: 6343232.636\n",
      "  load_time_ms: 0.631\n",
      "  sample_throughput: 645.604\n",
      "  sample_time_ms: 6195.747\n",
      "  update_time_ms: 3.84\n",
      "timestamp: 1643325272\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 132000\n",
      "training_iteration: 33\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 139968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-14-46\n",
      "done: false\n",
      "episode_len_mean: 148.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 1128\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.243469981352488\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012445425942319161\n",
      "        policy_loss: -0.07219181160752972\n",
      "        total_loss: 91.41412057240804\n",
      "        vf_explained_var: -0.060983620683352155\n",
      "        vf_loss: 91.48382303873699\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2733033418655395\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011979152671938937\n",
      "        policy_loss: -0.0687618500366807\n",
      "        total_loss: 82.34630875269572\n",
      "        vf_explained_var: 0.17318247218926747\n",
      "        vf_loss: 82.41267468770344\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2867183880011241\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013483130723862662\n",
      "        policy_loss: -0.09095595977967605\n",
      "        total_loss: 59.96123561859131\n",
      "        vf_explained_var: 0.169835351506869\n",
      "        vf_loss: 60.04949480056763\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 139968\n",
      "  num_agent_steps_trained: 139968\n",
      "  num_steps_sampled: 140000\n",
      "  num_steps_trained: 140000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 35\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.555555555555557\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 30.333333333333336\n",
      "  player_1: 25.199999999999996\n",
      "  player_2: 30.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 3.967333333333333\n",
      "  player_1: -2.0466666666666673\n",
      "  player_2: 1.079333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -27.866666666666664\n",
      "  player_1: -31.333333333333336\n",
      "  player_2: -31.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10170103593771898\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5266601183628312\n",
      "  mean_inference_ms: 1.859791128319673\n",
      "  mean_raw_obs_processing_ms: 0.24345047734095238\n",
      "time_since_restore: 220.10870003700256\n",
      "time_this_iter_s: 7.533140182495117\n",
      "time_total_s: 220.10870003700256\n",
      "timers:\n",
      "  learn_throughput: 708.005\n",
      "  learn_time_ms: 5649.679\n",
      "  load_throughput: 6318388.129\n",
      "  load_time_ms: 0.633\n",
      "  sample_throughput: 633.843\n",
      "  sample_time_ms: 6310.712\n",
      "  update_time_ms: 4.099\n",
      "timestamp: 1643325286\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 140000\n",
      "training_iteration: 35\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 147971\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-15-01\n",
      "done: false\n",
      "episode_len_mean: 153.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 1179\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2047709655761718\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013361450135616905\n",
      "        policy_loss: -0.07210883695166558\n",
      "        total_loss: 38.62861029783885\n",
      "        vf_explained_var: -0.06544109145800273\n",
      "        vf_loss: 38.69804712454478\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2044682057698568\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011052092082439534\n",
      "        policy_loss: -0.030244455505162478\n",
      "        total_loss: 68.9247878742218\n",
      "        vf_explained_var: 0.028188093105951946\n",
      "        vf_loss: 68.95282206535339\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2402367854118348\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011272139639123543\n",
      "        policy_loss: -0.0865573730521525\n",
      "        total_loss: 84.29358373006185\n",
      "        vf_explained_var: 0.1272335022687912\n",
      "        vf_loss: 84.37788623491923\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 147971\n",
      "  num_agent_steps_trained: 147971\n",
      "  num_steps_sampled: 148000\n",
      "  num_steps_trained: 148000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 37\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.58888888888889\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.26666666666667\n",
      "  player_1: 24.53333333333333\n",
      "  player_2: 33.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.2039999999999993\n",
      "  player_1: -2.176\n",
      "  player_2: 1.972\n",
      "policy_reward_min:\n",
      "  player_0: -27.866666666666664\n",
      "  player_1: -30.266666666666662\n",
      "  player_2: -29.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10209614416727934\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5164725959133183\n",
      "  mean_inference_ms: 1.868825173410315\n",
      "  mean_raw_obs_processing_ms: 0.24512107101042208\n",
      "time_since_restore: 234.9604036808014\n",
      "time_this_iter_s: 7.156153202056885\n",
      "time_total_s: 234.9604036808014\n",
      "timers:\n",
      "  learn_throughput: 681.798\n",
      "  learn_time_ms: 5866.843\n",
      "  load_throughput: 6339397.695\n",
      "  load_time_ms: 0.631\n",
      "  sample_throughput: 605.336\n",
      "  sample_time_ms: 6607.895\n",
      "  update_time_ms: 4.067\n",
      "timestamp: 1643325301\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 148000\n",
      "training_iteration: 37\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 155970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-15-14\n",
      "done: false\n",
      "episode_len_mean: 160.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 21\n",
      "episodes_total: 1224\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1491149719556173\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011796449593039143\n",
      "        policy_loss: -0.11737767502665519\n",
      "        total_loss: 80.26408067067464\n",
      "        vf_explained_var: 0.10197525978088379\n",
      "        vf_loss: 80.37909932772318\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1811859075228373\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011845433257287066\n",
      "        policy_loss: -0.009685416718324026\n",
      "        total_loss: 51.674898045857745\n",
      "        vf_explained_var: 0.2498652619123459\n",
      "        vf_loss: 51.68221428553263\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2181654739379884\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010189124289900065\n",
      "        policy_loss: -0.0792760259239003\n",
      "        total_loss: 52.98492489496867\n",
      "        vf_explained_var: -0.21424080590407055\n",
      "        vf_loss: 53.06216356277466\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 155970\n",
      "  num_agent_steps_trained: 155970\n",
      "  num_steps_sampled: 156000\n",
      "  num_steps_trained: 156000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 39\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.6375\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.26666666666667\n",
      "  player_1: 26.666666666666664\n",
      "  player_2: 33.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.775333333333333\n",
      "  player_1: -1.1526666666666672\n",
      "  player_2: 1.3773333333333329\n",
      "policy_reward_min:\n",
      "  player_0: -25.53333333333333\n",
      "  player_1: -27.066666666666663\n",
      "  player_2: -42.73333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10261966003824158\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5094146826236662\n",
      "  mean_inference_ms: 1.8821710030567758\n",
      "  mean_raw_obs_processing_ms: 0.2464667383523372\n",
      "time_since_restore: 247.5265440940857\n",
      "time_this_iter_s: 6.241071701049805\n",
      "time_total_s: 247.5265440940857\n",
      "timers:\n",
      "  learn_throughput: 679.513\n",
      "  learn_time_ms: 5886.57\n",
      "  load_throughput: 6387670.284\n",
      "  load_time_ms: 0.626\n",
      "  sample_throughput: 595.959\n",
      "  sample_time_ms: 6711.876\n",
      "  update_time_ms: 3.882\n",
      "timestamp: 1643325314\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 156000\n",
      "training_iteration: 39\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 163968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-15-27\n",
      "done: false\n",
      "episode_len_mean: 170.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 1273\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.131962612271309\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012585391631531214\n",
      "        policy_loss: -0.06438184672500938\n",
      "        total_loss: 50.1771053425471\n",
      "        vf_explained_var: 0.14523785809675852\n",
      "        vf_loss: 50.23897065003713\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1796384569009144\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010887970067612211\n",
      "        policy_loss: -0.06342019895712535\n",
      "        total_loss: 57.8423707262675\n",
      "        vf_explained_var: -0.08729309558868409\n",
      "        vf_loss: 57.90361299196879\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2322623658180236\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013730359695061149\n",
      "        policy_loss: -0.07698705570151408\n",
      "        total_loss: 54.58330522855123\n",
      "        vf_explained_var: -0.03486945947011312\n",
      "        vf_loss: 54.65754662354787\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 163968\n",
      "  num_agent_steps_trained: 163968\n",
      "  num_steps_sampled: 164000\n",
      "  num_steps_trained: 164000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 41\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.112499999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1375\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.333333333333336\n",
      "  player_1: 26.666666666666664\n",
      "  player_2: 26.266666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 4.752666666666666\n",
      "  player_1: -1.6553333333333333\n",
      "  player_2: -0.0973333333333338\n",
      "policy_reward_min:\n",
      "  player_0: -25.53333333333333\n",
      "  player_1: -48.666666666666664\n",
      "  player_2: -42.73333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10302137801436205\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5010209994874391\n",
      "  mean_inference_ms: 1.8913869010263857\n",
      "  mean_raw_obs_processing_ms: 0.247109585154736\n",
      "time_since_restore: 260.5206434726715\n",
      "time_this_iter_s: 6.534060478210449\n",
      "time_total_s: 260.5206434726715\n",
      "timers:\n",
      "  learn_throughput: 672.988\n",
      "  learn_time_ms: 5943.643\n",
      "  load_throughput: 6279368.216\n",
      "  load_time_ms: 0.637\n",
      "  sample_throughput: 592.197\n",
      "  sample_time_ms: 6754.506\n",
      "  update_time_ms: 4.188\n",
      "timestamp: 1643325327\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 164000\n",
      "training_iteration: 41\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 171968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-15-39\n",
      "done: false\n",
      "episode_len_mean: 168.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 1323\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1490650037924448\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012425542313085987\n",
      "        policy_loss: -0.08243232202560952\n",
      "        total_loss: 46.89897219975789\n",
      "        vf_explained_var: -0.05254948496818543\n",
      "        vf_loss: 46.97891922632853\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1631515073776244\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014522637228462068\n",
      "        policy_loss: -0.0957588362817963\n",
      "        total_loss: 79.56214833577474\n",
      "        vf_explained_var: 0.1864158308506012\n",
      "        vf_loss: 79.65500301361084\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2081288397312164\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013310936939178645\n",
      "        policy_loss: -0.06565014588957031\n",
      "        total_loss: 91.56026786168417\n",
      "        vf_explained_var: 0.09600718220074972\n",
      "        vf_loss: 91.62325584411622\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 171968\n",
      "  num_agent_steps_trained: 171968\n",
      "  num_steps_sampled: 172000\n",
      "  num_steps_trained: 172000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 43\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.0625\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 30.333333333333336\n",
      "  player_1: 20.333333333333332\n",
      "  player_2: 29.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 5.361333333333334\n",
      "  player_1: -4.168666666666667\n",
      "  player_2: 1.8073333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -21.0\n",
      "  player_1: -48.666666666666664\n",
      "  player_2: -27.133333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1032260858807542\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.49104035733995494\n",
      "  mean_inference_ms: 1.8936036202984292\n",
      "  mean_raw_obs_processing_ms: 0.24701007028463137\n",
      "time_since_restore: 272.9976363182068\n",
      "time_this_iter_s: 6.190081596374512\n",
      "time_total_s: 272.9976363182068\n",
      "timers:\n",
      "  learn_throughput: 679.733\n",
      "  learn_time_ms: 5884.664\n",
      "  load_throughput: 6263427.163\n",
      "  load_time_ms: 0.639\n",
      "  sample_throughput: 588.616\n",
      "  sample_time_ms: 6795.608\n",
      "  update_time_ms: 4.439\n",
      "timestamp: 1643325339\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 172000\n",
      "training_iteration: 43\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 179968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-15-52\n",
      "done: false\n",
      "episode_len_mean: 174.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 1365\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1209728489319484\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012000925995986714\n",
      "        policy_loss: -0.112899193217357\n",
      "        total_loss: 53.15856725056966\n",
      "        vf_explained_var: 0.06445092976093292\n",
      "        vf_loss: 53.26906634966532\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1215208822488785\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013286109319142678\n",
      "        policy_loss: -0.05807325048372149\n",
      "        total_loss: 73.4510717900594\n",
      "        vf_explained_var: 0.15725964804490408\n",
      "        vf_loss: 73.50648796081543\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1825953209400177\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012804108433482118\n",
      "        policy_loss: -0.09457712891201178\n",
      "        total_loss: 53.4803010781606\n",
      "        vf_explained_var: 0.09856296837329864\n",
      "        vf_loss: 53.57231728553772\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 179968\n",
      "  num_agent_steps_trained: 179968\n",
      "  num_steps_sampled: 180000\n",
      "  num_steps_trained: 180000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 45\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.349999999999994\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.125\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 30.333333333333336\n",
      "  player_1: 21.866666666666664\n",
      "  player_2: 29.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 5.423333333333334\n",
      "  player_1: -4.504666666666667\n",
      "  player_2: 2.0813333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -21.0\n",
      "  player_1: -42.06666666666666\n",
      "  player_2: -29.133333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10308674289343368\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.48403010767967175\n",
      "  mean_inference_ms: 1.8953589184507316\n",
      "  mean_raw_obs_processing_ms: 0.24713246332068972\n",
      "time_since_restore: 285.5978090763092\n",
      "time_this_iter_s: 6.410318374633789\n",
      "time_total_s: 285.5978090763092\n",
      "timers:\n",
      "  learn_throughput: 694.594\n",
      "  learn_time_ms: 5758.756\n",
      "  load_throughput: 6344911.883\n",
      "  load_time_ms: 0.63\n",
      "  sample_throughput: 598.589\n",
      "  sample_time_ms: 6682.378\n",
      "  update_time_ms: 4.25\n",
      "timestamp: 1643325352\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 180000\n",
      "training_iteration: 45\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 187968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-16-05\n",
      "done: false\n",
      "episode_len_mean: 176.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 21\n",
      "episodes_total: 1410\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0863555979728698\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011722959543488065\n",
      "        policy_loss: -0.06681416848053535\n",
      "        total_loss: 71.36489275296529\n",
      "        vf_explained_var: 0.06254389524459839\n",
      "        vf_loss: 71.42936261812847\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.052096252044042\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012315008410014342\n",
      "        policy_loss: -0.0729143976016591\n",
      "        total_loss: 70.70905508677164\n",
      "        vf_explained_var: 0.1299942473570506\n",
      "        vf_loss: 70.77950600941976\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1553404196103414\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013178515660741444\n",
      "        policy_loss: -0.0647164710611105\n",
      "        total_loss: 94.91578406016032\n",
      "        vf_explained_var: 0.07527746756871541\n",
      "        vf_loss: 94.9778647740682\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 187968\n",
      "  num_agent_steps_trained: 187968\n",
      "  num_steps_sampled: 188000\n",
      "  num_steps_trained: 188000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 47\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 23.87777777777778\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.2\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.53333333333333\n",
      "  player_1: 32.33333333333333\n",
      "  player_2: 32.6\n",
      "policy_reward_mean:\n",
      "  player_0: 3.5980000000000003\n",
      "  player_1: -1.892\n",
      "  player_2: 1.2940000000000003\n",
      "policy_reward_min:\n",
      "  player_0: -24.6\n",
      "  player_1: -42.06666666666666\n",
      "  player_2: -40.86666666666666\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10279010005633171\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.478362488978606\n",
      "  mean_inference_ms: 1.8929132774177273\n",
      "  mean_raw_obs_processing_ms: 0.24669239768661375\n",
      "time_since_restore: 298.37435960769653\n",
      "time_this_iter_s: 6.321040868759155\n",
      "time_total_s: 298.37435960769653\n",
      "timers:\n",
      "  learn_throughput: 717.123\n",
      "  learn_time_ms: 5577.842\n",
      "  load_throughput: 6410368.333\n",
      "  load_time_ms: 0.624\n",
      "  sample_throughput: 621.223\n",
      "  sample_time_ms: 6438.907\n",
      "  update_time_ms: 4.267\n",
      "timestamp: 1643325365\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 188000\n",
      "training_iteration: 47\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 195968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-16-18\n",
      "done: false\n",
      "episode_len_mean: 183.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 1456\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1294776753584543\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014309288969767901\n",
      "        policy_loss: -0.03653206517919898\n",
      "        total_loss: 42.04919194539388\n",
      "        vf_explained_var: 0.30450806200504305\n",
      "        vf_loss: 42.08286190668742\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0770191923777261\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017523948309014183\n",
      "        policy_loss: -0.1103832306394664\n",
      "        total_loss: 50.64606182098389\n",
      "        vf_explained_var: -0.0005396968126296997\n",
      "        vf_loss: 50.752940346399946\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.131415974299113\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011738422625185194\n",
      "        policy_loss: -0.07617789720495542\n",
      "        total_loss: 57.186966489156084\n",
      "        vf_explained_var: -0.33075911323229473\n",
      "        vf_loss: 57.2607966264089\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 195968\n",
      "  num_agent_steps_trained: 195968\n",
      "  num_steps_sampled: 196000\n",
      "  num_steps_trained: 196000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 49\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.0375\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.2\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.53333333333333\n",
      "  player_1: 32.33333333333333\n",
      "  player_2: 32.6\n",
      "policy_reward_mean:\n",
      "  player_0: 3.4166666666666674\n",
      "  player_1: -2.6933333333333334\n",
      "  player_2: 2.2766666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -28.666666666666664\n",
      "  player_1: -42.06666666666666\n",
      "  player_2: -40.86666666666666\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10307562683781075\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.47234520385309364\n",
      "  mean_inference_ms: 1.897393556807212\n",
      "  mean_raw_obs_processing_ms: 0.2469019595913267\n",
      "time_since_restore: 311.2305886745453\n",
      "time_this_iter_s: 6.301900148391724\n",
      "time_total_s: 311.2305886745453\n",
      "timers:\n",
      "  learn_throughput: 713.9\n",
      "  learn_time_ms: 5603.024\n",
      "  load_throughput: 6308647.063\n",
      "  load_time_ms: 0.634\n",
      "  sample_throughput: 624.338\n",
      "  sample_time_ms: 6406.79\n",
      "  update_time_ms: 4.734\n",
      "timestamp: 1643325378\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 196000\n",
      "training_iteration: 49\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 203968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-16-31\n",
      "done: false\n",
      "episode_len_mean: 185.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 21\n",
      "episodes_total: 1497\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0506920703252156\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016948768583267035\n",
      "        policy_loss: -0.06527908482744048\n",
      "        total_loss: 60.72089332898458\n",
      "        vf_explained_var: -0.05339914083480835\n",
      "        vf_loss: 60.782782834370934\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.032112263639768\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012037852633657167\n",
      "        policy_loss: -0.07822745171996455\n",
      "        total_loss: 36.37291656653086\n",
      "        vf_explained_var: 0.19450334350268045\n",
      "        vf_loss: 36.44873655001322\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1169413065910339\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009303873973719116\n",
      "        policy_loss: -0.054146477629741036\n",
      "        total_loss: 52.976042362848915\n",
      "        vf_explained_var: 0.09177458624045054\n",
      "        vf_loss: 53.02832793235779\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 203968\n",
      "  num_agent_steps_trained: 203968\n",
      "  num_steps_sampled: 204000\n",
      "  num_steps_trained: 204000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 51\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 23.92222222222222\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.2\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.6\n",
      "  player_1: 32.33333333333333\n",
      "  player_2: 28.0\n",
      "policy_reward_mean:\n",
      "  player_0: 5.166\n",
      "  player_1: -3.032\n",
      "  player_2: 0.866\n",
      "policy_reward_min:\n",
      "  player_0: -28.666666666666664\n",
      "  player_1: -31.4\n",
      "  player_2: -32.199999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10311593173258977\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4667562312870283\n",
      "  mean_inference_ms: 1.9019664474310225\n",
      "  mean_raw_obs_processing_ms: 0.24721376304838777\n",
      "time_since_restore: 324.080956697464\n",
      "time_this_iter_s: 6.4221086502075195\n",
      "time_total_s: 324.080956697464\n",
      "timers:\n",
      "  learn_throughput: 715.44\n",
      "  learn_time_ms: 5590.961\n",
      "  load_throughput: 6277723.48\n",
      "  load_time_ms: 0.637\n",
      "  sample_throughput: 625.596\n",
      "  sample_time_ms: 6393.9\n",
      "  update_time_ms: 4.514\n",
      "timestamp: 1643325391\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 204000\n",
      "training_iteration: 51\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 211968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-16-45\n",
      "done: false\n",
      "episode_len_mean: 176.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 1545\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0764370775222778\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011687500137025684\n",
      "        policy_loss: -0.09889924630522728\n",
      "        total_loss: 76.7324939695994\n",
      "        vf_explained_var: -0.08062787810961405\n",
      "        vf_loss: 76.82905590693156\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0034585837523142\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012075935336130593\n",
      "        policy_loss: -0.08105950693289439\n",
      "        total_loss: 63.45984644889832\n",
      "        vf_explained_var: 0.038191896279652915\n",
      "        vf_loss: 63.53849058310191\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0668001683553059\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011887034413202855\n",
      "        policy_loss: -0.027220099369684857\n",
      "        total_loss: 47.520349985758465\n",
      "        vf_explained_var: 0.2853630954027176\n",
      "        vf_loss: 47.54519276936849\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 211968\n",
      "  num_agent_steps_trained: 211968\n",
      "  num_steps_sampled: 212000\n",
      "  num_steps_trained: 212000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 53\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.31111111111111\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.666666666666664\n",
      "  player_1: 26.666666666666664\n",
      "  player_2: 29.133333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 6.4639999999999995\n",
      "  player_1: -4.83\n",
      "  player_2: 1.3659999999999997\n",
      "policy_reward_min:\n",
      "  player_0: -22.53333333333333\n",
      "  player_1: -33.93333333333333\n",
      "  player_2: -32.199999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10307373650397818\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4606207112399609\n",
      "  mean_inference_ms: 1.9026488380703102\n",
      "  mean_raw_obs_processing_ms: 0.24764323219076087\n",
      "time_since_restore: 338.2577078342438\n",
      "time_this_iter_s: 6.637676239013672\n",
      "time_total_s: 338.2577078342438\n",
      "timers:\n",
      "  learn_throughput: 697.275\n",
      "  learn_time_ms: 5736.622\n",
      "  load_throughput: 6129559.022\n",
      "  load_time_ms: 0.653\n",
      "  sample_throughput: 611.803\n",
      "  sample_time_ms: 6538.05\n",
      "  update_time_ms: 4.146\n",
      "timestamp: 1643325405\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 212000\n",
      "training_iteration: 53\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 219970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-16-59\n",
      "done: false\n",
      "episode_len_mean: 182.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 1585\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0478055729468663\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013433494264249533\n",
      "        policy_loss: 0.02945435371560355\n",
      "        total_loss: 30.576061493555706\n",
      "        vf_explained_var: 0.23572687566280365\n",
      "        vf_loss: 30.543920437494915\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9662421176830928\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01576725004594967\n",
      "        policy_loss: -0.08543909418551872\n",
      "        total_loss: 23.64999992529551\n",
      "        vf_explained_var: 0.0803858749071757\n",
      "        vf_loss: 23.732285575866698\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0656499801079433\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01245568764904038\n",
      "        policy_loss: -0.1380553446469518\n",
      "        total_loss: 47.50618708292643\n",
      "        vf_explained_var: 0.21831998149553936\n",
      "        vf_loss: 47.64175147692362\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 219970\n",
      "  num_agent_steps_trained: 219970\n",
      "  num_steps_sampled: 220000\n",
      "  num_steps_trained: 220000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 55\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.03333333333333\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.111111111111114\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.666666666666664\n",
      "  player_1: 27.333333333333336\n",
      "  player_2: 29.133333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 4.705333333333333\n",
      "  player_1: -3.0666666666666673\n",
      "  player_2: 1.361333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -22.53333333333333\n",
      "  player_1: -33.93333333333333\n",
      "  player_2: -35.800000000000004\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10331291510611619\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4563760882029019\n",
      "  mean_inference_ms: 1.9090100375242582\n",
      "  mean_raw_obs_processing_ms: 0.24831451971933802\n",
      "time_since_restore: 352.4692802429199\n",
      "time_this_iter_s: 7.256039142608643\n",
      "time_total_s: 352.4692802429199\n",
      "timers:\n",
      "  learn_throughput: 680.894\n",
      "  learn_time_ms: 5874.63\n",
      "  load_throughput: 5973515.631\n",
      "  load_time_ms: 0.67\n",
      "  sample_throughput: 599.98\n",
      "  sample_time_ms: 6666.887\n",
      "  update_time_ms: 4.899\n",
      "timestamp: 1643325419\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 220000\n",
      "training_iteration: 55\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 227968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-17-13\n",
      "done: false\n",
      "episode_len_mean: 176.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 1634\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9788697936137517\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011559220342084398\n",
      "        policy_loss: -0.0949748745188117\n",
      "        total_loss: 56.32193707784017\n",
      "        vf_explained_var: 0.0640524031718572\n",
      "        vf_loss: 56.41460014820099\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9729871239264806\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012793321480930899\n",
      "        policy_loss: -0.067697270559147\n",
      "        total_loss: 74.00409832636515\n",
      "        vf_explained_var: 0.04506775140762329\n",
      "        vf_loss: 74.06923730214437\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.058430829246839\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012922771388925879\n",
      "        policy_loss: -0.08422288893877218\n",
      "        total_loss: 30.774069256782532\n",
      "        vf_explained_var: 0.1118252565463384\n",
      "        vf_loss: 30.85570787747701\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 227968\n",
      "  num_agent_steps_trained: 227968\n",
      "  num_steps_sampled: 228000\n",
      "  num_steps_trained: 228000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 57\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.200000000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.14444444444445\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 27.333333333333336\n",
      "  player_2: 28.53333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 4.626666666666668\n",
      "  player_1: -0.9033333333333338\n",
      "  player_2: -0.7233333333333337\n",
      "policy_reward_min:\n",
      "  player_0: -20.93333333333333\n",
      "  player_1: -29.0\n",
      "  player_2: -35.800000000000004\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10413123929671428\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.45247815617134135\n",
      "  mean_inference_ms: 1.917856958278091\n",
      "  mean_raw_obs_processing_ms: 0.24887719618968585\n",
      "time_since_restore: 365.6408586502075\n",
      "time_this_iter_s: 6.723308324813843\n",
      "time_total_s: 365.6408586502075\n",
      "timers:\n",
      "  learn_throughput: 677.015\n",
      "  learn_time_ms: 5908.289\n",
      "  load_throughput: 5957395.071\n",
      "  load_time_ms: 0.671\n",
      "  sample_throughput: 593.683\n",
      "  sample_time_ms: 6737.607\n",
      "  update_time_ms: 5.301\n",
      "timestamp: 1643325433\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 228000\n",
      "training_iteration: 57\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 235969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-17-26\n",
      "done: false\n",
      "episode_len_mean: 168.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 1680\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9967003484567006\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013373358039389131\n",
      "        policy_loss: -0.11041403052086632\n",
      "        total_loss: 49.75476536432902\n",
      "        vf_explained_var: 0.08805794318517049\n",
      "        vf_loss: 49.86250507990519\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9535520982742309\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012978384973515252\n",
      "        policy_loss: -0.06297826215624809\n",
      "        total_loss: 95.82632729530334\n",
      "        vf_explained_var: 0.08168180942535401\n",
      "        vf_loss: 95.88671024004618\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0463756660620371\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013334054978913627\n",
      "        policy_loss: -0.06774863272362078\n",
      "        total_loss: 73.4294313621521\n",
      "        vf_explained_var: -0.01862223168214162\n",
      "        vf_loss: 73.49451283613841\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 235969\n",
      "  num_agent_steps_trained: 235969\n",
      "  num_steps_sampled: 236000\n",
      "  num_steps_trained: 236000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 59\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.08888888888889\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 29.0\n",
      "  player_2: 28.53333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 5.414666666666665\n",
      "  player_1: -0.9893333333333335\n",
      "  player_2: -1.4253333333333336\n",
      "policy_reward_min:\n",
      "  player_0: -26.666666666666664\n",
      "  player_1: -34.333333333333336\n",
      "  player_2: -42.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10446707325505983\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4485392649039986\n",
      "  mean_inference_ms: 1.9233621156541838\n",
      "  mean_raw_obs_processing_ms: 0.24943202202552128\n",
      "time_since_restore: 378.8247301578522\n",
      "time_this_iter_s: 6.5030295848846436\n",
      "time_total_s: 378.8247301578522\n",
      "timers:\n",
      "  learn_throughput: 672.738\n",
      "  learn_time_ms: 5945.851\n",
      "  load_throughput: 6013554.608\n",
      "  load_time_ms: 0.665\n",
      "  sample_throughput: 590.671\n",
      "  sample_time_ms: 6771.957\n",
      "  update_time_ms: 5.103\n",
      "timestamp: 1643325446\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 236000\n",
      "training_iteration: 59\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 243968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-17-39\n",
      "done: false\n",
      "episode_len_mean: 176.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 21\n",
      "episodes_total: 1722\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9679272850354512\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01238035130193263\n",
      "        policy_loss: -0.019202006643948454\n",
      "        total_loss: 23.25346237341563\n",
      "        vf_explained_var: 0.20171293437480928\n",
      "        vf_loss: 23.270188407897948\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9393914570411046\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010657597796477021\n",
      "        policy_loss: -0.1355238404062887\n",
      "        total_loss: 23.63267560640971\n",
      "        vf_explained_var: 0.2212301516532898\n",
      "        vf_loss: 23.766068048477173\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0057523455222448\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01276247971793642\n",
      "        policy_loss: -0.036033748771684866\n",
      "        total_loss: 29.218725214004518\n",
      "        vf_explained_var: -0.22865806460380556\n",
      "        vf_loss: 29.252206292152405\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 243968\n",
      "  num_agent_steps_trained: 243968\n",
      "  num_steps_sampled: 244000\n",
      "  num_steps_trained: 244000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 61\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.900000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.12222222222223\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 29.0\n",
      "  player_2: 26.79999999999999\n",
      "policy_reward_mean:\n",
      "  player_0: 5.407333333333334\n",
      "  player_1: -2.356666666666667\n",
      "  player_2: -0.05066666666666695\n",
      "policy_reward_min:\n",
      "  player_0: -26.666666666666664\n",
      "  player_1: -34.333333333333336\n",
      "  player_2: -42.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10450464235673267\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.44534451316963497\n",
      "  mean_inference_ms: 1.922863346436842\n",
      "  mean_raw_obs_processing_ms: 0.24949678134742154\n",
      "time_since_restore: 392.117299079895\n",
      "time_this_iter_s: 6.671338081359863\n",
      "time_total_s: 392.117299079895\n",
      "timers:\n",
      "  learn_throughput: 668.073\n",
      "  learn_time_ms: 5987.369\n",
      "  load_throughput: 6100805.818\n",
      "  load_time_ms: 0.656\n",
      "  sample_throughput: 586.248\n",
      "  sample_time_ms: 6823.047\n",
      "  update_time_ms: 5.033\n",
      "timestamp: 1643325459\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 244000\n",
      "training_iteration: 61\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 251968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-17-54\n",
      "done: false\n",
      "episode_len_mean: 188.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1763\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9128638726472854\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014121698798050298\n",
      "        policy_loss: -0.08472589815656344\n",
      "        total_loss: 42.11360535621643\n",
      "        vf_explained_var: 0.3027867150306702\n",
      "        vf_loss: 42.19550668080648\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9699267307917278\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014892973539559347\n",
      "        policy_loss: -0.07555881140132745\n",
      "        total_loss: 48.60960533301036\n",
      "        vf_explained_var: -0.1796436736981074\n",
      "        vf_loss: 48.6821853685379\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0289779847860336\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012898955455893884\n",
      "        policy_loss: -0.1059316076276203\n",
      "        total_loss: 37.69312394460042\n",
      "        vf_explained_var: 0.3873212269941966\n",
      "        vf_loss: 37.796475529670715\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 251968\n",
      "  num_agent_steps_trained: 251968\n",
      "  num_steps_sampled: 252000\n",
      "  num_steps_trained: 252000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 63\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.89\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.17\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.0\n",
      "  player_1: 29.0\n",
      "  player_2: 26.79999999999999\n",
      "policy_reward_mean:\n",
      "  player_0: 5.707999999999999\n",
      "  player_1: -1.6800000000000006\n",
      "  player_2: -1.0280000000000002\n",
      "policy_reward_min:\n",
      "  player_0: -25.6\n",
      "  player_1: -24.8\n",
      "  player_2: -37.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1048999496433522\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4415149813911034\n",
      "  mean_inference_ms: 1.9278735601007266\n",
      "  mean_raw_obs_processing_ms: 0.2501181629668523\n",
      "time_since_restore: 407.39549565315247\n",
      "time_this_iter_s: 7.61260724067688\n",
      "time_total_s: 407.39549565315247\n",
      "timers:\n",
      "  learn_throughput: 656.901\n",
      "  learn_time_ms: 6089.194\n",
      "  load_throughput: 6293265.314\n",
      "  load_time_ms: 0.636\n",
      "  sample_throughput: 583.897\n",
      "  sample_time_ms: 6850.523\n",
      "  update_time_ms: 5.869\n",
      "timestamp: 1643325474\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 252000\n",
      "training_iteration: 63\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 259968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-18-09\n",
      "done: false\n",
      "episode_len_mean: 192.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 1807\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8961002824703852\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011034492885593251\n",
      "        policy_loss: -0.15698050785732146\n",
      "        total_loss: 38.92300026257833\n",
      "        vf_explained_var: 0.08152968347072602\n",
      "        vf_loss: 39.07777357578278\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9597181862592697\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017201837788961713\n",
      "        policy_loss: -0.034378726937187215\n",
      "        total_loss: 46.259613269170124\n",
      "        vf_explained_var: 0.08995827356974284\n",
      "        vf_loss: 46.290552085240684\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0218104688326517\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01432919109262313\n",
      "        policy_loss: -0.04125646624714136\n",
      "        total_loss: 32.529483489990234\n",
      "        vf_explained_var: 0.4045853519439697\n",
      "        vf_loss: 32.567873907089236\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 259968\n",
      "  num_agent_steps_trained: 259968\n",
      "  num_steps_sampled: 260000\n",
      "  num_steps_trained: 260000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 65\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.255555555555556\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.15555555555556\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.0\n",
      "  player_1: 28.73333333333333\n",
      "  player_2: 26.79999999999999\n",
      "policy_reward_mean:\n",
      "  player_0: 5.540666666666666\n",
      "  player_1: -0.8353333333333337\n",
      "  player_2: -1.7053333333333336\n",
      "policy_reward_min:\n",
      "  player_0: -25.6\n",
      "  player_1: -38.333333333333336\n",
      "  player_2: -34.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10552786068904041\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4387581452209378\n",
      "  mean_inference_ms: 1.9383365808377595\n",
      "  mean_raw_obs_processing_ms: 0.25150255577830555\n",
      "time_since_restore: 421.96581053733826\n",
      "time_this_iter_s: 7.063159465789795\n",
      "time_total_s: 421.96581053733826\n",
      "timers:\n",
      "  learn_throughput: 652.955\n",
      "  learn_time_ms: 6125.997\n",
      "  load_throughput: 6435696.037\n",
      "  load_time_ms: 0.622\n",
      "  sample_throughput: 572.234\n",
      "  sample_time_ms: 6990.15\n",
      "  update_time_ms: 5.111\n",
      "timestamp: 1643325489\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 260000\n",
      "training_iteration: 65\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 267968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-18-23\n",
      "done: false\n",
      "episode_len_mean: 192.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 17\n",
      "episodes_total: 1843\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8778874729077021\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013916538981672298\n",
      "        policy_loss: -0.07376104573408762\n",
      "        total_loss: 46.53956689039866\n",
      "        vf_explained_var: -0.15812283198038737\n",
      "        vf_loss: 46.610544551213586\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8863182892402013\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01777818938568089\n",
      "        policy_loss: -0.018341081260393063\n",
      "        total_loss: 44.98769486665726\n",
      "        vf_explained_var: -0.12921126067638397\n",
      "        vf_loss: 45.002480148474376\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.937438584168752\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010050382134591586\n",
      "        policy_loss: -0.1290004703340431\n",
      "        total_loss: 33.26525737285614\n",
      "        vf_explained_var: 0.10627235988775889\n",
      "        vf_loss: 33.39224758783976\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 267968\n",
      "  num_agent_steps_trained: 267968\n",
      "  num_steps_sampled: 268000\n",
      "  num_steps_trained: 268000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 67\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.922222222222224\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.15555555555556\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.0\n",
      "  player_1: 28.73333333333333\n",
      "  player_2: 27.066666666666663\n",
      "policy_reward_mean:\n",
      "  player_0: 5.867333333333335\n",
      "  player_1: -2.0446666666666666\n",
      "  player_2: -0.8226666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -23.733333333333327\n",
      "  player_1: -38.333333333333336\n",
      "  player_2: -37.93333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10615878230945024\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.43754138183782265\n",
      "  mean_inference_ms: 1.9463054665370558\n",
      "  mean_raw_obs_processing_ms: 0.25227936310372473\n",
      "time_since_restore: 435.76897287368774\n",
      "time_this_iter_s: 6.769323110580444\n",
      "time_total_s: 435.76897287368774\n",
      "timers:\n",
      "  learn_throughput: 646.734\n",
      "  learn_time_ms: 6184.927\n",
      "  load_throughput: 5878903.918\n",
      "  load_time_ms: 0.68\n",
      "  sample_throughput: 568.361\n",
      "  sample_time_ms: 7037.784\n",
      "  update_time_ms: 4.803\n",
      "timestamp: 1643325503\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 268000\n",
      "training_iteration: 67\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 275968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-18-36\n",
      "done: false\n",
      "episode_len_mean: 202.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 19\n",
      "episodes_total: 1883\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8750817541281383\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0119837390944114\n",
      "        policy_loss: -0.047599946409463884\n",
      "        total_loss: 43.81559640010198\n",
      "        vf_explained_var: 0.2603597621122996\n",
      "        vf_loss: 43.860799730618794\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9131541782617569\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013534176507528172\n",
      "        policy_loss: -0.12499878600239754\n",
      "        total_loss: 21.999845496813457\n",
      "        vf_explained_var: 0.042532452742258704\n",
      "        vf_loss: 22.122137457529703\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.96537355919679\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013559490589001749\n",
      "        policy_loss: -0.0561500980417865\n",
      "        total_loss: 52.4513561185201\n",
      "        vf_explained_var: 0.03986221174399058\n",
      "        vf_loss: 52.504794406890866\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 275968\n",
      "  num_agent_steps_trained: 275968\n",
      "  num_steps_sampled: 276000\n",
      "  num_steps_trained: 276000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 69\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.244444444444447\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.13333333333334\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.46666666666666\n",
      "  player_1: 28.73333333333333\n",
      "  player_2: 29.133333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 6.280666666666666\n",
      "  player_1: -2.1933333333333334\n",
      "  player_2: -1.087333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -23.733333333333327\n",
      "  player_1: -29.666666666666664\n",
      "  player_2: -37.93333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10643179855513632\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4350747269663671\n",
      "  mean_inference_ms: 1.950706550791875\n",
      "  mean_raw_obs_processing_ms: 0.25275371014975645\n",
      "time_since_restore: 449.1429283618927\n",
      "time_this_iter_s: 6.943349361419678\n",
      "time_total_s: 449.1429283618927\n",
      "timers:\n",
      "  learn_throughput: 645.76\n",
      "  learn_time_ms: 6194.253\n",
      "  load_throughput: 5852653.318\n",
      "  load_time_ms: 0.683\n",
      "  sample_throughput: 568.75\n",
      "  sample_time_ms: 7032.973\n",
      "  update_time_ms: 4.637\n",
      "timestamp: 1643325516\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 276000\n",
      "training_iteration: 69\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 283968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-18-49\n",
      "done: false\n",
      "episode_len_mean: 208.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 1926\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.902332558631897\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014784663338008007\n",
      "        policy_loss: -0.07857218231928224\n",
      "        total_loss: 41.51181907018026\n",
      "        vf_explained_var: -0.11580636064211527\n",
      "        vf_loss: 41.58743411064148\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9126531650622686\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01434316576547341\n",
      "        policy_loss: -0.11880959873708585\n",
      "        total_loss: 31.33832124233246\n",
      "        vf_explained_var: 0.29086409787336986\n",
      "        vf_loss: 31.45426235516866\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0069191519419352\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014717643161469217\n",
      "        policy_loss: -0.028361195599039397\n",
      "        total_loss: 28.56792084058126\n",
      "        vf_explained_var: -0.07390057424704234\n",
      "        vf_loss: 28.593338464101155\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 283968\n",
      "  num_agent_steps_trained: 283968\n",
      "  num_steps_sampled: 284000\n",
      "  num_steps_trained: 284000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 71\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.325\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.125\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.46666666666666\n",
      "  player_1: 23.133333333333336\n",
      "  player_2: 29.133333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 7.464666666666668\n",
      "  player_1: -2.7073333333333336\n",
      "  player_2: -1.7573333333333325\n",
      "policy_reward_min:\n",
      "  player_0: -23.733333333333327\n",
      "  player_1: -29.666666666666664\n",
      "  player_2: -37.93333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10662395287651302\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4321744707347478\n",
      "  mean_inference_ms: 1.953159455048663\n",
      "  mean_raw_obs_processing_ms: 0.2529814396813621\n",
      "time_since_restore: 461.87788224220276\n",
      "time_this_iter_s: 6.317238092422485\n",
      "time_total_s: 461.87788224220276\n",
      "timers:\n",
      "  learn_throughput: 651.247\n",
      "  learn_time_ms: 6142.063\n",
      "  load_throughput: 5893773.625\n",
      "  load_time_ms: 0.679\n",
      "  sample_throughput: 567.61\n",
      "  sample_time_ms: 7047.096\n",
      "  update_time_ms: 4.59\n",
      "timestamp: 1643325529\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 284000\n",
      "training_iteration: 71\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 291968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-19-03\n",
      "done: false\n",
      "episode_len_mean: 197.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 18\n",
      "episodes_total: 1961\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8181817285219828\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014857862200338386\n",
      "        policy_loss: -0.09331951975822449\n",
      "        total_loss: 22.802693939208986\n",
      "        vf_explained_var: 0.014067799051602681\n",
      "        vf_loss: 22.89304194132487\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9511431314547857\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013010657863730254\n",
      "        policy_loss: -0.06855507552313309\n",
      "        total_loss: 43.35525094191233\n",
      "        vf_explained_var: 0.010822781324386598\n",
      "        vf_loss: 43.42120379924774\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9466130151351293\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011170470188584053\n",
      "        policy_loss: -0.07430091668851674\n",
      "        total_loss: 41.74472813606262\n",
      "        vf_explained_var: 0.01929659187793732\n",
      "        vf_loss: 41.816795081297556\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 291968\n",
      "  num_agent_steps_trained: 291968\n",
      "  num_steps_sampled: 292000\n",
      "  num_steps_trained: 292000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 73\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.200000000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.2\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 30.666666666666664\n",
      "  player_1: 23.133333333333336\n",
      "  player_2: 29.133333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 8.074666666666667\n",
      "  player_1: -1.6833333333333338\n",
      "  player_2: -3.391333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -23.46666666666667\n",
      "  player_1: -22.666666666666668\n",
      "  player_2: -25.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10663409931864773\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4295306573322318\n",
      "  mean_inference_ms: 1.9556266152942599\n",
      "  mean_raw_obs_processing_ms: 0.25296991781436645\n",
      "time_since_restore: 475.8229134082794\n",
      "time_this_iter_s: 7.165371656417847\n",
      "time_total_s: 475.8229134082794\n",
      "timers:\n",
      "  learn_throughput: 664.024\n",
      "  learn_time_ms: 6023.881\n",
      "  load_throughput: 5838396.437\n",
      "  load_time_ms: 0.685\n",
      "  sample_throughput: 577.897\n",
      "  sample_time_ms: 6921.648\n",
      "  update_time_ms: 3.887\n",
      "timestamp: 1643325543\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 292000\n",
      "training_iteration: 73\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 299969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-19-16\n",
      "done: false\n",
      "episode_len_mean: 191.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2004\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7893860812981923\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01331908671592828\n",
      "        policy_loss: -0.07510854418079059\n",
      "        total_loss: 61.97131892681122\n",
      "        vf_explained_var: 0.07996050397555034\n",
      "        vf_loss: 62.0437638870875\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8738352368275325\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015089999377111477\n",
      "        policy_loss: -0.038251929810891545\n",
      "        total_loss: 30.642607094446817\n",
      "        vf_explained_var: 0.11811839898427327\n",
      "        vf_loss: 30.677840989430745\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9822510276238123\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01224831064590641\n",
      "        policy_loss: -0.09622224388178438\n",
      "        total_loss: 61.0521136935552\n",
      "        vf_explained_var: -0.05084062457084656\n",
      "        vf_loss: 61.1458865181605\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 299969\n",
      "  num_agent_steps_trained: 299969\n",
      "  num_steps_sampled: 300000\n",
      "  num_steps_trained: 300000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 75\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.549999999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.2\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 38.666666666666664\n",
      "  player_1: 23.133333333333336\n",
      "  player_2: 22.0\n",
      "policy_reward_mean:\n",
      "  player_0: 8.110666666666665\n",
      "  player_1: -0.2513333333333339\n",
      "  player_2: -4.859333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -23.46666666666667\n",
      "  player_1: -27.333333333333336\n",
      "  player_2: -39.06666666666666\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10691574651881788\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.42713759075080665\n",
      "  mean_inference_ms: 1.960718428339373\n",
      "  mean_raw_obs_processing_ms: 0.2535596868934703\n",
      "time_since_restore: 488.8024170398712\n",
      "time_this_iter_s: 6.5093302726745605\n",
      "time_total_s: 488.8024170398712\n",
      "timers:\n",
      "  learn_throughput: 680.938\n",
      "  learn_time_ms: 5874.251\n",
      "  load_throughput: 5757254.727\n",
      "  load_time_ms: 0.695\n",
      "  sample_throughput: 591.087\n",
      "  sample_time_ms: 6767.19\n",
      "  update_time_ms: 3.886\n",
      "timestamp: 1643325556\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 300000\n",
      "training_iteration: 75\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 307969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-19-29\n",
      "done: false\n",
      "episode_len_mean: 192.84\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 2048\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8190984086195627\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014815559016121067\n",
      "        policy_loss: -0.05031955675532421\n",
      "        total_loss: 34.41374084790548\n",
      "        vf_explained_var: -0.08181902865568796\n",
      "        vf_loss: 34.46109753926595\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9037808674573898\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014374324774792816\n",
      "        policy_loss: -0.10037055845061937\n",
      "        total_loss: 36.707043755849206\n",
      "        vf_explained_var: 0.3098715114593506\n",
      "        vf_loss: 36.80453953742981\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9945508040984472\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01352549314332767\n",
      "        policy_loss: -0.09743033233253906\n",
      "        total_loss: 57.498691940307616\n",
      "        vf_explained_var: 0.15010198255379995\n",
      "        vf_loss: 57.593417453765866\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 307969\n",
      "  num_agent_steps_trained: 307969\n",
      "  num_steps_sampled: 308000\n",
      "  num_steps_trained: 308000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 77\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.575\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.125\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 38.666666666666664\n",
      "  player_1: 33.06666666666666\n",
      "  player_2: 22.0\n",
      "policy_reward_mean:\n",
      "  player_0: 8.428\n",
      "  player_1: 0.2939999999999998\n",
      "  player_2: -5.7219999999999995\n",
      "policy_reward_min:\n",
      "  player_0: -26.933333333333337\n",
      "  player_1: -32.53333333333333\n",
      "  player_2: -39.06666666666666\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10716292789810486\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4252878072026452\n",
      "  mean_inference_ms: 1.9634000394678526\n",
      "  mean_raw_obs_processing_ms: 0.25404782345618426\n",
      "time_since_restore: 501.9394392967224\n",
      "time_this_iter_s: 6.439975023269653\n",
      "time_total_s: 501.9394392967224\n",
      "timers:\n",
      "  learn_throughput: 688.49\n",
      "  learn_time_ms: 5809.813\n",
      "  load_throughput: 6340116.393\n",
      "  load_time_ms: 0.631\n",
      "  sample_throughput: 598.181\n",
      "  sample_time_ms: 6686.942\n",
      "  update_time_ms: 4.171\n",
      "timestamp: 1643325569\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 308000\n",
      "training_iteration: 77\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 315968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-19-43\n",
      "done: false\n",
      "episode_len_mean: 192.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 2091\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8495819455385208\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014555592434941598\n",
      "        policy_loss: -0.039424917635818325\n",
      "        total_loss: 45.983304368654885\n",
      "        vf_explained_var: 0.07478217899799347\n",
      "        vf_loss: 46.01981806834539\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8317520229021708\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014545273833718966\n",
      "        policy_loss: -0.11256259685382247\n",
      "        total_loss: 41.484883550008135\n",
      "        vf_explained_var: -0.05503718654314677\n",
      "        vf_loss: 41.594537038803104\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9485344644387563\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01335250820799653\n",
      "        policy_loss: -0.07869824849069118\n",
      "        total_loss: 70.26486923535664\n",
      "        vf_explained_var: 0.048097625573476153\n",
      "        vf_loss: 70.34089690526326\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 315968\n",
      "  num_agent_steps_trained: 315968\n",
      "  num_steps_sampled: 316000\n",
      "  num_steps_trained: 316000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 79\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.37777777777778\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.14444444444445\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.53333333333333\n",
      "  player_1: 33.06666666666666\n",
      "  player_2: 28.133333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 6.148666666666667\n",
      "  player_1: 0.5166666666666663\n",
      "  player_2: -3.665333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -28.53333333333333\n",
      "  player_1: -32.53333333333333\n",
      "  player_2: -43.533333333333324\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10712454545728888\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4228654319030037\n",
      "  mean_inference_ms: 1.964264587037912\n",
      "  mean_raw_obs_processing_ms: 0.2541759326885934\n",
      "time_since_restore: 515.7477035522461\n",
      "time_this_iter_s: 7.07708740234375\n",
      "time_total_s: 515.7477035522461\n",
      "timers:\n",
      "  learn_throughput: 682.845\n",
      "  learn_time_ms: 5857.843\n",
      "  load_throughput: 6299882.092\n",
      "  load_time_ms: 0.635\n",
      "  sample_throughput: 599.404\n",
      "  sample_time_ms: 6673.297\n",
      "  update_time_ms: 4.168\n",
      "timestamp: 1643325583\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 316000\n",
      "training_iteration: 79\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 323968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-19-57\n",
      "done: false\n",
      "episode_len_mean: 186.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 2134\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8453601245085398\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014534685218247129\n",
      "        policy_loss: -0.016423708020399014\n",
      "        total_loss: 50.14238927523295\n",
      "        vf_explained_var: 0.07211561183134715\n",
      "        vf_loss: 50.155905952453615\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9047556255261103\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014921865411063967\n",
      "        policy_loss: -0.10185051347749928\n",
      "        total_loss: 27.638152219454447\n",
      "        vf_explained_var: 0.21201250473658242\n",
      "        vf_loss: 27.737018284797667\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9632076384623846\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013650719422839757\n",
      "        policy_loss: -0.10409826737828552\n",
      "        total_loss: 46.44679417292277\n",
      "        vf_explained_var: 0.03514304002126058\n",
      "        vf_loss: 46.54816292921702\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 323968\n",
      "  num_agent_steps_trained: 323968\n",
      "  num_steps_sampled: 324000\n",
      "  num_steps_trained: 324000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 81\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.755555555555553\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.2\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 27.46666666666667\n",
      "  player_1: 29.399999999999995\n",
      "  player_2: 28.133333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 4.115333333333333\n",
      "  player_1: 0.3333333333333327\n",
      "  player_2: -1.448666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -29.8\n",
      "  player_1: -32.53333333333333\n",
      "  player_2: -43.533333333333324\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10715866617420526\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.42044835126623004\n",
      "  mean_inference_ms: 1.965638953964049\n",
      "  mean_raw_obs_processing_ms: 0.25418136279663917\n",
      "time_since_restore: 529.0221507549286\n",
      "time_this_iter_s: 6.666136026382446\n",
      "time_total_s: 529.0221507549286\n",
      "timers:\n",
      "  learn_throughput: 677.345\n",
      "  learn_time_ms: 5905.409\n",
      "  load_throughput: 6209184.308\n",
      "  load_time_ms: 0.644\n",
      "  sample_throughput: 596.223\n",
      "  sample_time_ms: 6708.9\n",
      "  update_time_ms: 4.401\n",
      "timestamp: 1643325597\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 324000\n",
      "training_iteration: 81\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 331968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-20-12\n",
      "done: false\n",
      "episode_len_mean: 175.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 2182\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7396999822060267\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013333607652990716\n",
      "        policy_loss: -0.07769455100720127\n",
      "        total_loss: 41.153507348696394\n",
      "        vf_explained_var: 0.1549412602186203\n",
      "        vf_loss: 41.228535294532776\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8739569238821665\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01371114490759435\n",
      "        policy_loss: -0.02384818240844955\n",
      "        total_loss: 27.974089857737223\n",
      "        vf_explained_var: 0.012126710017522175\n",
      "        vf_loss: 27.995195709864298\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.998199635942777\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015498380023661109\n",
      "        policy_loss: -0.10588781967371082\n",
      "        total_loss: 64.72277726173401\n",
      "        vf_explained_var: 0.1638489822546641\n",
      "        vf_loss: 64.82556575457255\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 331968\n",
      "  num_agent_steps_trained: 331968\n",
      "  num_steps_sampled: 332000\n",
      "  num_steps_trained: 332000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 83\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.663636363636364\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.10909090909092\n",
      "  vram_util_percent0: 0.1005859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.199999999999996\n",
      "  player_1: 22.066666666666663\n",
      "  player_2: 38.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 6.084666666666667\n",
      "  player_1: 0.33866666666666634\n",
      "  player_2: -3.423333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -29.8\n",
      "  player_1: -34.800000000000004\n",
      "  player_2: -43.533333333333324\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10731508833394585\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.41872623033702316\n",
      "  mean_inference_ms: 1.9695548099709592\n",
      "  mean_raw_obs_processing_ms: 0.2546183452939943\n",
      "time_since_restore: 543.8646426200867\n",
      "time_this_iter_s: 8.172471761703491\n",
      "time_total_s: 543.8646426200867\n",
      "timers:\n",
      "  learn_throughput: 669.104\n",
      "  learn_time_ms: 5978.143\n",
      "  load_throughput: 6231092.293\n",
      "  load_time_ms: 0.642\n",
      "  sample_throughput: 593.495\n",
      "  sample_time_ms: 6739.733\n",
      "  update_time_ms: 4.681\n",
      "timestamp: 1643325612\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 332000\n",
      "training_iteration: 83\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 339968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-20-30\n",
      "done: false\n",
      "episode_len_mean: 167.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 2229\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.77373451034228\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013270651183872056\n",
      "        policy_loss: -0.07970768019557\n",
      "        total_loss: 50.81776266415914\n",
      "        vf_explained_var: 0.18509992599487304\n",
      "        vf_loss: 50.89481641451518\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.812594085931778\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01343337496960885\n",
      "        policy_loss: -0.080691645629704\n",
      "        total_loss: 94.94334571520487\n",
      "        vf_explained_var: 0.2993207363287608\n",
      "        vf_loss: 95.02135145187378\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9619042273362478\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01709401966681726\n",
      "        policy_loss: -0.05271581334993243\n",
      "        total_loss: 47.735457503000895\n",
      "        vf_explained_var: 0.24988564729690552\n",
      "        vf_loss: 47.78475471496582\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 339968\n",
      "  num_agent_steps_trained: 339968\n",
      "  num_steps_sampled: 340000\n",
      "  num_steps_trained: 340000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 85\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.650000000000006\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.0\n",
      "  vram_util_percent0: 0.09856499565972222\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.199999999999996\n",
      "  player_1: 34.26666666666667\n",
      "  player_2: 38.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 7.605333333333333\n",
      "  player_1: -0.878666666666667\n",
      "  player_2: -3.726666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -27.333333333333336\n",
      "  player_1: -34.800000000000004\n",
      "  player_2: -36.6\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10764556305880174\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.41733511376237137\n",
      "  mean_inference_ms: 1.976156200913548\n",
      "  mean_raw_obs_processing_ms: 0.25564689565130083\n",
      "time_since_restore: 561.76953125\n",
      "time_this_iter_s: 9.433472871780396\n",
      "time_total_s: 561.76953125\n",
      "timers:\n",
      "  learn_throughput: 620.161\n",
      "  learn_time_ms: 6449.933\n",
      "  load_throughput: 6193597.165\n",
      "  load_time_ms: 0.646\n",
      "  sample_throughput: 567.078\n",
      "  sample_time_ms: 7053.701\n",
      "  update_time_ms: 4.764\n",
      "timestamp: 1643325630\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 340000\n",
      "training_iteration: 85\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 347968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-20-48\n",
      "done: false\n",
      "episode_len_mean: 182.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 2.9999999999999982\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 2270\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7106895409027736\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014181677993328777\n",
      "        policy_loss: -0.11570257219796379\n",
      "        total_loss: 27.95762864748637\n",
      "        vf_explained_var: -0.01326573113600413\n",
      "        vf_loss: 28.070494869550068\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9287126100063324\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014440970457168684\n",
      "        policy_loss: -0.07629054181277752\n",
      "        total_loss: 28.17676577091217\n",
      "        vf_explained_var: 0.11614542484283447\n",
      "        vf_loss: 28.25016832033793\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9454231389363607\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013214813816002315\n",
      "        policy_loss: -0.08512346695022037\n",
      "        total_loss: 45.863547569910686\n",
      "        vf_explained_var: 0.01794008751710256\n",
      "        vf_loss: 45.94602836608887\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 347968\n",
      "  num_agent_steps_trained: 347968\n",
      "  num_steps_sampled: 348000\n",
      "  num_steps_trained: 348000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 87\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.618181818181824\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 53.963636363636354\n",
      "  vram_util_percent0: 0.10015684185606061\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.199999999999996\n",
      "  player_1: 34.26666666666667\n",
      "  player_2: 26.0\n",
      "policy_reward_mean:\n",
      "  player_0: 6.903333333333333\n",
      "  player_1: -1.5146666666666675\n",
      "  player_2: -2.3886666666666674\n",
      "policy_reward_min:\n",
      "  player_0: -24.066666666666663\n",
      "  player_1: -34.800000000000004\n",
      "  player_2: -35.73333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10830925671385176\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.41674102589206313\n",
      "  mean_inference_ms: 1.9843270506767738\n",
      "  mean_raw_obs_processing_ms: 0.25652209441454926\n",
      "time_since_restore: 579.8782923221588\n",
      "time_this_iter_s: 9.14337682723999\n",
      "time_total_s: 579.8782923221588\n",
      "timers:\n",
      "  learn_throughput: 579.037\n",
      "  learn_time_ms: 6908.024\n",
      "  load_throughput: 6086640.546\n",
      "  load_time_ms: 0.657\n",
      "  sample_throughput: 528.216\n",
      "  sample_time_ms: 7572.656\n",
      "  update_time_ms: 4.457\n",
      "timestamp: 1643325648\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 348000\n",
      "training_iteration: 87\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 355968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-21-01\n",
      "done: false\n",
      "episode_len_mean: 194.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 2310\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7462689580519994\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014282270385192532\n",
      "        policy_loss: -0.07753420908004045\n",
      "        total_loss: 54.37046521981557\n",
      "        vf_explained_var: 0.019438740213712058\n",
      "        vf_loss: 54.44514273643494\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8769228853782018\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014845965547816984\n",
      "        policy_loss: -0.09327442883824309\n",
      "        total_loss: 34.109924437204995\n",
      "        vf_explained_var: 0.13086399614810942\n",
      "        vf_loss: 34.200229552586876\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9531512246529261\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017309903531701518\n",
      "        policy_loss: -0.057756376673157014\n",
      "        total_loss: 50.218832108179726\n",
      "        vf_explained_var: 0.09116882900396983\n",
      "        vf_loss: 50.27312679608663\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 355968\n",
      "  num_agent_steps_trained: 355968\n",
      "  num_steps_sampled: 356000\n",
      "  num_steps_trained: 356000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 89\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.4125\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.0\n",
      "  vram_util_percent0: 0.09932454427083333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 35.666666666666664\n",
      "  player_1: 34.26666666666667\n",
      "  player_2: 24.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 6.963333333333333\n",
      "  player_1: -1.5686666666666667\n",
      "  player_2: -2.394666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -27.733333333333327\n",
      "  player_1: -29.333333333333332\n",
      "  player_2: -35.73333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10895192652061178\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.41608766606873315\n",
      "  mean_inference_ms: 1.9936665581393676\n",
      "  mean_raw_obs_processing_ms: 0.2574235964828134\n",
      "time_since_restore: 593.1919090747833\n",
      "time_this_iter_s: 6.571348667144775\n",
      "time_total_s: 593.1919090747833\n",
      "timers:\n",
      "  learn_throughput: 583.402\n",
      "  learn_time_ms: 6856.337\n",
      "  load_throughput: 5982675.177\n",
      "  load_time_ms: 0.669\n",
      "  sample_throughput: 510.758\n",
      "  sample_time_ms: 7831.492\n",
      "  update_time_ms: 5.993\n",
      "timestamp: 1643325661\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 356000\n",
      "training_iteration: 89\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 363970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-21-14\n",
      "done: false\n",
      "episode_len_mean: 194.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2353\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.732830151716868\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015570262752686782\n",
      "        policy_loss: -0.06922965430965027\n",
      "        total_loss: 44.54394980430603\n",
      "        vf_explained_var: 0.12156155606110891\n",
      "        vf_loss: 44.610065383911135\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.926836661696434\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013746591604440256\n",
      "        policy_loss: -0.0781422426427404\n",
      "        total_loss: 35.772892413139346\n",
      "        vf_explained_var: 0.0929097855091095\n",
      "        vf_loss: 35.848285457293194\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9165875607728958\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01362429713139734\n",
      "        policy_loss: -0.09291042022407055\n",
      "        total_loss: 35.644336606661476\n",
      "        vf_explained_var: -0.019528362353642782\n",
      "        vf_loss: 35.734522178967794\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 363970\n",
      "  num_agent_steps_trained: 363970\n",
      "  num_steps_sampled: 364000\n",
      "  num_steps_trained: 364000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 91\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.644444444444446\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.0\n",
      "  vram_util_percent0: 0.09912109375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 35.666666666666664\n",
      "  player_1: 32.333333333333336\n",
      "  player_2: 24.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 6.958666666666666\n",
      "  player_1: 0.052666666666666126\n",
      "  player_2: -4.011333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -27.733333333333327\n",
      "  player_1: -28.666666666666664\n",
      "  player_2: -35.73333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10889211842009335\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.41451733549907127\n",
      "  mean_inference_ms: 1.9951224019598204\n",
      "  mean_raw_obs_processing_ms: 0.2576524398086534\n",
      "time_since_restore: 606.2714810371399\n",
      "time_this_iter_s: 6.622610092163086\n",
      "time_total_s: 606.2714810371399\n",
      "timers:\n",
      "  learn_throughput: 584.501\n",
      "  learn_time_ms: 6843.447\n",
      "  load_throughput: 6049549.634\n",
      "  load_time_ms: 0.661\n",
      "  sample_throughput: 514.838\n",
      "  sample_time_ms: 7769.441\n",
      "  update_time_ms: 5.864\n",
      "timestamp: 1643325674\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 364000\n",
      "training_iteration: 91\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 371968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-21-28\n",
      "done: false\n",
      "episode_len_mean: 184.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 2395\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7698595082759857\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014194989729746036\n",
      "        policy_loss: -0.07844940021634102\n",
      "        total_loss: 46.13694685300191\n",
      "        vf_explained_var: -0.035182422796885174\n",
      "        vf_loss: 46.21255738894145\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8216886339584987\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015339291320124175\n",
      "        policy_loss: -0.09583206169928114\n",
      "        total_loss: 32.04922256787618\n",
      "        vf_explained_var: 0.385866303841273\n",
      "        vf_loss: 32.14198688189189\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9134250370661418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014850205390333333\n",
      "        policy_loss: -0.10694699635108312\n",
      "        total_loss: 58.350229082107546\n",
      "        vf_explained_var: 0.10290415028731029\n",
      "        vf_loss: 58.45420474370321\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 371968\n",
      "  num_agent_steps_trained: 371968\n",
      "  num_steps_sampled: 372000\n",
      "  num_steps_trained: 372000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 93\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.91111111111111\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.0\n",
      "  vram_util_percent0: 0.09912109375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.0\n",
      "  player_1: 32.333333333333336\n",
      "  player_2: 24.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 6.318666666666667\n",
      "  player_1: 0.4266666666666665\n",
      "  player_2: -3.745333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -27.733333333333327\n",
      "  player_1: -28.666666666666664\n",
      "  player_2: -32.73333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10912383532446952\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4127442209818625\n",
      "  mean_inference_ms: 1.995911761505422\n",
      "  mean_raw_obs_processing_ms: 0.25768872078003113\n",
      "time_since_restore: 619.9697399139404\n",
      "time_this_iter_s: 7.02926230430603\n",
      "time_total_s: 619.9697399139404\n",
      "timers:\n",
      "  learn_throughput: 591.697\n",
      "  learn_time_ms: 6760.219\n",
      "  load_throughput: 6032148.995\n",
      "  load_time_ms: 0.663\n",
      "  sample_throughput: 516.593\n",
      "  sample_time_ms: 7743.033\n",
      "  update_time_ms: 5.531\n",
      "timestamp: 1643325688\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 372000\n",
      "training_iteration: 93\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 379968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-21-43\n",
      "done: false\n",
      "episode_len_mean: 193.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 14\n",
      "episodes_total: 2429\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7680132442712784\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01641420090518351\n",
      "        policy_loss: -0.08377260444064935\n",
      "        total_loss: 32.687724760373435\n",
      "        vf_explained_var: -0.01919971267382304\n",
      "        vf_loss: 32.76821456114451\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7487729867299397\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013539368854277806\n",
      "        policy_loss: -0.04777461419192453\n",
      "        total_loss: 18.931519242922466\n",
      "        vf_explained_var: 0.008897703886032105\n",
      "        vf_loss: 18.976585930188495\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8741027686993281\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01488138815385658\n",
      "        policy_loss: -0.10373149859408537\n",
      "        total_loss: 32.25546059290568\n",
      "        vf_explained_var: 0.024295849204063417\n",
      "        vf_loss: 32.356216073036194\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 379968\n",
      "  num_agent_steps_trained: 379968\n",
      "  num_steps_sampled: 380000\n",
      "  num_steps_trained: 380000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 95\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.35\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.42999999999999\n",
      "  vram_util_percent0: 0.09912109375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.0\n",
      "  player_1: 18.866666666666667\n",
      "  player_2: 21.866666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 7.918666666666668\n",
      "  player_1: -1.0233333333333332\n",
      "  player_2: -3.8953333333333338\n",
      "policy_reward_min:\n",
      "  player_0: -22.666666666666664\n",
      "  player_1: -35.333333333333336\n",
      "  player_2: -32.73333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10888838955138695\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.41117243490595806\n",
      "  mean_inference_ms: 1.9941895399822558\n",
      "  mean_raw_obs_processing_ms: 0.25735245719213146\n",
      "time_since_restore: 635.3338372707367\n",
      "time_this_iter_s: 7.932343244552612\n",
      "time_total_s: 635.3338372707367\n",
      "timers:\n",
      "  learn_throughput: 614.463\n",
      "  learn_time_ms: 6509.745\n",
      "  load_throughput: 6082227.378\n",
      "  load_time_ms: 0.658\n",
      "  sample_throughput: 530.226\n",
      "  sample_time_ms: 7543.953\n",
      "  update_time_ms: 5.522\n",
      "timestamp: 1643325703\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 380000\n",
      "training_iteration: 95\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 387968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-21-58\n",
      "done: false\n",
      "episode_len_mean: 205.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 19\n",
      "episodes_total: 2469\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7230687801043193\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016253737297944704\n",
      "        policy_loss: -0.08245054165832698\n",
      "        total_loss: 26.515120787620546\n",
      "        vf_explained_var: 0.1269559520483017\n",
      "        vf_loss: 26.59432043393453\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7828616265455882\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014343515533900867\n",
      "        policy_loss: -0.10193365332204848\n",
      "        total_loss: 29.09790942986806\n",
      "        vf_explained_var: -0.011874061226844788\n",
      "        vf_loss: 29.196974320411684\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8188006359338761\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021091013981240355\n",
      "        policy_loss: -0.051815755045972765\n",
      "        total_loss: 30.278499751091005\n",
      "        vf_explained_var: -0.18552336474259695\n",
      "        vf_loss: 30.326097338994344\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 387968\n",
      "  num_agent_steps_trained: 387968\n",
      "  num_steps_sampled: 388000\n",
      "  num_steps_trained: 388000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 97\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.38888888888889\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.5\n",
      "  vram_util_percent0: 0.09912109375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.0\n",
      "  player_1: 30.666666666666668\n",
      "  player_2: 21.866666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 8.181333333333335\n",
      "  player_1: 0.15733333333333327\n",
      "  player_2: -5.338666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -18.733333333333327\n",
      "  player_1: -35.333333333333336\n",
      "  player_2: -32.73333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.109200265857375\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.41037550862809247\n",
      "  mean_inference_ms: 1.9992321716898565\n",
      "  mean_raw_obs_processing_ms: 0.25785607704729263\n",
      "time_since_restore: 649.8368382453918\n",
      "time_this_iter_s: 7.054443359375\n",
      "time_total_s: 649.8368382453918\n",
      "timers:\n",
      "  learn_throughput: 649.614\n",
      "  learn_time_ms: 6157.499\n",
      "  load_throughput: 6141900.718\n",
      "  load_time_ms: 0.651\n",
      "  sample_throughput: 552.915\n",
      "  sample_time_ms: 7234.387\n",
      "  update_time_ms: 5.406\n",
      "timestamp: 1643325718\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 388000\n",
      "training_iteration: 97\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 395969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-22-12\n",
      "done: false\n",
      "episode_len_mean: 208.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 21\n",
      "episodes_total: 2513\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7596785428126653\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015602935389695934\n",
      "        policy_loss: -0.08324424522618452\n",
      "        total_loss: 23.575312062899272\n",
      "        vf_explained_var: 0.15548249920209248\n",
      "        vf_loss: 23.655435558160146\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8248167928059896\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01214001001504141\n",
      "        policy_loss: -0.017279216622312864\n",
      "        total_loss: 38.70263405799866\n",
      "        vf_explained_var: -0.08957072198390961\n",
      "        vf_loss: 38.71748574177424\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9100546723604203\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01661902942913397\n",
      "        policy_loss: -0.10080229291071495\n",
      "        total_loss: 38.87905524571737\n",
      "        vf_explained_var: 0.20058485468228657\n",
      "        vf_loss: 38.974871740341186\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 395969\n",
      "  num_agent_steps_trained: 395969\n",
      "  num_steps_sampled: 396000\n",
      "  num_steps_trained: 396000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 99\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.533333333333335\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.6\n",
      "  vram_util_percent0: 0.09912109375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.666666666666664\n",
      "  player_1: 30.666666666666668\n",
      "  player_2: 20.133333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 8.198666666666666\n",
      "  player_1: -1.2773333333333334\n",
      "  player_2: -3.9213333333333344\n",
      "policy_reward_min:\n",
      "  player_0: -18.46666666666667\n",
      "  player_1: -21.066666666666663\n",
      "  player_2: -35.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1096370285851335\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40991410073187834\n",
      "  mean_inference_ms: 2.0067377794248067\n",
      "  mean_raw_obs_processing_ms: 0.2588461824757112\n",
      "time_since_restore: 664.0862879753113\n",
      "time_this_iter_s: 6.957494497299194\n",
      "time_total_s: 664.0862879753113\n",
      "timers:\n",
      "  learn_throughput: 642.153\n",
      "  learn_time_ms: 6229.046\n",
      "  load_throughput: 6279368.216\n",
      "  load_time_ms: 0.637\n",
      "  sample_throughput: 564.831\n",
      "  sample_time_ms: 7081.76\n",
      "  update_time_ms: 3.803\n",
      "timestamp: 1643325732\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 396000\n",
      "training_iteration: 99\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 403969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-22-27\n",
      "done: false\n",
      "episode_len_mean: 190.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 2557\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7085741678873698\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015566375421470487\n",
      "        policy_loss: -0.07762061069409053\n",
      "        total_loss: 38.08415065765381\n",
      "        vf_explained_var: -0.1971417228380839\n",
      "        vf_loss: 38.158658165931705\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8122073984146119\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014933645360312463\n",
      "        policy_loss: -0.06047424091647069\n",
      "        total_loss: 21.18661260445913\n",
      "        vf_explained_var: -0.04845286210378011\n",
      "        vf_loss: 21.244100131988525\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8655835314591726\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017092558254274384\n",
      "        policy_loss: -0.07239497222316761\n",
      "        total_loss: 35.24018048604329\n",
      "        vf_explained_var: 0.18836845338344574\n",
      "        vf_loss: 35.3074476591746\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 403969\n",
      "  num_agent_steps_trained: 403969\n",
      "  num_steps_sampled: 404000\n",
      "  num_steps_trained: 404000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 101\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.62222222222222\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.644444444444446\n",
      "  vram_util_percent0: 0.09912109375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.666666666666664\n",
      "  player_1: 24.666666666666664\n",
      "  player_2: 22.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 8.220666666666666\n",
      "  player_1: -0.5753333333333334\n",
      "  player_2: -4.645333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -17.266666666666666\n",
      "  player_1: -21.066666666666663\n",
      "  player_2: -35.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1103090190590838\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40938422120950585\n",
      "  mean_inference_ms: 2.0140142746993916\n",
      "  mean_raw_obs_processing_ms: 0.2593582173418893\n",
      "time_since_restore: 678.4703733921051\n",
      "time_this_iter_s: 7.014457941055298\n",
      "time_total_s: 678.4703733921051\n",
      "timers:\n",
      "  learn_throughput: 630.478\n",
      "  learn_time_ms: 6344.397\n",
      "  load_throughput: 6229704.059\n",
      "  load_time_ms: 0.642\n",
      "  sample_throughput: 554.908\n",
      "  sample_time_ms: 7208.408\n",
      "  update_time_ms: 3.783\n",
      "timestamp: 1643325747\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 404000\n",
      "training_iteration: 101\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 411968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-22-41\n",
      "done: false\n",
      "episode_len_mean: 179.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 2600\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7054934142033259\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016701267524173696\n",
      "        policy_loss: -0.02873288815530638\n",
      "        total_loss: 26.968665153185526\n",
      "        vf_explained_var: 0.12693100690841674\n",
      "        vf_loss: 26.994057699839274\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8023584367831548\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013560880373327866\n",
      "        policy_loss: -0.06256847620941698\n",
      "        total_loss: 27.732243022918702\n",
      "        vf_explained_var: -0.1934045825401942\n",
      "        vf_loss: 27.792099339167276\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9153985722859701\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013625956420316167\n",
      "        policy_loss: -0.14056476935278625\n",
      "        total_loss: 20.434619783560436\n",
      "        vf_explained_var: 0.08009201566378275\n",
      "        vf_loss: 20.571096749305724\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 411968\n",
      "  num_agent_steps_trained: 411968\n",
      "  num_steps_sampled: 412000\n",
      "  num_steps_trained: 412000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 103\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.855555555555554\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.7\n",
      "  vram_util_percent0: 0.09912109375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.666666666666664\n",
      "  player_1: 24.666666666666664\n",
      "  player_2: 22.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 8.370000000000001\n",
      "  player_1: -0.31600000000000045\n",
      "  player_2: -5.054000000000001\n",
      "policy_reward_min:\n",
      "  player_0: -16.0\n",
      "  player_1: -32.4\n",
      "  player_2: -35.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11032552385375485\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40872509771267806\n",
      "  mean_inference_ms: 2.0166296144948443\n",
      "  mean_raw_obs_processing_ms: 0.25973222820908787\n",
      "time_since_restore: 692.8453872203827\n",
      "time_this_iter_s: 7.305668830871582\n",
      "time_total_s: 692.8453872203827\n",
      "timers:\n",
      "  learn_throughput: 625.479\n",
      "  learn_time_ms: 6395.095\n",
      "  load_throughput: 6167413.888\n",
      "  load_time_ms: 0.649\n",
      "  sample_throughput: 548.236\n",
      "  sample_time_ms: 7296.132\n",
      "  update_time_ms: 3.885\n",
      "timestamp: 1643325761\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 412000\n",
      "training_iteration: 103\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 419970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-22-56\n",
      "done: false\n",
      "episode_len_mean: 178.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 2645\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6966511722405752\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014911346617039675\n",
      "        policy_loss: -0.04483823934841591\n",
      "        total_loss: 31.485055996576946\n",
      "        vf_explained_var: 0.29089502155780794\n",
      "        vf_loss: 31.526911776860555\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8059350168704986\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014958415047246187\n",
      "        policy_loss: -0.05355088050787648\n",
      "        total_loss: 20.81808261235555\n",
      "        vf_explained_var: -0.0580713027715683\n",
      "        vf_loss: 20.868641735712686\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8234391796588898\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014496136666821258\n",
      "        policy_loss: -0.07936571419239044\n",
      "        total_loss: 32.76127059936523\n",
      "        vf_explained_var: 0.07518881201744079\n",
      "        vf_loss: 32.83628753662109\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 419970\n",
      "  num_agent_steps_trained: 419970\n",
      "  num_steps_sampled: 420000\n",
      "  num_steps_trained: 420000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 105\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.600000000000005\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.844444444444434\n",
      "  vram_util_percent0: 0.09912109375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 27.199999999999996\n",
      "  player_1: 24.666666666666664\n",
      "  player_2: 21.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 5.366666666666666\n",
      "  player_1: 0.9506666666666667\n",
      "  player_2: -3.3173333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -18.666666666666664\n",
      "  player_1: -32.4\n",
      "  player_2: -35.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11063074117885492\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.407825225242854\n",
      "  mean_inference_ms: 2.0184486129475925\n",
      "  mean_raw_obs_processing_ms: 0.25978867786272347\n",
      "time_since_restore: 707.4254324436188\n",
      "time_this_iter_s: 7.1432695388793945\n",
      "time_total_s: 707.4254324436188\n",
      "timers:\n",
      "  learn_throughput: 632.631\n",
      "  learn_time_ms: 6322.797\n",
      "  load_throughput: 6199318.627\n",
      "  load_time_ms: 0.645\n",
      "  sample_throughput: 547.095\n",
      "  sample_time_ms: 7311.338\n",
      "  update_time_ms: 3.861\n",
      "timestamp: 1643325776\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 420000\n",
      "training_iteration: 105\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 427968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-23-09\n",
      "done: false\n",
      "episode_len_mean: 188.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 21\n",
      "episodes_total: 2684\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6781411474943161\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0156940776901835\n",
      "        policy_loss: -0.05169637103953088\n",
      "        total_loss: 25.58065531571706\n",
      "        vf_explained_var: 0.13075157423814138\n",
      "        vf_loss: 25.629213024775186\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.799257628719012\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015790213303892717\n",
      "        policy_loss: -0.055366396211708586\n",
      "        total_loss: 32.83226814746857\n",
      "        vf_explained_var: 0.10313382466634115\n",
      "        vf_loss: 32.884476540883384\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.822510783871015\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0134466795164706\n",
      "        policy_loss: -0.07696688561545063\n",
      "        total_loss: 26.454698770840963\n",
      "        vf_explained_var: 0.09151853064695994\n",
      "        vf_loss: 26.527631611824035\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 427968\n",
      "  num_agent_steps_trained: 427968\n",
      "  num_steps_sampled: 428000\n",
      "  num_steps_trained: 428000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 107\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.087500000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.725\n",
      "  vram_util_percent0: 0.09912109375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 23.666666666666668\n",
      "  player_1: 30.799999999999997\n",
      "  player_2: 21.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 4.593999999999999\n",
      "  player_1: 1.2999999999999998\n",
      "  player_2: -2.8939999999999992\n",
      "policy_reward_min:\n",
      "  player_0: -18.666666666666664\n",
      "  player_1: -19.400000000000002\n",
      "  player_2: -32.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11052607768357119\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40621057702516133\n",
      "  mean_inference_ms: 2.0180365675949044\n",
      "  mean_raw_obs_processing_ms: 0.2596051405109497\n",
      "time_since_restore: 721.0151879787445\n",
      "time_this_iter_s: 6.743410110473633\n",
      "time_total_s: 721.0151879787445\n",
      "timers:\n",
      "  learn_throughput: 639.315\n",
      "  learn_time_ms: 6256.696\n",
      "  load_throughput: 6199776.801\n",
      "  load_time_ms: 0.645\n",
      "  sample_throughput: 558.017\n",
      "  sample_time_ms: 7168.234\n",
      "  update_time_ms: 4.603\n",
      "timestamp: 1643325789\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 428000\n",
      "training_iteration: 107\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 435968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-23-24\n",
      "done: false\n",
      "episode_len_mean: 199.84\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 18\n",
      "episodes_total: 2720\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6929825625816981\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015382713375911407\n",
      "        policy_loss: -0.09806829096749425\n",
      "        total_loss: 21.54630855878194\n",
      "        vf_explained_var: -0.019667133887608847\n",
      "        vf_loss: 21.641300282478333\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8408195783694585\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014008148761810542\n",
      "        policy_loss: -0.08897135930446287\n",
      "        total_loss: 29.964046424229938\n",
      "        vf_explained_var: 0.05534261604150136\n",
      "        vf_loss: 30.05021637757619\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8381391483545303\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015817570223055858\n",
      "        policy_loss: -0.10519588612020016\n",
      "        total_loss: 43.91601985931396\n",
      "        vf_explained_var: 0.10836141347885132\n",
      "        vf_loss: 44.016470715204875\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 435968\n",
      "  num_agent_steps_trained: 435968\n",
      "  num_steps_sampled: 436000\n",
      "  num_steps_trained: 436000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 109\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.53\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.61\n",
      "  vram_util_percent0: 0.09912109375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.46666666666666\n",
      "  player_1: 30.799999999999997\n",
      "  player_2: 21.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 4.390666666666667\n",
      "  player_1: 1.0946666666666667\n",
      "  player_2: -2.485333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -21.13333333333333\n",
      "  player_1: -19.400000000000002\n",
      "  player_2: -32.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11052752131435083\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4051867438206064\n",
      "  mean_inference_ms: 2.0200152169416916\n",
      "  mean_raw_obs_processing_ms: 0.25977169534715955\n",
      "time_since_restore: 735.3805940151215\n",
      "time_this_iter_s: 7.30455207824707\n",
      "time_total_s: 735.3805940151215\n",
      "timers:\n",
      "  learn_throughput: 636.335\n",
      "  learn_time_ms: 6285.993\n",
      "  load_throughput: 6195198.109\n",
      "  load_time_ms: 0.646\n",
      "  sample_throughput: 561.095\n",
      "  sample_time_ms: 7128.917\n",
      "  update_time_ms: 4.833\n",
      "timestamp: 1643325804\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 436000\n",
      "training_iteration: 109\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 443968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-23-38\n",
      "done: false\n",
      "episode_len_mean: 206.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 19\n",
      "episodes_total: 2759\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6618523508310318\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018791224999998423\n",
      "        policy_loss: -0.08457155914821972\n",
      "        total_loss: 31.84539204120636\n",
      "        vf_explained_var: 0.3635231872399648\n",
      "        vf_loss: 31.92620532353719\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7683862036466599\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016874893907891722\n",
      "        policy_loss: -0.09053727136769642\n",
      "        total_loss: 35.866059436798096\n",
      "        vf_explained_var: 0.16351901630560556\n",
      "        vf_loss: 35.95322142918905\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7792776052157084\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014391680692572966\n",
      "        policy_loss: -0.05623913019895554\n",
      "        total_loss: 31.903944622675578\n",
      "        vf_explained_var: 0.17430612524350483\n",
      "        vf_loss: 31.955865904490153\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 443968\n",
      "  num_agent_steps_trained: 443968\n",
      "  num_steps_sampled: 444000\n",
      "  num_steps_trained: 444000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 111\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.49\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.71\n",
      "  vram_util_percent0: 0.09912109375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.46666666666666\n",
      "  player_1: 27.666666666666668\n",
      "  player_2: 22.066666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 6.9479999999999995\n",
      "  player_1: -1.4959999999999996\n",
      "  player_2: -2.452\n",
      "policy_reward_min:\n",
      "  player_0: -21.13333333333333\n",
      "  player_1: -25.933333333333334\n",
      "  player_2: -29.93333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11057859478805193\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40368708893981614\n",
      "  mean_inference_ms: 2.021489438850494\n",
      "  mean_raw_obs_processing_ms: 0.25987694605377887\n",
      "time_since_restore: 749.38338804245\n",
      "time_this_iter_s: 7.437348127365112\n",
      "time_total_s: 749.38338804245\n",
      "timers:\n",
      "  learn_throughput: 639.265\n",
      "  learn_time_ms: 6257.19\n",
      "  load_throughput: 6175813.885\n",
      "  load_time_ms: 0.648\n",
      "  sample_throughput: 564.88\n",
      "  sample_time_ms: 7081.156\n",
      "  update_time_ms: 6.493\n",
      "timestamp: 1643325818\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 444000\n",
      "training_iteration: 111\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 451970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-23-53\n",
      "done: false\n",
      "episode_len_mean: 196.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 2808\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6251702028512954\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013385157289709848\n",
      "        policy_loss: -0.07124140030393998\n",
      "        total_loss: 37.41335785230001\n",
      "        vf_explained_var: 0.2704846163590749\n",
      "        vf_loss: 37.48192210197449\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7531779732306798\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018986805425871958\n",
      "        policy_loss: -0.0740118526838099\n",
      "        total_loss: 49.87045292695363\n",
      "        vf_explained_var: 0.12090672254562378\n",
      "        vf_loss: 49.94066693147023\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8058218934138616\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01387626092807409\n",
      "        policy_loss: -0.08234770182830592\n",
      "        total_loss: 44.51756917317709\n",
      "        vf_explained_var: 0.24921865304311117\n",
      "        vf_loss: 44.59575415293376\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 451970\n",
      "  num_agent_steps_trained: 451970\n",
      "  num_steps_sampled: 452000\n",
      "  num_steps_trained: 452000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 113\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.666666666666664\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.85555555555555\n",
      "  vram_util_percent0: 0.09912109375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.666666666666668\n",
      "  player_1: 20.333333333333336\n",
      "  player_2: 22.066666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 7.640666666666666\n",
      "  player_1: -2.7473333333333336\n",
      "  player_2: -1.8933333333333338\n",
      "policy_reward_min:\n",
      "  player_0: -17.733333333333327\n",
      "  player_1: -31.0\n",
      "  player_2: -31.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11109039360155179\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40266872508094664\n",
      "  mean_inference_ms: 2.0262780134568676\n",
      "  mean_raw_obs_processing_ms: 0.260261752355918\n",
      "time_since_restore: 764.1301455497742\n",
      "time_this_iter_s: 7.372944593429565\n",
      "time_total_s: 764.1301455497742\n",
      "timers:\n",
      "  learn_throughput: 637.154\n",
      "  learn_time_ms: 6277.915\n",
      "  load_throughput: 6247799.501\n",
      "  load_time_ms: 0.64\n",
      "  sample_throughput: 558.285\n",
      "  sample_time_ms: 7164.796\n",
      "  update_time_ms: 6.437\n",
      "timestamp: 1643325833\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 452000\n",
      "training_iteration: 113\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 459970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-24-07\n",
      "done: false\n",
      "episode_len_mean: 179.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 2853\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6349091726541519\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012815519999294337\n",
      "        policy_loss: -0.08300453164925178\n",
      "        total_loss: 33.94507337729136\n",
      "        vf_explained_var: 0.2655210610230764\n",
      "        vf_loss: 34.0255150826772\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7888160532712937\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015748503729909515\n",
      "        policy_loss: -0.08051254086662084\n",
      "        total_loss: 41.388875500361124\n",
      "        vf_explained_var: -0.16028265873591105\n",
      "        vf_loss: 41.46623839696248\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7839097648859024\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013043766472461964\n",
      "        policy_loss: -0.0261150217622829\n",
      "        total_loss: 22.350975200335185\n",
      "        vf_explained_var: 0.24164389352003735\n",
      "        vf_loss: 22.371220598220827\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 459970\n",
      "  num_agent_steps_trained: 459970\n",
      "  num_steps_sampled: 460000\n",
      "  num_steps_trained: 460000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 115\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.57777777777778\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.03333333333333\n",
      "  vram_util_percent0: 0.09912109375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 26.0\n",
      "  player_2: 23.066666666666663\n",
      "policy_reward_mean:\n",
      "  player_0: 8.746666666666666\n",
      "  player_1: -2.2173333333333334\n",
      "  player_2: -3.529333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -17.733333333333327\n",
      "  player_1: -31.0\n",
      "  player_2: -31.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11120820176403125\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4028956346896997\n",
      "  mean_inference_ms: 2.0292347493276783\n",
      "  mean_raw_obs_processing_ms: 0.2607620642900384\n",
      "time_since_restore: 778.676064491272\n",
      "time_this_iter_s: 7.14090895652771\n",
      "time_total_s: 778.676064491272\n",
      "timers:\n",
      "  learn_throughput: 637.686\n",
      "  learn_time_ms: 6272.677\n",
      "  load_throughput: 6190854.613\n",
      "  load_time_ms: 0.646\n",
      "  sample_throughput: 559.106\n",
      "  sample_time_ms: 7154.284\n",
      "  update_time_ms: 6.869\n",
      "timestamp: 1643325847\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 460000\n",
      "training_iteration: 115\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 467968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-24-21\n",
      "done: false\n",
      "episode_len_mean: 176.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 2896\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6933922956387202\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016048996856285764\n",
      "        policy_loss: -0.05099550044784943\n",
      "        total_loss: 29.466070903142292\n",
      "        vf_explained_var: -0.09315930426120758\n",
      "        vf_loss: 29.51385666211446\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7998538718620936\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017534598612176825\n",
      "        policy_loss: -0.053747289526897175\n",
      "        total_loss: 29.209669405619305\n",
      "        vf_explained_var: 0.059038890600204466\n",
      "        vf_loss: 29.25990972836812\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8355213711659114\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013503600367015072\n",
      "        policy_loss: -0.11171880166356762\n",
      "        total_loss: 36.238156112035114\n",
      "        vf_explained_var: 0.11416339794794718\n",
      "        vf_loss: 36.34379849433899\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 467968\n",
      "  num_agent_steps_trained: 467968\n",
      "  num_steps_sampled: 468000\n",
      "  num_steps_trained: 468000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 117\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.95\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.1\n",
      "  vram_util_percent0: 0.09912109375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 26.0\n",
      "  player_2: 23.066666666666663\n",
      "policy_reward_mean:\n",
      "  player_0: 8.018\n",
      "  player_1: -1.9579999999999995\n",
      "  player_2: -3.06\n",
      "policy_reward_min:\n",
      "  player_0: -17.733333333333327\n",
      "  player_1: -30.666666666666668\n",
      "  player_2: -25.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11149163274690034\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4022099020788071\n",
      "  mean_inference_ms: 2.031079084773199\n",
      "  mean_raw_obs_processing_ms: 0.2608677535016217\n",
      "time_since_restore: 792.5848276615143\n",
      "time_this_iter_s: 6.89098596572876\n",
      "time_total_s: 792.5848276615143\n",
      "timers:\n",
      "  learn_throughput: 635.737\n",
      "  learn_time_ms: 6291.908\n",
      "  load_throughput: 6130454.927\n",
      "  load_time_ms: 0.652\n",
      "  sample_throughput: 556.31\n",
      "  sample_time_ms: 7190.242\n",
      "  update_time_ms: 6.203\n",
      "timestamp: 1643325861\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 468000\n",
      "training_iteration: 117\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 475968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-24-36\n",
      "done: false\n",
      "episode_len_mean: 179.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 2943\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6538744082053503\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01252581538031369\n",
      "        policy_loss: -0.11429871002833049\n",
      "        total_loss: 42.00445931275686\n",
      "        vf_explained_var: 0.08840981185436249\n",
      "        vf_loss: 42.1162530374527\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8003452446063359\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015695992553270564\n",
      "        policy_loss: -0.0773985988413915\n",
      "        total_loss: 48.812036736806235\n",
      "        vf_explained_var: -0.020785752336184182\n",
      "        vf_loss: 48.88629644711812\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7779443019628525\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011843968732379088\n",
      "        policy_loss: -0.04246782668866217\n",
      "        total_loss: 62.93173670609792\n",
      "        vf_explained_var: 0.11688015441099803\n",
      "        vf_loss: 62.96887493451436\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 475968\n",
      "  num_agent_steps_trained: 475968\n",
      "  num_steps_sampled: 476000\n",
      "  num_steps_trained: 476000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 119\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.466666666666665\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.833333333333336\n",
      "  vram_util_percent0: 0.09901258680555555\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 19.933333333333334\n",
      "  player_2: 21.0\n",
      "policy_reward_mean:\n",
      "  player_0: 7.566666666666667\n",
      "  player_1: -0.5653333333333331\n",
      "  player_2: -4.001333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -11.800000000000004\n",
      "  player_1: -26.666666666666664\n",
      "  player_2: -35.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11176694425442044\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4018777790757852\n",
      "  mean_inference_ms: 2.035168256880051\n",
      "  mean_raw_obs_processing_ms: 0.26148874705390207\n",
      "time_since_restore: 807.1407392024994\n",
      "time_this_iter_s: 7.285757064819336\n",
      "time_total_s: 807.1407392024994\n",
      "timers:\n",
      "  learn_throughput: 634.682\n",
      "  learn_time_ms: 6302.366\n",
      "  load_throughput: 6097923.164\n",
      "  load_time_ms: 0.656\n",
      "  sample_throughput: 554.201\n",
      "  sample_time_ms: 7217.599\n",
      "  update_time_ms: 6.006\n",
      "timestamp: 1643325876\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 476000\n",
      "training_iteration: 119\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 483970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-24-49\n",
      "done: false\n",
      "episode_len_mean: 179.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2985\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6418037937084834\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016755215265400563\n",
      "        policy_loss: -0.07783445226649444\n",
      "        total_loss: 36.19498492399851\n",
      "        vf_explained_var: -0.083082009156545\n",
      "        vf_loss: 36.26946818351745\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7318489720424016\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014516601643238876\n",
      "        policy_loss: -0.06355231280128161\n",
      "        total_loss: 37.19473555088043\n",
      "        vf_explained_var: 0.043178624908129376\n",
      "        vf_loss: 37.255384472211205\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8242160912354787\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01313678692886242\n",
      "        policy_loss: -0.10209666571269432\n",
      "        total_loss: 36.06171162923177\n",
      "        vf_explained_var: -0.12097828030586243\n",
      "        vf_loss: 36.157896982828774\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 483970\n",
      "  num_agent_steps_trained: 483970\n",
      "  num_steps_sampled: 484000\n",
      "  num_steps_trained: 484000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 121\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.799999999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.25555555555556\n",
      "  vram_util_percent0: 0.09794560185185186\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 19.6\n",
      "  player_2: 21.0\n",
      "policy_reward_mean:\n",
      "  player_0: 8.18\n",
      "  player_1: -0.15\n",
      "  player_2: -5.029999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -24.666666666666664\n",
      "  player_1: -29.333333333333336\n",
      "  player_2: -35.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11168107252608209\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40112471541088107\n",
      "  mean_inference_ms: 2.0356340044469783\n",
      "  mean_raw_obs_processing_ms: 0.2615907861865111\n",
      "time_since_restore: 820.2580840587616\n",
      "time_this_iter_s: 6.523184537887573\n",
      "time_total_s: 820.2580840587616\n",
      "timers:\n",
      "  learn_throughput: 643.223\n",
      "  learn_time_ms: 6218.688\n",
      "  load_throughput: 6165374.1\n",
      "  load_time_ms: 0.649\n",
      "  sample_throughput: 554.252\n",
      "  sample_time_ms: 7216.929\n",
      "  update_time_ms: 4.298\n",
      "timestamp: 1643325889\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 484000\n",
      "training_iteration: 121\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 491968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-25-03\n",
      "done: false\n",
      "episode_len_mean: 183.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3026\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6355778880914053\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013100910937667626\n",
      "        policy_loss: -0.06204320872357736\n",
      "        total_loss: 42.718412607510885\n",
      "        vf_explained_var: 0.20659447054068247\n",
      "        vf_loss: 42.777835141817725\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6973746118942896\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013773526180502814\n",
      "        policy_loss: -0.07477999510864416\n",
      "        total_loss: 37.42349872748057\n",
      "        vf_explained_var: -0.028385183811187743\n",
      "        vf_loss: 37.495524284044905\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8201865881681443\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013345344706619168\n",
      "        policy_loss: -0.08738968582513432\n",
      "        total_loss: 39.54597285270691\n",
      "        vf_explained_var: 0.001008214553197225\n",
      "        vf_loss: 39.627357117335\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 491968\n",
      "  num_agent_steps_trained: 491968\n",
      "  num_steps_sampled: 492000\n",
      "  num_steps_trained: 492000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 123\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.610000000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.09970703125000001\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.0\n",
      "  player_1: 28.133333333333336\n",
      "  player_2: 21.0\n",
      "policy_reward_mean:\n",
      "  player_0: 9.064666666666664\n",
      "  player_1: -0.6013333333333337\n",
      "  player_2: -5.463333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -24.666666666666664\n",
      "  player_1: -29.333333333333336\n",
      "  player_2: -35.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11169878980538536\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40024947853640247\n",
      "  mean_inference_ms: 2.0356843509924865\n",
      "  mean_raw_obs_processing_ms: 0.2617045207753685\n",
      "time_since_restore: 834.3558351993561\n",
      "time_this_iter_s: 7.452474117279053\n",
      "time_total_s: 834.3558351993561\n",
      "timers:\n",
      "  learn_throughput: 647.998\n",
      "  learn_time_ms: 6172.854\n",
      "  load_throughput: 6021324.337\n",
      "  load_time_ms: 0.664\n",
      "  sample_throughput: 567.81\n",
      "  sample_time_ms: 7044.606\n",
      "  update_time_ms: 4.271\n",
      "timestamp: 1643325903\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 492000\n",
      "training_iteration: 123\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 499968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-25-21\n",
      "done: false\n",
      "episode_len_mean: 174.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 3078\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6202145334084829\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013023941186499997\n",
      "        policy_loss: -0.08406986733277638\n",
      "        total_loss: 47.42012714703878\n",
      "        vf_explained_var: 0.2884832207361857\n",
      "        vf_loss: 47.50159225304922\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7117800213893255\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016349249364653748\n",
      "        policy_loss: -0.06067117014278968\n",
      "        total_loss: 27.913567070960998\n",
      "        vf_explained_var: 0.027118512789408366\n",
      "        vf_loss: 27.97096819559733\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8349310525258382\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013025691591568224\n",
      "        policy_loss: -0.07751928092911839\n",
      "        total_loss: 34.24779193242391\n",
      "        vf_explained_var: -0.08325429658095042\n",
      "        vf_loss: 34.3194499429067\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 499968\n",
      "  num_agent_steps_trained: 499968\n",
      "  num_steps_sampled: 500000\n",
      "  num_steps_trained: 500000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 125\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.336363636363636\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.13636363636363\n",
      "  vram_util_percent0: 0.09842566287878789\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 35.666666666666664\n",
      "  player_1: 28.133333333333336\n",
      "  player_2: 16.066666666666663\n",
      "policy_reward_mean:\n",
      "  player_0: 9.731333333333332\n",
      "  player_1: -0.050666666666666735\n",
      "  player_2: -6.680666666666668\n",
      "policy_reward_min:\n",
      "  player_0: -18.266666666666662\n",
      "  player_1: -24.0\n",
      "  player_2: -29.666666666666668\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11191583207319276\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4000299363146123\n",
      "  mean_inference_ms: 2.0422480232575357\n",
      "  mean_raw_obs_processing_ms: 0.26267422597308376\n",
      "time_since_restore: 851.6343071460724\n",
      "time_this_iter_s: 8.773804187774658\n",
      "time_total_s: 851.6343071460724\n",
      "timers:\n",
      "  learn_throughput: 623.635\n",
      "  learn_time_ms: 6414.011\n",
      "  load_throughput: 5993147.103\n",
      "  load_time_ms: 0.667\n",
      "  sample_throughput: 556.954\n",
      "  sample_time_ms: 7181.923\n",
      "  update_time_ms: 4.099\n",
      "timestamp: 1643325921\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 500000\n",
      "training_iteration: 125\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 507970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-25-38\n",
      "done: false\n",
      "episode_len_mean: 168.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 3124\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6484733812014262\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015002315944220754\n",
      "        policy_loss: 0.0018287151299106578\n",
      "        total_loss: 18.72432603677114\n",
      "        vf_explained_var: 0.01758563220500946\n",
      "        vf_loss: 18.71949693044027\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.749833751519521\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017626307758309437\n",
      "        policy_loss: -0.10091469903554147\n",
      "        total_loss: 22.673288667996726\n",
      "        vf_explained_var: 0.2808586891492208\n",
      "        vf_loss: 22.770677989323932\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7918603006998698\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01378244710913757\n",
      "        policy_loss: -0.06371046844869852\n",
      "        total_loss: 26.135923922856648\n",
      "        vf_explained_var: 0.25194136798381805\n",
      "        vf_loss: 26.193432499567667\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 507970\n",
      "  num_agent_steps_trained: 507970\n",
      "  num_steps_sampled: 508000\n",
      "  num_steps_trained: 508000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 127\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.16363636363636\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.2\n",
      "  vram_util_percent0: 0.09977213541666666\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 35.666666666666664\n",
      "  player_1: 24.199999999999996\n",
      "  player_2: 20.06666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 10.021333333333333\n",
      "  player_1: -1.2786666666666666\n",
      "  player_2: -5.7426666666666675\n",
      "policy_reward_min:\n",
      "  player_0: -15.0\n",
      "  player_1: -25.666666666666668\n",
      "  player_2: -27.333333333333332\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11228156369146688\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3998965826209573\n",
      "  mean_inference_ms: 2.052521650903864\n",
      "  mean_raw_obs_processing_ms: 0.26357287886892367\n",
      "time_since_restore: 868.6903085708618\n",
      "time_this_iter_s: 8.919550657272339\n",
      "time_total_s: 868.6903085708618\n",
      "timers:\n",
      "  learn_throughput: 598.247\n",
      "  learn_time_ms: 6686.205\n",
      "  load_throughput: 5926041.468\n",
      "  load_time_ms: 0.675\n",
      "  sample_throughput: 536.642\n",
      "  sample_time_ms: 7453.753\n",
      "  update_time_ms: 4.38\n",
      "timestamp: 1643325938\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 508000\n",
      "training_iteration: 127\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 515968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-25-54\n",
      "done: false\n",
      "episode_len_mean: 173.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 3169\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6643266136447589\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017421361354951405\n",
      "        policy_loss: -0.1073475746313731\n",
      "        total_loss: 29.83004223982493\n",
      "        vf_explained_var: 0.12331965784231821\n",
      "        vf_loss: 29.933905239105226\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.723812588651975\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019981132501088725\n",
      "        policy_loss: -0.06727413132165869\n",
      "        total_loss: 28.497471340497334\n",
      "        vf_explained_var: 0.08061832547187806\n",
      "        vf_loss: 28.56074919462204\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7439852182070414\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012926914919225964\n",
      "        policy_loss: -0.04132845420390367\n",
      "        total_loss: 30.23875619570414\n",
      "        vf_explained_var: -0.008994693756103516\n",
      "        vf_loss: 30.274267455736798\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 515968\n",
      "  num_agent_steps_trained: 515968\n",
      "  num_steps_sampled: 516000\n",
      "  num_steps_trained: 516000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 129\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.840000000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.0\n",
      "  vram_util_percent0: 0.09912109375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.333333333333336\n",
      "  player_1: 24.199999999999996\n",
      "  player_2: 20.06666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 10.69266666666667\n",
      "  player_1: -2.895333333333333\n",
      "  player_2: -4.797333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -16.333333333333332\n",
      "  player_1: -27.666666666666664\n",
      "  player_2: -27.333333333333332\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11303076981599056\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4010041733550411\n",
      "  mean_inference_ms: 2.065750714752401\n",
      "  mean_raw_obs_processing_ms: 0.26506277176560944\n",
      "time_since_restore: 885.2168664932251\n",
      "time_this_iter_s: 8.003403902053833\n",
      "time_total_s: 885.2168664932251\n",
      "timers:\n",
      "  learn_throughput: 586.914\n",
      "  learn_time_ms: 6815.304\n",
      "  load_throughput: 5637126.537\n",
      "  load_time_ms: 0.71\n",
      "  sample_throughput: 513.373\n",
      "  sample_time_ms: 7791.602\n",
      "  update_time_ms: 4.641\n",
      "timestamp: 1643325954\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 516000\n",
      "training_iteration: 129\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 523968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-26-12\n",
      "done: false\n",
      "episode_len_mean: 182.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 3210\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6375215951601664\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017136937271751102\n",
      "        policy_loss: -0.06667443076769511\n",
      "        total_loss: 23.073523661295575\n",
      "        vf_explained_var: 0.3858616884549459\n",
      "        vf_loss: 23.136770811080932\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7528121674060821\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016598143669081463\n",
      "        policy_loss: -0.08778389331962293\n",
      "        total_loss: 23.975364480018616\n",
      "        vf_explained_var: -0.11279137055079143\n",
      "        vf_loss: 24.059828969637554\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6989811809857687\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013352890622390987\n",
      "        policy_loss: -0.08824308195772271\n",
      "        total_loss: 21.115017212231955\n",
      "        vf_explained_var: -0.029557304779688518\n",
      "        vf_loss: 21.19725144704183\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 523968\n",
      "  num_agent_steps_trained: 523968\n",
      "  num_steps_sampled: 524000\n",
      "  num_steps_trained: 524000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 131\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.89090909090909\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.04545454545455\n",
      "  vram_util_percent0: 0.09858842329545457\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.333333333333336\n",
      "  player_1: 23.0\n",
      "  player_2: 18.0\n",
      "policy_reward_mean:\n",
      "  player_0: 8.722666666666667\n",
      "  player_1: -1.6833333333333333\n",
      "  player_2: -4.0393333333333326\n",
      "policy_reward_min:\n",
      "  player_0: -16.333333333333332\n",
      "  player_1: -27.666666666666668\n",
      "  player_2: -48.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1139532166378338\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40146772377032547\n",
      "  mean_inference_ms: 2.079587148237256\n",
      "  mean_raw_obs_processing_ms: 0.26618594641466714\n",
      "time_since_restore: 902.434910774231\n",
      "time_this_iter_s: 9.013082504272461\n",
      "time_total_s: 902.434910774231\n",
      "timers:\n",
      "  learn_throughput: 559.2\n",
      "  learn_time_ms: 7153.082\n",
      "  load_throughput: 5297343.311\n",
      "  load_time_ms: 0.755\n",
      "  sample_throughput: 497.362\n",
      "  sample_time_ms: 8042.435\n",
      "  update_time_ms: 4.931\n",
      "timestamp: 1643325972\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 524000\n",
      "training_iteration: 131\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 531968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-26-28\n",
      "done: false\n",
      "episode_len_mean: 194.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 3253\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6377274481455485\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021191224216169893\n",
      "        policy_loss: -0.06945522438734769\n",
      "        total_loss: 47.14280685106913\n",
      "        vf_explained_var: 0.14111867189407348\n",
      "        vf_loss: 47.20802381515503\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7629921778043112\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022983440293452683\n",
      "        policy_loss: -0.09681972814102968\n",
      "        total_loss: 32.10584524949392\n",
      "        vf_explained_var: 0.15446363449096678\n",
      "        vf_loss: 32.19806848526001\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.715396936138471\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012848973046250952\n",
      "        policy_loss: -0.0821651264031728\n",
      "        total_loss: 37.02510272026062\n",
      "        vf_explained_var: 0.20520107011000316\n",
      "        vf_loss: 37.101485617955525\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 531968\n",
      "  num_agent_steps_trained: 531968\n",
      "  num_steps_sampled: 532000\n",
      "  num_steps_trained: 532000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 133\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.78\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.0\n",
      "  vram_util_percent0: 0.09928385416666666\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.0\n",
      "  player_1: 23.0\n",
      "  player_2: 17.46666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 7.337999999999999\n",
      "  player_1: -0.7559999999999997\n",
      "  player_2: -3.582\n",
      "policy_reward_min:\n",
      "  player_0: -16.2\n",
      "  player_1: -27.666666666666668\n",
      "  player_2: -48.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11473934647418778\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4024159779356847\n",
      "  mean_inference_ms: 2.0937615003243555\n",
      "  mean_raw_obs_processing_ms: 0.2675414067607116\n",
      "time_since_restore: 918.5997722148895\n",
      "time_this_iter_s: 7.965347528457642\n",
      "time_total_s: 918.5997722148895\n",
      "timers:\n",
      "  learn_throughput: 547.109\n",
      "  learn_time_ms: 7311.162\n",
      "  load_throughput: 4592219.85\n",
      "  load_time_ms: 0.871\n",
      "  sample_throughput: 474.476\n",
      "  sample_time_ms: 8430.344\n",
      "  update_time_ms: 5.254\n",
      "timestamp: 1643325988\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 532000\n",
      "training_iteration: 133\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 539968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-26-44\n",
      "done: false\n",
      "episode_len_mean: 193.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3291\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6456930847962697\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016477082384961553\n",
      "        policy_loss: -0.08867308097581068\n",
      "        total_loss: 31.55001804033915\n",
      "        vf_explained_var: 0.2570952787001928\n",
      "        vf_loss: 31.63374821662903\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7513350478808085\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015064492009975462\n",
      "        policy_loss: -0.04489608313733091\n",
      "        total_loss: 19.160472994645435\n",
      "        vf_explained_var: 0.08550465007623037\n",
      "        vf_loss: 19.200849786599477\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7392641931772232\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014861599857589074\n",
      "        policy_loss: -0.10887960164186855\n",
      "        total_loss: 26.635800256729127\n",
      "        vf_explained_var: -0.19299649576346078\n",
      "        vf_loss: 26.737991894086203\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 539968\n",
      "  num_agent_steps_trained: 539968\n",
      "  num_steps_sampled: 540000\n",
      "  num_steps_trained: 540000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 135\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.559999999999995\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.07000000000001\n",
      "  vram_util_percent0: 0.09928385416666666\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.0\n",
      "  player_1: 16.0\n",
      "  player_2: 17.46666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 7.866666666666665\n",
      "  player_1: -2.185333333333334\n",
      "  player_2: -2.6813333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -16.2\n",
      "  player_1: -28.333333333333336\n",
      "  player_2: -41.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11527530278228176\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40304980070163415\n",
      "  mean_inference_ms: 2.1049357867427223\n",
      "  mean_raw_obs_processing_ms: 0.2686617040386886\n",
      "time_since_restore: 934.5244181156158\n",
      "time_this_iter_s: 7.943936824798584\n",
      "time_total_s: 934.5244181156158\n",
      "timers:\n",
      "  learn_throughput: 558.935\n",
      "  learn_time_ms: 7156.468\n",
      "  load_throughput: 4503346.128\n",
      "  load_time_ms: 0.888\n",
      "  sample_throughput: 474.69\n",
      "  sample_time_ms: 8426.544\n",
      "  update_time_ms: 5.517\n",
      "timestamp: 1643326004\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 540000\n",
      "training_iteration: 135\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 547968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-27-00\n",
      "done: false\n",
      "episode_len_mean: 188.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 3338\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6118900402386983\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015398879313985816\n",
      "        policy_loss: -0.07980504028188685\n",
      "        total_loss: 43.37696772575379\n",
      "        vf_explained_var: 0.21595761636892954\n",
      "        vf_loss: 43.45215340296427\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7156885641813279\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01537794367401754\n",
      "        policy_loss: -0.07300196962353463\n",
      "        total_loss: 29.000582138697307\n",
      "        vf_explained_var: 0.2891641773780187\n",
      "        vf_loss: 29.06897089322408\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.762659916083018\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016089035080988388\n",
      "        policy_loss: -0.08173049378596867\n",
      "        total_loss: 35.13315466562907\n",
      "        vf_explained_var: -0.12166901548703511\n",
      "        vf_loss: 35.207645196914676\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 547968\n",
      "  num_agent_steps_trained: 547968\n",
      "  num_steps_sampled: 548000\n",
      "  num_steps_trained: 548000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 137\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.22\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.1\n",
      "  vram_util_percent0: 0.09928385416666666\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 38.0\n",
      "  player_1: 23.46666666666667\n",
      "  player_2: 20.333333333333332\n",
      "policy_reward_mean:\n",
      "  player_0: 9.150666666666664\n",
      "  player_1: -2.463333333333334\n",
      "  player_2: -3.687333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -23.93333333333333\n",
      "  player_1: -28.333333333333336\n",
      "  player_2: -43.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11634477768970701\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4037049333053497\n",
      "  mean_inference_ms: 2.1193195621915253\n",
      "  mean_raw_obs_processing_ms: 0.2698763240512583\n",
      "time_since_restore: 950.6224589347839\n",
      "time_this_iter_s: 7.951845169067383\n",
      "time_total_s: 950.6224589347839\n",
      "timers:\n",
      "  learn_throughput: 567.087\n",
      "  learn_time_ms: 7053.587\n",
      "  load_throughput: 4374078.632\n",
      "  load_time_ms: 0.914\n",
      "  sample_throughput: 479.572\n",
      "  sample_time_ms: 8340.767\n",
      "  update_time_ms: 5.416\n",
      "timestamp: 1643326020\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 548000\n",
      "training_iteration: 137\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 555968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-27-18\n",
      "done: false\n",
      "episode_len_mean: 181.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 3380\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6658900594711303\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015532314494436528\n",
      "        policy_loss: -0.09274072987337907\n",
      "        total_loss: 32.87600932677587\n",
      "        vf_explained_var: -0.053813220461209614\n",
      "        vf_loss: 32.96409037033717\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7372892791032791\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01578565548081921\n",
      "        policy_loss: -0.05444603946059942\n",
      "        total_loss: 23.869932903448742\n",
      "        vf_explained_var: 0.2818374043703079\n",
      "        vf_loss: 23.919643508593243\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7440592336654663\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01084400594193236\n",
      "        policy_loss: -0.04189393723383546\n",
      "        total_loss: 26.50741234302521\n",
      "        vf_explained_var: 0.26074053506056466\n",
      "        vf_loss: 26.544426574707032\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 555968\n",
      "  num_agent_steps_trained: 555968\n",
      "  num_steps_sampled: 556000\n",
      "  num_steps_trained: 556000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 139\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.863636363636363\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.08181818181819\n",
      "  vram_util_percent0: 0.09928385416666666\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 38.0\n",
      "  player_1: 23.46666666666667\n",
      "  player_2: 20.333333333333332\n",
      "policy_reward_mean:\n",
      "  player_0: 8.984666666666667\n",
      "  player_1: -2.2993333333333332\n",
      "  player_2: -3.6853333333333342\n",
      "policy_reward_min:\n",
      "  player_0: -23.93333333333333\n",
      "  player_1: -31.0\n",
      "  player_2: -43.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1165690099444499\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40451339647339646\n",
      "  mean_inference_ms: 2.1288607511278483\n",
      "  mean_raw_obs_processing_ms: 0.2711640777029312\n",
      "time_since_restore: 968.3834855556488\n",
      "time_this_iter_s: 9.477730989456177\n",
      "time_total_s: 968.3834855556488\n",
      "timers:\n",
      "  learn_throughput: 557.379\n",
      "  learn_time_ms: 7176.449\n",
      "  load_throughput: 4350824.927\n",
      "  load_time_ms: 0.919\n",
      "  sample_throughput: 486.079\n",
      "  sample_time_ms: 8229.112\n",
      "  update_time_ms: 5.526\n",
      "timestamp: 1643326038\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 556000\n",
      "training_iteration: 139\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 563968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-27-37\n",
      "done: false\n",
      "episode_len_mean: 178.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 3424\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7020163319508235\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014629717898936481\n",
      "        policy_loss: -0.1344773180099825\n",
      "        total_loss: 33.01643821875254\n",
      "        vf_explained_var: 0.18230670948823294\n",
      "        vf_loss: 33.14652691364288\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7520577818155288\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01659236812374123\n",
      "        policy_loss: -0.06300327624815205\n",
      "        total_loss: 28.73061043739319\n",
      "        vf_explained_var: 0.05461465179920197\n",
      "        vf_loss: 28.788635997772218\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7534942736228307\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012381663988958281\n",
      "        policy_loss: -0.04616790358132372\n",
      "        total_loss: 27.781642662684124\n",
      "        vf_explained_var: 0.050594027837117514\n",
      "        vf_loss: 27.82223889350891\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 563968\n",
      "  num_agent_steps_trained: 563968\n",
      "  num_steps_sampled: 564000\n",
      "  num_steps_trained: 564000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 141\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.390909090909087\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.40909090909091\n",
      "  vram_util_percent0: 0.09928385416666666\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 28.333333333333336\n",
      "  player_2: 19.599999999999998\n",
      "policy_reward_mean:\n",
      "  player_0: 7.954666666666665\n",
      "  player_1: -2.0473333333333343\n",
      "  player_2: -2.907333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -22.0\n",
      "  player_1: -31.0\n",
      "  player_2: -27.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11735657489350365\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40588676135467494\n",
      "  mean_inference_ms: 2.145840340445445\n",
      "  mean_raw_obs_processing_ms: 0.2727184173364088\n",
      "time_since_restore: 986.8456041812897\n",
      "time_this_iter_s: 8.683501243591309\n",
      "time_total_s: 986.8456041812897\n",
      "timers:\n",
      "  learn_throughput: 550.853\n",
      "  learn_time_ms: 7261.464\n",
      "  load_throughput: 4078475.301\n",
      "  load_time_ms: 0.981\n",
      "  sample_throughput: 469.773\n",
      "  sample_time_ms: 8514.75\n",
      "  update_time_ms: 5.441\n",
      "timestamp: 1643326057\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 564000\n",
      "training_iteration: 141\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 571968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-27-52\n",
      "done: false\n",
      "episode_len_mean: 181.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 3469\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6338322835167249\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014001289496679116\n",
      "        policy_loss: -0.08428423031854133\n",
      "        total_loss: 26.680848712921144\n",
      "        vf_explained_var: -0.04087676028410594\n",
      "        vf_loss: 26.76093243598938\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7294925421476364\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013915995314384115\n",
      "        policy_loss: -0.023209225631629426\n",
      "        total_loss: 37.25710999170939\n",
      "        vf_explained_var: -0.13345839738845824\n",
      "        vf_loss: 37.27614410718282\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6948941073815028\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011929643632071949\n",
      "        policy_loss: -0.1070985318844517\n",
      "        total_loss: 29.674402187665304\n",
      "        vf_explained_var: 0.007154918909072876\n",
      "        vf_loss: 29.776132556597393\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 571968\n",
      "  num_agent_steps_trained: 571968\n",
      "  num_steps_sampled: 572000\n",
      "  num_steps_trained: 572000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 143\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.512500000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.9\n",
      "  vram_util_percent0: 0.09928385416666667\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 28.333333333333336\n",
      "  player_2: 21.06666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 7.194666666666666\n",
      "  player_1: -1.9773333333333334\n",
      "  player_2: -2.2173333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -22.0\n",
      "  player_1: -27.133333333333333\n",
      "  player_2: -21.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11831513210852447\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40748626578055513\n",
      "  mean_inference_ms: 2.164923226656743\n",
      "  mean_raw_obs_processing_ms: 0.2745551529076859\n",
      "time_since_restore: 1002.5666651725769\n",
      "time_this_iter_s: 6.9355432987213135\n",
      "time_total_s: 1002.5666651725769\n",
      "timers:\n",
      "  learn_throughput: 554.673\n",
      "  learn_time_ms: 7211.462\n",
      "  load_throughput: 4624497.919\n",
      "  load_time_ms: 0.865\n",
      "  sample_throughput: 468.64\n",
      "  sample_time_ms: 8535.335\n",
      "  update_time_ms: 5.143\n",
      "timestamp: 1643326072\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 572000\n",
      "training_iteration: 143\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 579968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-28-07\n",
      "done: false\n",
      "episode_len_mean: 179.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 3513\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6496132349967957\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0140547673357014\n",
      "        policy_loss: -0.08071188640470306\n",
      "        total_loss: 33.91433723926544\n",
      "        vf_explained_var: -0.030658321380615236\n",
      "        vf_loss: 33.99083278814952\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7351964648564656\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01605826739539528\n",
      "        policy_loss: -0.05504901097466548\n",
      "        total_loss: 20.334743574460347\n",
      "        vf_explained_var: 0.039220729867617286\n",
      "        vf_loss: 20.38497501770655\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7310009676218033\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013291802154975206\n",
      "        policy_loss: -0.03776097509389122\n",
      "        total_loss: 41.34468562444051\n",
      "        vf_explained_var: -0.008376474777857463\n",
      "        vf_loss: 41.37646514574687\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 579968\n",
      "  num_agent_steps_trained: 579968\n",
      "  num_steps_sampled: 580000\n",
      "  num_steps_trained: 580000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 145\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.419999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.010000000000005\n",
      "  vram_util_percent0: 0.09928385416666666\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 21.866666666666664\n",
      "  player_2: 23.46666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 7.852666666666669\n",
      "  player_1: -3.313333333333334\n",
      "  player_2: -1.539333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -22.0\n",
      "  player_1: -27.333333333333332\n",
      "  player_2: -28.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11874542131268552\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40816907658648155\n",
      "  mean_inference_ms: 2.1714863753799163\n",
      "  mean_raw_obs_processing_ms: 0.27536977747359714\n",
      "time_since_restore: 1017.2344019412994\n",
      "time_this_iter_s: 7.677778959274292\n",
      "time_total_s: 1017.2344019412994\n",
      "timers:\n",
      "  learn_throughput: 559.427\n",
      "  learn_time_ms: 7150.177\n",
      "  load_throughput: 4451370.655\n",
      "  load_time_ms: 0.899\n",
      "  sample_throughput: 481.382\n",
      "  sample_time_ms: 8309.403\n",
      "  update_time_ms: 4.872\n",
      "timestamp: 1643326087\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 580000\n",
      "training_iteration: 145\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 587968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-28-21\n",
      "done: false\n",
      "episode_len_mean: 174.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 3560\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.611315426727136\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017258144846079244\n",
      "        policy_loss: -0.09145709152022997\n",
      "        total_loss: 30.89855565706889\n",
      "        vf_explained_var: 0.04295552889506022\n",
      "        vf_loss: 30.984835324287413\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7901785393555959\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015832090244184655\n",
      "        policy_loss: -0.08735643944392602\n",
      "        total_loss: 36.817400239308675\n",
      "        vf_explained_var: 0.2503568387031555\n",
      "        vf_loss: 36.90000713030497\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6972723066806793\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011277213944510246\n",
      "        policy_loss: -0.07087484617562344\n",
      "        total_loss: 33.713401465415956\n",
      "        vf_explained_var: 0.08410340170065563\n",
      "        vf_loss: 33.77920141379038\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 587968\n",
      "  num_agent_steps_trained: 587968\n",
      "  num_steps_sampled: 588000\n",
      "  num_steps_trained: 588000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 147\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.375\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.1\n",
      "  vram_util_percent0: 0.09928385416666667\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 27.666666666666664\n",
      "  player_1: 19.0\n",
      "  player_2: 23.46666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 9.564\n",
      "  player_1: -4.014\n",
      "  player_2: -2.55\n",
      "policy_reward_min:\n",
      "  player_0: -15.0\n",
      "  player_1: -27.333333333333332\n",
      "  player_2: -28.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11868642797083374\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40774153436518246\n",
      "  mean_inference_ms: 2.1730765678271737\n",
      "  mean_raw_obs_processing_ms: 0.2755882186092564\n",
      "time_since_restore: 1030.5882141590118\n",
      "time_this_iter_s: 6.384469032287598\n",
      "time_total_s: 1030.5882141590118\n",
      "timers:\n",
      "  learn_throughput: 576.013\n",
      "  learn_time_ms: 6944.292\n",
      "  load_throughput: 4661632.676\n",
      "  load_time_ms: 0.858\n",
      "  sample_throughput: 490.847\n",
      "  sample_time_ms: 8149.174\n",
      "  update_time_ms: 5.3\n",
      "timestamp: 1643326101\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 588000\n",
      "training_iteration: 147\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 595970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-28-33\n",
      "done: false\n",
      "episode_len_mean: 172.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 3606\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5996456916133562\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013010993686721121\n",
      "        policy_loss: -0.03171409733593464\n",
      "        total_loss: 35.26189768473307\n",
      "        vf_explained_var: -0.013591829538345337\n",
      "        vf_loss: 35.28970864613851\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7159151325623194\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016299793685089742\n",
      "        policy_loss: -0.03723330477873484\n",
      "        total_loss: 47.894965403874714\n",
      "        vf_explained_var: 0.2600845758120219\n",
      "        vf_loss: 47.927308791478474\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7211779516935348\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011039967258438992\n",
      "        policy_loss: -0.13188876740634442\n",
      "        total_loss: 38.51252457936605\n",
      "        vf_explained_var: 0.2449090035756429\n",
      "        vf_loss: 38.6394453382492\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 595970\n",
      "  num_agent_steps_trained: 595970\n",
      "  num_steps_sampled: 596000\n",
      "  num_steps_trained: 596000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 149\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.387500000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.1125\n",
      "  vram_util_percent0: 0.09891764322916667\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 19.133333333333333\n",
      "  player_2: 32.53333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 8.231333333333334\n",
      "  player_1: -2.8406666666666665\n",
      "  player_2: -2.3906666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -24.133333333333333\n",
      "  player_1: -27.066666666666663\n",
      "  player_2: -28.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11815565954241551\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40635461239896215\n",
      "  mean_inference_ms: 2.168696115020131\n",
      "  mean_raw_obs_processing_ms: 0.2751234116168373\n",
      "time_since_restore: 1043.2600014209747\n",
      "time_this_iter_s: 6.289097785949707\n",
      "time_total_s: 1043.2600014209747\n",
      "timers:\n",
      "  learn_throughput: 612.538\n",
      "  learn_time_ms: 6530.211\n",
      "  load_throughput: 4773985.146\n",
      "  load_time_ms: 0.838\n",
      "  sample_throughput: 513.811\n",
      "  sample_time_ms: 7784.961\n",
      "  update_time_ms: 4.887\n",
      "timestamp: 1643326113\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 596000\n",
      "training_iteration: 149\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 603968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-28-47\n",
      "done: false\n",
      "episode_len_mean: 176.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 3649\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5994598047931989\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014730986657777976\n",
      "        policy_loss: -0.07595550457326074\n",
      "        total_loss: 32.66706477324168\n",
      "        vf_explained_var: 0.18391549547513325\n",
      "        vf_loss: 32.738600873152414\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7583357286453247\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015439799779403612\n",
      "        policy_loss: -0.06407059154473245\n",
      "        total_loss: 35.97795435587565\n",
      "        vf_explained_var: 0.020271276036898295\n",
      "        vf_loss: 36.037393232981366\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6902926673491796\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01320470405170757\n",
      "        policy_loss: -0.07076605430493753\n",
      "        total_loss: 22.113104450702668\n",
      "        vf_explained_var: -0.055768962502479556\n",
      "        vf_loss: 22.177928587595623\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 603968\n",
      "  num_agent_steps_trained: 603968\n",
      "  num_steps_sampled: 604000\n",
      "  num_steps_trained: 604000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 151\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.344444444444445\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.1\n",
      "  vram_util_percent0: 0.09908492476851853\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 19.133333333333333\n",
      "  player_2: 32.53333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 8.154666666666667\n",
      "  player_1: -3.075333333333333\n",
      "  player_2: -2.0793333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -24.133333333333333\n",
      "  player_1: -29.93333333333333\n",
      "  player_2: -21.666666666666668\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11819048238991715\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4053610358541105\n",
      "  mean_inference_ms: 2.1658903625851145\n",
      "  mean_raw_obs_processing_ms: 0.2748803742899678\n",
      "time_since_restore: 1056.4664301872253\n",
      "time_this_iter_s: 6.598849534988403\n",
      "time_total_s: 1056.4664301872253\n",
      "timers:\n",
      "  learn_throughput: 654.719\n",
      "  learn_time_ms: 6109.494\n",
      "  load_throughput: 5516099.293\n",
      "  load_time_ms: 0.725\n",
      "  sample_throughput: 558.239\n",
      "  sample_time_ms: 7165.395\n",
      "  update_time_ms: 4.846\n",
      "timestamp: 1643326127\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 604000\n",
      "training_iteration: 151\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 611968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-29-03\n",
      "done: false\n",
      "episode_len_mean: 181.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3690\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6263784402608872\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013825865543512918\n",
      "        policy_loss: -0.07966618305072189\n",
      "        total_loss: 31.440532506306965\n",
      "        vf_explained_var: 0.13241057395935057\n",
      "        vf_loss: 31.51605094273885\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6981394843260447\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015401572159162257\n",
      "        policy_loss: -0.09130531243979931\n",
      "        total_loss: 33.420955692927045\n",
      "        vf_explained_var: 0.3892114049196243\n",
      "        vf_loss: 33.50764072100321\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7258534491062164\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014146713110785641\n",
      "        policy_loss: -0.06121305202133954\n",
      "        total_loss: 31.868595293362937\n",
      "        vf_explained_var: 0.2463585106531779\n",
      "        vf_loss: 31.923442192077637\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 611968\n",
      "  num_agent_steps_trained: 611968\n",
      "  num_steps_sampled: 612000\n",
      "  num_steps_trained: 612000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 153\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.583333333333332\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.70833333333334\n",
      "  vram_util_percent0: 0.10065375434027779\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 25.46666666666667\n",
      "  player_2: 32.53333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 7.993333333333332\n",
      "  player_1: -1.870666666666667\n",
      "  player_2: -3.122666666666668\n",
      "policy_reward_min:\n",
      "  player_0: -17.333333333333332\n",
      "  player_1: -29.93333333333333\n",
      "  player_2: -25.53333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11834151329602839\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40465551994967064\n",
      "  mean_inference_ms: 2.1676515699051015\n",
      "  mean_raw_obs_processing_ms: 0.27499366803102476\n",
      "time_since_restore: 1072.4885635375977\n",
      "time_this_iter_s: 9.45780897140503\n",
      "time_total_s: 1072.4885635375977\n",
      "timers:\n",
      "  learn_throughput: 649.753\n",
      "  learn_time_ms: 6156.19\n",
      "  load_throughput: 5491903.499\n",
      "  load_time_ms: 0.728\n",
      "  sample_throughput: 588.858\n",
      "  sample_time_ms: 6792.807\n",
      "  update_time_ms: 5.077\n",
      "timestamp: 1643326143\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 612000\n",
      "training_iteration: 153\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 619968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-29-20\n",
      "done: false\n",
      "episode_len_mean: 183.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 3735\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6036636982361475\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014574707509570241\n",
      "        policy_loss: -0.07699398636817932\n",
      "        total_loss: 31.820872820218405\n",
      "        vf_explained_var: 0.43305373926957447\n",
      "        vf_loss: 31.89349419434865\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7211441198984782\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01389209394936885\n",
      "        policy_loss: -0.0701429919525981\n",
      "        total_loss: 39.91826187769572\n",
      "        vf_explained_var: 0.31149014135201775\n",
      "        vf_loss: 39.98423690160116\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6938443805774053\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012884804761874875\n",
      "        policy_loss: -0.10472067308146507\n",
      "        total_loss: 20.42310167948405\n",
      "        vf_explained_var: 0.22672699471314747\n",
      "        vf_loss: 20.52202425003052\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 619968\n",
      "  num_agent_steps_trained: 619968\n",
      "  num_steps_sampled: 620000\n",
      "  num_steps_trained: 620000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 155\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.663636363636364\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.70909090909092\n",
      "  vram_util_percent0: 0.10370797821969698\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.333333333333336\n",
      "  player_1: 25.46666666666667\n",
      "  player_2: 19.333333333333332\n",
      "policy_reward_mean:\n",
      "  player_0: 8.489333333333333\n",
      "  player_1: -3.3506666666666662\n",
      "  player_2: -2.138666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -17.333333333333332\n",
      "  player_1: -31.666666666666664\n",
      "  player_2: -28.53333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1188104063769978\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4050749737069265\n",
      "  mean_inference_ms: 2.1759521982180927\n",
      "  mean_raw_obs_processing_ms: 0.2757551475027715\n",
      "time_since_restore: 1090.0929412841797\n",
      "time_this_iter_s: 9.016283988952637\n",
      "time_total_s: 1090.0929412841797\n",
      "timers:\n",
      "  learn_throughput: 627.489\n",
      "  learn_time_ms: 6374.612\n",
      "  load_throughput: 5606046.714\n",
      "  load_time_ms: 0.714\n",
      "  sample_throughput: 554.185\n",
      "  sample_time_ms: 7217.806\n",
      "  update_time_ms: 4.975\n",
      "timestamp: 1643326160\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 620000\n",
      "training_iteration: 155\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 627968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-29-37\n",
      "done: false\n",
      "episode_len_mean: 176.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 3782\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6121113663911819\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01752058822427595\n",
      "        policy_loss: -0.05681890508470436\n",
      "        total_loss: 27.509530668258666\n",
      "        vf_explained_var: 0.10208500901858011\n",
      "        vf_loss: 27.561093295415244\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.754798882206281\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014040361922895919\n",
      "        policy_loss: -0.10642801697055498\n",
      "        total_loss: 30.338802995681764\n",
      "        vf_explained_var: 0.1220092503229777\n",
      "        vf_loss: 30.4410187180837\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.687766593893369\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01158318232998378\n",
      "        policy_loss: -0.0350524717476219\n",
      "        total_loss: 24.217913722991945\n",
      "        vf_explained_var: 0.21080365300178527\n",
      "        vf_loss: 24.247753648757936\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 627968\n",
      "  num_agent_steps_trained: 627968\n",
      "  num_steps_sampled: 628000\n",
      "  num_steps_trained: 628000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 157\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.880000000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.730000000000004\n",
      "  vram_util_percent0: 0.105126953125\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.333333333333336\n",
      "  player_1: 25.46666666666667\n",
      "  player_2: 17.066666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 7.757333333333334\n",
      "  player_1: -2.162666666666667\n",
      "  player_2: -2.594666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -16.93333333333333\n",
      "  player_1: -31.666666666666664\n",
      "  player_2: -28.53333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11945844203402038\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4059906700703195\n",
      "  mean_inference_ms: 2.187006882857971\n",
      "  mean_raw_obs_processing_ms: 0.2770028339169022\n",
      "time_since_restore: 1107.149876832962\n",
      "time_this_iter_s: 8.002265453338623\n",
      "time_total_s: 1107.149876832962\n",
      "timers:\n",
      "  learn_throughput: 599.661\n",
      "  learn_time_ms: 6670.439\n",
      "  load_throughput: 5380762.027\n",
      "  load_time_ms: 0.743\n",
      "  sample_throughput: 528.712\n",
      "  sample_time_ms: 7565.553\n",
      "  update_time_ms: 4.564\n",
      "timestamp: 1643326177\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 628000\n",
      "training_iteration: 157\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 635968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-29-54\n",
      "done: false\n",
      "episode_len_mean: 180.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 3827\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6217638184626897\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019111252625213335\n",
      "        policy_loss: -0.018298108614981174\n",
      "        total_loss: 39.09715753555298\n",
      "        vf_explained_var: 0.23827175974845885\n",
      "        vf_loss: 39.10972268740336\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6340167184670766\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013702196698769828\n",
      "        policy_loss: -0.046697206520475446\n",
      "        total_loss: 25.23678879817327\n",
      "        vf_explained_var: 0.1735347491502762\n",
      "        vf_loss: 25.279375570615134\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.682163916627566\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011019640219407545\n",
      "        policy_loss: -0.1488463480770588\n",
      "        total_loss: 46.43690619468689\n",
      "        vf_explained_var: 0.34744621952374777\n",
      "        vf_loss: 46.58079342365265\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 635968\n",
      "  num_agent_steps_trained: 635968\n",
      "  num_steps_sampled: 636000\n",
      "  num_steps_trained: 636000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 159\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.130000000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.75\n",
      "  vram_util_percent0: 0.10050455729166667\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 19.133333333333336\n",
      "  player_2: 31.800000000000004\n",
      "policy_reward_mean:\n",
      "  player_0: 7.87\n",
      "  player_1: -1.7739999999999998\n",
      "  player_2: -3.0959999999999996\n",
      "policy_reward_min:\n",
      "  player_0: -20.666666666666664\n",
      "  player_1: -31.666666666666664\n",
      "  player_2: -31.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1197211720349411\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40690253571882123\n",
      "  mean_inference_ms: 2.196952754389483\n",
      "  mean_raw_obs_processing_ms: 0.2779559056644413\n",
      "time_since_restore: 1123.4638142585754\n",
      "time_this_iter_s: 7.9958343505859375\n",
      "time_total_s: 1123.4638142585754\n",
      "timers:\n",
      "  learn_throughput: 575.405\n",
      "  learn_time_ms: 6951.622\n",
      "  load_throughput: 5231925.656\n",
      "  load_time_ms: 0.765\n",
      "  sample_throughput: 504.461\n",
      "  sample_time_ms: 7929.25\n",
      "  update_time_ms: 4.84\n",
      "timestamp: 1643326194\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 636000\n",
      "training_iteration: 159\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 643968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-30-10\n",
      "done: false\n",
      "episode_len_mean: 179.12\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 3873\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6214612209796906\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01513832441831255\n",
      "        policy_loss: 0.004566651938172678\n",
      "        total_loss: 39.14187612533569\n",
      "        vf_explained_var: 0.22825538158416747\n",
      "        vf_loss: 39.13276816050212\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7310284654299418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016801852429302558\n",
      "        policy_loss: -0.10661491368276378\n",
      "        total_loss: 33.588825736045834\n",
      "        vf_explained_var: 0.12140115300814311\n",
      "        vf_loss: 33.69039992809296\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6629705478747686\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012499066854021142\n",
      "        policy_loss: -0.11479628525674343\n",
      "        total_loss: 27.420910781224567\n",
      "        vf_explained_var: 0.04571706096331279\n",
      "        vf_loss: 27.530082548459372\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 643968\n",
      "  num_agent_steps_trained: 643968\n",
      "  num_steps_sampled: 644000\n",
      "  num_steps_trained: 644000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 161\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.359999999999996\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.73\n",
      "  vram_util_percent0: 0.10304361979166668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.0\n",
      "  player_1: 18.733333333333334\n",
      "  player_2: 31.800000000000004\n",
      "policy_reward_mean:\n",
      "  player_0: 8.250000000000002\n",
      "  player_1: -2.238\n",
      "  player_2: -3.012\n",
      "policy_reward_min:\n",
      "  player_0: -20.666666666666664\n",
      "  player_1: -26.333333333333336\n",
      "  player_2: -31.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12041618427069331\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4077676640737485\n",
      "  mean_inference_ms: 2.2080708732733787\n",
      "  mean_raw_obs_processing_ms: 0.279040696598329\n",
      "time_since_restore: 1139.4347929954529\n",
      "time_this_iter_s: 7.865704774856567\n",
      "time_total_s: 1139.4347929954529\n",
      "timers:\n",
      "  learn_throughput: 557.537\n",
      "  learn_time_ms: 7174.407\n",
      "  load_throughput: 4836047.504\n",
      "  load_time_ms: 0.827\n",
      "  sample_throughput: 485.893\n",
      "  sample_time_ms: 8232.27\n",
      "  update_time_ms: 5.227\n",
      "timestamp: 1643326210\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 644000\n",
      "training_iteration: 161\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 651968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-30-26\n",
      "done: false\n",
      "episode_len_mean: 168.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 3923\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6313341735800108\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018413468873283514\n",
      "        policy_loss: -0.08156999493638674\n",
      "        total_loss: 39.98362443129221\n",
      "        vf_explained_var: -0.04677510956923167\n",
      "        vf_loss: 40.05966992378235\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6871081360181173\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018537975145839786\n",
      "        policy_loss: -0.09824825717757145\n",
      "        total_loss: 36.1089204788208\n",
      "        vf_explained_var: 0.17900300661722818\n",
      "        vf_loss: 36.201607364018756\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7001476915677388\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01324602370877983\n",
      "        policy_loss: -0.0414898934494704\n",
      "        total_loss: 24.0297674703598\n",
      "        vf_explained_var: 0.23196133732795715\n",
      "        vf_loss: 24.065296654701232\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 651968\n",
      "  num_agent_steps_trained: 651968\n",
      "  num_steps_sampled: 652000\n",
      "  num_steps_trained: 652000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 163\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.25\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.760000000000005\n",
      "  vram_util_percent0: 0.10174153645833332\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.0\n",
      "  player_1: 25.133333333333333\n",
      "  player_2: 19.266666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 7.774\n",
      "  player_1: -1.824\n",
      "  player_2: -2.95\n",
      "policy_reward_min:\n",
      "  player_0: -16.2\n",
      "  player_1: -20.333333333333336\n",
      "  player_2: -31.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12087535022722104\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4078727220220597\n",
      "  mean_inference_ms: 2.2183606123138593\n",
      "  mean_raw_obs_processing_ms: 0.2799467896386788\n",
      "time_since_restore: 1155.5567278862\n",
      "time_this_iter_s: 8.055867433547974\n",
      "time_total_s: 1155.5567278862\n",
      "timers:\n",
      "  learn_throughput: 558.16\n",
      "  learn_time_ms: 7166.398\n",
      "  load_throughput: 4663706.01\n",
      "  load_time_ms: 0.858\n",
      "  sample_throughput: 472.771\n",
      "  sample_time_ms: 8460.749\n",
      "  update_time_ms: 5.092\n",
      "timestamp: 1643326226\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 652000\n",
      "training_iteration: 163\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 659968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-30-47\n",
      "done: false\n",
      "episode_len_mean: 167.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 3968\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6090553013483683\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0181972402123453\n",
      "        policy_loss: -0.0947480506900077\n",
      "        total_loss: 32.330795481999715\n",
      "        vf_explained_var: 0.20067669689655304\n",
      "        vf_loss: 32.420084393819174\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7243910378217697\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015096740957420399\n",
      "        policy_loss: -0.05176681055376927\n",
      "        total_loss: 17.706942222913106\n",
      "        vf_explained_var: -0.07154064238071442\n",
      "        vf_loss: 17.754180075327554\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.695920311609904\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0137494424575405\n",
      "        policy_loss: -0.07866307297721505\n",
      "        total_loss: 38.21282779057821\n",
      "        vf_explained_var: 0.3454738118251165\n",
      "        vf_loss: 38.28530348141988\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 659968\n",
      "  num_agent_steps_trained: 659968\n",
      "  num_steps_sampled: 660000\n",
      "  num_steps_trained: 660000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 165\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 33.85384615384616\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.76923076923077\n",
      "  vram_util_percent0: 0.10123697916666669\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.0\n",
      "  player_1: 25.133333333333333\n",
      "  player_2: 22.133333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 6.7973333333333334\n",
      "  player_1: -1.604666666666667\n",
      "  player_2: -2.1926666666666663\n",
      "policy_reward_min:\n",
      "  player_0: -16.2\n",
      "  player_1: -29.133333333333333\n",
      "  player_2: -27.733333333333327\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12118478633361994\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.408586831691347\n",
      "  mean_inference_ms: 2.2287783009602093\n",
      "  mean_raw_obs_processing_ms: 0.2811866975879644\n",
      "time_since_restore: 1176.5494384765625\n",
      "time_this_iter_s: 10.808703422546387\n",
      "time_total_s: 1176.5494384765625\n",
      "timers:\n",
      "  learn_throughput: 536.066\n",
      "  learn_time_ms: 7461.761\n",
      "  load_throughput: 4245891.583\n",
      "  load_time_ms: 0.942\n",
      "  sample_throughput: 469.661\n",
      "  sample_time_ms: 8516.789\n",
      "  update_time_ms: 5.456\n",
      "timestamp: 1643326247\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 660000\n",
      "training_iteration: 165\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 667968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-31-04\n",
      "done: false\n",
      "episode_len_mean: 174.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 4013\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.564009549220403\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016403292155588094\n",
      "        policy_loss: -0.07768068067729474\n",
      "        total_loss: 24.27815488497416\n",
      "        vf_explained_var: 0.12702832460403443\n",
      "        vf_loss: 24.350914510091147\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7212062265475591\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014625154473275568\n",
      "        policy_loss: -0.05727966325978438\n",
      "        total_loss: 21.35677733898163\n",
      "        vf_explained_var: 0.23557405948638915\n",
      "        vf_loss: 21.409669551849365\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7073407445351283\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013421949692892668\n",
      "        policy_loss: -0.06380167311367889\n",
      "        total_loss: 25.774374690055847\n",
      "        vf_explained_var: 0.017723307609558106\n",
      "        vf_loss: 25.832136527697244\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 667968\n",
      "  num_agent_steps_trained: 667968\n",
      "  num_steps_sampled: 668000\n",
      "  num_steps_trained: 668000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 167\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.919999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 54.980000000000004\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.333333333333332\n",
      "  player_1: 23.4\n",
      "  player_2: 22.133333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 8.211333333333334\n",
      "  player_1: -2.7786666666666666\n",
      "  player_2: -2.4326666666666674\n",
      "policy_reward_min:\n",
      "  player_0: -15.866666666666664\n",
      "  player_1: -29.133333333333333\n",
      "  player_2: -31.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12198321927072171\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4098342464877009\n",
      "  mean_inference_ms: 2.2420180047874667\n",
      "  mean_raw_obs_processing_ms: 0.2825022021000796\n",
      "time_since_restore: 1193.2327771186829\n",
      "time_this_iter_s: 7.717225074768066\n",
      "time_total_s: 1193.2327771186829\n",
      "timers:\n",
      "  learn_throughput: 538.615\n",
      "  learn_time_ms: 7426.455\n",
      "  load_throughput: 4231007.994\n",
      "  load_time_ms: 0.945\n",
      "  sample_throughput: 460.969\n",
      "  sample_time_ms: 8677.365\n",
      "  update_time_ms: 5.591\n",
      "timestamp: 1643326264\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 668000\n",
      "training_iteration: 167\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 675969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-31-19\n",
      "done: false\n",
      "episode_len_mean: 173.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 4060\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5712694284319878\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012508476610213013\n",
      "        policy_loss: -0.07768343682866544\n",
      "        total_loss: 25.395064627329507\n",
      "        vf_explained_var: 0.08832159241040548\n",
      "        vf_loss: 25.46899560292562\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7467562977472941\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014272540080128238\n",
      "        policy_loss: -0.03464189162633071\n",
      "        total_loss: 27.404361403783163\n",
      "        vf_explained_var: -0.43933713654677076\n",
      "        vf_loss: 27.434721557299294\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6607148732741673\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01283277179266103\n",
      "        policy_loss: -0.1198547082239141\n",
      "        total_loss: 28.169786852200826\n",
      "        vf_explained_var: 0.22849178552627564\n",
      "        vf_loss: 28.283866809209186\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 675969\n",
      "  num_agent_steps_trained: 675969\n",
      "  num_steps_sampled: 676000\n",
      "  num_steps_trained: 676000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 169\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.144444444444446\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.13333333333333\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.333333333333332\n",
      "  player_1: 25.599999999999998\n",
      "  player_2: 23.399999999999995\n",
      "policy_reward_mean:\n",
      "  player_0: 9.071333333333333\n",
      "  player_1: -3.706666666666666\n",
      "  player_2: -2.3646666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -15.866666666666664\n",
      "  player_1: -31.799999999999997\n",
      "  player_2: -31.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12261559068019955\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.41073948226119195\n",
      "  mean_inference_ms: 2.2522281754785185\n",
      "  mean_raw_obs_processing_ms: 0.2835438527491933\n",
      "time_since_restore: 1208.4175753593445\n",
      "time_this_iter_s: 7.107991456985474\n",
      "time_total_s: 1208.4175753593445\n",
      "timers:\n",
      "  learn_throughput: 544.074\n",
      "  learn_time_ms: 7351.939\n",
      "  load_throughput: 4216334.347\n",
      "  load_time_ms: 0.949\n",
      "  sample_throughput: 465.359\n",
      "  sample_time_ms: 8595.52\n",
      "  update_time_ms: 5.481\n",
      "timestamp: 1643326279\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 676000\n",
      "training_iteration: 169\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 683968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-31-34\n",
      "done: false\n",
      "episode_len_mean: 175.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 4105\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.606667046546936\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015312745653827732\n",
      "        policy_loss: -0.08983855168024699\n",
      "        total_loss: 39.095132220586144\n",
      "        vf_explained_var: 0.22148167550563813\n",
      "        vf_loss: 39.18037693977356\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7271599229176839\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014606218309199902\n",
      "        policy_loss: -0.07695629139741261\n",
      "        total_loss: 35.381574347813924\n",
      "        vf_explained_var: 0.06561710735162099\n",
      "        vf_loss: 35.4541486676534\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.737219702800115\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01717554122135425\n",
      "        policy_loss: -0.08683413880411535\n",
      "        total_loss: 43.58844003041585\n",
      "        vf_explained_var: 0.058038359880447386\n",
      "        vf_loss: 43.66754494349162\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 683968\n",
      "  num_agent_steps_trained: 683968\n",
      "  num_steps_sampled: 684000\n",
      "  num_steps_trained: 684000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 171\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 32.2\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.455555555555556\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.333333333333336\n",
      "  player_1: 25.599999999999998\n",
      "  player_2: 23.399999999999995\n",
      "policy_reward_mean:\n",
      "  player_0: 8.686666666666667\n",
      "  player_1: -3.877333333333334\n",
      "  player_2: -1.809333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -12.466666666666665\n",
      "  player_1: -32.33333333333333\n",
      "  player_2: -25.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12256621514563296\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4103724511404188\n",
      "  mean_inference_ms: 2.2538061659726356\n",
      "  mean_raw_obs_processing_ms: 0.28359948890996955\n",
      "time_since_restore: 1223.2166097164154\n",
      "time_this_iter_s: 7.003191947937012\n",
      "time_total_s: 1223.2166097164154\n",
      "timers:\n",
      "  learn_throughput: 549.015\n",
      "  learn_time_ms: 7285.774\n",
      "  load_throughput: 4310692.703\n",
      "  load_time_ms: 0.928\n",
      "  sample_throughput: 471.843\n",
      "  sample_time_ms: 8477.399\n",
      "  update_time_ms: 5.0\n",
      "timestamp: 1643326294\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 684000\n",
      "training_iteration: 171\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 691968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-31-49\n",
      "done: false\n",
      "episode_len_mean: 166.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 4157\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5908844844500224\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017258208512776037\n",
      "        policy_loss: 0.0011009964222709339\n",
      "        total_loss: 26.548980452219645\n",
      "        vf_explained_var: 0.2947232049703598\n",
      "        vf_loss: 26.542701953252156\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7282895014683406\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015091538055191147\n",
      "        policy_loss: -0.10588511998144289\n",
      "        total_loss: 21.961406887372334\n",
      "        vf_explained_var: 0.009339998364448548\n",
      "        vf_loss: 22.062764647801718\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6833732038736343\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012950620810169422\n",
      "        policy_loss: -0.07377988448444132\n",
      "        total_loss: 35.71411899884542\n",
      "        vf_explained_var: 0.04068195660909017\n",
      "        vf_loss: 35.78207106590271\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 691968\n",
      "  num_agent_steps_trained: 691968\n",
      "  num_steps_sampled: 692000\n",
      "  num_steps_trained: 692000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 173\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.266666666666666\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.544444444444444\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 30.333333333333332\n",
      "  player_1: 22.400000000000002\n",
      "  player_2: 19.266666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 9.66\n",
      "  player_1: -3.946\n",
      "  player_2: -2.714\n",
      "policy_reward_min:\n",
      "  player_0: -15.400000000000002\n",
      "  player_1: -32.33333333333333\n",
      "  player_2: -25.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1222691818217648\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40965218844543316\n",
      "  mean_inference_ms: 2.252868579046652\n",
      "  mean_raw_obs_processing_ms: 0.28351300722651585\n",
      "time_since_restore: 1238.0021679401398\n",
      "time_this_iter_s: 7.797077655792236\n",
      "time_total_s: 1238.0021679401398\n",
      "timers:\n",
      "  learn_throughput: 556.642\n",
      "  learn_time_ms: 7185.951\n",
      "  load_throughput: 4520942.064\n",
      "  load_time_ms: 0.885\n",
      "  sample_throughput: 481.502\n",
      "  sample_time_ms: 8307.338\n",
      "  update_time_ms: 4.827\n",
      "timestamp: 1643326309\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 692000\n",
      "training_iteration: 173\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 699968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-32-03\n",
      "done: false\n",
      "episode_len_mean: 160.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 4205\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6041353851556778\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015050073235761375\n",
      "        policy_loss: -0.07825841430885096\n",
      "        total_loss: 34.380318031311035\n",
      "        vf_explained_var: -0.014195168217023213\n",
      "        vf_loss: 34.45406119982402\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.714208647608757\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01876942954651895\n",
      "        policy_loss: -0.09662100232516725\n",
      "        total_loss: 35.61335696538289\n",
      "        vf_explained_var: 0.12836231668790182\n",
      "        vf_loss: 35.70434703509013\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7207138327757517\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015679290042905827\n",
      "        policy_loss: -0.0808560472043852\n",
      "        total_loss: 31.47339361667633\n",
      "        vf_explained_var: -0.22895223955313365\n",
      "        vf_loss: 31.54719417413076\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 699968\n",
      "  num_agent_steps_trained: 699968\n",
      "  num_steps_sampled: 700000\n",
      "  num_steps_trained: 700000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 175\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.475\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.5\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666668\n",
      "  player_1: 16.8\n",
      "  player_2: 27.0\n",
      "policy_reward_mean:\n",
      "  player_0: 9.37\n",
      "  player_1: -3.5799999999999996\n",
      "  player_2: -2.7900000000000005\n",
      "policy_reward_min:\n",
      "  player_0: -15.400000000000002\n",
      "  player_1: -35.0\n",
      "  player_2: -30.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12284223127018916\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40994284962597694\n",
      "  mean_inference_ms: 2.2570491576882814\n",
      "  mean_raw_obs_processing_ms: 0.28406570476648935\n",
      "time_since_restore: 1252.091795682907\n",
      "time_this_iter_s: 6.390679597854614\n",
      "time_total_s: 1252.091795682907\n",
      "timers:\n",
      "  learn_throughput: 606.851\n",
      "  learn_time_ms: 6591.4\n",
      "  load_throughput: 5265092.107\n",
      "  load_time_ms: 0.76\n",
      "  sample_throughput: 501.811\n",
      "  sample_time_ms: 7971.129\n",
      "  update_time_ms: 4.278\n",
      "timestamp: 1643326323\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 700000\n",
      "training_iteration: 175\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 707968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-32-17\n",
      "done: false\n",
      "episode_len_mean: 162.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 4255\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.597340408762296\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014752764157421498\n",
      "        policy_loss: -0.01756500427921613\n",
      "        total_loss: 37.36679523309072\n",
      "        vf_explained_var: 0.06836531162261963\n",
      "        vf_loss: 37.37993439833323\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7275040372212728\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018040094393581965\n",
      "        policy_loss: -0.09522827583054702\n",
      "        total_loss: 44.412388873100284\n",
      "        vf_explained_var: -0.04169020434220632\n",
      "        vf_loss: 44.50220501422882\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7043919984499614\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014748955136795605\n",
      "        policy_loss: -0.07395563498139382\n",
      "        total_loss: 39.71574607849121\n",
      "        vf_explained_var: 0.2432230438788732\n",
      "        vf_loss: 39.78306449890137\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 707968\n",
      "  num_agent_steps_trained: 707968\n",
      "  num_steps_sampled: 708000\n",
      "  num_steps_trained: 708000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 177\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.34444444444444\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.2\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666668\n",
      "  player_1: 16.733333333333334\n",
      "  player_2: 27.0\n",
      "policy_reward_mean:\n",
      "  player_0: 9.161333333333335\n",
      "  player_1: -4.2026666666666666\n",
      "  player_2: -1.9586666666666661\n",
      "policy_reward_min:\n",
      "  player_0: -11.733333333333334\n",
      "  player_1: -35.0\n",
      "  player_2: -30.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12281379510504419\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4097677016950616\n",
      "  mean_inference_ms: 2.2562509262877213\n",
      "  mean_raw_obs_processing_ms: 0.2840262737547165\n",
      "time_since_restore: 1265.7486488819122\n",
      "time_this_iter_s: 6.9573376178741455\n",
      "time_total_s: 1265.7486488819122\n",
      "timers:\n",
      "  learn_throughput: 629.454\n",
      "  learn_time_ms: 6354.712\n",
      "  load_throughput: 5473091.929\n",
      "  load_time_ms: 0.731\n",
      "  sample_throughput: 545.533\n",
      "  sample_time_ms: 7332.28\n",
      "  update_time_ms: 4.062\n",
      "timestamp: 1643326337\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 708000\n",
      "training_iteration: 177\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 715968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-32-31\n",
      "done: false\n",
      "episode_len_mean: 167.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 4300\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6049558279911676\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01700204792553753\n",
      "        policy_loss: -0.08389667166940247\n",
      "        total_loss: 39.34494981129964\n",
      "        vf_explained_var: 0.300256307721138\n",
      "        vf_loss: 39.42374585469564\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6939822125434876\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014641991864688559\n",
      "        policy_loss: -0.09041460612167915\n",
      "        total_loss: 37.19053369839986\n",
      "        vf_explained_var: 0.07196547170480093\n",
      "        vf_loss: 37.27655544916789\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7048150781790415\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014066180306241828\n",
      "        policy_loss: -0.04868515189426641\n",
      "        total_loss: 24.208680216471354\n",
      "        vf_explained_var: -0.0726644750436147\n",
      "        vf_loss: 24.251035668055216\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 715968\n",
      "  num_agent_steps_trained: 715968\n",
      "  num_steps_sampled: 716000\n",
      "  num_steps_trained: 716000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 179\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.955555555555556\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.2\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 27.666666666666668\n",
      "  player_1: 20.733333333333334\n",
      "  player_2: 20.133333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 7.676666666666668\n",
      "  player_1: -3.2453333333333334\n",
      "  player_2: -1.4313333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -15.533333333333331\n",
      "  player_1: -30.666666666666668\n",
      "  player_2: -30.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12273035544411023\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4095463225769974\n",
      "  mean_inference_ms: 2.254790113294191\n",
      "  mean_raw_obs_processing_ms: 0.2840226722746249\n",
      "time_since_restore: 1279.6968505382538\n",
      "time_this_iter_s: 7.096616983413696\n",
      "time_total_s: 1279.6968505382538\n",
      "timers:\n",
      "  learn_throughput: 639.329\n",
      "  learn_time_ms: 6256.559\n",
      "  load_throughput: 5863494.216\n",
      "  load_time_ms: 0.682\n",
      "  sample_throughput: 558.747\n",
      "  sample_time_ms: 7158.877\n",
      "  update_time_ms: 4.006\n",
      "timestamp: 1643326351\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 716000\n",
      "training_iteration: 179\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 723968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-32-46\n",
      "done: false\n",
      "episode_len_mean: 168.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 4349\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5901953548192977\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014631901562739854\n",
      "        policy_loss: -0.04301762069265048\n",
      "        total_loss: 32.13834152062734\n",
      "        vf_explained_var: 0.13754178702831268\n",
      "        vf_loss: 32.17696933110555\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7180942193667094\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018487217333410324\n",
      "        policy_loss: -0.10482564367664357\n",
      "        total_loss: 22.27713577270508\n",
      "        vf_explained_var: -0.10923442383607229\n",
      "        vf_loss: 22.376415203412375\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6709200455745061\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012938193040217521\n",
      "        policy_loss: -0.0934999515923361\n",
      "        total_loss: 30.936891787846882\n",
      "        vf_explained_var: 0.07551191627979278\n",
      "        vf_loss: 31.024569586118062\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 723968\n",
      "  num_agent_steps_trained: 723968\n",
      "  num_steps_sampled: 724000\n",
      "  num_steps_trained: 724000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 181\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.444444444444443\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.2\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 25.0\n",
      "  player_1: 21.733333333333327\n",
      "  player_2: 18.666666666666668\n",
      "policy_reward_mean:\n",
      "  player_0: 5.308666666666666\n",
      "  player_1: -1.2453333333333336\n",
      "  player_2: -1.063333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -17.2\n",
      "  player_1: -20.6\n",
      "  player_2: -30.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12246787823459541\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40878317077207355\n",
      "  mean_inference_ms: 2.2537462854406702\n",
      "  mean_raw_obs_processing_ms: 0.2838465444455858\n",
      "time_since_restore: 1294.4114599227905\n",
      "time_this_iter_s: 6.7267906665802\n",
      "time_total_s: 1294.4114599227905\n",
      "timers:\n",
      "  learn_throughput: 640.964\n",
      "  learn_time_ms: 6240.596\n",
      "  load_throughput: 6200235.042\n",
      "  load_time_ms: 0.645\n",
      "  sample_throughput: 557.48\n",
      "  sample_time_ms: 7175.151\n",
      "  update_time_ms: 3.935\n",
      "timestamp: 1643326366\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 724000\n",
      "training_iteration: 181\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 731968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-33-00\n",
      "done: false\n",
      "episode_len_mean: 164.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 4399\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5580627826849619\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015892502141074752\n",
      "        policy_loss: -0.05277856469154358\n",
      "        total_loss: 39.67197740236918\n",
      "        vf_explained_var: 0.20421633859475455\n",
      "        vf_loss: 39.71998856226603\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.721211696267128\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016518494663603042\n",
      "        policy_loss: -0.10729870524257422\n",
      "        total_loss: 31.298235839207965\n",
      "        vf_explained_var: -0.03965915282567342\n",
      "        vf_loss: 31.400579233169555\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6819995198647181\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013935285032083205\n",
      "        policy_loss: -0.08519625896277527\n",
      "        total_loss: 17.70074925104777\n",
      "        vf_explained_var: 0.26461064219474795\n",
      "        vf_loss: 17.77967472235362\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 731968\n",
      "  num_agent_steps_trained: 731968\n",
      "  num_steps_sampled: 732000\n",
      "  num_steps_trained: 732000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 183\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.68888888888889\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.2\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.333333333333336\n",
      "  player_1: 25.866666666666667\n",
      "  player_2: 18.666666666666668\n",
      "policy_reward_mean:\n",
      "  player_0: 5.419333333333334\n",
      "  player_1: -1.1986666666666668\n",
      "  player_2: -1.2206666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -17.2\n",
      "  player_1: -23.666666666666668\n",
      "  player_2: -27.266666666666673\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12260307362618655\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4082128278217108\n",
      "  mean_inference_ms: 2.252783663338496\n",
      "  mean_raw_obs_processing_ms: 0.28371246807660294\n",
      "time_since_restore: 1308.5635285377502\n",
      "time_this_iter_s: 7.2118024826049805\n",
      "time_total_s: 1308.5635285377502\n",
      "timers:\n",
      "  learn_throughput: 646.147\n",
      "  learn_time_ms: 6190.542\n",
      "  load_throughput: 6152035.496\n",
      "  load_time_ms: 0.65\n",
      "  sample_throughput: 560.855\n",
      "  sample_time_ms: 7131.966\n",
      "  update_time_ms: 3.905\n",
      "timestamp: 1643326380\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 732000\n",
      "training_iteration: 183\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 739968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-33-14\n",
      "done: false\n",
      "episode_len_mean: 161.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 4446\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5597812746961911\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015993735425205765\n",
      "        policy_loss: -0.08323665890221794\n",
      "        total_loss: 32.48685524622599\n",
      "        vf_explained_var: 0.22689224600791932\n",
      "        vf_loss: 32.56529374440511\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6841853918631872\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01977512528179951\n",
      "        policy_loss: -0.10265738279869159\n",
      "        total_loss: 27.746593640645344\n",
      "        vf_explained_var: 0.27207507451375323\n",
      "        vf_loss: 27.843318519592284\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7222844219207764\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014811259804531195\n",
      "        policy_loss: -0.09161188076560696\n",
      "        total_loss: 30.337074149449666\n",
      "        vf_explained_var: 0.21228778938452403\n",
      "        vf_loss: 30.422020835876467\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 739968\n",
      "  num_agent_steps_trained: 739968\n",
      "  num_steps_sampled: 740000\n",
      "  num_steps_trained: 740000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 185\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.266666666666666\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.2\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.333333333333336\n",
      "  player_1: 25.866666666666667\n",
      "  player_2: 22.266666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 7.975333333333333\n",
      "  player_1: -3.5446666666666657\n",
      "  player_2: -1.4306666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -15.199999999999996\n",
      "  player_1: -32.53333333333333\n",
      "  player_2: -32.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12253080749834988\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4076046656379374\n",
      "  mean_inference_ms: 2.252076864436537\n",
      "  mean_raw_obs_processing_ms: 0.2835905521441454\n",
      "time_since_restore: 1323.115924835205\n",
      "time_this_iter_s: 7.170461416244507\n",
      "time_total_s: 1323.115924835205\n",
      "timers:\n",
      "  learn_throughput: 640.272\n",
      "  learn_time_ms: 6247.342\n",
      "  load_throughput: 5946836.807\n",
      "  load_time_ms: 0.673\n",
      "  sample_throughput: 564.809\n",
      "  sample_time_ms: 7082.038\n",
      "  update_time_ms: 4.039\n",
      "timestamp: 1643326394\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 740000\n",
      "training_iteration: 185\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 747968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-33-28\n",
      "done: false\n",
      "episode_len_mean: 165.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 4495\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5943792456388474\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01697826848302308\n",
      "        policy_loss: -0.08215334379735093\n",
      "        total_loss: 29.92022519270579\n",
      "        vf_explained_var: -0.014994477430979411\n",
      "        vf_loss: 29.997285049756368\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7178440103928249\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01815233474172904\n",
      "        policy_loss: -0.08283313930034637\n",
      "        total_loss: 34.64492467880249\n",
      "        vf_explained_var: -0.09015468239784241\n",
      "        vf_loss: 34.722312231063846\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7034124562144279\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012818194018369379\n",
      "        policy_loss: -0.08710480355347196\n",
      "        total_loss: 18.980157221158347\n",
      "        vf_explained_var: 0.013204761544863382\n",
      "        vf_loss: 19.061493913332622\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 747968\n",
      "  num_agent_steps_trained: 747968\n",
      "  num_steps_sampled: 748000\n",
      "  num_steps_trained: 748000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 187\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.15\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.2\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 29.666666666666664\n",
      "  player_1: 23.666666666666664\n",
      "  player_2: 22.266666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 8.758666666666665\n",
      "  player_1: -4.969333333333333\n",
      "  player_2: -0.7893333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -14.333333333333336\n",
      "  player_1: -32.53333333333333\n",
      "  player_2: -32.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12223226795146927\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4068257147497779\n",
      "  mean_inference_ms: 2.2492583593150655\n",
      "  mean_raw_obs_processing_ms: 0.283361427077615\n",
      "time_since_restore: 1336.539747953415\n",
      "time_this_iter_s: 6.841963529586792\n",
      "time_total_s: 1336.539747953415\n",
      "timers:\n",
      "  learn_throughput: 642.074\n",
      "  learn_time_ms: 6229.814\n",
      "  load_throughput: 6006234.919\n",
      "  load_time_ms: 0.666\n",
      "  sample_throughput: 561.712\n",
      "  sample_time_ms: 7121.089\n",
      "  update_time_ms: 5.177\n",
      "timestamp: 1643326408\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 748000\n",
      "training_iteration: 187\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 755968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-33-44\n",
      "done: false\n",
      "episode_len_mean: 170.82\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 21\n",
      "episodes_total: 4542\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5587218247850736\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016536877871356315\n",
      "        policy_loss: -0.08531790155296524\n",
      "        total_loss: 18.75195158322652\n",
      "        vf_explained_var: 0.1891271992524465\n",
      "        vf_loss: 18.832308452924092\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6840182675917943\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01507189211751118\n",
      "        policy_loss: -0.07380983967334032\n",
      "        total_loss: 24.14394317706426\n",
      "        vf_explained_var: -0.15040333211421966\n",
      "        vf_loss: 24.213231358528137\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6639937655131022\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011677528329485843\n",
      "        policy_loss: -0.0345972845206658\n",
      "        total_loss: 26.152173816363018\n",
      "        vf_explained_var: 0.010380495587984722\n",
      "        vf_loss: 26.18151619275411\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 755968\n",
      "  num_agent_steps_trained: 755968\n",
      "  num_steps_sampled: 756000\n",
      "  num_steps_trained: 756000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 189\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.779999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.1\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.333333333333336\n",
      "  player_1: 13.066666666666666\n",
      "  player_2: 21.2\n",
      "policy_reward_mean:\n",
      "  player_0: 8.144\n",
      "  player_1: -2.23\n",
      "  player_2: -2.9139999999999997\n",
      "policy_reward_min:\n",
      "  player_0: -9.333333333333334\n",
      "  player_1: -25.133333333333333\n",
      "  player_2: -20.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12227291655526124\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4065098620521171\n",
      "  mean_inference_ms: 2.247515806853153\n",
      "  mean_raw_obs_processing_ms: 0.28319205122417057\n",
      "time_since_restore: 1352.2112505435944\n",
      "time_this_iter_s: 7.776453971862793\n",
      "time_total_s: 1352.2112505435944\n",
      "timers:\n",
      "  learn_throughput: 626.363\n",
      "  learn_time_ms: 6386.074\n",
      "  load_throughput: 5492442.873\n",
      "  load_time_ms: 0.728\n",
      "  sample_throughput: 553.859\n",
      "  sample_time_ms: 7222.052\n",
      "  update_time_ms: 5.265\n",
      "timestamp: 1643326424\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 756000\n",
      "training_iteration: 189\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 763969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-33-57\n",
      "done: false\n",
      "episode_len_mean: 168.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 4592\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6052663159370423\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016182064595877097\n",
      "        policy_loss: -0.07451736202773948\n",
      "        total_loss: 26.25058453241984\n",
      "        vf_explained_var: 0.27520472089449566\n",
      "        vf_loss: 26.32024717171987\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7325710529088973\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019456470479951048\n",
      "        policy_loss: -0.10844869382369021\n",
      "        total_loss: 34.021994241078694\n",
      "        vf_explained_var: 0.3337358808517456\n",
      "        vf_loss: 34.12460604508718\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6516374293963114\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015346833487075273\n",
      "        policy_loss: -0.06471177173157533\n",
      "        total_loss: 27.299718532562256\n",
      "        vf_explained_var: 0.42900255660216013\n",
      "        vf_loss: 27.357524388631184\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 763969\n",
      "  num_agent_steps_trained: 763969\n",
      "  num_steps_sampled: 764000\n",
      "  num_steps_trained: 764000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 191\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.3\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.17777777777778\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 26.666666666666668\n",
      "  player_1: 16.0\n",
      "  player_2: 21.2\n",
      "policy_reward_mean:\n",
      "  player_0: 6.878666666666668\n",
      "  player_1: -0.9753333333333333\n",
      "  player_2: -2.9033333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -11.533333333333331\n",
      "  player_1: -17.8\n",
      "  player_2: -22.333333333333332\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12229012751113863\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40616060462591674\n",
      "  mean_inference_ms: 2.246537785079087\n",
      "  mean_raw_obs_processing_ms: 0.2831530093902247\n",
      "time_since_restore: 1365.8934288024902\n",
      "time_this_iter_s: 6.964364051818848\n",
      "time_total_s: 1365.8934288024902\n",
      "timers:\n",
      "  learn_throughput: 635.828\n",
      "  learn_time_ms: 6291.012\n",
      "  load_throughput: 5430574.222\n",
      "  load_time_ms: 0.737\n",
      "  sample_throughput: 558.126\n",
      "  sample_time_ms: 7166.842\n",
      "  update_time_ms: 5.487\n",
      "timestamp: 1643326437\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 764000\n",
      "training_iteration: 191\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 771968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-34-12\n",
      "done: false\n",
      "episode_len_mean: 163.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 4642\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5511996620893478\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015892696022395587\n",
      "        policy_loss: -0.09163465797901153\n",
      "        total_loss: 48.4671000512441\n",
      "        vf_explained_var: 0.31103175282478335\n",
      "        vf_loss: 48.553967002232866\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7166295822461446\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016760439772430497\n",
      "        policy_loss: -0.06817425058533748\n",
      "        total_loss: 53.27771467526754\n",
      "        vf_explained_var: 0.08436356087525686\n",
      "        vf_loss: 53.34086129029592\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6984542793035508\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015000282543320888\n",
      "        policy_loss: -0.07858734291046858\n",
      "        total_loss: 29.079862683614095\n",
      "        vf_explained_var: 0.0442263517777125\n",
      "        vf_loss: 29.151699789365132\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 771968\n",
      "  num_agent_steps_trained: 771968\n",
      "  num_steps_sampled: 772000\n",
      "  num_steps_trained: 772000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 193\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.66666666666667\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.288888888888884\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.800000000000004\n",
      "  player_1: 27.0\n",
      "  player_2: 21.066666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 6.3839999999999995\n",
      "  player_1: -1.494\n",
      "  player_2: -1.89\n",
      "policy_reward_min:\n",
      "  player_0: -18.0\n",
      "  player_1: -22.666666666666668\n",
      "  player_2: -22.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12242979792680213\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40580074744425443\n",
      "  mean_inference_ms: 2.24653497085875\n",
      "  mean_raw_obs_processing_ms: 0.2831771841684393\n",
      "time_since_restore: 1380.0042929649353\n",
      "time_this_iter_s: 6.6352715492248535\n",
      "time_total_s: 1380.0042929649353\n",
      "timers:\n",
      "  learn_throughput: 634.956\n",
      "  learn_time_ms: 6299.645\n",
      "  load_throughput: 5489208.219\n",
      "  load_time_ms: 0.729\n",
      "  sample_throughput: 553.045\n",
      "  sample_time_ms: 7232.691\n",
      "  update_time_ms: 5.556\n",
      "timestamp: 1643326452\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 772000\n",
      "training_iteration: 193\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 779968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-34-25\n",
      "done: false\n",
      "episode_len_mean: 160.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 4688\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5663255665699641\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018625216711467753\n",
      "        policy_loss: -0.0558563156115512\n",
      "        total_loss: 21.535922524134318\n",
      "        vf_explained_var: 0.27877765417099\n",
      "        vf_loss: 21.58619129975637\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7034559621413549\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018616680233036125\n",
      "        policy_loss: -0.06363615071090559\n",
      "        total_loss: 36.17222074826559\n",
      "        vf_explained_var: -0.1057303927342097\n",
      "        vf_loss: 36.230271883010865\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6639035232861836\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013454640518711283\n",
      "        policy_loss: -0.10117930519084135\n",
      "        total_loss: 35.61440119584402\n",
      "        vf_explained_var: 0.12314422508080801\n",
      "        vf_loss: 35.70952605406443\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 779968\n",
      "  num_agent_steps_trained: 779968\n",
      "  num_steps_sampled: 780000\n",
      "  num_steps_trained: 780000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 195\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 23.833333333333332\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.3\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.800000000000004\n",
      "  player_1: 27.0\n",
      "  player_2: 21.066666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 6.597333333333335\n",
      "  player_1: -2.0926666666666662\n",
      "  player_2: -1.5046666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -18.0\n",
      "  player_1: -28.333333333333332\n",
      "  player_2: -22.666666666666668\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12218089924411218\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.404994641485778\n",
      "  mean_inference_ms: 2.2436244103006207\n",
      "  mean_raw_obs_processing_ms: 0.2828139421725415\n",
      "time_since_restore: 1393.4252934455872\n",
      "time_this_iter_s: 6.618597507476807\n",
      "time_total_s: 1393.4252934455872\n",
      "timers:\n",
      "  learn_throughput: 644.59\n",
      "  learn_time_ms: 6205.495\n",
      "  load_throughput: 5651367.939\n",
      "  load_time_ms: 0.708\n",
      "  sample_throughput: 563.526\n",
      "  sample_time_ms: 7098.164\n",
      "  update_time_ms: 5.408\n",
      "timestamp: 1643326465\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 780000\n",
      "training_iteration: 195\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 787968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-34-38\n",
      "done: false\n",
      "episode_len_mean: 172.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 4733\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5767191253105799\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015817131601046034\n",
      "        policy_loss: -0.09211975711708267\n",
      "        total_loss: 43.06402447382609\n",
      "        vf_explained_var: -0.04913204471270243\n",
      "        vf_loss: 43.15139944076538\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7547333228588105\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017215583048042996\n",
      "        policy_loss: -0.05835208194019894\n",
      "        total_loss: 30.340455071131387\n",
      "        vf_explained_var: 0.35237761159737907\n",
      "        vf_loss: 30.39364240328471\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6815921384096145\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01381769474014603\n",
      "        policy_loss: -0.10483341281612714\n",
      "        total_loss: 31.47375305334727\n",
      "        vf_explained_var: 0.4717102458079656\n",
      "        vf_loss: 31.572368392944338\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 787968\n",
      "  num_agent_steps_trained: 787968\n",
      "  num_steps_sampled: 788000\n",
      "  num_steps_trained: 788000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 197\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.388888888888886\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.3\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 30.666666666666664\n",
      "  player_1: 24.333333333333332\n",
      "  player_2: 19.933333333333334\n",
      "policy_reward_mean:\n",
      "  player_0: 7.737333333333335\n",
      "  player_1: -3.1606666666666667\n",
      "  player_2: -1.5766666666666662\n",
      "policy_reward_min:\n",
      "  player_0: -15.466666666666669\n",
      "  player_1: -28.333333333333332\n",
      "  player_2: -30.333333333333332\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12186974197998386\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4043135318637466\n",
      "  mean_inference_ms: 2.2401125278506893\n",
      "  mean_raw_obs_processing_ms: 0.282516650748791\n",
      "time_since_restore: 1406.709780216217\n",
      "time_this_iter_s: 6.638742446899414\n",
      "time_total_s: 1406.709780216217\n",
      "timers:\n",
      "  learn_throughput: 645.838\n",
      "  learn_time_ms: 6193.502\n",
      "  load_throughput: 5643953.441\n",
      "  load_time_ms: 0.709\n",
      "  sample_throughput: 566.092\n",
      "  sample_time_ms: 7065.995\n",
      "  update_time_ms: 4.13\n",
      "timestamp: 1643326478\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 788000\n",
      "training_iteration: 197\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 795969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-34-52\n",
      "done: false\n",
      "episode_len_mean: 176.62\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 4782\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5630764496326447\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016501746428305826\n",
      "        policy_loss: -0.08414531363795201\n",
      "        total_loss: 26.417656491597494\n",
      "        vf_explained_var: 0.053840830127398175\n",
      "        vf_loss: 26.496851177215575\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7437417574723562\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018243938108180372\n",
      "        policy_loss: -0.07497108336072415\n",
      "        total_loss: 34.33509631156922\n",
      "        vf_explained_var: -0.059621588786443074\n",
      "        vf_loss: 34.40459420522054\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6443624832232793\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012769986849246682\n",
      "        policy_loss: -0.08677253320192298\n",
      "        total_loss: 24.970951763788857\n",
      "        vf_explained_var: 0.10951850891113281\n",
      "        vf_loss: 25.051977917353312\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 795969\n",
      "  num_agent_steps_trained: 795969\n",
      "  num_steps_sampled: 796000\n",
      "  num_steps_trained: 796000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 199\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.125\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.2\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.333333333333332\n",
      "  player_1: 20.266666666666666\n",
      "  player_2: 24.2\n",
      "policy_reward_mean:\n",
      "  player_0: 7.658\n",
      "  player_1: -4.1819999999999995\n",
      "  player_2: -0.4759999999999998\n",
      "policy_reward_min:\n",
      "  player_0: -13.533333333333335\n",
      "  player_1: -29.666666666666668\n",
      "  player_2: -30.333333333333332\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12162746486636263\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4036505994334314\n",
      "  mean_inference_ms: 2.2389755404725284\n",
      "  mean_raw_obs_processing_ms: 0.28240741192112095\n",
      "time_since_restore: 1420.2366752624512\n",
      "time_this_iter_s: 6.577667236328125\n",
      "time_total_s: 1420.2366752624512\n",
      "timers:\n",
      "  learn_throughput: 666.921\n",
      "  learn_time_ms: 5997.709\n",
      "  load_throughput: 6049985.936\n",
      "  load_time_ms: 0.661\n",
      "  sample_throughput: 576.577\n",
      "  sample_time_ms: 6937.5\n",
      "  update_time_ms: 3.997\n",
      "timestamp: 1643326492\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 796000\n",
      "training_iteration: 199\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 803968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-35-05\n",
      "done: false\n",
      "episode_len_mean: 163.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 4834\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5487000239888827\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017752132668805036\n",
      "        policy_loss: -0.1303358668414876\n",
      "        total_loss: 57.315772663752234\n",
      "        vf_explained_var: 0.08059462944666544\n",
      "        vf_loss: 57.44078293800354\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6992600544293721\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016141839502312603\n",
      "        policy_loss: -0.05137705886736512\n",
      "        total_loss: 29.18474152247111\n",
      "        vf_explained_var: 0.24254280845324197\n",
      "        vf_loss: 29.231276067097983\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6085858591397604\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014120685763018628\n",
      "        policy_loss: -0.0959799802924196\n",
      "        total_loss: 33.51789189656576\n",
      "        vf_explained_var: 0.2763675991694132\n",
      "        vf_loss: 33.60751745700836\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 803968\n",
      "  num_agent_steps_trained: 803968\n",
      "  num_steps_sampled: 804000\n",
      "  num_steps_trained: 804000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 201\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 23.9\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.2\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.333333333333336\n",
      "  player_1: 21.8\n",
      "  player_2: 24.2\n",
      "policy_reward_mean:\n",
      "  player_0: 8.003333333333336\n",
      "  player_1: -3.558666666666667\n",
      "  player_2: -1.4446666666666663\n",
      "policy_reward_min:\n",
      "  player_0: -12.933333333333334\n",
      "  player_1: -29.666666666666668\n",
      "  player_2: -28.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12195803878828251\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4029996733044389\n",
      "  mean_inference_ms: 2.2389252888229536\n",
      "  mean_raw_obs_processing_ms: 0.2821435552581864\n",
      "time_since_restore: 1433.1850938796997\n",
      "time_this_iter_s: 6.497617244720459\n",
      "time_total_s: 1433.1850938796997\n",
      "timers:\n",
      "  learn_throughput: 674.642\n",
      "  learn_time_ms: 5929.067\n",
      "  load_throughput: 6117489.881\n",
      "  load_time_ms: 0.654\n",
      "  sample_throughput: 588.292\n",
      "  sample_time_ms: 6799.345\n",
      "  update_time_ms: 3.806\n",
      "timestamp: 1643326505\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 804000\n",
      "training_iteration: 201\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 811968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-35-18\n",
      "done: false\n",
      "episode_len_mean: 158.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 4881\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5681897386908531\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01937556512382192\n",
      "        policy_loss: -0.08191982245848825\n",
      "        total_loss: 24.00707532564799\n",
      "        vf_explained_var: 0.15745774924755096\n",
      "        vf_loss: 24.08318239370982\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7178618146975835\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017688220326724985\n",
      "        policy_loss: -0.09650178407008449\n",
      "        total_loss: 34.683574765523275\n",
      "        vf_explained_var: 0.05689760128657023\n",
      "        vf_loss: 34.77477024714152\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6645010995864868\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01545217815997603\n",
      "        policy_loss: -0.07873473493537556\n",
      "        total_loss: 42.167156399091084\n",
      "        vf_explained_var: 0.046120537916819256\n",
      "        vf_loss: 42.23893763065338\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 811968\n",
      "  num_agent_steps_trained: 811968\n",
      "  num_steps_sampled: 812000\n",
      "  num_steps_trained: 812000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 203\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.733333333333334\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.27777777777778\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.333333333333336\n",
      "  player_1: 21.8\n",
      "  player_2: 20.0\n",
      "policy_reward_mean:\n",
      "  player_0: 7.535999999999999\n",
      "  player_1: -2.452\n",
      "  player_2: -2.084\n",
      "policy_reward_min:\n",
      "  player_0: -19.066666666666663\n",
      "  player_1: -29.666666666666664\n",
      "  player_2: -30.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12166947929621935\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4022730771487244\n",
      "  mean_inference_ms: 2.234113472924517\n",
      "  mean_raw_obs_processing_ms: 0.28170659867037584\n",
      "time_since_restore: 1446.5561616420746\n",
      "time_this_iter_s: 6.938355207443237\n",
      "time_total_s: 1446.5561616420746\n",
      "timers:\n",
      "  learn_throughput: 681.493\n",
      "  learn_time_ms: 5869.462\n",
      "  load_throughput: 6167413.888\n",
      "  load_time_ms: 0.649\n",
      "  sample_throughput: 601.835\n",
      "  sample_time_ms: 6646.34\n",
      "  update_time_ms: 3.696\n",
      "timestamp: 1643326518\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 812000\n",
      "training_iteration: 203\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 819968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-35-31\n",
      "done: false\n",
      "episode_len_mean: 161.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 4932\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5370052109162012\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013754757187366972\n",
      "        policy_loss: -0.13106426421552897\n",
      "        total_loss: 32.79586562315623\n",
      "        vf_explained_var: 0.26587389469146727\n",
      "        vf_loss: 32.92280337492625\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7124199205636979\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014838864290470903\n",
      "        policy_loss: -0.08119158593937754\n",
      "        total_loss: 43.11771436691284\n",
      "        vf_explained_var: 0.048939621448516844\n",
      "        vf_loss: 43.19445408185323\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6371381768584251\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012577496485513014\n",
      "        policy_loss: -0.018036813003321488\n",
      "        total_loss: 41.30159442106883\n",
      "        vf_explained_var: 0.20146909972031912\n",
      "        vf_loss: 41.31397125005722\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 819968\n",
      "  num_agent_steps_trained: 819968\n",
      "  num_steps_sampled: 820000\n",
      "  num_steps_trained: 820000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 205\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.150000000000006\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 24.53333333333333\n",
      "  player_2: 21.666666666666668\n",
      "policy_reward_mean:\n",
      "  player_0: 6.471333333333335\n",
      "  player_1: -1.5406666666666666\n",
      "  player_2: -1.9306666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -19.066666666666663\n",
      "  player_1: -25.666666666666664\n",
      "  player_2: -30.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12154502494067937\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4014277714611733\n",
      "  mean_inference_ms: 2.230797469973488\n",
      "  mean_raw_obs_processing_ms: 0.28127615369180625\n",
      "time_since_restore: 1459.499034166336\n",
      "time_this_iter_s: 6.490094184875488\n",
      "time_total_s: 1459.499034166336\n",
      "timers:\n",
      "  learn_throughput: 686.539\n",
      "  learn_time_ms: 5826.326\n",
      "  load_throughput: 6220003.707\n",
      "  load_time_ms: 0.643\n",
      "  sample_throughput: 601.909\n",
      "  sample_time_ms: 6645.525\n",
      "  update_time_ms: 4.0\n",
      "timestamp: 1643326531\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 820000\n",
      "training_iteration: 205\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 827970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-35-44\n",
      "done: false\n",
      "episode_len_mean: 163.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 4979\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5411325983206431\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017338963646700928\n",
      "        policy_loss: -0.07690066955983639\n",
      "        total_loss: 33.783811740875244\n",
      "        vf_explained_var: 0.13988056500752766\n",
      "        vf_loss: 33.85551094373067\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6725817920764288\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016355042428028453\n",
      "        policy_loss: -0.10370452001380424\n",
      "        total_loss: 22.944580243428547\n",
      "        vf_explained_var: -0.05829847613970439\n",
      "        vf_loss: 23.04337819258372\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6247312474250794\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013196079390739518\n",
      "        policy_loss: -0.053155500842258334\n",
      "        total_loss: 30.54102831840515\n",
      "        vf_explained_var: 0.12038925886154175\n",
      "        vf_loss: 30.58824546655019\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 827970\n",
      "  num_agent_steps_trained: 827970\n",
      "  num_steps_sampled: 828000\n",
      "  num_steps_trained: 828000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 207\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.3125\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.333333333333336\n",
      "  player_1: 24.53333333333333\n",
      "  player_2: 21.666666666666668\n",
      "policy_reward_mean:\n",
      "  player_0: 7.828\n",
      "  player_1: -3.49\n",
      "  player_2: -1.338\n",
      "policy_reward_min:\n",
      "  player_0: -20.666666666666668\n",
      "  player_1: -32.0\n",
      "  player_2: -29.666666666666668\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12139033804901192\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40076004559643275\n",
      "  mean_inference_ms: 2.229385790246732\n",
      "  mean_raw_obs_processing_ms: 0.2808530233881963\n",
      "time_since_restore: 1472.4930379390717\n",
      "time_this_iter_s: 6.329458951950073\n",
      "time_total_s: 1472.4930379390717\n",
      "timers:\n",
      "  learn_throughput: 690.724\n",
      "  learn_time_ms: 5791.024\n",
      "  load_throughput: 6155647.037\n",
      "  load_time_ms: 0.65\n",
      "  sample_throughput: 603.03\n",
      "  sample_time_ms: 6633.172\n",
      "  update_time_ms: 3.977\n",
      "timestamp: 1643326544\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 828000\n",
      "training_iteration: 207\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 835970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-35-57\n",
      "done: false\n",
      "episode_len_mean: 157.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 5033\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.530021167198817\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015181456650461769\n",
      "        policy_loss: -0.1276081993430853\n",
      "        total_loss: 26.009412956237792\n",
      "        vf_explained_var: 0.23397884368896485\n",
      "        vf_loss: 26.1324667485555\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6596981976429621\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01551815212641183\n",
      "        policy_loss: -0.0004348600407441457\n",
      "        total_loss: 26.85280774752299\n",
      "        vf_explained_var: 0.239459867477417\n",
      "        vf_loss: 26.848587395350137\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.636909949183464\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013785209262509094\n",
      "        policy_loss: -0.07308034640736878\n",
      "        total_loss: 40.98814245223999\n",
      "        vf_explained_var: 0.026201464335123697\n",
      "        vf_loss: 41.05501936594645\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 835970\n",
      "  num_agent_steps_trained: 835970\n",
      "  num_steps_sampled: 836000\n",
      "  num_steps_trained: 836000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 209\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.025\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.333333333333336\n",
      "  player_1: 27.73333333333333\n",
      "  player_2: 19.066666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 8.058\n",
      "  player_1: -4.078000000000001\n",
      "  player_2: -0.9800000000000001\n",
      "policy_reward_min:\n",
      "  player_0: -22.466666666666665\n",
      "  player_1: -32.0\n",
      "  player_2: -27.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12149251960944402\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40050582556729\n",
      "  mean_inference_ms: 2.2281331374027307\n",
      "  mean_raw_obs_processing_ms: 0.28094388134476633\n",
      "time_since_restore: 1485.2798881530762\n",
      "time_this_iter_s: 6.343896150588989\n",
      "time_total_s: 1485.2798881530762\n",
      "timers:\n",
      "  learn_throughput: 698.348\n",
      "  learn_time_ms: 5727.801\n",
      "  load_throughput: 6213093.36\n",
      "  load_time_ms: 0.644\n",
      "  sample_throughput: 609.929\n",
      "  sample_time_ms: 6558.135\n",
      "  update_time_ms: 4.19\n",
      "timestamp: 1643326557\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 836000\n",
      "training_iteration: 209\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 843968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-36-10\n",
      "done: false\n",
      "episode_len_mean: 154.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 5083\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5589436914523442\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017012778483622242\n",
      "        policy_loss: -0.0930949819739908\n",
      "        total_loss: 31.853878812789915\n",
      "        vf_explained_var: 0.36358705719312034\n",
      "        vf_loss: 31.941869684855142\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6875026440620422\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016803909960017335\n",
      "        policy_loss: -0.10084723416094979\n",
      "        total_loss: 10.839555575052897\n",
      "        vf_explained_var: -0.03913555443286896\n",
      "        vf_loss: 10.935361563364665\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6314519451061884\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012498154966257669\n",
      "        policy_loss: -0.06110821201310804\n",
      "        total_loss: 53.220647977193195\n",
      "        vf_explained_var: 0.2534603307644526\n",
      "        vf_loss: 53.276131820678714\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 843968\n",
      "  num_agent_steps_trained: 843968\n",
      "  num_steps_sampled: 844000\n",
      "  num_steps_trained: 844000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 211\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.41111111111111\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.0\n",
      "  player_1: 27.73333333333333\n",
      "  player_2: 23.599999999999998\n",
      "policy_reward_mean:\n",
      "  player_0: 6.082666666666666\n",
      "  player_1: -1.7073333333333331\n",
      "  player_2: -1.3753333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -22.466666666666665\n",
      "  player_1: -24.8\n",
      "  player_2: -28.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12124321972662294\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3995711215086372\n",
      "  mean_inference_ms: 2.2246148465276967\n",
      "  mean_raw_obs_processing_ms: 0.28056923186261273\n",
      "time_since_restore: 1498.1166853904724\n",
      "time_this_iter_s: 6.3358869552612305\n",
      "time_total_s: 1498.1166853904724\n",
      "timers:\n",
      "  learn_throughput: 698.479\n",
      "  learn_time_ms: 5726.726\n",
      "  load_throughput: 6289726.325\n",
      "  load_time_ms: 0.636\n",
      "  sample_throughput: 611.974\n",
      "  sample_time_ms: 6536.221\n",
      "  update_time_ms: 4.125\n",
      "timestamp: 1643326570\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 844000\n",
      "training_iteration: 211\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 851968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-36-23\n",
      "done: false\n",
      "episode_len_mean: 158.12\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 5130\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5409615037838618\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014424895604970516\n",
      "        policy_loss: -0.057758296603957815\n",
      "        total_loss: 31.141036785443625\n",
      "        vf_explained_var: 0.1525186938047409\n",
      "        vf_loss: 31.194467708269755\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6527350888649622\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013292756956805837\n",
      "        policy_loss: -0.07916822584656377\n",
      "        total_loss: 11.591953500906627\n",
      "        vf_explained_var: 0.08830166518688203\n",
      "        vf_loss: 11.66713396469752\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6233069519201915\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014528460416509337\n",
      "        policy_loss: -0.07185310043705007\n",
      "        total_loss: 31.37014146486918\n",
      "        vf_explained_var: 0.21338822921117148\n",
      "        vf_loss: 31.435456771850586\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 851968\n",
      "  num_agent_steps_trained: 851968\n",
      "  num_steps_sampled: 852000\n",
      "  num_steps_trained: 852000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 213\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.325\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 20.0\n",
      "  player_2: 23.599999999999998\n",
      "policy_reward_mean:\n",
      "  player_0: 7.283333333333332\n",
      "  player_1: -1.9526666666666668\n",
      "  player_2: -2.3306666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -10.2\n",
      "  player_1: -24.8\n",
      "  player_2: -28.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12085810814143277\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3985717680423392\n",
      "  mean_inference_ms: 2.220297940901256\n",
      "  mean_raw_obs_processing_ms: 0.28001922816403185\n",
      "time_since_restore: 1510.7365984916687\n",
      "time_this_iter_s: 6.259092330932617\n",
      "time_total_s: 1510.7365984916687\n",
      "timers:\n",
      "  learn_throughput: 707.368\n",
      "  learn_time_ms: 5654.762\n",
      "  load_throughput: 6223926.399\n",
      "  load_time_ms: 0.643\n",
      "  sample_throughput: 613.903\n",
      "  sample_time_ms: 6515.688\n",
      "  update_time_ms: 4.241\n",
      "timestamp: 1643326583\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 852000\n",
      "training_iteration: 213\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 859970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-36-36\n",
      "done: false\n",
      "episode_len_mean: 168.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 5179\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.507598355213801\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013686205609384767\n",
      "        policy_loss: -0.09454696738471588\n",
      "        total_loss: 35.853811480204264\n",
      "        vf_explained_var: 0.21850770711898804\n",
      "        vf_loss: 35.94425274848938\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7066013932228088\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018573589727345644\n",
      "        policy_loss: -0.07038023597250383\n",
      "        total_loss: 44.577988084157305\n",
      "        vf_explained_var: 0.21689031918843588\n",
      "        vf_loss: 44.64279630343119\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.596043733159701\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01397631619605818\n",
      "        policy_loss: -0.11042584435393413\n",
      "        total_loss: 34.75699039141337\n",
      "        vf_explained_var: 0.24442903498808544\n",
      "        vf_loss: 34.86112693786621\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 859970\n",
      "  num_agent_steps_trained: 859970\n",
      "  num_steps_sampled: 860000\n",
      "  num_steps_trained: 860000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 215\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.25\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 20.0\n",
      "  player_2: 22.93333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 7.809999999999999\n",
      "  player_1: -3.1639999999999997\n",
      "  player_2: -1.6459999999999997\n",
      "policy_reward_min:\n",
      "  player_0: -12.733333333333338\n",
      "  player_1: -29.666666666666668\n",
      "  player_2: -28.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12076731891693455\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.39843393156028045\n",
      "  mean_inference_ms: 2.218718736973077\n",
      "  mean_raw_obs_processing_ms: 0.2800512600184439\n",
      "time_since_restore: 1523.4922590255737\n",
      "time_this_iter_s: 6.306156635284424\n",
      "time_total_s: 1523.4922590255737\n",
      "timers:\n",
      "  learn_throughput: 709.47\n",
      "  learn_time_ms: 5638.008\n",
      "  load_throughput: 6207805.817\n",
      "  load_time_ms: 0.644\n",
      "  sample_throughput: 620.323\n",
      "  sample_time_ms: 6448.255\n",
      "  update_time_ms: 3.923\n",
      "timestamp: 1643326596\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 860000\n",
      "training_iteration: 215\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 867968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-36-49\n",
      "done: false\n",
      "episode_len_mean: 165.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 5231\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5680798240502676\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01576399062081327\n",
      "        policy_loss: -0.01804585026887556\n",
      "        total_loss: 26.104851140975953\n",
      "        vf_explained_var: -0.05987776696681976\n",
      "        vf_loss: 26.118167839050294\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6643598912159602\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014937099759348526\n",
      "        policy_loss: -0.07377705797553062\n",
      "        total_loss: 34.45402644634247\n",
      "        vf_explained_var: 0.23814196527004242\n",
      "        vf_loss: 34.52332218647003\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.574088942805926\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012084881306797494\n",
      "        policy_loss: -0.07972199275468786\n",
      "        total_loss: 31.353964120546976\n",
      "        vf_explained_var: 0.0647643256187439\n",
      "        vf_loss: 31.428247847557067\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 867968\n",
      "  num_agent_steps_trained: 867968\n",
      "  num_steps_sampled: 868000\n",
      "  num_steps_trained: 868000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 217\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.737499999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 29.0\n",
      "  player_1: 17.93333333333333\n",
      "  player_2: 28.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 7.375333333333333\n",
      "  player_1: -3.604666666666667\n",
      "  player_2: -0.7706666666666672\n",
      "policy_reward_min:\n",
      "  player_0: -21.866666666666667\n",
      "  player_1: -36.666666666666664\n",
      "  player_2: -21.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12067493098188958\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.39758479016474463\n",
      "  mean_inference_ms: 2.2167903624163117\n",
      "  mean_raw_obs_processing_ms: 0.2794912996539961\n",
      "time_since_restore: 1536.2386474609375\n",
      "time_this_iter_s: 6.356721639633179\n",
      "time_total_s: 1536.2386474609375\n",
      "timers:\n",
      "  learn_throughput: 711.374\n",
      "  learn_time_ms: 5622.925\n",
      "  load_throughput: 6309121.54\n",
      "  load_time_ms: 0.634\n",
      "  sample_throughput: 624.453\n",
      "  sample_time_ms: 6405.604\n",
      "  update_time_ms: 3.989\n",
      "timestamp: 1643326609\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 868000\n",
      "training_iteration: 217\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 875968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-37-01\n",
      "done: false\n",
      "episode_len_mean: 154.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 5282\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5363284704089165\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015638943689773442\n",
      "        policy_loss: -0.07890059350368878\n",
      "        total_loss: 29.50947684605916\n",
      "        vf_explained_var: 0.1340495906273524\n",
      "        vf_loss: 29.58368603070577\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7068389813105266\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02056516600753336\n",
      "        policy_loss: -0.07099342352400223\n",
      "        total_loss: 35.280978428522744\n",
      "        vf_explained_var: 0.31307392577330273\n",
      "        vf_loss: 35.34580237229665\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6187467827399572\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014524276737017014\n",
      "        policy_loss: -0.08964687345859905\n",
      "        total_loss: 36.739012482961016\n",
      "        vf_explained_var: 0.11912713487943014\n",
      "        vf_loss: 36.822123260498046\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 875968\n",
      "  num_agent_steps_trained: 875968\n",
      "  num_steps_sampled: 876000\n",
      "  num_steps_trained: 876000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 219\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.5875\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 27.0\n",
      "  player_1: 17.93333333333333\n",
      "  player_2: 28.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 6.888000000000001\n",
      "  player_1: -3.0680000000000005\n",
      "  player_2: -0.8200000000000002\n",
      "policy_reward_min:\n",
      "  player_0: -21.866666666666667\n",
      "  player_1: -36.666666666666664\n",
      "  player_2: -21.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12052596788072677\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3966807527575641\n",
      "  mean_inference_ms: 2.2130449090756668\n",
      "  mean_raw_obs_processing_ms: 0.278968803119512\n",
      "time_since_restore: 1549.0637073516846\n",
      "time_this_iter_s: 6.3545262813568115\n",
      "time_total_s: 1549.0637073516846\n",
      "timers:\n",
      "  learn_throughput: 710.477\n",
      "  learn_time_ms: 5630.017\n",
      "  load_throughput: 6469446.651\n",
      "  load_time_ms: 0.618\n",
      "  sample_throughput: 624.281\n",
      "  sample_time_ms: 6407.376\n",
      "  update_time_ms: 3.816\n",
      "timestamp: 1643326621\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 876000\n",
      "training_iteration: 219\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 883969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-37-14\n",
      "done: false\n",
      "episode_len_mean: 151.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 5335\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5122409515579541\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014443027803269311\n",
      "        policy_loss: -0.12742302009370177\n",
      "        total_loss: 31.37322141647339\n",
      "        vf_explained_var: 0.20775497376918792\n",
      "        vf_loss: 31.49631156762441\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6732599298159282\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014335344493386705\n",
      "        policy_loss: -0.03279503506763528\n",
      "        total_loss: 28.88736747423808\n",
      "        vf_explained_var: 0.1216005402803421\n",
      "        vf_loss: 28.913711663881937\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6153760107358297\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013204737851410756\n",
      "        policy_loss: -0.05177819480498632\n",
      "        total_loss: 32.14515840530395\n",
      "        vf_explained_var: -0.13618942558765412\n",
      "        vf_loss: 32.19099444389343\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 883969\n",
      "  num_agent_steps_trained: 883969\n",
      "  num_steps_sampled: 884000\n",
      "  num_steps_trained: 884000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 221\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.2\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 23.666666666666664\n",
      "  player_1: 15.466666666666669\n",
      "  player_2: 23.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 5.466666666666668\n",
      "  player_1: -3.0093333333333327\n",
      "  player_2: 0.5426666666666669\n",
      "policy_reward_min:\n",
      "  player_0: -21.266666666666662\n",
      "  player_1: -27.8\n",
      "  player_2: -21.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12030164262925236\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.39585906110547214\n",
      "  mean_inference_ms: 2.208334292206\n",
      "  mean_raw_obs_processing_ms: 0.278573510555262\n",
      "time_since_restore: 1561.7661743164062\n",
      "time_this_iter_s: 6.32187032699585\n",
      "time_total_s: 1561.7661743164062\n",
      "timers:\n",
      "  learn_throughput: 711.946\n",
      "  learn_time_ms: 5618.407\n",
      "  load_throughput: 6385725.269\n",
      "  load_time_ms: 0.626\n",
      "  sample_throughput: 625.445\n",
      "  sample_time_ms: 6395.444\n",
      "  update_time_ms: 3.857\n",
      "timestamp: 1643326634\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 884000\n",
      "training_iteration: 221\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 891968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-37-27\n",
      "done: false\n",
      "episode_len_mean: 152.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 5384\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5242644823590914\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01679357932537338\n",
      "        policy_loss: -0.08116272004631658\n",
      "        total_loss: 24.89370034535726\n",
      "        vf_explained_var: 0.20570766905943552\n",
      "        vf_loss: 24.969824873606363\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6859767565131187\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015471694917641799\n",
      "        policy_loss: -0.041199133787304165\n",
      "        total_loss: 26.069490860303244\n",
      "        vf_explained_var: -0.24375083029270173\n",
      "        vf_loss: 26.10372789780299\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6016714996099473\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01591482562982738\n",
      "        policy_loss: -0.06408468410993616\n",
      "        total_loss: 30.417474357287087\n",
      "        vf_explained_var: -0.16673147718111675\n",
      "        vf_loss: 30.474397479693096\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 891968\n",
      "  num_agent_steps_trained: 891968\n",
      "  num_steps_sampled: 892000\n",
      "  num_steps_trained: 892000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 223\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.2125\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 23.666666666666664\n",
      "  player_1: 18.133333333333333\n",
      "  player_2: 21.06666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 7.258000000000002\n",
      "  player_1: -2.4339999999999993\n",
      "  player_2: -1.8239999999999994\n",
      "policy_reward_min:\n",
      "  player_0: -21.266666666666662\n",
      "  player_1: -27.666666666666664\n",
      "  player_2: -22.866666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12025422373655757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.39539634320769096\n",
      "  mean_inference_ms: 2.2079165272864767\n",
      "  mean_raw_obs_processing_ms: 0.27853948596618183\n",
      "time_since_restore: 1574.4629442691803\n",
      "time_this_iter_s: 6.3484296798706055\n",
      "time_total_s: 1574.4629442691803\n",
      "timers:\n",
      "  learn_throughput: 711.337\n",
      "  learn_time_ms: 5623.211\n",
      "  load_throughput: 6447319.96\n",
      "  load_time_ms: 0.62\n",
      "  sample_throughput: 625.456\n",
      "  sample_time_ms: 6395.337\n",
      "  update_time_ms: 3.829\n",
      "timestamp: 1643326647\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 892000\n",
      "training_iteration: 223\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 899968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-37-40\n",
      "done: false\n",
      "episode_len_mean: 157.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 5435\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5201724495490392\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0165471251113199\n",
      "        policy_loss: -0.12162279698376854\n",
      "        total_loss: 33.218275677363074\n",
      "        vf_explained_var: 0.2367998351653417\n",
      "        vf_loss: 33.334934390385946\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.660583175222079\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015464271656698354\n",
      "        policy_loss: -0.0515040218581756\n",
      "        total_loss: 28.493173044522603\n",
      "        vf_explained_var: 0.23583456714948017\n",
      "        vf_loss: 28.53771814028422\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6057045197486878\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01433683665052324\n",
      "        policy_loss: -0.07189352288842202\n",
      "        total_loss: 25.898647753397622\n",
      "        vf_explained_var: 0.2713583157459895\n",
      "        vf_loss: 25.964089827537535\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 899968\n",
      "  num_agent_steps_trained: 899968\n",
      "  num_steps_sampled: 900000\n",
      "  num_steps_trained: 900000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 225\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.775000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.333333333333336\n",
      "  player_1: 24.066666666666663\n",
      "  player_2: 17.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 7.134666666666666\n",
      "  player_1: -0.7373333333333335\n",
      "  player_2: -3.397333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -17.333333333333336\n",
      "  player_1: -22.733333333333334\n",
      "  player_2: -27.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12002577952223445\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.39499895482857555\n",
      "  mean_inference_ms: 2.2060825307423584\n",
      "  mean_raw_obs_processing_ms: 0.27841484679112594\n",
      "time_since_restore: 1587.136960029602\n",
      "time_this_iter_s: 6.289243221282959\n",
      "time_total_s: 1587.136960029602\n",
      "timers:\n",
      "  learn_throughput: 712.464\n",
      "  learn_time_ms: 5614.318\n",
      "  load_throughput: 6564112.837\n",
      "  load_time_ms: 0.609\n",
      "  sample_throughput: 625.419\n",
      "  sample_time_ms: 6395.711\n",
      "  update_time_ms: 3.827\n",
      "timestamp: 1643326660\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 900000\n",
      "training_iteration: 225\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 907968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-37-52\n",
      "done: false\n",
      "episode_len_mean: 155.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 5488\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5243235771854718\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016820164666828365\n",
      "        policy_loss: -0.09371596350334585\n",
      "        total_loss: 48.17592537244161\n",
      "        vf_explained_var: 0.23790944973627726\n",
      "        vf_loss: 48.26459556897481\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7017398720979691\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017331653214997994\n",
      "        policy_loss: -0.05853460744023323\n",
      "        total_loss: 31.827000478108722\n",
      "        vf_explained_var: 0.23369956115881602\n",
      "        vf_loss: 31.877735754648846\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5635049660007159\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013843972074188965\n",
      "        policy_loss: -0.08045278044883161\n",
      "        total_loss: 27.46965779622396\n",
      "        vf_explained_var: 0.15345765829086302\n",
      "        vf_loss: 27.543880837758383\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 907968\n",
      "  num_agent_steps_trained: 907968\n",
      "  num_steps_sampled: 908000\n",
      "  num_steps_trained: 908000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 227\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.825\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.333333333333336\n",
      "  player_1: 24.46666666666667\n",
      "  player_2: 16.933333333333334\n",
      "policy_reward_mean:\n",
      "  player_0: 7.413333333333332\n",
      "  player_1: -3.1246666666666663\n",
      "  player_2: -1.2886666666666673\n",
      "policy_reward_min:\n",
      "  player_0: -22.8\n",
      "  player_1: -30.666666666666664\n",
      "  player_2: -27.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12023787696195354\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.394279818699718\n",
      "  mean_inference_ms: 2.2048330792894193\n",
      "  mean_raw_obs_processing_ms: 0.2781874600393888\n",
      "time_since_restore: 1599.8869180679321\n",
      "time_this_iter_s: 6.338013410568237\n",
      "time_total_s: 1599.8869180679321\n",
      "timers:\n",
      "  learn_throughput: 711.586\n",
      "  learn_time_ms: 5621.242\n",
      "  load_throughput: 6289019.005\n",
      "  load_time_ms: 0.636\n",
      "  sample_throughput: 625.829\n",
      "  sample_time_ms: 6391.526\n",
      "  update_time_ms: 3.885\n",
      "timestamp: 1643326672\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 908000\n",
      "training_iteration: 227\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 915970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-38-05\n",
      "done: false\n",
      "episode_len_mean: 150.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 5544\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5357356831431389\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014015477830713887\n",
      "        policy_loss: -0.12569024197757245\n",
      "        total_loss: 46.519695383707685\n",
      "        vf_explained_var: 0.35198495705922445\n",
      "        vf_loss: 46.63907884279887\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6806861617167791\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013874199225429039\n",
      "        policy_loss: -0.06054772761960824\n",
      "        total_loss: 53.411414019266765\n",
      "        vf_explained_var: 0.11932324727376302\n",
      "        vf_loss: 53.46571812311808\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.592635567287604\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01383292963309638\n",
      "        policy_loss: -0.060587055285771685\n",
      "        total_loss: 48.11209356307983\n",
      "        vf_explained_var: 0.12783953388532002\n",
      "        vf_loss: 48.16645563761393\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 915970\n",
      "  num_agent_steps_trained: 915970\n",
      "  num_steps_sampled: 916000\n",
      "  num_steps_trained: 916000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 229\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.6625\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.06666666666666\n",
      "  player_1: 25.266666666666666\n",
      "  player_2: 41.53333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 7.844000000000001\n",
      "  player_1: -3.4780000000000006\n",
      "  player_2: -1.3659999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -24.53333333333333\n",
      "  player_1: -30.666666666666664\n",
      "  player_2: -21.266666666666666\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12006171750761986\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.39346461536581595\n",
      "  mean_inference_ms: 2.1997761036619043\n",
      "  mean_raw_obs_processing_ms: 0.27769474856837273\n",
      "time_since_restore: 1612.7619404792786\n",
      "time_this_iter_s: 6.3210532665252686\n",
      "time_total_s: 1612.7619404792786\n",
      "timers:\n",
      "  learn_throughput: 710.941\n",
      "  learn_time_ms: 5626.348\n",
      "  load_throughput: 6282895.555\n",
      "  load_time_ms: 0.637\n",
      "  sample_throughput: 624.956\n",
      "  sample_time_ms: 6400.453\n",
      "  update_time_ms: 3.883\n",
      "timestamp: 1643326685\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 916000\n",
      "training_iteration: 229\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 923968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-38-18\n",
      "done: false\n",
      "episode_len_mean: 149.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 28\n",
      "episodes_total: 5596\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5440699468056361\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016687157238314588\n",
      "        policy_loss: -0.07057888072604934\n",
      "        total_loss: 25.761425425211588\n",
      "        vf_explained_var: 0.2796235301097234\n",
      "        vf_loss: 25.824495159784952\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6299010827143987\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01294684164819488\n",
      "        policy_loss: -0.071040517129004\n",
      "        total_loss: 37.83287473678589\n",
      "        vf_explained_var: 0.09865812083085378\n",
      "        vf_loss: 37.898089256286625\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6154761702815692\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015300013463202049\n",
      "        policy_loss: -0.09915745501716931\n",
      "        total_loss: 47.36668930371602\n",
      "        vf_explained_var: 0.35207627832889554\n",
      "        vf_loss: 47.45896162350972\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 923968\n",
      "  num_agent_steps_trained: 923968\n",
      "  num_steps_sampled: 924000\n",
      "  num_steps_trained: 924000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 231\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.5\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 27.333333333333336\n",
      "  player_1: 25.266666666666666\n",
      "  player_2: 41.53333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 8.496\n",
      "  player_1: -2.9420000000000006\n",
      "  player_2: -2.554\n",
      "policy_reward_min:\n",
      "  player_0: -24.53333333333333\n",
      "  player_1: -28.666666666666668\n",
      "  player_2: -21.266666666666666\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1195448848974468\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3927430104467916\n",
      "  mean_inference_ms: 2.195817265196129\n",
      "  mean_raw_obs_processing_ms: 0.2773591456091708\n",
      "time_since_restore: 1625.3906095027924\n",
      "time_this_iter_s: 6.3182525634765625\n",
      "time_total_s: 1625.3906095027924\n",
      "timers:\n",
      "  learn_throughput: 711.946\n",
      "  learn_time_ms: 5618.403\n",
      "  load_throughput: 6001078.8\n",
      "  load_time_ms: 0.667\n",
      "  sample_throughput: 625.866\n",
      "  sample_time_ms: 6391.145\n",
      "  update_time_ms: 4.033\n",
      "timestamp: 1643326698\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 924000\n",
      "training_iteration: 231\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 931968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-38-31\n",
      "done: false\n",
      "episode_len_mean: 150.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 5650\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5168031299114227\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013700017158989795\n",
      "        policy_loss: -0.10665076720217864\n",
      "        total_loss: 26.60232569853465\n",
      "        vf_explained_var: -0.21140962322553\n",
      "        vf_loss: 26.702811261812847\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6999507160981496\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014715980278592819\n",
      "        policy_loss: -0.05545727135768781\n",
      "        total_loss: 39.64329652150472\n",
      "        vf_explained_var: 0.11581773300965627\n",
      "        vf_loss: 39.69213144302368\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5605159484346708\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012757073974317988\n",
      "        policy_loss: -0.057663898679117365\n",
      "        total_loss: 40.55849738438924\n",
      "        vf_explained_var: 0.3779061782360077\n",
      "        vf_loss: 40.610420309702555\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 931968\n",
      "  num_agent_steps_trained: 931968\n",
      "  num_steps_sampled: 932000\n",
      "  num_steps_trained: 932000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 233\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.599999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 27.333333333333332\n",
      "  player_1: 21.666666666666664\n",
      "  player_2: 24.2\n",
      "policy_reward_mean:\n",
      "  player_0: 9.078\n",
      "  player_1: -3.4059999999999997\n",
      "  player_2: -2.6720000000000006\n",
      "policy_reward_min:\n",
      "  player_0: -12.2\n",
      "  player_1: -28.666666666666668\n",
      "  player_2: -26.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11949717335504906\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.39220219961328145\n",
      "  mean_inference_ms: 2.1926335992048744\n",
      "  mean_raw_obs_processing_ms: 0.2772267754106011\n",
      "time_since_restore: 1638.1382863521576\n",
      "time_this_iter_s: 6.297177076339722\n",
      "time_total_s: 1638.1382863521576\n",
      "timers:\n",
      "  learn_throughput: 711.057\n",
      "  learn_time_ms: 5625.429\n",
      "  load_throughput: 5938206.916\n",
      "  load_time_ms: 0.674\n",
      "  sample_throughput: 625.261\n",
      "  sample_time_ms: 6397.325\n",
      "  update_time_ms: 4.147\n",
      "timestamp: 1643326711\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 932000\n",
      "training_iteration: 233\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 939970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-38-44\n",
      "done: false\n",
      "episode_len_mean: 152.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 5699\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49893741895755134\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014633404354851033\n",
      "        policy_loss: -0.08944196394955119\n",
      "        total_loss: 26.27842558860779\n",
      "        vf_explained_var: 0.35670553664366406\n",
      "        vf_loss: 26.361282555262246\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6985624386866888\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014835758819438827\n",
      "        policy_loss: -0.060871571277578675\n",
      "        total_loss: 24.22709519704183\n",
      "        vf_explained_var: -0.028330336809158325\n",
      "        vf_loss: 24.28129059791565\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5927197696765264\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013066719898039689\n",
      "        policy_loss: -0.07594647789994875\n",
      "        total_loss: 30.229110488891603\n",
      "        vf_explained_var: 0.029557143449783326\n",
      "        vf_loss: 30.29917692820231\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 939970\n",
      "  num_agent_steps_trained: 939970\n",
      "  num_steps_sampled: 940000\n",
      "  num_steps_trained: 940000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 235\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.225\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.0\n",
      "  player_1: 27.266666666666666\n",
      "  player_2: 28.799999999999997\n",
      "policy_reward_mean:\n",
      "  player_0: 6.537333333333335\n",
      "  player_1: -0.4526666666666664\n",
      "  player_2: -3.0846666666666662\n",
      "policy_reward_min:\n",
      "  player_0: -15.199999999999996\n",
      "  player_1: -27.599999999999994\n",
      "  player_2: -26.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1195349094840555\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.39180273787457426\n",
      "  mean_inference_ms: 2.190944107071683\n",
      "  mean_raw_obs_processing_ms: 0.2772150242878312\n",
      "time_since_restore: 1650.8573079109192\n",
      "time_this_iter_s: 6.343458652496338\n",
      "time_total_s: 1650.8573079109192\n",
      "timers:\n",
      "  learn_throughput: 710.339\n",
      "  learn_time_ms: 5631.117\n",
      "  load_throughput: 5737958.207\n",
      "  load_time_ms: 0.697\n",
      "  sample_throughput: 625.445\n",
      "  sample_time_ms: 6395.451\n",
      "  update_time_ms: 4.185\n",
      "timestamp: 1643326724\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 940000\n",
      "training_iteration: 235\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 947968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-38-56\n",
      "done: false\n",
      "episode_len_mean: 150.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 5752\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5268206315239271\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015100899011498163\n",
      "        policy_loss: -0.09624078587939341\n",
      "        total_loss: 25.63988595644633\n",
      "        vf_explained_var: 0.1722825986146927\n",
      "        vf_loss: 25.729331172307333\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7093206930160523\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013953404906848543\n",
      "        policy_loss: -0.059399845525622365\n",
      "        total_loss: 25.792971351941425\n",
      "        vf_explained_var: 0.027907341917355857\n",
      "        vf_loss: 25.84609215736389\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.626958055794239\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014371388964773359\n",
      "        policy_loss: -0.06141997912277778\n",
      "        total_loss: 31.556552693049113\n",
      "        vf_explained_var: 0.10994534730911255\n",
      "        vf_loss: 31.611505559285483\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 947968\n",
      "  num_agent_steps_trained: 947968\n",
      "  num_steps_sampled: 948000\n",
      "  num_steps_trained: 948000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 237\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.887500000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.0\n",
      "  player_1: 27.266666666666666\n",
      "  player_2: 28.799999999999997\n",
      "policy_reward_mean:\n",
      "  player_0: 6.6626666666666665\n",
      "  player_1: -1.5473333333333334\n",
      "  player_2: -2.1153333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -15.199999999999996\n",
      "  player_1: -30.066666666666666\n",
      "  player_2: -27.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11933847983828832\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3912172553436134\n",
      "  mean_inference_ms: 2.187899227787845\n",
      "  mean_raw_obs_processing_ms: 0.27661687912571836\n",
      "time_since_restore: 1663.631221294403\n",
      "time_this_iter_s: 6.336050748825073\n",
      "time_total_s: 1663.631221294403\n",
      "timers:\n",
      "  learn_throughput: 710.563\n",
      "  learn_time_ms: 5629.341\n",
      "  load_throughput: 5960569.865\n",
      "  load_time_ms: 0.671\n",
      "  sample_throughput: 624.4\n",
      "  sample_time_ms: 6406.147\n",
      "  update_time_ms: 3.987\n",
      "timestamp: 1643326736\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 948000\n",
      "training_iteration: 237\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 955968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-39-09\n",
      "done: false\n",
      "episode_len_mean: 152.6\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 5804\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5128521645069122\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013725854336804939\n",
      "        policy_loss: -0.09447355210160216\n",
      "        total_loss: 34.48219327290853\n",
      "        vf_explained_var: 0.06008617122968038\n",
      "        vf_loss: 34.57048988978068\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6674542055527369\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013889632330610767\n",
      "        policy_loss: -0.10775868327667316\n",
      "        total_loss: 43.43195702870687\n",
      "        vf_explained_var: 0.20696838557720185\n",
      "        vf_loss: 43.53346555074056\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5684290442864101\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012440509248942059\n",
      "        policy_loss: 0.015556137793076534\n",
      "        total_loss: 29.070662938753763\n",
      "        vf_explained_var: -0.16227953096230824\n",
      "        vf_loss: 29.049508593877157\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 955968\n",
      "  num_agent_steps_trained: 955968\n",
      "  num_steps_sampled: 956000\n",
      "  num_steps_trained: 956000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 239\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.26666666666667\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 38.0\n",
      "  player_1: 19.0\n",
      "  player_2: 21.866666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 7.901333333333335\n",
      "  player_1: -3.458666666666667\n",
      "  player_2: -1.4426666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -21.93333333333333\n",
      "  player_1: -32.0\n",
      "  player_2: -27.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1191084024179209\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3902850125128715\n",
      "  mean_inference_ms: 2.1858457276033785\n",
      "  mean_raw_obs_processing_ms: 0.2762538690462921\n",
      "time_since_restore: 1676.3652601242065\n",
      "time_this_iter_s: 6.420418739318848\n",
      "time_total_s: 1676.3652601242065\n",
      "timers:\n",
      "  learn_throughput: 712.572\n",
      "  learn_time_ms: 5613.468\n",
      "  load_throughput: 5954223.658\n",
      "  load_time_ms: 0.672\n",
      "  sample_throughput: 626.994\n",
      "  sample_time_ms: 6379.643\n",
      "  update_time_ms: 3.961\n",
      "timestamp: 1643326749\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 956000\n",
      "training_iteration: 239\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 963970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-39-22\n",
      "done: false\n",
      "episode_len_mean: 156.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 5858\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5162054956952731\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01593863062060033\n",
      "        policy_loss: -0.10008862132827441\n",
      "        total_loss: 28.16267702738444\n",
      "        vf_explained_var: 0.3829217839241028\n",
      "        vf_loss: 28.255593112309775\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6403304141759872\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013428695470211097\n",
      "        policy_loss: -0.07333185701242959\n",
      "        total_loss: 25.405073653856913\n",
      "        vf_explained_var: 0.28528333266576134\n",
      "        vf_loss: 25.472362546920778\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5571748429536819\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010683828608301459\n",
      "        policy_loss: -0.12138667868139844\n",
      "        total_loss: 30.051323515574136\n",
      "        vf_explained_var: 0.2333917901913325\n",
      "        vf_loss: 30.16790257136027\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 963970\n",
      "  num_agent_steps_trained: 963970\n",
      "  num_steps_sampled: 964000\n",
      "  num_steps_trained: 964000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 241\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.1875\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 38.0\n",
      "  player_1: 24.0\n",
      "  player_2: 21.866666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 5.923333333333333\n",
      "  player_1: -3.1286666666666667\n",
      "  player_2: 0.2053333333333331\n",
      "policy_reward_min:\n",
      "  player_0: -21.93333333333333\n",
      "  player_1: -32.0\n",
      "  player_2: -23.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11890350821879403\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3895247713139294\n",
      "  mean_inference_ms: 2.1827725148326453\n",
      "  mean_raw_obs_processing_ms: 0.27605900544259954\n",
      "time_since_restore: 1689.171083688736\n",
      "time_this_iter_s: 6.329117774963379\n",
      "time_total_s: 1689.171083688736\n",
      "timers:\n",
      "  learn_throughput: 710.402\n",
      "  learn_time_ms: 5630.611\n",
      "  load_throughput: 6011615.307\n",
      "  load_time_ms: 0.665\n",
      "  sample_throughput: 624.226\n",
      "  sample_time_ms: 6407.935\n",
      "  update_time_ms: 3.789\n",
      "timestamp: 1643326762\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 964000\n",
      "training_iteration: 241\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 971968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-39-35\n",
      "done: false\n",
      "episode_len_mean: 146.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 5915\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5286099320650101\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014227319717565478\n",
      "        policy_loss: -0.08357090366072953\n",
      "        total_loss: 31.285679683685302\n",
      "        vf_explained_var: -0.01982968350251516\n",
      "        vf_loss: 31.362848196029663\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6784538809458415\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015995913399347046\n",
      "        policy_loss: -0.08513057192166647\n",
      "        total_loss: 30.874638322194418\n",
      "        vf_explained_var: 0.22783921817938488\n",
      "        vf_loss: 30.952570816675824\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6062759588162104\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015477680330883838\n",
      "        policy_loss: -0.0559315963772436\n",
      "        total_loss: 24.78445564587911\n",
      "        vf_explained_var: 0.3728852732976278\n",
      "        vf_loss: 24.833422530492147\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 971968\n",
      "  num_agent_steps_trained: 971968\n",
      "  num_steps_sampled: 972000\n",
      "  num_steps_trained: 972000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 243\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.6\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.333333333333332\n",
      "  player_1: 24.0\n",
      "  player_2: 19.133333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 7.095333333333333\n",
      "  player_1: -2.8986666666666667\n",
      "  player_2: -1.1966666666666668\n",
      "policy_reward_min:\n",
      "  player_0: -13.0\n",
      "  player_1: -27.333333333333332\n",
      "  player_2: -32.86666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11877466365466924\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3889997484456184\n",
      "  mean_inference_ms: 2.1800048837180435\n",
      "  mean_raw_obs_processing_ms: 0.27561188061633696\n",
      "time_since_restore: 1701.798547744751\n",
      "time_this_iter_s: 6.287705659866333\n",
      "time_total_s: 1701.798547744751\n",
      "timers:\n",
      "  learn_throughput: 711.848\n",
      "  learn_time_ms: 5619.181\n",
      "  load_throughput: 6078481.214\n",
      "  load_time_ms: 0.658\n",
      "  sample_throughput: 625.182\n",
      "  sample_time_ms: 6398.14\n",
      "  update_time_ms: 3.594\n",
      "timestamp: 1643326775\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 972000\n",
      "training_iteration: 243\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 979971\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-39-48\n",
      "done: false\n",
      "episode_len_mean: 149.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 5967\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5169498067100843\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01382738871332549\n",
      "        policy_loss: -0.06364961825311184\n",
      "        total_loss: 32.145764910380045\n",
      "        vf_explained_var: -0.08245740989844004\n",
      "        vf_loss: 32.20319201787313\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6607499980926513\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01343799169776806\n",
      "        policy_loss: -0.0779762855777517\n",
      "        total_loss: 52.69464062054952\n",
      "        vf_explained_var: 0.3221644115447998\n",
      "        vf_loss: 52.766569957733154\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5702051152785619\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013821991484286021\n",
      "        policy_loss: -0.060029720688859624\n",
      "        total_loss: 53.501041367848714\n",
      "        vf_explained_var: -0.011103079319000245\n",
      "        vf_loss: 53.55485123316447\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 979971\n",
      "  num_agent_steps_trained: 979971\n",
      "  num_steps_sampled: 980000\n",
      "  num_steps_trained: 980000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 245\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.787499999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 35.133333333333326\n",
      "  player_1: 27.0\n",
      "  player_2: 22.93333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 7.683333333333334\n",
      "  player_1: -3.0306666666666673\n",
      "  player_2: -1.6526666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -12.0\n",
      "  player_1: -32.266666666666666\n",
      "  player_2: -28.333333333333332\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1185544161509998\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3881709116710439\n",
      "  mean_inference_ms: 2.1781812173783983\n",
      "  mean_raw_obs_processing_ms: 0.2752274784311192\n",
      "time_since_restore: 1715.1296751499176\n",
      "time_this_iter_s: 6.374932527542114\n",
      "time_total_s: 1715.1296751499176\n",
      "timers:\n",
      "  learn_throughput: 706.96\n",
      "  learn_time_ms: 5658.029\n",
      "  load_throughput: 6208265.246\n",
      "  load_time_ms: 0.644\n",
      "  sample_throughput: 619.33\n",
      "  sample_time_ms: 6458.596\n",
      "  update_time_ms: 3.654\n",
      "timestamp: 1643326788\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 980000\n",
      "training_iteration: 245\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 987969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-40-02\n",
      "done: false\n",
      "episode_len_mean: 155.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 6017\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5003961243232091\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014939995833374268\n",
      "        policy_loss: -0.07367053025402129\n",
      "        total_loss: 28.847521282831828\n",
      "        vf_explained_var: 0.3340196253856023\n",
      "        vf_loss: 28.914468784332275\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6613124140103658\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01650464257307832\n",
      "        policy_loss: -0.07587721320800483\n",
      "        total_loss: 21.77781423727671\n",
      "        vf_explained_var: 0.13711489915847777\n",
      "        vf_loss: 21.84626441001892\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.550223094522953\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012724318551142156\n",
      "        policy_loss: -0.047356439977884295\n",
      "        total_loss: 35.029784070650734\n",
      "        vf_explained_var: 0.2094308586915334\n",
      "        vf_loss: 35.07141463279724\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 987969\n",
      "  num_agent_steps_trained: 987969\n",
      "  num_steps_sampled: 988000\n",
      "  num_steps_trained: 988000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 247\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.074999999999996\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.5\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 37.333333333333336\n",
      "  player_1: 27.0\n",
      "  player_2: 22.93333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 7.2906666666666675\n",
      "  player_1: -4.425333333333334\n",
      "  player_2: 0.1346666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -18.400000000000002\n",
      "  player_1: -32.266666666666666\n",
      "  player_2: -29.93333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11860761191514607\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3881161039713347\n",
      "  mean_inference_ms: 2.1780498738956977\n",
      "  mean_raw_obs_processing_ms: 0.2753147606371822\n",
      "time_since_restore: 1729.317126274109\n",
      "time_this_iter_s: 6.6515679359436035\n",
      "time_total_s: 1729.317126274109\n",
      "timers:\n",
      "  learn_throughput: 690.926\n",
      "  learn_time_ms: 5789.332\n",
      "  load_throughput: 6183780.915\n",
      "  load_time_ms: 0.647\n",
      "  sample_throughput: 608.712\n",
      "  sample_time_ms: 6571.254\n",
      "  update_time_ms: 3.83\n",
      "timestamp: 1643326802\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 988000\n",
      "training_iteration: 247\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 995968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-40-15\n",
      "done: false\n",
      "episode_len_mean: 152.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 30\n",
      "episodes_total: 6072\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5244084903597832\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014476139515536487\n",
      "        policy_loss: -0.05329065829515457\n",
      "        total_loss: 42.691600557963056\n",
      "        vf_explained_var: 0.29975634018580116\n",
      "        vf_loss: 42.738376998901366\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6177805962165197\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01480741839498247\n",
      "        policy_loss: -0.09741453538804004\n",
      "        total_loss: 28.186459312438966\n",
      "        vf_explained_var: 0.1420218978325526\n",
      "        vf_loss: 28.27721063931783\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5727838493386904\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014819434795548052\n",
      "        policy_loss: -0.0883613779845958\n",
      "        total_loss: 24.636573723157248\n",
      "        vf_explained_var: 0.14707263847192129\n",
      "        vf_loss: 24.718266299565634\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 995968\n",
      "  num_agent_steps_trained: 995968\n",
      "  num_steps_sampled: 996000\n",
      "  num_steps_trained: 996000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 249\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.612499999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4875\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 37.333333333333336\n",
      "  player_1: 21.0\n",
      "  player_2: 20.46666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 6.966\n",
      "  player_1: -2.4999999999999996\n",
      "  player_2: -1.4660000000000002\n",
      "policy_reward_min:\n",
      "  player_0: -18.666666666666668\n",
      "  player_1: -24.666666666666668\n",
      "  player_2: -29.93333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1186617433661471\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.38791039285339607\n",
      "  mean_inference_ms: 2.177535078736389\n",
      "  mean_raw_obs_processing_ms: 0.2752727489393024\n",
      "time_since_restore: 1742.0237419605255\n",
      "time_this_iter_s: 6.336386203765869\n",
      "time_total_s: 1742.0237419605255\n",
      "timers:\n",
      "  learn_throughput: 691.176\n",
      "  learn_time_ms: 5787.236\n",
      "  load_throughput: 5991220.941\n",
      "  load_time_ms: 0.668\n",
      "  sample_throughput: 605.533\n",
      "  sample_time_ms: 6605.753\n",
      "  update_time_ms: 3.918\n",
      "timestamp: 1643326815\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 996000\n",
      "training_iteration: 249\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1003969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-40-28\n",
      "done: false\n",
      "episode_len_mean: 150.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 6125\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.535079950094223\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01493590956995225\n",
      "        policy_loss: -0.0548809211080273\n",
      "        total_loss: 43.1091357866923\n",
      "        vf_explained_var: 0.22813496907552083\n",
      "        vf_loss: 43.15729525883992\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6291948624451955\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015632738332399945\n",
      "        policy_loss: -0.04730901977823426\n",
      "        total_loss: 32.176887658437096\n",
      "        vf_explained_var: 0.3220922495921453\n",
      "        vf_loss: 32.21716194152832\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5699851943055788\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012781633994957433\n",
      "        policy_loss: -0.14031720427796246\n",
      "        total_loss: 51.578370819091795\n",
      "        vf_explained_var: 0.36383884767691294\n",
      "        vf_loss: 51.712935848236086\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1003969\n",
      "  num_agent_steps_trained: 1003969\n",
      "  num_steps_sampled: 1004000\n",
      "  num_steps_trained: 1004000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 251\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.199999999999996\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4875\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 30.333333333333336\n",
      "  player_1: 22.733333333333334\n",
      "  player_2: 35.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 6.122000000000001\n",
      "  player_1: -2.802\n",
      "  player_2: -0.3199999999999997\n",
      "policy_reward_min:\n",
      "  player_0: -18.666666666666668\n",
      "  player_1: -23.733333333333334\n",
      "  player_2: -25.666666666666668\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11843613031773594\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.387279033258323\n",
      "  mean_inference_ms: 2.174901120642139\n",
      "  mean_raw_obs_processing_ms: 0.2750489872954759\n",
      "time_since_restore: 1754.7581803798676\n",
      "time_this_iter_s: 6.158909559249878\n",
      "time_total_s: 1754.7581803798676\n",
      "timers:\n",
      "  learn_throughput: 692.496\n",
      "  learn_time_ms: 5776.206\n",
      "  load_throughput: 6208035.523\n",
      "  load_time_ms: 0.644\n",
      "  sample_throughput: 605.695\n",
      "  sample_time_ms: 6603.984\n",
      "  update_time_ms: 4.017\n",
      "timestamp: 1643326828\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1004000\n",
      "training_iteration: 251\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1011968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-40-41\n",
      "done: false\n",
      "episode_len_mean: 151.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 6176\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5097271390755971\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013764286603356378\n",
      "        policy_loss: -0.06432549628118674\n",
      "        total_loss: 25.05509966850281\n",
      "        vf_explained_var: 0.31524061103661855\n",
      "        vf_loss: 25.113231185277304\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.631192161043485\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01533900572360532\n",
      "        policy_loss: -0.08975922283095618\n",
      "        total_loss: 24.950738495190937\n",
      "        vf_explained_var: 0.2767918888727824\n",
      "        vf_loss: 25.033595210711162\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5166198296348253\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011823534539677591\n",
      "        policy_loss: -0.046086747338995336\n",
      "        total_loss: 37.33342063585917\n",
      "        vf_explained_var: 0.16294759611288706\n",
      "        vf_loss: 37.37418678601583\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1011968\n",
      "  num_agent_steps_trained: 1011968\n",
      "  num_steps_sampled: 1012000\n",
      "  num_steps_trained: 1012000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 253\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.062499999999996\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4875\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 30.333333333333336\n",
      "  player_1: 22.733333333333334\n",
      "  player_2: 35.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 7.546666666666666\n",
      "  player_1: -5.3613333333333335\n",
      "  player_2: 0.8146666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -14.333333333333332\n",
      "  player_1: -23.733333333333334\n",
      "  player_2: -29.399999999999995\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11827413420728522\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.38667629330423753\n",
      "  mean_inference_ms: 2.17212982422385\n",
      "  mean_raw_obs_processing_ms: 0.27504058955820504\n",
      "time_since_restore: 1767.417141675949\n",
      "time_this_iter_s: 6.30250883102417\n",
      "time_total_s: 1767.417141675949\n",
      "timers:\n",
      "  learn_throughput: 692.255\n",
      "  learn_time_ms: 5778.218\n",
      "  load_throughput: 6218389.918\n",
      "  load_time_ms: 0.643\n",
      "  sample_throughput: 606.91\n",
      "  sample_time_ms: 6590.764\n",
      "  update_time_ms: 4.586\n",
      "timestamp: 1643326841\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1012000\n",
      "training_iteration: 253\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1019968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-40-54\n",
      "done: false\n",
      "episode_len_mean: 159.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 6225\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5351559353868166\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013588930845966539\n",
      "        policy_loss: -0.06432847223244607\n",
      "        total_loss: 35.605607767105106\n",
      "        vf_explained_var: 0.4345697929461797\n",
      "        vf_loss: 35.663821318944294\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6517787011464437\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015788497525651875\n",
      "        policy_loss: -0.08293330182088539\n",
      "        total_loss: 33.219753499031064\n",
      "        vf_explained_var: 0.1743687528371811\n",
      "        vf_loss: 33.29558185895284\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5725902338822683\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014820556497118862\n",
      "        policy_loss: -0.08331523577372234\n",
      "        total_loss: 49.28252825737\n",
      "        vf_explained_var: -0.052385902404785155\n",
      "        vf_loss: 49.35917452335357\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1019968\n",
      "  num_agent_steps_trained: 1019968\n",
      "  num_steps_sampled: 1020000\n",
      "  num_steps_trained: 1020000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 255\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.112499999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.462500000000006\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 29.666666666666668\n",
      "  player_1: 18.666666666666664\n",
      "  player_2: 28.46666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 7.096666666666667\n",
      "  player_1: -2.7613333333333334\n",
      "  player_2: -1.3353333333333337\n",
      "policy_reward_min:\n",
      "  player_0: -15.13333333333333\n",
      "  player_1: -22.0\n",
      "  player_2: -29.399999999999995\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1180245686846275\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3862426136286948\n",
      "  mean_inference_ms: 2.170471549045217\n",
      "  mean_raw_obs_processing_ms: 0.2748357014893818\n",
      "time_since_restore: 1780.7097716331482\n",
      "time_this_iter_s: 6.358882904052734\n",
      "time_total_s: 1780.7097716331482\n",
      "timers:\n",
      "  learn_throughput: 690.051\n",
      "  learn_time_ms: 5796.669\n",
      "  load_throughput: 6221156.927\n",
      "  load_time_ms: 0.643\n",
      "  sample_throughput: 607.564\n",
      "  sample_time_ms: 6583.664\n",
      "  update_time_ms: 4.506\n",
      "timestamp: 1643326854\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1020000\n",
      "training_iteration: 255\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1027968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-41-07\n",
      "done: false\n",
      "episode_len_mean: 156.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 30\n",
      "episodes_total: 6279\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5391153951485952\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014171655380941957\n",
      "        policy_loss: -0.05455591031039755\n",
      "        total_loss: 36.21848583539327\n",
      "        vf_explained_var: 0.20907711287339528\n",
      "        vf_loss: 36.266664514541624\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6196107921997706\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013056138037585091\n",
      "        policy_loss: -0.12106176574404041\n",
      "        total_loss: 39.99742748101552\n",
      "        vf_explained_var: 0.1869675346215566\n",
      "        vf_loss: 40.112613945007325\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5788980468114218\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013375202099171778\n",
      "        policy_loss: -0.05151627968996763\n",
      "        total_loss: 33.121726751327515\n",
      "        vf_explained_var: 0.17447124024232227\n",
      "        vf_loss: 33.16722451210022\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1027968\n",
      "  num_agent_steps_trained: 1027968\n",
      "  num_steps_sampled: 1028000\n",
      "  num_steps_trained: 1028000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 257\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 23.58888888888889\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.455555555555556\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.333333333333336\n",
      "  player_1: 18.666666666666664\n",
      "  player_2: 28.46666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 6.776666666666666\n",
      "  player_1: -3.0373333333333328\n",
      "  player_2: -0.7393333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -15.13333333333333\n",
      "  player_1: -37.0\n",
      "  player_2: -28.333333333333332\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11791937925955931\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.38538461954428416\n",
      "  mean_inference_ms: 2.1672489733548295\n",
      "  mean_raw_obs_processing_ms: 0.2743739690904183\n",
      "time_since_restore: 1793.429438829422\n",
      "time_this_iter_s: 6.368396282196045\n",
      "time_total_s: 1793.429438829422\n",
      "timers:\n",
      "  learn_throughput: 706.903\n",
      "  learn_time_ms: 5658.487\n",
      "  load_throughput: 6248963.051\n",
      "  load_time_ms: 0.64\n",
      "  sample_throughput: 618.895\n",
      "  sample_time_ms: 6463.134\n",
      "  update_time_ms: 4.537\n",
      "timestamp: 1643326867\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1028000\n",
      "training_iteration: 257\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1035968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-41-20\n",
      "done: false\n",
      "episode_len_mean: 151.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 6330\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5262139549851418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016787307462339716\n",
      "        policy_loss: -0.06229143674019724\n",
      "        total_loss: 31.987041918436685\n",
      "        vf_explained_var: 0.2736042173703512\n",
      "        vf_loss: 32.041779143015546\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6308779096603394\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014580957395840718\n",
      "        policy_loss: -0.12276739153157298\n",
      "        total_loss: 24.33944318930308\n",
      "        vf_explained_var: 0.08916676143805186\n",
      "        vf_loss: 24.455649371147157\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.564097459812959\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013950385531258339\n",
      "        policy_loss: -0.0821949071312944\n",
      "        total_loss: 29.091721817652385\n",
      "        vf_explained_var: 0.2754093390703201\n",
      "        vf_loss: 29.167638867696127\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1035968\n",
      "  num_agent_steps_trained: 1035968\n",
      "  num_steps_sampled: 1036000\n",
      "  num_steps_trained: 1036000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 259\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.237499999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.5\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.333333333333336\n",
      "  player_1: 16.733333333333334\n",
      "  player_2: 26.933333333333334\n",
      "policy_reward_mean:\n",
      "  player_0: 8.379333333333333\n",
      "  player_1: -4.868666666666666\n",
      "  player_2: -0.5106666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -17.466666666666665\n",
      "  player_1: -37.0\n",
      "  player_2: -27.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11793421318791067\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.38484051904460587\n",
      "  mean_inference_ms: 2.1648636589739763\n",
      "  mean_raw_obs_processing_ms: 0.27407407740994616\n",
      "time_since_restore: 1806.2907667160034\n",
      "time_this_iter_s: 6.3556671142578125\n",
      "time_total_s: 1806.2907667160034\n",
      "timers:\n",
      "  learn_throughput: 705.395\n",
      "  learn_time_ms: 5670.579\n",
      "  load_throughput: 6449550.609\n",
      "  load_time_ms: 0.62\n",
      "  sample_throughput: 619.625\n",
      "  sample_time_ms: 6455.521\n",
      "  update_time_ms: 4.329\n",
      "timestamp: 1643326880\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1036000\n",
      "training_iteration: 259\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1043968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-41-32\n",
      "done: false\n",
      "episode_len_mean: 155.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 6380\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5364983034133911\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016803510340804982\n",
      "        policy_loss: -0.07147654501876484\n",
      "        total_loss: 36.137879056930544\n",
      "        vf_explained_var: 0.3074989116191864\n",
      "        vf_loss: 36.20179419517517\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5954072469472885\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0129646455359034\n",
      "        policy_loss: -0.040020809397101405\n",
      "        total_loss: 28.335509587923685\n",
      "        vf_explained_var: 0.19496247947216033\n",
      "        vf_loss: 28.36969617843628\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.555867782831192\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012503486092305139\n",
      "        policy_loss: -0.1109799776226282\n",
      "        total_loss: 31.578056418100992\n",
      "        vf_explained_var: 0.14151133437951405\n",
      "        vf_loss: 31.68340986251831\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1043968\n",
      "  num_agent_steps_trained: 1043968\n",
      "  num_steps_sampled: 1044000\n",
      "  num_steps_trained: 1044000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 261\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.25\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.475\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 45.0\n",
      "  player_1: 17.866666666666664\n",
      "  player_2: 16.46666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 8.693333333333333\n",
      "  player_1: -3.894666666666667\n",
      "  player_2: -1.7986666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -19.133333333333336\n",
      "  player_1: -31.66666666666667\n",
      "  player_2: -33.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1179078866974136\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.38464308246879425\n",
      "  mean_inference_ms: 2.1642215621751744\n",
      "  mean_raw_obs_processing_ms: 0.2741291010537144\n",
      "time_since_restore: 1818.9022841453552\n",
      "time_this_iter_s: 6.231171369552612\n",
      "time_total_s: 1818.9022841453552\n",
      "timers:\n",
      "  learn_throughput: 706.249\n",
      "  learn_time_ms: 5663.728\n",
      "  load_throughput: 6467700.848\n",
      "  load_time_ms: 0.618\n",
      "  sample_throughput: 621.789\n",
      "  sample_time_ms: 6433.047\n",
      "  update_time_ms: 4.23\n",
      "timestamp: 1643326892\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1044000\n",
      "training_iteration: 261\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1051970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-41-45\n",
      "done: false\n",
      "episode_len_mean: 158.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 28\n",
      "episodes_total: 6433\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5013025419910749\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013640922637447754\n",
      "        policy_loss: -0.02800418062756459\n",
      "        total_loss: 44.70846528371175\n",
      "        vf_explained_var: 0.4174051755666733\n",
      "        vf_loss: 44.73033090273539\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6259302935997645\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016377765695518368\n",
      "        policy_loss: -0.11301803059875964\n",
      "        total_loss: 47.63603667259216\n",
      "        vf_explained_var: 0.33530135174592335\n",
      "        vf_loss: 47.741684684753416\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5803536540269851\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015387978013750398\n",
      "        policy_loss: -0.047125702972213426\n",
      "        total_loss: 41.11930565516154\n",
      "        vf_explained_var: 0.30320096969604493\n",
      "        vf_loss: 41.159507004419964\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1051970\n",
      "  num_agent_steps_trained: 1051970\n",
      "  num_steps_sampled: 1052000\n",
      "  num_steps_trained: 1052000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 263\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 23.933333333333334\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.455555555555556\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 45.0\n",
      "  player_1: 31.53333333333333\n",
      "  player_2: 22.933333333333334\n",
      "policy_reward_mean:\n",
      "  player_0: 8.131333333333334\n",
      "  player_1: -4.248666666666667\n",
      "  player_2: -0.8826666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -20.333333333333332\n",
      "  player_1: -33.06666666666666\n",
      "  player_2: -33.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1177477014479736\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3841005403215687\n",
      "  mean_inference_ms: 2.1614803399810465\n",
      "  mean_raw_obs_processing_ms: 0.2738181872716243\n",
      "time_since_restore: 1831.6324625015259\n",
      "time_this_iter_s: 6.362170457839966\n",
      "time_total_s: 1831.6324625015259\n",
      "timers:\n",
      "  learn_throughput: 705.869\n",
      "  learn_time_ms: 5666.775\n",
      "  load_throughput: 6389373.143\n",
      "  load_time_ms: 0.626\n",
      "  sample_throughput: 620.541\n",
      "  sample_time_ms: 6445.985\n",
      "  update_time_ms: 4.785\n",
      "timestamp: 1643326905\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1052000\n",
      "training_iteration: 263\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1059968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-41-58\n",
      "done: false\n",
      "episode_len_mean: 161.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 6480\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48393765499194463\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014109994268983428\n",
      "        policy_loss: -0.08282590406636398\n",
      "        total_loss: 25.969648175239563\n",
      "        vf_explained_var: 0.13904283861319225\n",
      "        vf_loss: 26.04612448692322\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6052236057321231\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013529445543099428\n",
      "        policy_loss: -0.04908067093230784\n",
      "        total_loss: 34.499910252888995\n",
      "        vf_explained_var: 0.3243746048212051\n",
      "        vf_loss: 34.542902886072795\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5231080681085587\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01354275392047081\n",
      "        policy_loss: -0.061385946590453384\n",
      "        total_loss: 31.665246466000873\n",
      "        vf_explained_var: 0.20049804071585337\n",
      "        vf_loss: 31.720537877082826\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1059968\n",
      "  num_agent_steps_trained: 1059968\n",
      "  num_steps_sampled: 1060000\n",
      "  num_steps_trained: 1060000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 265\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.25\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4875\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 30.333333333333332\n",
      "  player_1: 31.53333333333333\n",
      "  player_2: 22.933333333333334\n",
      "policy_reward_mean:\n",
      "  player_0: 7.079333333333334\n",
      "  player_1: -3.222666666666667\n",
      "  player_2: -0.8566666666666669\n",
      "policy_reward_min:\n",
      "  player_0: -20.333333333333332\n",
      "  player_1: -33.06666666666666\n",
      "  player_2: -30.666666666666668\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11756636028259639\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3836725262363339\n",
      "  mean_inference_ms: 2.159823370846332\n",
      "  mean_raw_obs_processing_ms: 0.27364825016479594\n",
      "time_since_restore: 1844.2314112186432\n",
      "time_this_iter_s: 6.252488851547241\n",
      "time_total_s: 1844.2314112186432\n",
      "timers:\n",
      "  learn_throughput: 714.629\n",
      "  learn_time_ms: 5597.311\n",
      "  load_throughput: 6453520.022\n",
      "  load_time_ms: 0.62\n",
      "  sample_throughput: 625.512\n",
      "  sample_time_ms: 6394.759\n",
      "  update_time_ms: 4.825\n",
      "timestamp: 1643326918\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1060000\n",
      "training_iteration: 265\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1067968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-42-11\n",
      "done: false\n",
      "episode_len_mean: 158.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 6531\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5284710797667503\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014459060923254583\n",
      "        policy_loss: -0.046565218446776274\n",
      "        total_loss: 31.655745911598206\n",
      "        vf_explained_var: 0.11139922022819519\n",
      "        vf_loss: 31.69580445130666\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6357227857907614\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015359832808112565\n",
      "        policy_loss: -0.0761029290035367\n",
      "        total_loss: 32.34007774670919\n",
      "        vf_explained_var: -0.003451194167137146\n",
      "        vf_loss: 32.409269110361734\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5727408115069071\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015495781532881664\n",
      "        policy_loss: -0.0972286587736259\n",
      "        total_loss: 32.914305941263834\n",
      "        vf_explained_var: -0.12233913143475851\n",
      "        vf_loss: 33.004561529159545\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1067968\n",
      "  num_agent_steps_trained: 1067968\n",
      "  num_steps_sampled: 1068000\n",
      "  num_steps_trained: 1068000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 267\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.662499999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.475\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 30.333333333333332\n",
      "  player_1: 21.599999999999998\n",
      "  player_2: 21.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 6.377333333333334\n",
      "  player_1: -1.8246666666666667\n",
      "  player_2: -1.5526666666666669\n",
      "policy_reward_min:\n",
      "  player_0: -24.666666666666664\n",
      "  player_1: -25.666666666666668\n",
      "  player_2: -28.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11712201183362372\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.38287028173164145\n",
      "  mean_inference_ms: 2.155635962782841\n",
      "  mean_raw_obs_processing_ms: 0.2732860419250448\n",
      "time_since_restore: 1857.093269586563\n",
      "time_this_iter_s: 6.346403360366821\n",
      "time_total_s: 1857.093269586563\n",
      "timers:\n",
      "  learn_throughput: 712.795\n",
      "  learn_time_ms: 5611.715\n",
      "  load_throughput: 6386211.412\n",
      "  load_time_ms: 0.626\n",
      "  sample_throughput: 624.935\n",
      "  sample_time_ms: 6400.668\n",
      "  update_time_ms: 4.749\n",
      "timestamp: 1643326931\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1068000\n",
      "training_iteration: 267\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1075968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-42-24\n",
      "done: false\n",
      "episode_len_mean: 153.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 6584\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5119523158669472\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013872107052737686\n",
      "        policy_loss: -0.07606110360783835\n",
      "        total_loss: 57.28989454428355\n",
      "        vf_explained_var: 0.2079187649488449\n",
      "        vf_loss: 57.359713373184206\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.625470137099425\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014043916571502753\n",
      "        policy_loss: -0.09210218300732474\n",
      "        total_loss: 52.382832703590395\n",
      "        vf_explained_var: 0.06315433084964753\n",
      "        vf_loss: 52.46861530621847\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5814232920606931\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014056581113448678\n",
      "        policy_loss: -0.06414236197868983\n",
      "        total_loss: 65.57745728174845\n",
      "        vf_explained_var: 0.13076614260673522\n",
      "        vf_loss: 65.63527374903362\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1075968\n",
      "  num_agent_steps_trained: 1075968\n",
      "  num_steps_sampled: 1076000\n",
      "  num_steps_trained: 1076000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 269\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.033333333333335\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.48888888888889\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.8\n",
      "  player_1: 27.666666666666668\n",
      "  player_2: 22.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 5.916\n",
      "  player_1: -1.6459999999999995\n",
      "  player_2: -1.2699999999999998\n",
      "policy_reward_min:\n",
      "  player_0: -24.666666666666664\n",
      "  player_1: -42.599999999999994\n",
      "  player_2: -36.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11719695455935147\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.38264262649821323\n",
      "  mean_inference_ms: 2.155209966480298\n",
      "  mean_raw_obs_processing_ms: 0.27308910389950314\n",
      "time_since_restore: 1869.861186504364\n",
      "time_this_iter_s: 6.402211666107178\n",
      "time_total_s: 1869.861186504364\n",
      "timers:\n",
      "  learn_throughput: 713.711\n",
      "  learn_time_ms: 5604.511\n",
      "  load_throughput: 6370691.475\n",
      "  load_time_ms: 0.628\n",
      "  sample_throughput: 626.715\n",
      "  sample_time_ms: 6382.485\n",
      "  update_time_ms: 4.85\n",
      "timestamp: 1643326944\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1076000\n",
      "training_iteration: 269\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1083968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-42-37\n",
      "done: false\n",
      "episode_len_mean: 148.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 6641\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5300664414962133\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01659892174046642\n",
      "        policy_loss: -0.037418461423367265\n",
      "        total_loss: 42.9365282980601\n",
      "        vf_explained_var: 0.20145133892695108\n",
      "        vf_loss: 42.966477077802026\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6481158256530761\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013994781353976577\n",
      "        policy_loss: -0.10137194545318683\n",
      "        total_loss: 28.722053295771282\n",
      "        vf_explained_var: 0.034203545649846394\n",
      "        vf_loss: 28.817127548853556\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.560544273853302\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014445516433208117\n",
      "        policy_loss: -0.07320317744432638\n",
      "        total_loss: 35.337527459462486\n",
      "        vf_explained_var: 0.05571843524773916\n",
      "        vf_loss: 35.40422986666361\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1083968\n",
      "  num_agent_steps_trained: 1083968\n",
      "  num_steps_sampled: 1084000\n",
      "  num_steps_trained: 1084000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 271\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.2\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.5\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.0\n",
      "  player_1: 27.666666666666668\n",
      "  player_2: 22.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 7.751333333333334\n",
      "  player_1: -2.7606666666666673\n",
      "  player_2: -1.9906666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -19.733333333333338\n",
      "  player_1: -42.599999999999994\n",
      "  player_2: -36.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11713723385876346\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3822018307279379\n",
      "  mean_inference_ms: 2.1539669211364507\n",
      "  mean_raw_obs_processing_ms: 0.27266787344380233\n",
      "time_since_restore: 1882.656955242157\n",
      "time_this_iter_s: 6.313064098358154\n",
      "time_total_s: 1882.656955242157\n",
      "timers:\n",
      "  learn_throughput: 712.899\n",
      "  learn_time_ms: 5610.89\n",
      "  load_throughput: 6165600.676\n",
      "  load_time_ms: 0.649\n",
      "  sample_throughput: 624.927\n",
      "  sample_time_ms: 6400.752\n",
      "  update_time_ms: 4.901\n",
      "timestamp: 1643326957\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1084000\n",
      "training_iteration: 271\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1091972\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-42-49\n",
      "done: false\n",
      "episode_len_mean: 148.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 31\n",
      "episodes_total: 6695\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5344861729939778\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017665290686812416\n",
      "        policy_loss: -0.09825427036732436\n",
      "        total_loss: 42.95113044420878\n",
      "        vf_explained_var: 0.0531550540526708\n",
      "        vf_loss: 43.041435372034705\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6394029120604198\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014931103832665637\n",
      "        policy_loss: -0.07544314820629855\n",
      "        total_loss: 49.9439634736379\n",
      "        vf_explained_var: 0.19237345854441326\n",
      "        vf_loss: 50.01268753051758\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5394638257225355\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012434458917365797\n",
      "        policy_loss: -0.09922532517462969\n",
      "        total_loss: 34.50584813515345\n",
      "        vf_explained_var: 0.14264221409956615\n",
      "        vf_loss: 34.599477875232694\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1091972\n",
      "  num_agent_steps_trained: 1091972\n",
      "  num_steps_sampled: 1092000\n",
      "  num_steps_trained: 1092000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 273\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.874999999999996\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.4875\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.0\n",
      "  player_1: 20.333333333333332\n",
      "  player_2: 20.266666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 7.342000000000001\n",
      "  player_1: -1.9520000000000002\n",
      "  player_2: -2.3899999999999992\n",
      "policy_reward_min:\n",
      "  player_0: -13.333333333333332\n",
      "  player_1: -31.0\n",
      "  player_2: -23.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.117105880477894\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.38166401555624974\n",
      "  mean_inference_ms: 2.1512756905136263\n",
      "  mean_raw_obs_processing_ms: 0.2724666759989737\n",
      "time_since_restore: 1895.4106013774872\n",
      "time_this_iter_s: 6.306774616241455\n",
      "time_total_s: 1895.4106013774872\n",
      "timers:\n",
      "  learn_throughput: 712.396\n",
      "  learn_time_ms: 5614.851\n",
      "  load_throughput: 6176950.775\n",
      "  load_time_ms: 0.648\n",
      "  sample_throughput: 623.774\n",
      "  sample_time_ms: 6412.575\n",
      "  update_time_ms: 3.805\n",
      "timestamp: 1643326969\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1092000\n",
      "training_iteration: 273\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1099968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-43-02\n",
      "done: false\n",
      "episode_len_mean: 145.54\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 6749\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5215279928843181\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013667289871797645\n",
      "        policy_loss: -0.08857958426078161\n",
      "        total_loss: 42.428292748133345\n",
      "        vf_explained_var: 0.019805851380030316\n",
      "        vf_loss: 42.51072153409322\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.592344748278459\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013498936290073591\n",
      "        policy_loss: -0.0689393354828159\n",
      "        total_loss: 32.43377986431122\n",
      "        vf_explained_var: 0.22231391191482544\n",
      "        vf_loss: 32.49664456367493\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.544294361770153\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011951053776239557\n",
      "        policy_loss: -0.05381285440176725\n",
      "        total_loss: 22.97795152982076\n",
      "        vf_explained_var: 0.19061832865079245\n",
      "        vf_loss: 23.02638644059499\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1099968\n",
      "  num_agent_steps_trained: 1099968\n",
      "  num_steps_sampled: 1100000\n",
      "  num_steps_trained: 1100000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 275\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.400000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.5\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 26.0\n",
      "  player_1: 21.266666666666666\n",
      "  player_2: 23.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 6.958666666666667\n",
      "  player_1: -1.8213333333333332\n",
      "  player_2: -2.137333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -21.93333333333333\n",
      "  player_1: -28.333333333333332\n",
      "  player_2: -21.333333333333332\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11721245605945114\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.38136135201388544\n",
      "  mean_inference_ms: 2.1513582884906026\n",
      "  mean_raw_obs_processing_ms: 0.27277638697113055\n",
      "time_since_restore: 1908.151252746582\n",
      "time_this_iter_s: 6.402981519699097\n",
      "time_total_s: 1908.151252746582\n",
      "timers:\n",
      "  learn_throughput: 710.612\n",
      "  learn_time_ms: 5628.949\n",
      "  load_throughput: 6074959.626\n",
      "  load_time_ms: 0.658\n",
      "  sample_throughput: 624.42\n",
      "  sample_time_ms: 6405.948\n",
      "  update_time_ms: 3.727\n",
      "timestamp: 1643326982\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1100000\n",
      "training_iteration: 275\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1107969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-43-15\n",
      "done: false\n",
      "episode_len_mean: 144.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 6800\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5145051247874896\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013828325724534806\n",
      "        policy_loss: -0.06317063748836517\n",
      "        total_loss: 29.933309335708618\n",
      "        vf_explained_var: -0.032852132519086204\n",
      "        vf_loss: 29.99025717417399\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6143764618039131\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014002983791409254\n",
      "        policy_loss: -0.07519217438995839\n",
      "        total_loss: 29.59269385019938\n",
      "        vf_explained_var: 0.23161951959133148\n",
      "        vf_loss: 29.66158456802368\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5686962502201398\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015221308417015583\n",
      "        policy_loss: -0.04122956040004889\n",
      "        total_loss: 32.08306482474009\n",
      "        vf_explained_var: 0.1938289213180542\n",
      "        vf_loss: 32.11744469801585\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1107969\n",
      "  num_agent_steps_trained: 1107969\n",
      "  num_steps_sampled: 1108000\n",
      "  num_steps_trained: 1108000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 277\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.324999999999996\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.5\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 27.666666666666668\n",
      "  player_1: 27.666666666666664\n",
      "  player_2: 25.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 6.525333333333334\n",
      "  player_1: -2.4186666666666667\n",
      "  player_2: -1.1066666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -21.93333333333333\n",
      "  player_1: -26.333333333333336\n",
      "  player_2: -20.333333333333332\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11708389034760858\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3810069147460353\n",
      "  mean_inference_ms: 2.149736690544552\n",
      "  mean_raw_obs_processing_ms: 0.2725535833280919\n",
      "time_since_restore: 1920.9681692123413\n",
      "time_this_iter_s: 6.3407087326049805\n",
      "time_total_s: 1920.9681692123413\n",
      "timers:\n",
      "  learn_throughput: 711.217\n",
      "  learn_time_ms: 5624.159\n",
      "  load_throughput: 6056975.342\n",
      "  load_time_ms: 0.66\n",
      "  sample_throughput: 623.287\n",
      "  sample_time_ms: 6417.586\n",
      "  update_time_ms: 3.66\n",
      "timestamp: 1643326995\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1108000\n",
      "training_iteration: 277\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1115968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-43-28\n",
      "done: false\n",
      "episode_len_mean: 149.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 6853\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5116523909568786\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013562810742469083\n",
      "        policy_loss: -0.04903062759898603\n",
      "        total_loss: 28.653123548825583\n",
      "        vf_explained_var: 0.43025945882002514\n",
      "        vf_loss: 28.696050901412963\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6147969990968705\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014224673301323492\n",
      "        policy_loss: -0.11551165415594976\n",
      "        total_loss: 29.036776882807413\n",
      "        vf_explained_var: -0.0607047313451767\n",
      "        vf_loss: 29.14588768005371\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.525705093840758\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015363503021744692\n",
      "        policy_loss: -0.03741355581829945\n",
      "        total_loss: 25.722753224372862\n",
      "        vf_explained_var: -0.25758658985296884\n",
      "        vf_loss: 25.753253231048586\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1115968\n",
      "  num_agent_steps_trained: 1115968\n",
      "  num_steps_sampled: 1116000\n",
      "  num_steps_trained: 1116000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 279\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.125\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.5\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.0\n",
      "  player_1: 27.666666666666664\n",
      "  player_2: 25.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 5.758666666666667\n",
      "  player_1: -1.953333333333334\n",
      "  player_2: -0.8053333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -14.333333333333336\n",
      "  player_1: -27.333333333333336\n",
      "  player_2: -31.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1166925936678783\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3804551334489447\n",
      "  mean_inference_ms: 2.1455777722346734\n",
      "  mean_raw_obs_processing_ms: 0.27198332385588736\n",
      "time_since_restore: 1933.6524984836578\n",
      "time_this_iter_s: 6.31588339805603\n",
      "time_total_s: 1933.6524984836578\n",
      "timers:\n",
      "  learn_throughput: 711.908\n",
      "  learn_time_ms: 5618.7\n",
      "  load_throughput: 5870469.925\n",
      "  load_time_ms: 0.681\n",
      "  sample_throughput: 623.659\n",
      "  sample_time_ms: 6413.756\n",
      "  update_time_ms: 3.63\n",
      "timestamp: 1643327008\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1116000\n",
      "training_iteration: 279\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1123968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-43-41\n",
      "done: false\n",
      "episode_len_mean: 156.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 6902\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5320119710763296\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01365553865682159\n",
      "        policy_loss: -0.043882091715931895\n",
      "        total_loss: 29.625271553993226\n",
      "        vf_explained_var: -0.014279118180274964\n",
      "        vf_loss: 29.663008732795717\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5795478203892708\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013376712636566784\n",
      "        policy_loss: -0.09908678882134457\n",
      "        total_loss: 35.302671931584676\n",
      "        vf_explained_var: -0.19377228021621704\n",
      "        vf_loss: 35.39573908487956\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5448431894183159\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012720759414272228\n",
      "        policy_loss: -0.018931254331643384\n",
      "        total_loss: 29.27452146053314\n",
      "        vf_explained_var: 0.3353253088394801\n",
      "        vf_loss: 29.28772801876068\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1123968\n",
      "  num_agent_steps_trained: 1123968\n",
      "  num_steps_sampled: 1124000\n",
      "  num_steps_trained: 1124000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 281\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.644444444444446\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.5\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.333333333333336\n",
      "  player_1: 18.53333333333333\n",
      "  player_2: 26.2\n",
      "policy_reward_mean:\n",
      "  player_0: 6.874666666666668\n",
      "  player_1: -2.4493333333333336\n",
      "  player_2: -1.4253333333333336\n",
      "policy_reward_min:\n",
      "  player_0: -14.0\n",
      "  player_1: -40.266666666666666\n",
      "  player_2: -31.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1167091571169167\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3801245940692754\n",
      "  mean_inference_ms: 2.1461409389552344\n",
      "  mean_raw_obs_processing_ms: 0.27197597655607025\n",
      "time_since_restore: 1946.846438884735\n",
      "time_this_iter_s: 6.678391933441162\n",
      "time_total_s: 1946.846438884735\n",
      "timers:\n",
      "  learn_throughput: 708.244\n",
      "  learn_time_ms: 5647.768\n",
      "  load_throughput: 6023918.71\n",
      "  load_time_ms: 0.664\n",
      "  sample_throughput: 623.213\n",
      "  sample_time_ms: 6418.352\n",
      "  update_time_ms: 3.678\n",
      "timestamp: 1643327021\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1124000\n",
      "training_iteration: 281\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1131968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-43-55\n",
      "done: false\n",
      "episode_len_mean: 162.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 6953\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5401230364044507\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014953471201843059\n",
      "        policy_loss: -0.09719548315508292\n",
      "        total_loss: 28.609372987747193\n",
      "        vf_explained_var: 0.19085227449735007\n",
      "        vf_loss: 28.699839644432068\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6342056646943093\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015859918851444187\n",
      "        policy_loss: -0.09891670710717639\n",
      "        total_loss: 27.589611568450927\n",
      "        vf_explained_var: 0.0933232061068217\n",
      "        vf_loss: 27.681391332944234\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5328973412513733\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014428140007100106\n",
      "        policy_loss: -0.09414752890666325\n",
      "        total_loss: 41.793008489608766\n",
      "        vf_explained_var: 0.3202998967965444\n",
      "        vf_loss: 41.88066362698873\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1131968\n",
      "  num_agent_steps_trained: 1131968\n",
      "  num_steps_sampled: 1132000\n",
      "  num_steps_trained: 1132000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 283\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.3125\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.5\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.333333333333336\n",
      "  player_1: 16.866666666666664\n",
      "  player_2: 26.2\n",
      "policy_reward_mean:\n",
      "  player_0: 5.762666666666665\n",
      "  player_1: -1.7873333333333334\n",
      "  player_2: -0.9753333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -19.466666666666665\n",
      "  player_1: -40.266666666666666\n",
      "  player_2: -28.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11713343820889122\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3804317829200828\n",
      "  mean_inference_ms: 2.1482690509392928\n",
      "  mean_raw_obs_processing_ms: 0.2724155214856181\n",
      "time_since_restore: 1960.3473806381226\n",
      "time_this_iter_s: 6.293737888336182\n",
      "time_total_s: 1960.3473806381226\n",
      "timers:\n",
      "  learn_throughput: 700.259\n",
      "  learn_time_ms: 5712.175\n",
      "  load_throughput: 6070783.037\n",
      "  load_time_ms: 0.659\n",
      "  sample_throughput: 613.346\n",
      "  sample_time_ms: 6521.604\n",
      "  update_time_ms: 3.724\n",
      "timestamp: 1643327035\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1132000\n",
      "training_iteration: 283\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1139968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-44-07\n",
      "done: false\n",
      "episode_len_mean: 159.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 7003\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5236343707640966\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013521161907525012\n",
      "        policy_loss: -0.13307428206472346\n",
      "        total_loss: 23.08968784650167\n",
      "        vf_explained_var: 0.2644063272078832\n",
      "        vf_loss: 23.216677695910136\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.576361012061437\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013176242491899757\n",
      "        policy_loss: -0.04395630379517873\n",
      "        total_loss: 24.778061230977375\n",
      "        vf_explained_var: 0.020547292828559875\n",
      "        vf_loss: 24.816088081995645\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5370520256956418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015159439371068402\n",
      "        policy_loss: -0.05047000505340596\n",
      "        total_loss: 41.66219757715861\n",
      "        vf_explained_var: 0.190737220843633\n",
      "        vf_loss: 41.70584569295247\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1139968\n",
      "  num_agent_steps_trained: 1139968\n",
      "  num_steps_sampled: 1140000\n",
      "  num_steps_trained: 1140000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 285\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.3375\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.5\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 25.333333333333336\n",
      "  player_1: 19.266666666666666\n",
      "  player_2: 24.133333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 5.336\n",
      "  player_1: -0.41\n",
      "  player_2: -1.9260000000000002\n",
      "policy_reward_min:\n",
      "  player_0: -19.53333333333333\n",
      "  player_1: -33.0\n",
      "  player_2: -28.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11698734220226165\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3801799468091007\n",
      "  mean_inference_ms: 2.146374665466149\n",
      "  mean_raw_obs_processing_ms: 0.2724499287919598\n",
      "time_since_restore: 1973.0188279151917\n",
      "time_this_iter_s: 6.256203889846802\n",
      "time_total_s: 1973.0188279151917\n",
      "timers:\n",
      "  learn_throughput: 701.45\n",
      "  learn_time_ms: 5702.477\n",
      "  load_throughput: 6030414.435\n",
      "  load_time_ms: 0.663\n",
      "  sample_throughput: 612.509\n",
      "  sample_time_ms: 6530.513\n",
      "  update_time_ms: 3.724\n",
      "timestamp: 1643327047\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1140000\n",
      "training_iteration: 285\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1147968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-44-20\n",
      "done: false\n",
      "episode_len_mean: 161.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 7054\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.511612842977047\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014334460465512773\n",
      "        policy_loss: -0.11885204821825028\n",
      "        total_loss: 36.74091276327769\n",
      "        vf_explained_var: 0.2063328512509664\n",
      "        vf_loss: 36.853314536412555\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6175269800424575\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01776765921859348\n",
      "        policy_loss: -0.07424970881392558\n",
      "        total_loss: 33.33256200790405\n",
      "        vf_explained_var: 0.03738499204317729\n",
      "        vf_loss: 33.39881629625956\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5582252793510755\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014478398231003666\n",
      "        policy_loss: -0.024006580694889028\n",
      "        total_loss: 37.92274618625641\n",
      "        vf_explained_var: 0.3125681513547897\n",
      "        vf_loss: 37.940237396558125\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1147968\n",
      "  num_agent_steps_trained: 1147968\n",
      "  num_steps_sampled: 1148000\n",
      "  num_steps_trained: 1148000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 287\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.137500000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.5\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 25.333333333333332\n",
      "  player_1: 19.266666666666666\n",
      "  player_2: 24.133333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 7.145333333333333\n",
      "  player_1: -0.41866666666666646\n",
      "  player_2: -3.726666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -19.53333333333333\n",
      "  player_1: -33.0\n",
      "  player_2: -25.666666666666668\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11661597849837267\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.37958189755059024\n",
      "  mean_inference_ms: 2.143672993578254\n",
      "  mean_raw_obs_processing_ms: 0.2720563339960543\n",
      "time_since_restore: 1985.8128681182861\n",
      "time_this_iter_s: 6.288986444473267\n",
      "time_total_s: 1985.8128681182861\n",
      "timers:\n",
      "  learn_throughput: 702.991\n",
      "  learn_time_ms: 5689.97\n",
      "  load_throughput: 5943044.988\n",
      "  load_time_ms: 0.673\n",
      "  sample_throughput: 613.711\n",
      "  sample_time_ms: 6517.721\n",
      "  update_time_ms: 3.835\n",
      "timestamp: 1643327060\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1148000\n",
      "training_iteration: 287\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1155968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-44-33\n",
      "done: false\n",
      "episode_len_mean: 163.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 7103\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5078122134009997\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014177724111365654\n",
      "        policy_loss: -0.07707199825439602\n",
      "        total_loss: 46.109203526178995\n",
      "        vf_explained_var: 0.21462101101875306\n",
      "        vf_loss: 46.17989549636841\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5855816063284874\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014573437281341745\n",
      "        policy_loss: -0.05703918584001561\n",
      "        total_loss: 21.5110804049174\n",
      "        vf_explained_var: 0.17690572738647461\n",
      "        vf_loss: 21.5615615940094\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5840825287501017\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015559815306478413\n",
      "        policy_loss: -0.05706144834558169\n",
      "        total_loss: 47.65375909487406\n",
      "        vf_explained_var: 0.3408836704492569\n",
      "        vf_loss: 47.703818295796715\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1155968\n",
      "  num_agent_steps_trained: 1155968\n",
      "  num_steps_sampled: 1156000\n",
      "  num_steps_trained: 1156000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 289\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.11111111111111\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.5\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 25.333333333333332\n",
      "  player_1: 19.266666666666666\n",
      "  player_2: 18.0\n",
      "policy_reward_mean:\n",
      "  player_0: 6.5133333333333345\n",
      "  player_1: -0.20466666666666644\n",
      "  player_2: -3.308666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -30.0\n",
      "  player_1: -22.0\n",
      "  player_2: -27.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1165351207009072\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3791494456380824\n",
      "  mean_inference_ms: 2.1426792472120733\n",
      "  mean_raw_obs_processing_ms: 0.27194249228233597\n",
      "time_since_restore: 1998.6738409996033\n",
      "time_this_iter_s: 6.349022388458252\n",
      "time_total_s: 1998.6738409996033\n",
      "timers:\n",
      "  learn_throughput: 701.355\n",
      "  learn_time_ms: 5703.249\n",
      "  load_throughput: 6154743.754\n",
      "  load_time_ms: 0.65\n",
      "  sample_throughput: 612.714\n",
      "  sample_time_ms: 6528.331\n",
      "  update_time_ms: 3.896\n",
      "timestamp: 1643327073\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1156000\n",
      "training_iteration: 289\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1163968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-44-46\n",
      "done: false\n",
      "episode_len_mean: 163.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 7151\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5090899169445038\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01808277398493374\n",
      "        policy_loss: -0.08273687474429607\n",
      "        total_loss: 29.311492727597553\n",
      "        vf_explained_var: 0.11702641646067301\n",
      "        vf_loss: 29.386092263857524\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5759659881393114\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014227283935973294\n",
      "        policy_loss: -0.033213261334846415\n",
      "        total_loss: 34.0843212779363\n",
      "        vf_explained_var: 0.18307725568612418\n",
      "        vf_loss: 34.11113227367401\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5804669353365898\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015291894068476837\n",
      "        policy_loss: -0.12781605142789582\n",
      "        total_loss: 29.931042648951212\n",
      "        vf_explained_var: 0.25494588832060494\n",
      "        vf_loss: 30.051977650324503\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1163968\n",
      "  num_agent_steps_trained: 1163968\n",
      "  num_steps_sampled: 1164000\n",
      "  num_steps_trained: 1164000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 291\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.5125\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.5125\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 24.333333333333332\n",
      "  player_1: 18.933333333333334\n",
      "  player_2: 18.333333333333332\n",
      "policy_reward_mean:\n",
      "  player_0: 5.194\n",
      "  player_1: 0.14599999999999944\n",
      "  player_2: -2.3400000000000003\n",
      "policy_reward_min:\n",
      "  player_0: -30.0\n",
      "  player_1: -21.0\n",
      "  player_2: -27.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11613983906045214\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3782983586868507\n",
      "  mean_inference_ms: 2.1383864608168635\n",
      "  mean_raw_obs_processing_ms: 0.2714945008601572\n",
      "time_since_restore: 2011.5025210380554\n",
      "time_this_iter_s: 6.337948799133301\n",
      "time_total_s: 2011.5025210380554\n",
      "timers:\n",
      "  learn_throughput: 703.411\n",
      "  learn_time_ms: 5686.574\n",
      "  load_throughput: 6203674.013\n",
      "  load_time_ms: 0.645\n",
      "  sample_throughput: 613.217\n",
      "  sample_time_ms: 6522.982\n",
      "  update_time_ms: 3.851\n",
      "timestamp: 1643327086\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1164000\n",
      "training_iteration: 291\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1171968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-44-59\n",
      "done: false\n",
      "episode_len_mean: 161.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 7199\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47514500727256137\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013564594496009856\n",
      "        policy_loss: -0.07270733138546348\n",
      "        total_loss: 39.04702978134155\n",
      "        vf_explained_var: 0.06875074525674184\n",
      "        vf_loss: 39.11363272984823\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6147795544068019\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013359697163423334\n",
      "        policy_loss: -0.07685524756709734\n",
      "        total_loss: 26.168159116109212\n",
      "        vf_explained_var: 0.3927707076072693\n",
      "        vf_loss: 26.239002553621926\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5083600355188052\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01403001575217786\n",
      "        policy_loss: -0.047726322854869066\n",
      "        total_loss: 21.115199019114176\n",
      "        vf_explained_var: 0.16525969584782918\n",
      "        vf_loss: 21.156611828804017\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1171968\n",
      "  num_agent_steps_trained: 1171968\n",
      "  num_steps_sampled: 1172000\n",
      "  num_steps_trained: 1172000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 293\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.375\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.5\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 30.333333333333336\n",
      "  player_1: 21.133333333333336\n",
      "  player_2: 20.733333333333334\n",
      "policy_reward_mean:\n",
      "  player_0: 6.856\n",
      "  player_1: -1.0340000000000003\n",
      "  player_2: -2.822\n",
      "policy_reward_min:\n",
      "  player_0: -20.0\n",
      "  player_1: -21.266666666666666\n",
      "  player_2: -39.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11639957168384002\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.37825765631659924\n",
      "  mean_inference_ms: 2.1397211139618277\n",
      "  mean_raw_obs_processing_ms: 0.2715626064477122\n",
      "time_since_restore: 2024.2504937648773\n",
      "time_this_iter_s: 6.281261444091797\n",
      "time_total_s: 2024.2504937648773\n",
      "timers:\n",
      "  learn_throughput: 712.869\n",
      "  learn_time_ms: 5611.126\n",
      "  load_throughput: 6175359.246\n",
      "  load_time_ms: 0.648\n",
      "  sample_throughput: 622.929\n",
      "  sample_time_ms: 6421.278\n",
      "  update_time_ms: 3.875\n",
      "timestamp: 1643327099\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1172000\n",
      "training_iteration: 293\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1179968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-45-12\n",
      "done: false\n",
      "episode_len_mean: 163.12\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 7248\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5181885714332263\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01612216895170756\n",
      "        policy_loss: -0.10728372312771778\n",
      "        total_loss: 31.209210427602134\n",
      "        vf_explained_var: 0.09671484371026356\n",
      "        vf_loss: 31.309239133199057\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5804355576634407\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01365381390588785\n",
      "        policy_loss: -0.07048076666891574\n",
      "        total_loss: 38.71287068684896\n",
      "        vf_explained_var: 0.1935970077912013\n",
      "        vf_loss: 38.777207142512005\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5710650700330734\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016422014146986234\n",
      "        policy_loss: -0.06044649966061115\n",
      "        total_loss: 36.57196931838989\n",
      "        vf_explained_var: 0.34499561846256255\n",
      "        vf_loss: 36.62502616564433\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1179968\n",
      "  num_agent_steps_trained: 1179968\n",
      "  num_steps_sampled: 1180000\n",
      "  num_steps_trained: 1180000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 295\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 23.733333333333334\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.544444444444444\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 30.333333333333336\n",
      "  player_1: 21.133333333333336\n",
      "  player_2: 20.733333333333334\n",
      "policy_reward_mean:\n",
      "  player_0: 7.351333333333335\n",
      "  player_1: -0.6086666666666665\n",
      "  player_2: -3.7426666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -17.466666666666665\n",
      "  player_1: -27.333333333333336\n",
      "  player_2: -39.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11642642468420535\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.37809867993391244\n",
      "  mean_inference_ms: 2.1396904584087757\n",
      "  mean_raw_obs_processing_ms: 0.2714300049679633\n",
      "time_since_restore: 2037.066339969635\n",
      "time_this_iter_s: 6.337272644042969\n",
      "time_total_s: 2037.066339969635\n",
      "timers:\n",
      "  learn_throughput: 711.004\n",
      "  learn_time_ms: 5625.849\n",
      "  load_throughput: 6238970.659\n",
      "  load_time_ms: 0.641\n",
      "  sample_throughput: 622.543\n",
      "  sample_time_ms: 6425.258\n",
      "  update_time_ms: 3.9\n",
      "timestamp: 1643327112\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1180000\n",
      "training_iteration: 295\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1187971\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-45-25\n",
      "done: false\n",
      "episode_len_mean: 154.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 7304\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5121525524059931\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015379292053530988\n",
      "        policy_loss: -0.057723873481154445\n",
      "        total_loss: 48.405044403076175\n",
      "        vf_explained_var: 0.08762241502602895\n",
      "        vf_loss: 48.455847641626995\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6134229506055514\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015538877102408151\n",
      "        policy_loss: -0.08129394853798051\n",
      "        total_loss: 36.72073768933614\n",
      "        vf_explained_var: 0.21969479699929556\n",
      "        vf_loss: 36.7950390625\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5460111137231191\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01427462931528377\n",
      "        policy_loss: -0.04261319741296272\n",
      "        total_loss: 37.75332005182902\n",
      "        vf_explained_var: 0.09672888894875845\n",
      "        vf_loss: 37.78950964609782\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1187971\n",
      "  num_agent_steps_trained: 1187971\n",
      "  num_steps_sampled: 1188000\n",
      "  num_steps_trained: 1188000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 297\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.25\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.525\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 27.0\n",
      "  player_2: 22.066666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 7.770666666666667\n",
      "  player_1: -1.077333333333333\n",
      "  player_2: -3.6933333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -23.06666666666667\n",
      "  player_1: -27.333333333333336\n",
      "  player_2: -28.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11657950591321152\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3778798576157127\n",
      "  mean_inference_ms: 2.1386442301415833\n",
      "  mean_raw_obs_processing_ms: 0.27137893453332773\n",
      "time_since_restore: 2050.065719604492\n",
      "time_this_iter_s: 6.545285940170288\n",
      "time_total_s: 2050.065719604492\n",
      "timers:\n",
      "  learn_throughput: 708.501\n",
      "  learn_time_ms: 5645.725\n",
      "  load_throughput: 6330308.267\n",
      "  load_time_ms: 0.632\n",
      "  sample_throughput: 620.936\n",
      "  sample_time_ms: 6441.889\n",
      "  update_time_ms: 3.867\n",
      "timestamp: 1643327125\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1188000\n",
      "training_iteration: 297\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1195969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-45-38\n",
      "done: false\n",
      "episode_len_mean: 156.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 7350\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5108805106083552\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014052054508139615\n",
      "        policy_loss: -0.07565205120791992\n",
      "        total_loss: 32.75195280234019\n",
      "        vf_explained_var: 0.036278988122940066\n",
      "        vf_loss: 32.821281472841896\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6008035555481911\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01623867111601006\n",
      "        policy_loss: -0.035453412383794784\n",
      "        total_loss: 25.251589741706848\n",
      "        vf_explained_var: 0.15660276889801025\n",
      "        vf_loss: 25.2797355333964\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5585638127724329\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015776292665955604\n",
      "        policy_loss: -0.0723239201058944\n",
      "        total_loss: 25.6218012825648\n",
      "        vf_explained_var: 0.287931098540624\n",
      "        vf_loss: 25.68702596664429\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1195969\n",
      "  num_agent_steps_trained: 1195969\n",
      "  num_steps_sampled: 1196000\n",
      "  num_steps_trained: 1196000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 299\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.233333333333334\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.51111111111111\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 37.0\n",
      "  player_1: 27.0\n",
      "  player_2: 23.53333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 8.758666666666665\n",
      "  player_1: -1.9873333333333334\n",
      "  player_2: -3.7713333333333328\n",
      "policy_reward_min:\n",
      "  player_0: -23.06666666666667\n",
      "  player_1: -25.53333333333333\n",
      "  player_2: -29.666666666666668\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11648588557941751\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3775239477623529\n",
      "  mean_inference_ms: 2.137610998244218\n",
      "  mean_raw_obs_processing_ms: 0.27153774390528457\n",
      "time_since_restore: 2062.884607553482\n",
      "time_this_iter_s: 6.427558898925781\n",
      "time_total_s: 2062.884607553482\n",
      "timers:\n",
      "  learn_throughput: 709.074\n",
      "  learn_time_ms: 5641.159\n",
      "  load_throughput: 6296808.287\n",
      "  load_time_ms: 0.635\n",
      "  sample_throughput: 620.185\n",
      "  sample_time_ms: 6449.684\n",
      "  update_time_ms: 3.873\n",
      "timestamp: 1643327138\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1196000\n",
      "training_iteration: 299\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1203968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-45-50\n",
      "done: false\n",
      "episode_len_mean: 160.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 7401\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5052712657054266\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014716912489941348\n",
      "        policy_loss: -0.06184372344675163\n",
      "        total_loss: 28.429972151120502\n",
      "        vf_explained_var: 0.1420597187678019\n",
      "        vf_loss: 28.485193289120993\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6083412933349609\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01554996166333248\n",
      "        policy_loss: -0.07145063120794172\n",
      "        total_loss: 24.32891103108724\n",
      "        vf_explained_var: 0.34094956119855246\n",
      "        vf_loss: 24.39336424032847\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5234477504094441\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014390677761775805\n",
      "        policy_loss: -0.07028912989112238\n",
      "        total_loss: 41.33030520121257\n",
      "        vf_explained_var: -0.014924154082934061\n",
      "        vf_loss: 41.3941184870402\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1203968\n",
      "  num_agent_steps_trained: 1203968\n",
      "  num_steps_sampled: 1204000\n",
      "  num_steps_trained: 1204000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 301\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.175\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.5\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 37.0\n",
      "  player_1: 27.666666666666668\n",
      "  player_2: 23.53333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 7.612666666666667\n",
      "  player_1: -0.7453333333333338\n",
      "  player_2: -3.8673333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -16.333333333333332\n",
      "  player_1: -26.0\n",
      "  player_2: -29.666666666666668\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11608871845763423\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.377143363090193\n",
      "  mean_inference_ms: 2.1343261600679075\n",
      "  mean_raw_obs_processing_ms: 0.27123504368971224\n",
      "time_since_restore: 2075.684904575348\n",
      "time_this_iter_s: 6.362067461013794\n",
      "time_total_s: 2075.684904575348\n",
      "timers:\n",
      "  learn_throughput: 710.964\n",
      "  learn_time_ms: 5626.165\n",
      "  load_throughput: 6139877.768\n",
      "  load_time_ms: 0.651\n",
      "  sample_throughput: 620.28\n",
      "  sample_time_ms: 6448.701\n",
      "  update_time_ms: 3.85\n",
      "timestamp: 1643327150\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1204000\n",
      "training_iteration: 301\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1211968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-46-03\n",
      "done: false\n",
      "episode_len_mean: 156.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 7454\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47490008642276127\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013579098304247358\n",
      "        policy_loss: -0.04977487849692504\n",
      "        total_loss: 23.467712626457214\n",
      "        vf_explained_var: 0.26142344137032825\n",
      "        vf_loss: 23.5113769086202\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5753851905465126\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014906296308861186\n",
      "        policy_loss: -0.07514842480421066\n",
      "        total_loss: 54.278912216822306\n",
      "        vf_explained_var: -0.09263488094011943\n",
      "        vf_loss: 54.347352520624796\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5585988992452622\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014466288572390719\n",
      "        policy_loss: -0.11860608632365863\n",
      "        total_loss: 37.173406244913735\n",
      "        vf_explained_var: 0.10525146166483561\n",
      "        vf_loss: 37.28550261179606\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1211968\n",
      "  num_agent_steps_trained: 1211968\n",
      "  num_steps_sampled: 1212000\n",
      "  num_steps_trained: 1212000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 303\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.737499999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.5625\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.666666666666664\n",
      "  player_1: 27.666666666666668\n",
      "  player_2: 20.53333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 7.261333333333333\n",
      "  player_1: -0.22866666666666677\n",
      "  player_2: -4.0326666666666675\n",
      "policy_reward_min:\n",
      "  player_0: -16.333333333333332\n",
      "  player_1: -35.06666666666666\n",
      "  player_2: -31.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11629590683307305\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3768163636448802\n",
      "  mean_inference_ms: 2.135178623152487\n",
      "  mean_raw_obs_processing_ms: 0.2709055568139041\n",
      "time_since_restore: 2088.4971113204956\n",
      "time_this_iter_s: 6.349585056304932\n",
      "time_total_s: 2088.4971113204956\n",
      "timers:\n",
      "  learn_throughput: 708.945\n",
      "  learn_time_ms: 5642.19\n",
      "  load_throughput: 6045407.899\n",
      "  load_time_ms: 0.662\n",
      "  sample_throughput: 619.738\n",
      "  sample_time_ms: 6454.336\n",
      "  update_time_ms: 3.883\n",
      "timestamp: 1643327163\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1212000\n",
      "training_iteration: 303\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1219968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-46-16\n",
      "done: false\n",
      "episode_len_mean: 155.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 7506\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47280735393365225\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015004735979947934\n",
      "        policy_loss: -0.09076622242418428\n",
      "        total_loss: 32.536896901130675\n",
      "        vf_explained_var: 0.01120472013950348\n",
      "        vf_loss: 32.62091117699941\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.543715671847264\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016480696873986744\n",
      "        policy_loss: -0.06198010956868529\n",
      "        total_loss: 36.03281525929769\n",
      "        vf_explained_var: 0.4952488700548808\n",
      "        vf_loss: 36.08737886269887\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5106760071714719\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013928007859977427\n",
      "        policy_loss: -0.0057284891481200854\n",
      "        total_loss: 25.1093097893397\n",
      "        vf_explained_var: 0.23212416986624398\n",
      "        vf_loss: 25.1087708012263\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1219968\n",
      "  num_agent_steps_trained: 1219968\n",
      "  num_steps_sampled: 1220000\n",
      "  num_steps_trained: 1220000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 305\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.412499999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.666666666666664\n",
      "  player_1: 19.599999999999998\n",
      "  player_2: 20.53333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 9.156666666666668\n",
      "  player_1: -2.563333333333333\n",
      "  player_2: -3.5933333333333337\n",
      "policy_reward_min:\n",
      "  player_0: -15.93333333333333\n",
      "  player_1: -35.06666666666666\n",
      "  player_2: -31.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11594005076063944\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3762974479714984\n",
      "  mean_inference_ms: 2.1315648347086382\n",
      "  mean_raw_obs_processing_ms: 0.27077694653681134\n",
      "time_since_restore: 2101.2017862796783\n",
      "time_this_iter_s: 6.284234046936035\n",
      "time_total_s: 2101.2017862796783\n",
      "timers:\n",
      "  learn_throughput: 710.077\n",
      "  learn_time_ms: 5633.191\n",
      "  load_throughput: 6058725.218\n",
      "  load_time_ms: 0.66\n",
      "  sample_throughput: 620.004\n",
      "  sample_time_ms: 6451.571\n",
      "  update_time_ms: 3.997\n",
      "timestamp: 1643327176\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1220000\n",
      "training_iteration: 305\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1227969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-46-29\n",
      "done: false\n",
      "episode_len_mean: 156.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 7557\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5102753959099452\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014561937954065185\n",
      "        policy_loss: -0.020625282345960536\n",
      "        total_loss: 35.87396766026815\n",
      "        vf_explained_var: 0.20154538253943124\n",
      "        vf_loss: 35.88803976376851\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5850601287682852\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01491390310656546\n",
      "        policy_loss: -0.1154624654725194\n",
      "        total_loss: 29.05712237040202\n",
      "        vf_explained_var: 0.3921108714739482\n",
      "        vf_loss: 29.165873494148254\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5342428201436996\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013912137638678056\n",
      "        policy_loss: -0.07217761573071281\n",
      "        total_loss: 30.084895022710164\n",
      "        vf_explained_var: 0.21010464231173198\n",
      "        vf_loss: 30.15081233819326\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1227969\n",
      "  num_agent_steps_trained: 1227969\n",
      "  num_steps_sampled: 1228000\n",
      "  num_steps_trained: 1228000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 307\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.924999999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.575\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.666666666666668\n",
      "  player_1: 19.599999999999998\n",
      "  player_2: 17.333333333333332\n",
      "policy_reward_mean:\n",
      "  player_0: 9.030000000000001\n",
      "  player_1: -1.0460000000000005\n",
      "  player_2: -4.984\n",
      "policy_reward_min:\n",
      "  player_0: -15.93333333333333\n",
      "  player_1: -23.333333333333336\n",
      "  player_2: -31.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11589154754222296\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3759156975202916\n",
      "  mean_inference_ms: 2.130998877077405\n",
      "  mean_raw_obs_processing_ms: 0.27074575393486366\n",
      "time_since_restore: 2114.058549642563\n",
      "time_this_iter_s: 6.325350522994995\n",
      "time_total_s: 2114.058549642563\n",
      "timers:\n",
      "  learn_throughput: 711.946\n",
      "  learn_time_ms: 5618.402\n",
      "  load_throughput: 6139653.078\n",
      "  load_time_ms: 0.652\n",
      "  sample_throughput: 620.903\n",
      "  sample_time_ms: 6442.228\n",
      "  update_time_ms: 3.997\n",
      "timestamp: 1643327189\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1228000\n",
      "training_iteration: 307\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1235968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-46-42\n",
      "done: false\n",
      "episode_len_mean: 156.6\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 7606\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5041397062937418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014147609160196832\n",
      "        policy_loss: -0.04933408606797457\n",
      "        total_loss: 35.076597259839374\n",
      "        vf_explained_var: 0.43466130991776786\n",
      "        vf_loss: 35.11956478118896\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6253332795699438\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01596354079976057\n",
      "        policy_loss: -0.08368876269708077\n",
      "        total_loss: 35.29777728398641\n",
      "        vf_explained_var: 0.10540590643882751\n",
      "        vf_loss: 35.37428215185801\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5132570180296898\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014992628002849718\n",
      "        policy_loss: -0.0397834824770689\n",
      "        total_loss: 21.77726430018743\n",
      "        vf_explained_var: -0.11229229807853698\n",
      "        vf_loss: 21.810301128228506\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1235968\n",
      "  num_agent_steps_trained: 1235968\n",
      "  num_steps_sampled: 1236000\n",
      "  num_steps_trained: 1236000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 309\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.299999999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.5375\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 29.333333333333332\n",
      "  player_1: 18.93333333333333\n",
      "  player_2: 19.333333333333332\n",
      "policy_reward_mean:\n",
      "  player_0: 7.506666666666666\n",
      "  player_1: -0.885333333333334\n",
      "  player_2: -3.6213333333333337\n",
      "policy_reward_min:\n",
      "  player_0: -19.333333333333336\n",
      "  player_1: -35.66666666666667\n",
      "  player_2: -31.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11592178907463016\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3758780745830218\n",
      "  mean_inference_ms: 2.131735393707879\n",
      "  mean_raw_obs_processing_ms: 0.2708266728566879\n",
      "time_since_restore: 2126.866332769394\n",
      "time_this_iter_s: 6.307544231414795\n",
      "time_total_s: 2126.866332769394\n",
      "timers:\n",
      "  learn_throughput: 711.527\n",
      "  learn_time_ms: 5621.709\n",
      "  load_throughput: 5966717.405\n",
      "  load_time_ms: 0.67\n",
      "  sample_throughput: 621.19\n",
      "  sample_time_ms: 6439.257\n",
      "  update_time_ms: 4.036\n",
      "timestamp: 1643327202\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1236000\n",
      "training_iteration: 309\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1243968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-46-55\n",
      "done: false\n",
      "episode_len_mean: 159.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 7654\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5120647974809011\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01498685042587264\n",
      "        policy_loss: -0.03357369108280788\n",
      "        total_loss: 26.52246643225352\n",
      "        vf_explained_var: 0.09617417971293131\n",
      "        vf_loss: 26.549295949935914\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5723961593707403\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014489698501686993\n",
      "        policy_loss: -0.04161778081208468\n",
      "        total_loss: 30.81018800417582\n",
      "        vf_explained_var: 0.1453042342265447\n",
      "        vf_loss: 30.845285199483236\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5173937399188677\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012324075609582602\n",
      "        policy_loss: -0.07107233899645507\n",
      "        total_loss: 21.219261050224304\n",
      "        vf_explained_var: 0.17782654225826264\n",
      "        vf_loss: 21.284787712891898\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1243968\n",
      "  num_agent_steps_trained: 1243968\n",
      "  num_steps_sampled: 1244000\n",
      "  num_steps_trained: 1244000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 311\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.125\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.5625\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 29.333333333333332\n",
      "  player_1: 19.399999999999995\n",
      "  player_2: 19.333333333333332\n",
      "policy_reward_mean:\n",
      "  player_0: 7.769333333333333\n",
      "  player_1: -2.474666666666667\n",
      "  player_2: -2.2946666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -19.333333333333336\n",
      "  player_1: -35.66666666666667\n",
      "  player_2: -23.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11583366449633145\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.37553419290335205\n",
      "  mean_inference_ms: 2.1295711426577992\n",
      "  mean_raw_obs_processing_ms: 0.27086081883313967\n",
      "time_since_restore: 2139.6586253643036\n",
      "time_this_iter_s: 6.297770261764526\n",
      "time_total_s: 2139.6586253643036\n",
      "timers:\n",
      "  learn_throughput: 710.226\n",
      "  learn_time_ms: 5632.011\n",
      "  load_throughput: 6008601.103\n",
      "  load_time_ms: 0.666\n",
      "  sample_throughput: 621.302\n",
      "  sample_time_ms: 6438.091\n",
      "  update_time_ms: 3.941\n",
      "timestamp: 1643327215\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1244000\n",
      "training_iteration: 311\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1251968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-47-07\n",
      "done: false\n",
      "episode_len_mean: 173.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 19\n",
      "episodes_total: 7697\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48089771926403047\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017639246718899812\n",
      "        policy_loss: -0.06539439774428804\n",
      "        total_loss: 33.85407387415568\n",
      "        vf_explained_var: 0.26655968685944875\n",
      "        vf_loss: 33.911530857086184\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5776036385695139\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014450628929929129\n",
      "        policy_loss: -0.10388608165085315\n",
      "        total_loss: 27.748987441062926\n",
      "        vf_explained_var: 0.2689013131459554\n",
      "        vf_loss: 27.84637096722921\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5086146214604378\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012728002733194141\n",
      "        policy_loss: -0.03426186134107411\n",
      "        total_loss: 24.74392236471176\n",
      "        vf_explained_var: 0.33773145397504173\n",
      "        vf_loss: 24.77245663801829\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1251968\n",
      "  num_agent_steps_trained: 1251968\n",
      "  num_steps_sampled: 1252000\n",
      "  num_steps_trained: 1252000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 313\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.7625\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.33333333333333\n",
      "  player_1: 19.399999999999995\n",
      "  player_2: 27.0\n",
      "policy_reward_mean:\n",
      "  player_0: 7.146666666666667\n",
      "  player_1: -2.1433333333333335\n",
      "  player_2: -2.003333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -31.0\n",
      "  player_1: -35.66666666666667\n",
      "  player_2: -31.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11590967891062405\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3752959580191366\n",
      "  mean_inference_ms: 2.127681753696496\n",
      "  mean_raw_obs_processing_ms: 0.27094319901291114\n",
      "time_since_restore: 2152.318427801132\n",
      "time_this_iter_s: 6.274555444717407\n",
      "time_total_s: 2152.318427801132\n",
      "timers:\n",
      "  learn_throughput: 711.634\n",
      "  learn_time_ms: 5620.871\n",
      "  load_throughput: 6133368.429\n",
      "  load_time_ms: 0.652\n",
      "  sample_throughput: 622.945\n",
      "  sample_time_ms: 6421.109\n",
      "  update_time_ms: 3.828\n",
      "timestamp: 1643327227\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1252000\n",
      "training_iteration: 313\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1259968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-47-20\n",
      "done: false\n",
      "episode_len_mean: 166.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 28\n",
      "episodes_total: 7753\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4932330839832624\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014557980179073032\n",
      "        policy_loss: -0.0747513866983354\n",
      "        total_loss: 22.401380047798156\n",
      "        vf_explained_var: -0.0037378027041753133\n",
      "        vf_loss: 22.46958034515381\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6007003993789355\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014589795704778226\n",
      "        policy_loss: -0.09384982425215033\n",
      "        total_loss: 25.662146757443747\n",
      "        vf_explained_var: -0.057283754348754885\n",
      "        vf_loss: 25.749431343078612\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5556140007575353\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016906343242480942\n",
      "        policy_loss: -0.08362797642747562\n",
      "        total_loss: 34.359043289820356\n",
      "        vf_explained_var: 0.240192418495814\n",
      "        vf_loss: 34.43506336847941\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1259968\n",
      "  num_agent_steps_trained: 1259968\n",
      "  num_steps_sampled: 1260000\n",
      "  num_steps_trained: 1260000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 315\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.5\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.588888888888896\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.33333333333333\n",
      "  player_1: 20.800000000000004\n",
      "  player_2: 27.0\n",
      "policy_reward_mean:\n",
      "  player_0: 7.016\n",
      "  player_1: -1.204\n",
      "  player_2: -2.8120000000000003\n",
      "policy_reward_min:\n",
      "  player_0: -31.0\n",
      "  player_1: -21.333333333333332\n",
      "  player_2: -31.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11572208469854839\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3748520050530685\n",
      "  mean_inference_ms: 2.125952820576963\n",
      "  mean_raw_obs_processing_ms: 0.2703501961401677\n",
      "time_since_restore: 2165.147089958191\n",
      "time_this_iter_s: 6.312283039093018\n",
      "time_total_s: 2165.147089958191\n",
      "timers:\n",
      "  learn_throughput: 711.673\n",
      "  learn_time_ms: 5620.557\n",
      "  load_throughput: 6151358.803\n",
      "  load_time_ms: 0.65\n",
      "  sample_throughput: 622.531\n",
      "  sample_time_ms: 6425.379\n",
      "  update_time_ms: 3.719\n",
      "timestamp: 1643327240\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1260000\n",
      "training_iteration: 315\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1267968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-47-33\n",
      "done: false\n",
      "episode_len_mean: 151.82\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 28\n",
      "episodes_total: 7806\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5144578754901886\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013649117525977392\n",
      "        policy_loss: -0.06660160924618443\n",
      "        total_loss: 51.940351467132565\n",
      "        vf_explained_var: 0.26989246666431427\n",
      "        vf_loss: 52.00081104278564\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5876137875517209\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014836865341300532\n",
      "        policy_loss: -0.1143427703777949\n",
      "        total_loss: 22.315622475941975\n",
      "        vf_explained_var: 0.2408908208211263\n",
      "        vf_loss: 22.42328866322835\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5059726256132125\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014839832525106733\n",
      "        policy_loss: -0.014163265759125352\n",
      "        total_loss: 51.71720149834951\n",
      "        vf_explained_var: 0.21927063703536986\n",
      "        vf_loss: 51.72468672116597\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1267968\n",
      "  num_agent_steps_trained: 1267968\n",
      "  num_steps_sampled: 1268000\n",
      "  num_steps_trained: 1268000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 317\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.5\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.587500000000006\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 37.333333333333336\n",
      "  player_1: 22.400000000000002\n",
      "  player_2: 22.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 6.800666666666667\n",
      "  player_1: -0.0033333333333335703\n",
      "  player_2: -3.797333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -20.53333333333333\n",
      "  player_1: -21.46666666666667\n",
      "  player_2: -35.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11581762496721139\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3747552424648871\n",
      "  mean_inference_ms: 2.125905419998824\n",
      "  mean_raw_obs_processing_ms: 0.27021701820535166\n",
      "time_since_restore: 2177.976654291153\n",
      "time_this_iter_s: 6.319872856140137\n",
      "time_total_s: 2177.976654291153\n",
      "timers:\n",
      "  learn_throughput: 710.353\n",
      "  learn_time_ms: 5631.003\n",
      "  load_throughput: 6088186.668\n",
      "  load_time_ms: 0.657\n",
      "  sample_throughput: 622.617\n",
      "  sample_time_ms: 6424.497\n",
      "  update_time_ms: 3.632\n",
      "timestamp: 1643327253\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1268000\n",
      "training_iteration: 317\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1275968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-47-46\n",
      "done: false\n",
      "episode_len_mean: 152.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 7853\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4857688917716344\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014039506908162016\n",
      "        policy_loss: -0.08799104142313202\n",
      "        total_loss: 27.99843995889028\n",
      "        vf_explained_var: 0.1803726859887441\n",
      "        vf_loss: 28.080113407770792\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.588085618019104\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016356336247141976\n",
      "        policy_loss: -0.09202776831885179\n",
      "        total_loss: 28.212102341651917\n",
      "        vf_explained_var: 0.16264044165611266\n",
      "        vf_loss: 28.29676980495453\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5176895184318224\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014187969632330351\n",
      "        policy_loss: -0.08559493207993607\n",
      "        total_loss: 23.043019058704377\n",
      "        vf_explained_var: 0.39749240656693774\n",
      "        vf_loss: 23.12222945690155\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1275968\n",
      "  num_agent_steps_trained: 1275968\n",
      "  num_steps_sampled: 1276000\n",
      "  num_steps_trained: 1276000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 319\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.433333333333337\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.588888888888896\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 37.333333333333336\n",
      "  player_1: 22.400000000000002\n",
      "  player_2: 22.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 6.29\n",
      "  player_1: -0.0020000000000000282\n",
      "  player_2: -3.2880000000000003\n",
      "policy_reward_min:\n",
      "  player_0: -20.53333333333333\n",
      "  player_1: -23.333333333333336\n",
      "  player_2: -35.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11568623418893606\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.37448384556674186\n",
      "  mean_inference_ms: 2.1247116388451355\n",
      "  mean_raw_obs_processing_ms: 0.270450654862371\n",
      "time_since_restore: 2190.8904960155487\n",
      "time_this_iter_s: 6.392286777496338\n",
      "time_total_s: 2190.8904960155487\n",
      "timers:\n",
      "  learn_throughput: 709.659\n",
      "  learn_time_ms: 5636.511\n",
      "  load_throughput: 6250359.884\n",
      "  load_time_ms: 0.64\n",
      "  sample_throughput: 622.513\n",
      "  sample_time_ms: 6425.567\n",
      "  update_time_ms: 3.62\n",
      "timestamp: 1643327266\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1276000\n",
      "training_iteration: 319\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1283968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-47-59\n",
      "done: false\n",
      "episode_len_mean: 163.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 7902\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4685874751210213\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014719840090571475\n",
      "        policy_loss: -0.07881571916397662\n",
      "        total_loss: 26.77747709910075\n",
      "        vf_explained_var: 0.3626527408758799\n",
      "        vf_loss: 26.849668951034545\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5604305973649025\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015530977638482\n",
      "        policy_loss: -0.07345320786504696\n",
      "        total_loss: 24.36716382185618\n",
      "        vf_explained_var: 0.0501371165116628\n",
      "        vf_loss: 24.43362819035848\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5263394504785538\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016097398275172356\n",
      "        policy_loss: -0.08739794397416215\n",
      "        total_loss: 24.912349495887756\n",
      "        vf_explained_var: 0.15336088418960572\n",
      "        vf_loss: 24.992503685951235\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1283968\n",
      "  num_agent_steps_trained: 1283968\n",
      "  num_steps_sampled: 1284000\n",
      "  num_steps_trained: 1284000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 321\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.950000000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.33333333333333\n",
      "  player_1: 22.866666666666667\n",
      "  player_2: 19.0\n",
      "policy_reward_mean:\n",
      "  player_0: 6.483999999999998\n",
      "  player_1: 0.294\n",
      "  player_2: -3.7779999999999996\n",
      "policy_reward_min:\n",
      "  player_0: -21.46666666666666\n",
      "  player_1: -23.333333333333336\n",
      "  player_2: -21.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11544886729636993\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.37389475318991167\n",
      "  mean_inference_ms: 2.122217665127635\n",
      "  mean_raw_obs_processing_ms: 0.27020309703961304\n",
      "time_since_restore: 2203.761137008667\n",
      "time_this_iter_s: 6.427522659301758\n",
      "time_total_s: 2203.761137008667\n",
      "timers:\n",
      "  learn_throughput: 709.167\n",
      "  learn_time_ms: 5640.423\n",
      "  load_throughput: 6333175.796\n",
      "  load_time_ms: 0.632\n",
      "  sample_throughput: 622.295\n",
      "  sample_time_ms: 6427.822\n",
      "  update_time_ms: 3.782\n",
      "timestamp: 1643327279\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1284000\n",
      "training_iteration: 321\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1291968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-48-13\n",
      "done: false\n",
      "episode_len_mean: 171.42\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 21\n",
      "episodes_total: 7948\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49121825754642484\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013867023122334883\n",
      "        policy_loss: -0.017414915536840755\n",
      "        total_loss: 44.53872071584066\n",
      "        vf_explained_var: -0.053706538081169125\n",
      "        vf_loss: 44.54989587783813\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.593252448340257\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014721119423775235\n",
      "        policy_loss: -0.119087176246879\n",
      "        total_loss: 28.878116499582926\n",
      "        vf_explained_var: 0.4069873430331548\n",
      "        vf_loss: 28.990579427083333\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5272694125771522\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012791774825834044\n",
      "        policy_loss: -0.08911067618678013\n",
      "        total_loss: 26.995242457389832\n",
      "        vf_explained_var: 0.15600737551848093\n",
      "        vf_loss: 27.078596703211467\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1291968\n",
      "  num_agent_steps_trained: 1291968\n",
      "  num_steps_sampled: 1292000\n",
      "  num_steps_trained: 1292000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 323\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.8875\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.33333333333333\n",
      "  player_1: 22.866666666666667\n",
      "  player_2: 19.333333333333332\n",
      "policy_reward_mean:\n",
      "  player_0: 7.411333333333333\n",
      "  player_1: -0.8746666666666667\n",
      "  player_2: -3.536666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -21.46666666666666\n",
      "  player_1: -27.333333333333332\n",
      "  player_2: -27.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11548083556460577\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.37370892171204145\n",
      "  mean_inference_ms: 2.1225881269915994\n",
      "  mean_raw_obs_processing_ms: 0.270098551866422\n",
      "time_since_restore: 2217.004405260086\n",
      "time_this_iter_s: 6.532821416854858\n",
      "time_total_s: 2217.004405260086\n",
      "timers:\n",
      "  learn_throughput: 703.987\n",
      "  learn_time_ms: 5681.921\n",
      "  load_throughput: 6260856.066\n",
      "  load_time_ms: 0.639\n",
      "  sample_throughput: 616.894\n",
      "  sample_time_ms: 6484.094\n",
      "  update_time_ms: 3.861\n",
      "timestamp: 1643327293\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1292000\n",
      "training_iteration: 323\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1299968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-48-26\n",
      "done: false\n",
      "episode_len_mean: 159.54\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 8002\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49606012483437856\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013667393240750458\n",
      "        policy_loss: -0.07871389662846923\n",
      "        total_loss: 38.006221243540445\n",
      "        vf_explained_var: 0.04620284100373586\n",
      "        vf_loss: 38.078784704208374\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5550912774602572\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014666571870360107\n",
      "        policy_loss: -0.053154959868018825\n",
      "        total_loss: 36.21172019640605\n",
      "        vf_explained_var: 0.017009104092915853\n",
      "        vf_loss: 36.25827534039815\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49308159867922463\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015168899994496315\n",
      "        policy_loss: -0.05496245658025146\n",
      "        total_loss: 25.665367727279662\n",
      "        vf_explained_var: -0.06409819026788076\n",
      "        vf_loss: 25.71350433031718\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1299968\n",
      "  num_agent_steps_trained: 1299968\n",
      "  num_steps_sampled: 1300000\n",
      "  num_steps_trained: 1300000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 325\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.287499999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.333333333333336\n",
      "  player_1: 17.599999999999998\n",
      "  player_2: 19.333333333333332\n",
      "policy_reward_mean:\n",
      "  player_0: 8.972666666666667\n",
      "  player_1: -1.967333333333333\n",
      "  player_2: -4.005333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -17.333333333333332\n",
      "  player_1: -30.333333333333336\n",
      "  player_2: -27.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11563353075819953\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3737067677769014\n",
      "  mean_inference_ms: 2.1231765934337914\n",
      "  mean_raw_obs_processing_ms: 0.270019817334593\n",
      "time_since_restore: 2230.066039085388\n",
      "time_this_iter_s: 6.412353754043579\n",
      "time_total_s: 2230.066039085388\n",
      "timers:\n",
      "  learn_throughput: 700.354\n",
      "  learn_time_ms: 5711.396\n",
      "  load_throughput: 6353562.069\n",
      "  load_time_ms: 0.63\n",
      "  sample_throughput: 613.616\n",
      "  sample_time_ms: 6518.73\n",
      "  update_time_ms: 3.834\n",
      "timestamp: 1643327306\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1300000\n",
      "training_iteration: 325\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1307968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-48-39\n",
      "done: false\n",
      "episode_len_mean: 151.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 8053\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5073578421274821\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015049747348190673\n",
      "        policy_loss: -0.0679382298514247\n",
      "        total_loss: 27.62975640932719\n",
      "        vf_explained_var: 0.11871252735455831\n",
      "        vf_loss: 27.690922463734946\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6001552030444145\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014805006025853799\n",
      "        policy_loss: -0.08293710857629776\n",
      "        total_loss: 33.78094089825948\n",
      "        vf_explained_var: 0.09854782164096833\n",
      "        vf_loss: 33.85721545537313\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5201361891627312\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014557322797869953\n",
      "        policy_loss: -0.026568986000493167\n",
      "        total_loss: 29.816068239212036\n",
      "        vf_explained_var: 0.1079221127430598\n",
      "        vf_loss: 29.836086428960165\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1307968\n",
      "  num_agent_steps_trained: 1307968\n",
      "  num_steps_sampled: 1308000\n",
      "  num_steps_trained: 1308000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 327\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.837500000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.333333333333336\n",
      "  player_1: 21.666666666666664\n",
      "  player_2: 18.666666666666668\n",
      "policy_reward_mean:\n",
      "  player_0: 12.253333333333332\n",
      "  player_1: -3.030666666666667\n",
      "  player_2: -6.222666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -16.666666666666664\n",
      "  player_1: -34.73333333333333\n",
      "  player_2: -27.666666666666668\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1154660586248361\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3731152014317626\n",
      "  mean_inference_ms: 2.1198259633422945\n",
      "  mean_raw_obs_processing_ms: 0.2696805554988197\n",
      "time_since_restore: 2243.043343782425\n",
      "time_this_iter_s: 6.458045721054077\n",
      "time_total_s: 2243.043343782425\n",
      "timers:\n",
      "  learn_throughput: 699.188\n",
      "  learn_time_ms: 5720.925\n",
      "  load_throughput: 6274436.591\n",
      "  load_time_ms: 0.638\n",
      "  sample_throughput: 612.516\n",
      "  sample_time_ms: 6530.438\n",
      "  update_time_ms: 3.882\n",
      "timestamp: 1643327319\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1308000\n",
      "training_iteration: 327\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1315969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-48-52\n",
      "done: false\n",
      "episode_len_mean: 161.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 8102\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5236651885509491\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015986675952978355\n",
      "        policy_loss: -0.05661867689030866\n",
      "        total_loss: 36.17996755282084\n",
      "        vf_explained_var: 0.2264695421854655\n",
      "        vf_loss: 36.22939220110575\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5955094511310259\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01604618239998672\n",
      "        policy_loss: -0.07183321667214235\n",
      "        total_loss: 26.561817679405213\n",
      "        vf_explained_var: -0.12495588501294454\n",
      "        vf_loss: 26.62643026192983\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.540497225522995\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016011006930590762\n",
      "        policy_loss: -0.09112385985131065\n",
      "        total_loss: 42.02089576721191\n",
      "        vf_explained_var: 0.3869470844666163\n",
      "        vf_loss: 42.104814853668216\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1315969\n",
      "  num_agent_steps_trained: 1315969\n",
      "  num_steps_sampled: 1316000\n",
      "  num_steps_trained: 1316000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 329\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.877777777777776\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 30.0\n",
      "  player_1: 24.53333333333333\n",
      "  player_2: 23.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 10.253999999999996\n",
      "  player_1: -3.1660000000000004\n",
      "  player_2: -4.088000000000001\n",
      "policy_reward_min:\n",
      "  player_0: -21.333333333333336\n",
      "  player_1: -34.73333333333333\n",
      "  player_2: -27.666666666666668\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11557038296661942\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3730390908865152\n",
      "  mean_inference_ms: 2.119499824547585\n",
      "  mean_raw_obs_processing_ms: 0.2698065734412822\n",
      "time_since_restore: 2256.2974326610565\n",
      "time_this_iter_s: 6.505168199539185\n",
      "time_total_s: 2256.2974326610565\n",
      "timers:\n",
      "  learn_throughput: 695.383\n",
      "  learn_time_ms: 5752.228\n",
      "  load_throughput: 6262491.975\n",
      "  load_time_ms: 0.639\n",
      "  sample_throughput: 608.994\n",
      "  sample_time_ms: 6568.205\n",
      "  update_time_ms: 3.934\n",
      "timestamp: 1643327332\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1316000\n",
      "training_iteration: 329\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1323968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-49-05\n",
      "done: false\n",
      "episode_len_mean: 164.62\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 8152\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5026136438051859\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01462431079081331\n",
      "        policy_loss: -0.08879818905765811\n",
      "        total_loss: 36.40318693478902\n",
      "        vf_explained_var: 0.423141540090243\n",
      "        vf_loss: 36.4854043674469\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5641046701868375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01562333173664077\n",
      "        policy_loss: -0.09879793295947215\n",
      "        total_loss: 25.803911507924397\n",
      "        vf_explained_var: 0.10852883120377858\n",
      "        vf_loss: 25.895678793589273\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5127844825387001\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01372439251051522\n",
      "        policy_loss: -0.0991477157905077\n",
      "        total_loss: 30.42303162574768\n",
      "        vf_explained_var: -0.07269344508647918\n",
      "        vf_loss: 30.516003449757893\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1323968\n",
      "  num_agent_steps_trained: 1323968\n",
      "  num_steps_sampled: 1324000\n",
      "  num_steps_trained: 1324000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 331\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.033333333333335\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 24.53333333333333\n",
      "  player_2: 23.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 7.709333333333334\n",
      "  player_1: -1.734666666666667\n",
      "  player_2: -2.9746666666666663\n",
      "policy_reward_min:\n",
      "  player_0: -21.733333333333338\n",
      "  player_1: -27.666666666666664\n",
      "  player_2: -32.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11559803287606155\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.37293412012425625\n",
      "  mean_inference_ms: 2.118509690179142\n",
      "  mean_raw_obs_processing_ms: 0.26952769368611\n",
      "time_since_restore: 2269.399718284607\n",
      "time_this_iter_s: 6.491757869720459\n",
      "time_total_s: 2269.399718284607\n",
      "timers:\n",
      "  learn_throughput: 692.948\n",
      "  learn_time_ms: 5772.439\n",
      "  load_throughput: 6285484.789\n",
      "  load_time_ms: 0.636\n",
      "  sample_throughput: 606.658\n",
      "  sample_time_ms: 6593.505\n",
      "  update_time_ms: 3.974\n",
      "timestamp: 1643327345\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1324000\n",
      "training_iteration: 331\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1331970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-49-18\n",
      "done: false\n",
      "episode_len_mean: 154.6\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 8206\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4792800604303678\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013558496830531416\n",
      "        policy_loss: -0.0997798509337008\n",
      "        total_loss: 31.76496750195821\n",
      "        vf_explained_var: 0.45804474194844563\n",
      "        vf_loss: 31.858646065394083\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5559308194120725\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014834238211406045\n",
      "        policy_loss: -0.07867588023344675\n",
      "        total_loss: 36.84839591821035\n",
      "        vf_explained_var: 0.27732035001118976\n",
      "        vf_loss: 36.920396844546\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.486821404894193\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014208813692675903\n",
      "        policy_loss: -0.043731770782421034\n",
      "        total_loss: 35.81702665408452\n",
      "        vf_explained_var: 0.46681697607040407\n",
      "        vf_loss: 35.85436452547709\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1331970\n",
      "  num_agent_steps_trained: 1331970\n",
      "  num_steps_sampled: 1332000\n",
      "  num_steps_trained: 1332000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 333\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.950000000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 24.133333333333336\n",
      "  player_2: 15.333333333333332\n",
      "policy_reward_mean:\n",
      "  player_0: 8.950000000000001\n",
      "  player_1: -1.234\n",
      "  player_2: -4.716\n",
      "policy_reward_min:\n",
      "  player_0: -21.733333333333338\n",
      "  player_1: -31.66666666666667\n",
      "  player_2: -34.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11556469612331334\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.37268645109902837\n",
      "  mean_inference_ms: 2.1178393938821656\n",
      "  mean_raw_obs_processing_ms: 0.2694785182075281\n",
      "time_since_restore: 2282.483261346817\n",
      "time_this_iter_s: 6.451327800750732\n",
      "time_total_s: 2282.483261346817\n",
      "timers:\n",
      "  learn_throughput: 693.828\n",
      "  learn_time_ms: 5765.118\n",
      "  load_throughput: 6330786.008\n",
      "  load_time_ms: 0.632\n",
      "  sample_throughput: 607.11\n",
      "  sample_time_ms: 6588.587\n",
      "  update_time_ms: 3.882\n",
      "timestamp: 1643327358\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1332000\n",
      "training_iteration: 333\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1339968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-49-31\n",
      "done: false\n",
      "episode_len_mean: 151.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 8257\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4850844474633535\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01571616440817208\n",
      "        policy_loss: -0.09801915913820267\n",
      "        total_loss: 25.362034616470336\n",
      "        vf_explained_var: 0.37608591198921204\n",
      "        vf_loss: 25.452981685002644\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5786068413654963\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014406270600447897\n",
      "        policy_loss: -0.042709339144639674\n",
      "        total_loss: 20.693044179280598\n",
      "        vf_explained_var: -0.05704447309176127\n",
      "        vf_loss: 20.729270776112873\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5298238296310107\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01379569488846452\n",
      "        policy_loss: -0.1045827554538846\n",
      "        total_loss: 38.03285075187683\n",
      "        vf_explained_var: 0.11600706398487091\n",
      "        vf_loss: 38.13122542063395\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1339968\n",
      "  num_agent_steps_trained: 1339968\n",
      "  num_steps_sampled: 1340000\n",
      "  num_steps_trained: 1340000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 335\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.975\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.0\n",
      "  player_1: 24.133333333333336\n",
      "  player_2: 20.0\n",
      "policy_reward_mean:\n",
      "  player_0: 8.800666666666668\n",
      "  player_1: -0.8693333333333334\n",
      "  player_2: -4.931333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -12.0\n",
      "  player_1: -31.66666666666667\n",
      "  player_2: -34.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1154261544010506\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.37262083295830223\n",
      "  mean_inference_ms: 2.117829686118097\n",
      "  mean_raw_obs_processing_ms: 0.2695523074374371\n",
      "time_since_restore: 2295.5572578907013\n",
      "time_this_iter_s: 6.428053140640259\n",
      "time_total_s: 2295.5572578907013\n",
      "timers:\n",
      "  learn_throughput: 694.342\n",
      "  learn_time_ms: 5760.852\n",
      "  load_throughput: 6231092.293\n",
      "  load_time_ms: 0.642\n",
      "  sample_throughput: 607.099\n",
      "  sample_time_ms: 6588.71\n",
      "  update_time_ms: 3.943\n",
      "timestamp: 1643327371\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1340000\n",
      "training_iteration: 335\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1347969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-49-45\n",
      "done: false\n",
      "episode_len_mean: 154.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 8310\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5235590823491414\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01675179715147048\n",
      "        policy_loss: -0.08070068219055732\n",
      "        total_loss: 35.3519101079305\n",
      "        vf_explained_var: 0.01275736133257548\n",
      "        vf_loss: 35.42507267951965\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5483107450604439\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014010341328509337\n",
      "        policy_loss: -0.0875396623617659\n",
      "        total_loss: 32.99440902233124\n",
      "        vf_explained_var: 0.071267076532046\n",
      "        vf_loss: 33.07564420223236\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5472459362943968\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01646306096342111\n",
      "        policy_loss: -0.0543216131379207\n",
      "        total_loss: 41.38420239766439\n",
      "        vf_explained_var: 0.10716210782527924\n",
      "        vf_loss: 41.43111568768819\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1347969\n",
      "  num_agent_steps_trained: 1347969\n",
      "  num_steps_sampled: 1348000\n",
      "  num_steps_trained: 1348000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 337\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.677777777777777\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 35.666666666666664\n",
      "  player_1: 24.866666666666667\n",
      "  player_2: 20.0\n",
      "policy_reward_mean:\n",
      "  player_0: 8.349333333333332\n",
      "  player_1: -1.5166666666666666\n",
      "  player_2: -3.8326666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -26.666666666666668\n",
      "  player_1: -24.133333333333333\n",
      "  player_2: -31.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11524323690305716\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.37227735003568413\n",
      "  mean_inference_ms: 2.1168805052117925\n",
      "  mean_raw_obs_processing_ms: 0.2692501850014251\n",
      "time_since_restore: 2308.657778978348\n",
      "time_this_iter_s: 6.444243907928467\n",
      "time_total_s: 2308.657778978348\n",
      "timers:\n",
      "  learn_throughput: 693.038\n",
      "  learn_time_ms: 5771.689\n",
      "  load_throughput: 6392538.007\n",
      "  load_time_ms: 0.626\n",
      "  sample_throughput: 606.128\n",
      "  sample_time_ms: 6599.264\n",
      "  update_time_ms: 3.972\n",
      "timestamp: 1643327385\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1348000\n",
      "training_iteration: 337\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1355970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-49-58\n",
      "done: false\n",
      "episode_len_mean: 151.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 8365\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48757610936959583\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013433575821945851\n",
      "        policy_loss: -0.0035819001123309135\n",
      "        total_loss: 36.77826013565063\n",
      "        vf_explained_var: -0.03671887616316478\n",
      "        vf_loss: 36.775796820322675\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5949811355272929\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013616274767021727\n",
      "        policy_loss: -0.1299195182695985\n",
      "        total_loss: 34.008600317637125\n",
      "        vf_explained_var: 0.21527583281199136\n",
      "        vf_loss: 34.132392450968425\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5203721551100413\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016177299974127285\n",
      "        policy_loss: -0.06359684627503157\n",
      "        total_loss: 39.57448781649271\n",
      "        vf_explained_var: 0.008794140021006267\n",
      "        vf_loss: 39.63080511411031\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1355970\n",
      "  num_agent_steps_trained: 1355970\n",
      "  num_steps_sampled: 1356000\n",
      "  num_steps_trained: 1356000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 339\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.4\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 35.666666666666664\n",
      "  player_1: 24.866666666666667\n",
      "  player_2: 23.46666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 9.134666666666668\n",
      "  player_1: -2.2213333333333334\n",
      "  player_2: -3.9133333333333336\n",
      "policy_reward_min:\n",
      "  player_0: -26.666666666666668\n",
      "  player_1: -24.333333333333336\n",
      "  player_2: -46.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11515216572237948\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3717750797540003\n",
      "  mean_inference_ms: 2.115018171257529\n",
      "  mean_raw_obs_processing_ms: 0.26906408529742415\n",
      "time_since_restore: 2321.4872391223907\n",
      "time_this_iter_s: 6.2774224281311035\n",
      "time_total_s: 2321.4872391223907\n",
      "timers:\n",
      "  learn_throughput: 698.428\n",
      "  learn_time_ms: 5727.15\n",
      "  load_throughput: 6449054.776\n",
      "  load_time_ms: 0.62\n",
      "  sample_throughput: 608.4\n",
      "  sample_time_ms: 6574.627\n",
      "  update_time_ms: 3.828\n",
      "timestamp: 1643327398\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1356000\n",
      "training_iteration: 339\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1363969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-50-11\n",
      "done: false\n",
      "episode_len_mean: 147.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 8416\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4410205151637395\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014886915543702344\n",
      "        policy_loss: -0.06479554108809679\n",
      "        total_loss: 23.142843855222065\n",
      "        vf_explained_var: 0.18216278711954753\n",
      "        vf_loss: 23.200940246582032\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5450605526566505\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014771945863055105\n",
      "        policy_loss: -0.07018852482239406\n",
      "        total_loss: 25.525746421813963\n",
      "        vf_explained_var: 0.24349871814250945\n",
      "        vf_loss: 25.5892875512441\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5389394969741503\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015295409126623175\n",
      "        policy_loss: -0.07667063921224326\n",
      "        total_loss: 33.29450255711873\n",
      "        vf_explained_var: 0.16710156420866648\n",
      "        vf_loss: 33.364290324846905\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1363969\n",
      "  num_agent_steps_trained: 1363969\n",
      "  num_steps_sampled: 1364000\n",
      "  num_steps_trained: 1364000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 341\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.51111111111111\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 21.599999999999998\n",
      "  player_2: 23.46666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 8.515333333333333\n",
      "  player_1: -1.4366666666666665\n",
      "  player_2: -4.078666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -22.93333333333333\n",
      "  player_1: -24.333333333333336\n",
      "  player_2: -46.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11507193767378363\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.37175490571727915\n",
      "  mean_inference_ms: 2.113867518193987\n",
      "  mean_raw_obs_processing_ms: 0.2692217488142396\n",
      "time_since_restore: 2334.5980932712555\n",
      "time_this_iter_s: 6.50441312789917\n",
      "time_total_s: 2334.5980932712555\n",
      "timers:\n",
      "  learn_throughput: 698.44\n",
      "  learn_time_ms: 5727.051\n",
      "  load_throughput: 6473440.599\n",
      "  load_time_ms: 0.618\n",
      "  sample_throughput: 610.219\n",
      "  sample_time_ms: 6555.028\n",
      "  update_time_ms: 3.637\n",
      "timestamp: 1643327411\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1364000\n",
      "training_iteration: 341\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1371968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-50-24\n",
      "done: false\n",
      "episode_len_mean: 155.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 8466\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47774470468362173\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014855447019993638\n",
      "        policy_loss: -0.05073535457253456\n",
      "        total_loss: 31.095162591934205\n",
      "        vf_explained_var: 0.08202410519123077\n",
      "        vf_loss: 31.139212776819864\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.58956050157547\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01701115714243012\n",
      "        policy_loss: -0.08679191549308599\n",
      "        total_loss: 36.23245038986206\n",
      "        vf_explained_var: 0.30733832240104675\n",
      "        vf_loss: 36.31158732732137\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4917390438914299\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015142221864131215\n",
      "        policy_loss: -0.10497653766535223\n",
      "        total_loss: 44.11101339181264\n",
      "        vf_explained_var: 0.15190590043862662\n",
      "        vf_loss: 44.20917626539866\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1371968\n",
      "  num_agent_steps_trained: 1371968\n",
      "  num_steps_sampled: 1372000\n",
      "  num_steps_trained: 1372000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 343\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.387500000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 37.333333333333336\n",
      "  player_1: 21.599999999999998\n",
      "  player_2: 22.666666666666668\n",
      "policy_reward_mean:\n",
      "  player_0: 8.298\n",
      "  player_1: -1.2559999999999998\n",
      "  player_2: -4.042000000000001\n",
      "policy_reward_min:\n",
      "  player_0: -23.266666666666666\n",
      "  player_1: -24.666666666666668\n",
      "  player_2: -29.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11511548286881167\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3717222279513311\n",
      "  mean_inference_ms: 2.1150529291772577\n",
      "  mean_raw_obs_processing_ms: 0.2692718869165979\n",
      "time_since_restore: 2347.719815969467\n",
      "time_this_iter_s: 6.4393720626831055\n",
      "time_total_s: 2347.719815969467\n",
      "timers:\n",
      "  learn_throughput: 698.42\n",
      "  learn_time_ms: 5727.21\n",
      "  load_throughput: 6240363.028\n",
      "  load_time_ms: 0.641\n",
      "  sample_throughput: 610.295\n",
      "  sample_time_ms: 6554.208\n",
      "  update_time_ms: 3.753\n",
      "timestamp: 1643327424\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1372000\n",
      "training_iteration: 343\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1379968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-50-37\n",
      "done: false\n",
      "episode_len_mean: 154.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 8520\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5059752587477366\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01409700796951256\n",
      "        policy_loss: -0.09328486612687509\n",
      "        total_loss: 30.007812334696453\n",
      "        vf_explained_var: 0.1579546046257019\n",
      "        vf_loss: 30.094753535588584\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5779437644282976\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014439406880383103\n",
      "        policy_loss: -0.06659636601805687\n",
      "        total_loss: 40.53986402034759\n",
      "        vf_explained_var: 0.20073131461938223\n",
      "        vf_loss: 40.59996276537577\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.511757298608621\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01442587594644768\n",
      "        policy_loss: -0.029439891893416642\n",
      "        total_loss: 38.32680559317271\n",
      "        vf_explained_var: 0.1227151519060135\n",
      "        vf_loss: 38.349753972689314\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1379968\n",
      "  num_agent_steps_trained: 1379968\n",
      "  num_steps_sampled: 1380000\n",
      "  num_steps_trained: 1380000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 345\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.6875\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 37.333333333333336\n",
      "  player_1: 30.53333333333333\n",
      "  player_2: 22.666666666666668\n",
      "policy_reward_mean:\n",
      "  player_0: 8.450000000000001\n",
      "  player_1: -0.18999999999999997\n",
      "  player_2: -5.259999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -23.266666666666666\n",
      "  player_1: -22.0\n",
      "  player_2: -37.86666666666666\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11503825958991333\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.37135782885483676\n",
      "  mean_inference_ms: 2.1135081356389254\n",
      "  mean_raw_obs_processing_ms: 0.2689567107205689\n",
      "time_since_restore: 2360.66778922081\n",
      "time_this_iter_s: 6.398788928985596\n",
      "time_total_s: 2360.66778922081\n",
      "timers:\n",
      "  learn_throughput: 699.715\n",
      "  learn_time_ms: 5716.616\n",
      "  load_throughput: 6281248.97\n",
      "  load_time_ms: 0.637\n",
      "  sample_throughput: 611.987\n",
      "  sample_time_ms: 6536.081\n",
      "  update_time_ms: 3.729\n",
      "timestamp: 1643327437\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1380000\n",
      "training_iteration: 345\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1387969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-50-51\n",
      "done: false\n",
      "episode_len_mean: 139.54\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 31\n",
      "episodes_total: 8581\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47223214417695997\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014484042200293742\n",
      "        policy_loss: -0.056262907506898045\n",
      "        total_loss: 47.15726735432943\n",
      "        vf_explained_var: 0.1932297784090042\n",
      "        vf_loss: 47.207012523015344\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5316591035326322\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013630626147791852\n",
      "        policy_loss: -0.06548027279165884\n",
      "        total_loss: 39.9538432264328\n",
      "        vf_explained_var: 0.2821810867389043\n",
      "        vf_loss: 40.01318964004517\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49446752965450286\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017244034333927707\n",
      "        policy_loss: -0.08637676014875373\n",
      "        total_loss: 48.38548287391663\n",
      "        vf_explained_var: 0.49732986609141033\n",
      "        vf_loss: 48.46410010973612\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1387969\n",
      "  num_agent_steps_trained: 1387969\n",
      "  num_steps_sampled: 1388000\n",
      "  num_steps_trained: 1388000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 347\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.97777777777778\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.666666666666664\n",
      "  player_1: 30.53333333333333\n",
      "  player_2: 18.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 10.142666666666669\n",
      "  player_1: 0.2386666666666668\n",
      "  player_2: -7.381333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -20.46666666666667\n",
      "  player_1: -24.933333333333334\n",
      "  player_2: -37.86666666666666\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11488779101090907\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3717349714706721\n",
      "  mean_inference_ms: 2.1134236705053255\n",
      "  mean_raw_obs_processing_ms: 0.26918151691424347\n",
      "time_since_restore: 2374.4419465065002\n",
      "time_this_iter_s: 7.130230903625488\n",
      "time_total_s: 2374.4419465065002\n",
      "timers:\n",
      "  learn_throughput: 692.065\n",
      "  learn_time_ms: 5779.801\n",
      "  load_throughput: 6259921.645\n",
      "  load_time_ms: 0.639\n",
      "  sample_throughput: 611.722\n",
      "  sample_time_ms: 6538.915\n",
      "  update_time_ms: 3.832\n",
      "timestamp: 1643327451\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1388000\n",
      "training_iteration: 347\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1395969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-51-04\n",
      "done: false\n",
      "episode_len_mean: 143.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 8633\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45384077886740365\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012996705443122968\n",
      "        policy_loss: -0.09196288597459594\n",
      "        total_loss: 34.35715194384257\n",
      "        vf_explained_var: 0.21787038028240205\n",
      "        vf_loss: 34.443266255060834\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5674287297328313\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018255272504073522\n",
      "        policy_loss: -0.0935903432018434\n",
      "        total_loss: 31.19507781823476\n",
      "        vf_explained_var: 0.29404357850551605\n",
      "        vf_loss: 31.280453211466472\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5204381435116132\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016394236424506138\n",
      "        policy_loss: -0.03344543165837725\n",
      "        total_loss: 27.197465580304463\n",
      "        vf_explained_var: 0.033289867838223776\n",
      "        vf_loss: 27.22353353182475\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1395969\n",
      "  num_agent_steps_trained: 1395969\n",
      "  num_steps_sampled: 1396000\n",
      "  num_steps_trained: 1396000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 349\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.675\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.666666666666664\n",
      "  player_1: 23.333333333333336\n",
      "  player_2: 16.066666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 10.748\n",
      "  player_1: -1.9879999999999998\n",
      "  player_2: -5.76\n",
      "policy_reward_min:\n",
      "  player_0: -10.93333333333333\n",
      "  player_1: -30.0\n",
      "  player_2: -33.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11500591484743306\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3715650841621956\n",
      "  mean_inference_ms: 2.1136907844965473\n",
      "  mean_raw_obs_processing_ms: 0.269068026164962\n",
      "time_since_restore: 2387.515965461731\n",
      "time_this_iter_s: 6.478597640991211\n",
      "time_total_s: 2387.515965461731\n",
      "timers:\n",
      "  learn_throughput: 689.137\n",
      "  learn_time_ms: 5804.362\n",
      "  load_throughput: 6297990.165\n",
      "  load_time_ms: 0.635\n",
      "  sample_throughput: 605.077\n",
      "  sample_time_ms: 6610.73\n",
      "  update_time_ms: 4.005\n",
      "timestamp: 1643327464\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1396000\n",
      "training_iteration: 349\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1403971\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-51-17\n",
      "done: false\n",
      "episode_len_mean: 157.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 8683\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4650393897294998\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014337586098921636\n",
      "        policy_loss: -0.031295339030524096\n",
      "        total_loss: 31.94924336751302\n",
      "        vf_explained_var: 0.175530167222023\n",
      "        vf_loss: 31.974086882273355\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.580857840180397\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012878151048205382\n",
      "        policy_loss: -0.1165775762250026\n",
      "        total_loss: 41.912767289479575\n",
      "        vf_explained_var: -0.040482346614201865\n",
      "        vf_loss: 42.02354970296224\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5234413199623426\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014381759375756612\n",
      "        policy_loss: -0.0598035986131678\n",
      "        total_loss: 26.208352735837302\n",
      "        vf_explained_var: 0.42333964506785077\n",
      "        vf_loss: 26.26168461004893\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1403971\n",
      "  num_agent_steps_trained: 1403971\n",
      "  num_steps_sampled: 1404000\n",
      "  num_steps_trained: 1404000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 351\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.5\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.625\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 30.666666666666664\n",
      "  player_1: 27.333333333333332\n",
      "  player_2: 14.599999999999998\n",
      "policy_reward_mean:\n",
      "  player_0: 9.302\n",
      "  player_1: -0.764\n",
      "  player_2: -5.537999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -19.666666666666668\n",
      "  player_1: -22.0\n",
      "  player_2: -31.800000000000004\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11509707575237112\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.37116909660128145\n",
      "  mean_inference_ms: 2.1127149512526917\n",
      "  mean_raw_obs_processing_ms: 0.2690062499908137\n",
      "time_since_restore: 2400.5299570560455\n",
      "time_this_iter_s: 6.408002853393555\n",
      "time_total_s: 2400.5299570560455\n",
      "timers:\n",
      "  learn_throughput: 690.416\n",
      "  learn_time_ms: 5793.612\n",
      "  load_throughput: 6269278.428\n",
      "  load_time_ms: 0.638\n",
      "  sample_throughput: 603.325\n",
      "  sample_time_ms: 6629.924\n",
      "  update_time_ms: 4.11\n",
      "timestamp: 1643327477\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1404000\n",
      "training_iteration: 351\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1411970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-51-30\n",
      "done: false\n",
      "episode_len_mean: 167.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 21\n",
      "episodes_total: 8729\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47253872036933897\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01402228875137856\n",
      "        policy_loss: -0.12697458771367867\n",
      "        total_loss: 26.94774564743042\n",
      "        vf_explained_var: 0.27653903245925904\n",
      "        vf_loss: 27.068410325050355\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5602753707766532\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015914218335798577\n",
      "        policy_loss: -0.016282522759089868\n",
      "        total_loss: 37.37767536799113\n",
      "        vf_explained_var: 0.14996172209580738\n",
      "        vf_loss: 37.386796674728394\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5372560441493988\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018375134141072825\n",
      "        policy_loss: -0.041281998051951325\n",
      "        total_loss: 25.963033922513326\n",
      "        vf_explained_var: 0.03318512817223867\n",
      "        vf_loss: 25.996047222614287\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1411970\n",
      "  num_agent_steps_trained: 1411970\n",
      "  num_steps_sampled: 1412000\n",
      "  num_steps_trained: 1412000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 353\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.0875\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 30.666666666666664\n",
      "  player_1: 27.333333333333332\n",
      "  player_2: 14.599999999999998\n",
      "policy_reward_mean:\n",
      "  player_0: 8.132\n",
      "  player_1: 0.5499999999999997\n",
      "  player_2: -5.682\n",
      "policy_reward_min:\n",
      "  player_0: -19.666666666666668\n",
      "  player_1: -24.666666666666668\n",
      "  player_2: -31.800000000000004\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11474127248407755\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3707190218009726\n",
      "  mean_inference_ms: 2.109526925793571\n",
      "  mean_raw_obs_processing_ms: 0.2686732355730726\n",
      "time_since_restore: 2413.485512971878\n",
      "time_this_iter_s: 6.346789360046387\n",
      "time_total_s: 2413.485512971878\n",
      "timers:\n",
      "  learn_throughput: 692.339\n",
      "  learn_time_ms: 5777.52\n",
      "  load_throughput: 6592744.42\n",
      "  load_time_ms: 0.607\n",
      "  sample_throughput: 604.794\n",
      "  sample_time_ms: 6613.826\n",
      "  update_time_ms: 3.98\n",
      "timestamp: 1643327490\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1412000\n",
      "training_iteration: 353\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1419970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-51-43\n",
      "done: false\n",
      "episode_len_mean: 162.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 8781\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4620029453436534\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015245307079170137\n",
      "        policy_loss: -0.07460636627317095\n",
      "        total_loss: 25.967176427841185\n",
      "        vf_explained_var: 0.38027751445770264\n",
      "        vf_loss: 26.03492222468058\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5680562722682952\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013162691683950905\n",
      "        policy_loss: -0.10566818925241629\n",
      "        total_loss: 40.076233166058856\n",
      "        vf_explained_var: 0.2694082436958949\n",
      "        vf_loss: 40.175978201230365\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5260333836078643\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015357954961579339\n",
      "        policy_loss: -0.03691399949758003\n",
      "        total_loss: 32.301008887290955\n",
      "        vf_explained_var: 0.1539231409629186\n",
      "        vf_loss: 32.33101195335388\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1419970\n",
      "  num_agent_steps_trained: 1419970\n",
      "  num_steps_sampled: 1420000\n",
      "  num_steps_trained: 1420000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 355\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.9125\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.625\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 30.0\n",
      "  player_1: 22.0\n",
      "  player_2: 26.0\n",
      "policy_reward_mean:\n",
      "  player_0: 8.598666666666666\n",
      "  player_1: -0.24333333333333357\n",
      "  player_2: -5.355333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -13.93333333333333\n",
      "  player_1: -37.0\n",
      "  player_2: -29.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11473927219675144\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.37057830172488765\n",
      "  mean_inference_ms: 2.110364364488111\n",
      "  mean_raw_obs_processing_ms: 0.2686145225220292\n",
      "time_since_restore: 2426.290480852127\n",
      "time_this_iter_s: 6.23682713508606\n",
      "time_total_s: 2426.290480852127\n",
      "timers:\n",
      "  learn_throughput: 693.647\n",
      "  learn_time_ms: 5766.622\n",
      "  load_throughput: 6622933.839\n",
      "  load_time_ms: 0.604\n",
      "  sample_throughput: 605.591\n",
      "  sample_time_ms: 6605.113\n",
      "  update_time_ms: 4.307\n",
      "timestamp: 1643327503\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1420000\n",
      "training_iteration: 355\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1427968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-51-56\n",
      "done: false\n",
      "episode_len_mean: 154.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 28\n",
      "episodes_total: 8831\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4718857554594676\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014634968262907933\n",
      "        policy_loss: -0.05039423494289319\n",
      "        total_loss: 34.37671724160512\n",
      "        vf_explained_var: 0.2103857417901357\n",
      "        vf_loss: 34.420525850454965\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5811570556958516\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014244078287044128\n",
      "        policy_loss: -0.056893504625186324\n",
      "        total_loss: 20.72282936414083\n",
      "        vf_explained_var: 0.22153873244921365\n",
      "        vf_loss: 20.773313024838764\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5040326582392057\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015430946627263135\n",
      "        policy_loss: -0.06552033612194161\n",
      "        total_loss: 30.02187391281128\n",
      "        vf_explained_var: 0.24860778490702312\n",
      "        vf_loss: 30.080450344085694\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1427968\n",
      "  num_agent_steps_trained: 1427968\n",
      "  num_steps_sampled: 1428000\n",
      "  num_steps_trained: 1428000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 357\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.6625\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 26.666666666666668\n",
      "  player_1: 25.666666666666664\n",
      "  player_2: 26.0\n",
      "policy_reward_mean:\n",
      "  player_0: 8.012\n",
      "  player_1: 0.8980000000000001\n",
      "  player_2: -5.910000000000001\n",
      "policy_reward_min:\n",
      "  player_0: -20.333333333333336\n",
      "  player_1: -37.0\n",
      "  player_2: -31.599999999999994\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11501280648511247\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3707499721806422\n",
      "  mean_inference_ms: 2.1114358734257506\n",
      "  mean_raw_obs_processing_ms: 0.2688201428382423\n",
      "time_since_restore: 2439.3517112731934\n",
      "time_this_iter_s: 6.4427807331085205\n",
      "time_total_s: 2439.3517112731934\n",
      "timers:\n",
      "  learn_throughput: 702.292\n",
      "  learn_time_ms: 5695.638\n",
      "  load_throughput: 6629476.429\n",
      "  load_time_ms: 0.603\n",
      "  sample_throughput: 607.467\n",
      "  sample_time_ms: 6584.721\n",
      "  update_time_ms: 4.196\n",
      "timestamp: 1643327516\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1428000\n",
      "training_iteration: 357\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1435968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-52-09\n",
      "done: false\n",
      "episode_len_mean: 156.28\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 8883\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4743365987141927\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014784590989535596\n",
      "        policy_loss: -0.11254996272424857\n",
      "        total_loss: 50.19871722539266\n",
      "        vf_explained_var: 0.1289339206616084\n",
      "        vf_loss: 50.304614302317304\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5628286124269167\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015170899423925827\n",
      "        policy_loss: -0.0634296431671828\n",
      "        total_loss: 44.76331070582072\n",
      "        vf_explained_var: 0.1013718823591868\n",
      "        vf_loss: 44.81991292794545\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5248607660333315\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013813512673380804\n",
      "        policy_loss: -0.07293220824562013\n",
      "        total_loss: 42.59308364868164\n",
      "        vf_explained_var: 0.1381236877044042\n",
      "        vf_loss: 42.65979969978333\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1435968\n",
      "  num_agent_steps_trained: 1435968\n",
      "  num_steps_sampled: 1436000\n",
      "  num_steps_trained: 1436000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 359\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.125\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.625\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 37.0\n",
      "  player_1: 25.666666666666664\n",
      "  player_2: 25.800000000000004\n",
      "policy_reward_mean:\n",
      "  player_0: 9.166000000000002\n",
      "  player_1: 0.38800000000000023\n",
      "  player_2: -6.554000000000001\n",
      "policy_reward_min:\n",
      "  player_0: -20.333333333333336\n",
      "  player_1: -24.0\n",
      "  player_2: -31.599999999999994\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1149612368124652\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.37051865198377665\n",
      "  mean_inference_ms: 2.111148939614372\n",
      "  mean_raw_obs_processing_ms: 0.26877657102826075\n",
      "time_since_restore: 2452.4632885456085\n",
      "time_this_iter_s: 6.492160797119141\n",
      "time_total_s: 2452.4632885456085\n",
      "timers:\n",
      "  learn_throughput: 702.013\n",
      "  learn_time_ms: 5697.898\n",
      "  load_throughput: 6585498.508\n",
      "  load_time_ms: 0.607\n",
      "  sample_throughput: 613.275\n",
      "  sample_time_ms: 6522.359\n",
      "  update_time_ms: 4.164\n",
      "timestamp: 1643327529\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1436000\n",
      "training_iteration: 359\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1443968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-52-22\n",
      "done: false\n",
      "episode_len_mean: 153.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 8937\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47117675046126045\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014690360975082513\n",
      "        policy_loss: -0.06299163460731506\n",
      "        total_loss: 36.34803759256999\n",
      "        vf_explained_var: 0.44007571736971535\n",
      "        vf_loss: 36.40441864331563\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5780652206142743\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01603819260393114\n",
      "        policy_loss: -0.06601181133805464\n",
      "        total_loss: 31.91848325252533\n",
      "        vf_explained_var: 0.2949552754561106\n",
      "        vf_loss: 31.97727800687154\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5044987826546034\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016894235125458484\n",
      "        policy_loss: -0.10540359153722724\n",
      "        total_loss: 30.42318518002828\n",
      "        vf_explained_var: 0.48880183001359306\n",
      "        vf_loss: 30.52098645210266\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1443968\n",
      "  num_agent_steps_trained: 1443968\n",
      "  num_steps_sampled: 1444000\n",
      "  num_steps_trained: 1444000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 361\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.433333333333334\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 37.0\n",
      "  player_1: 21.866666666666667\n",
      "  player_2: 18.133333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 10.249333333333334\n",
      "  player_1: -0.3246666666666668\n",
      "  player_2: -6.924666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -18.266666666666673\n",
      "  player_1: -24.0\n",
      "  player_2: -34.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11470843424747638\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.37002079790897996\n",
      "  mean_inference_ms: 2.1090202737000716\n",
      "  mean_raw_obs_processing_ms: 0.2683722175758826\n",
      "time_since_restore: 2465.567722797394\n",
      "time_this_iter_s: 6.507418870925903\n",
      "time_total_s: 2465.567722797394\n",
      "timers:\n",
      "  learn_throughput: 701.291\n",
      "  learn_time_ms: 5703.763\n",
      "  load_throughput: 6549250.888\n",
      "  load_time_ms: 0.611\n",
      "  sample_throughput: 613.479\n",
      "  sample_time_ms: 6520.186\n",
      "  update_time_ms: 4.22\n",
      "timestamp: 1643327542\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1444000\n",
      "training_iteration: 361\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1451969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-52-35\n",
      "done: false\n",
      "episode_len_mean: 160.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 8982\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4928520013888677\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017059296974305957\n",
      "        policy_loss: -0.05544041592938204\n",
      "        total_loss: 38.56851647059123\n",
      "        vf_explained_var: 0.3313614680369695\n",
      "        vf_loss: 38.61628012975057\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5382157667477926\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01486425517678678\n",
      "        policy_loss: -0.06934782024317732\n",
      "        total_loss: 26.96006260236104\n",
      "        vf_explained_var: 0.052144795060157775\n",
      "        vf_loss: 27.02272148132324\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5078076085448265\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01756923723693035\n",
      "        policy_loss: -0.09698322781672081\n",
      "        total_loss: 35.3893600877126\n",
      "        vf_explained_var: 0.15754682838916778\n",
      "        vf_loss: 35.47843737602234\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1451969\n",
      "  num_agent_steps_trained: 1451969\n",
      "  num_steps_sampled: 1452000\n",
      "  num_steps_trained: 1452000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 363\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.15\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6375\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.800000000000004\n",
      "  player_1: 20.0\n",
      "  player_2: 23.333333333333332\n",
      "policy_reward_mean:\n",
      "  player_0: 8.59\n",
      "  player_1: 1.006\n",
      "  player_2: -6.595999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -24.666666666666668\n",
      "  player_1: -24.0\n",
      "  player_2: -35.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11469527950156429\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3699312671077851\n",
      "  mean_inference_ms: 2.1096826971600224\n",
      "  mean_raw_obs_processing_ms: 0.2685053952726591\n",
      "time_since_restore: 2478.4300751686096\n",
      "time_this_iter_s: 6.38997483253479\n",
      "time_total_s: 2478.4300751686096\n",
      "timers:\n",
      "  learn_throughput: 702.376\n",
      "  learn_time_ms: 5694.955\n",
      "  load_throughput: 6510367.094\n",
      "  load_time_ms: 0.614\n",
      "  sample_throughput: 613.63\n",
      "  sample_time_ms: 6518.591\n",
      "  update_time_ms: 4.303\n",
      "timestamp: 1643327555\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1452000\n",
      "training_iteration: 363\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1459968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-52-48\n",
      "done: false\n",
      "episode_len_mean: 164.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 21\n",
      "episodes_total: 9029\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45245815634727476\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013698856353997447\n",
      "        policy_loss: -0.08908774552866816\n",
      "        total_loss: 36.16849402109782\n",
      "        vf_explained_var: 0.35717174887657166\n",
      "        vf_loss: 36.25141759554545\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5783377261956533\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01764306596177145\n",
      "        policy_loss: -0.06317959832803656\n",
      "        total_loss: 24.73426956176758\n",
      "        vf_explained_var: 0.21510839641094207\n",
      "        vf_loss: 24.789509835243226\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48510427902142206\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015084529964848723\n",
      "        policy_loss: -0.0856537967423598\n",
      "        total_loss: 24.079792369206746\n",
      "        vf_explained_var: 0.2840897923707962\n",
      "        vf_loss: 24.15865816752116\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1459968\n",
      "  num_agent_steps_trained: 1459968\n",
      "  num_steps_sampled: 1460000\n",
      "  num_steps_trained: 1460000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 365\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.433333333333334\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.67777777777778\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.800000000000004\n",
      "  player_1: 21.333333333333336\n",
      "  player_2: 23.333333333333332\n",
      "policy_reward_mean:\n",
      "  player_0: 8.113333333333333\n",
      "  player_1: 0.7993333333333331\n",
      "  player_2: -5.9126666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -29.93333333333333\n",
      "  player_1: -24.0\n",
      "  player_2: -35.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11475680029358598\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3702853475836499\n",
      "  mean_inference_ms: 2.1114957320169743\n",
      "  mean_raw_obs_processing_ms: 0.2689372268510129\n",
      "time_since_restore: 2491.5125443935394\n",
      "time_this_iter_s: 6.478769063949585\n",
      "time_total_s: 2491.5125443935394\n",
      "timers:\n",
      "  learn_throughput: 699.519\n",
      "  learn_time_ms: 5718.218\n",
      "  load_throughput: 6379654.727\n",
      "  load_time_ms: 0.627\n",
      "  sample_throughput: 612.371\n",
      "  sample_time_ms: 6531.993\n",
      "  update_time_ms: 4.049\n",
      "timestamp: 1643327568\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1460000\n",
      "training_iteration: 365\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1467968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-53-01\n",
      "done: false\n",
      "episode_len_mean: 158.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 9083\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4790561082959175\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016656351471783803\n",
      "        policy_loss: -0.08856264604255557\n",
      "        total_loss: 24.71043489297231\n",
      "        vf_explained_var: 0.1719207900762558\n",
      "        vf_loss: 24.79150230884552\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5774042395750681\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015003114796149035\n",
      "        policy_loss: -0.08958842985021571\n",
      "        total_loss: 25.60244450410207\n",
      "        vf_explained_var: -0.0959153046210607\n",
      "        vf_loss: 25.685281330744427\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5187690143783887\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016374131536279795\n",
      "        policy_loss: -0.024140377044677735\n",
      "        total_loss: 34.11879401842753\n",
      "        vf_explained_var: 0.0321174019575119\n",
      "        vf_loss: 34.13556593259175\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1467968\n",
      "  num_agent_steps_trained: 1467968\n",
      "  num_steps_sampled: 1468000\n",
      "  num_steps_trained: 1468000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 367\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.15\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6125\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.666666666666664\n",
      "  player_1: 24.53333333333333\n",
      "  player_2: 20.2\n",
      "policy_reward_mean:\n",
      "  player_0: 9.051333333333334\n",
      "  player_1: -0.6986666666666669\n",
      "  player_2: -5.352666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -29.93333333333333\n",
      "  player_1: -23.0\n",
      "  player_2: -32.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11512482189307571\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.37027504567625946\n",
      "  mean_inference_ms: 2.112501957365623\n",
      "  mean_raw_obs_processing_ms: 0.26883222240059645\n",
      "time_since_restore: 2504.5119202136993\n",
      "time_this_iter_s: 6.442636251449585\n",
      "time_total_s: 2504.5119202136993\n",
      "timers:\n",
      "  learn_throughput: 699.5\n",
      "  learn_time_ms: 5718.372\n",
      "  load_throughput: 6403761.976\n",
      "  load_time_ms: 0.625\n",
      "  sample_throughput: 611.059\n",
      "  sample_time_ms: 6546.009\n",
      "  update_time_ms: 4.032\n",
      "timestamp: 1643327581\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1468000\n",
      "training_iteration: 367\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1475968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-53-14\n",
      "done: false\n",
      "episode_len_mean: 152.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 9135\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44357556064923603\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013905531522468664\n",
      "        policy_loss: -0.09536493576442202\n",
      "        total_loss: 29.045619033177694\n",
      "        vf_explained_var: 0.4627008420228958\n",
      "        vf_loss: 29.134726473490396\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5334212135275205\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013955890089570554\n",
      "        policy_loss: -0.070798578299582\n",
      "        total_loss: 30.6690766954422\n",
      "        vf_explained_var: 0.2573561735947927\n",
      "        vf_loss: 30.73359544754028\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5380650359392166\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015347729828754382\n",
      "        policy_loss: -0.08544890229900678\n",
      "        total_loss: 28.848718372980752\n",
      "        vf_explained_var: 0.28513917803764344\n",
      "        vf_loss: 28.92726091066996\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1475968\n",
      "  num_agent_steps_trained: 1475968\n",
      "  num_steps_sampled: 1476000\n",
      "  num_steps_trained: 1476000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 369\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.125\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6375\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 40.666666666666664\n",
      "  player_1: 24.53333333333333\n",
      "  player_2: 20.2\n",
      "policy_reward_mean:\n",
      "  player_0: 10.711333333333334\n",
      "  player_1: -1.1326666666666667\n",
      "  player_2: -6.578666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -14.93333333333333\n",
      "  player_1: -24.46666666666667\n",
      "  player_2: -34.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11491963017854187\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3696438318122525\n",
      "  mean_inference_ms: 2.1103830698195543\n",
      "  mean_raw_obs_processing_ms: 0.2684365943261733\n",
      "time_since_restore: 2517.5512750148773\n",
      "time_this_iter_s: 6.425134897232056\n",
      "time_total_s: 2517.5512750148773\n",
      "timers:\n",
      "  learn_throughput: 700.452\n",
      "  learn_time_ms: 5710.601\n",
      "  load_throughput: 6413063.721\n",
      "  load_time_ms: 0.624\n",
      "  sample_throughput: 611.05\n",
      "  sample_time_ms: 6546.104\n",
      "  update_time_ms: 4.249\n",
      "timestamp: 1643327594\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1476000\n",
      "training_iteration: 369\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1483970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-53-27\n",
      "done: false\n",
      "episode_len_mean: 158.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 9183\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4676842204729716\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01391047398208078\n",
      "        policy_loss: -0.07282927936563889\n",
      "        total_loss: 31.43248042901357\n",
      "        vf_explained_var: 0.46497367819150287\n",
      "        vf_loss: 31.499049960772197\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5506528524557749\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014321518215609407\n",
      "        policy_loss: -0.05176657615850369\n",
      "        total_loss: 24.807502714792886\n",
      "        vf_explained_var: 0.12260701576868693\n",
      "        vf_loss: 24.852824573516845\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5238369139035542\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014664550914944811\n",
      "        policy_loss: -0.0840402129975458\n",
      "        total_loss: 27.64403718471527\n",
      "        vf_explained_var: 0.31059293071428934\n",
      "        vf_loss: 27.721478295326232\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1483970\n",
      "  num_agent_steps_trained: 1483970\n",
      "  num_steps_sampled: 1484000\n",
      "  num_steps_trained: 1484000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 371\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.375\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6375\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 40.666666666666664\n",
      "  player_1: 22.6\n",
      "  player_2: 16.0\n",
      "policy_reward_mean:\n",
      "  player_0: 10.468666666666666\n",
      "  player_1: -0.6093333333333335\n",
      "  player_2: -6.859333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -15.2\n",
      "  player_1: -25.333333333333332\n",
      "  player_2: -35.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1148038845618292\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3695552482551927\n",
      "  mean_inference_ms: 2.10912614488769\n",
      "  mean_raw_obs_processing_ms: 0.2683132067426465\n",
      "time_since_restore: 2530.45592713356\n",
      "time_this_iter_s: 6.304628610610962\n",
      "time_total_s: 2530.45592713356\n",
      "timers:\n",
      "  learn_throughput: 702.297\n",
      "  learn_time_ms: 5695.595\n",
      "  load_throughput: 6319340.088\n",
      "  load_time_ms: 0.633\n",
      "  sample_throughput: 611.638\n",
      "  sample_time_ms: 6539.82\n",
      "  update_time_ms: 4.183\n",
      "timestamp: 1643327607\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1484000\n",
      "training_iteration: 371\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1491968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-53-41\n",
      "done: false\n",
      "episode_len_mean: 159.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 9236\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46584640234708785\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015177044174466558\n",
      "        policy_loss: -0.05339565431078275\n",
      "        total_loss: 33.308442069689434\n",
      "        vf_explained_var: 0.05262227932612101\n",
      "        vf_loss: 33.35500785986582\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5605068341890971\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017089096931546616\n",
      "        policy_loss: -0.08543020373520752\n",
      "        total_loss: 32.168688713709514\n",
      "        vf_explained_var: 0.47293092012405397\n",
      "        vf_loss: 32.246428702672326\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5010276407996813\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015522652361675379\n",
      "        policy_loss: -0.05374198227189481\n",
      "        total_loss: 31.207662913004558\n",
      "        vf_explained_var: 0.3618662120898565\n",
      "        vf_loss: 31.25441968599955\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1491968\n",
      "  num_agent_steps_trained: 1491968\n",
      "  num_steps_sampled: 1492000\n",
      "  num_steps_trained: 1492000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 373\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.8375\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6125\n",
      "  vram_util_percent0: 0.10139973958333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 40.333333333333336\n",
      "  player_1: 22.6\n",
      "  player_2: 15.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 10.266666666666667\n",
      "  player_1: -1.6533333333333333\n",
      "  player_2: -5.613333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -15.2\n",
      "  player_1: -33.0\n",
      "  player_2: -44.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11481654762542194\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36943730337175223\n",
      "  mean_inference_ms: 2.1087979615691372\n",
      "  mean_raw_obs_processing_ms: 0.2681953208139419\n",
      "time_since_restore: 2543.4374747276306\n",
      "time_this_iter_s: 6.464603900909424\n",
      "time_total_s: 2543.4374747276306\n",
      "timers:\n",
      "  learn_throughput: 700.892\n",
      "  learn_time_ms: 5707.013\n",
      "  load_throughput: 6320292.334\n",
      "  load_time_ms: 0.633\n",
      "  sample_throughput: 612.815\n",
      "  sample_time_ms: 6527.257\n",
      "  update_time_ms: 4.198\n",
      "timestamp: 1643327621\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1492000\n",
      "training_iteration: 373\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1499968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-53-54\n",
      "done: false\n",
      "episode_len_mean: 148.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 9291\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4645264621575673\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012648498496792754\n",
      "        policy_loss: -0.06463256557782492\n",
      "        total_loss: 39.63450138727824\n",
      "        vf_explained_var: -0.18984722753365835\n",
      "        vf_loss: 39.69344212214152\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5579892758528392\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01619978190088053\n",
      "        policy_loss: -0.08639020790966849\n",
      "        total_loss: 33.4271235148112\n",
      "        vf_explained_var: -0.1735532295703888\n",
      "        vf_loss: 33.50622371037801\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5418637029329936\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020073897087058867\n",
      "        policy_loss: -0.09239347719276945\n",
      "        total_loss: 46.81700775782267\n",
      "        vf_explained_var: 0.2040025689204534\n",
      "        vf_loss: 46.900367933909095\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1499968\n",
      "  num_agent_steps_trained: 1499968\n",
      "  num_steps_sampled: 1500000\n",
      "  num_steps_trained: 1500000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 375\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.212500000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.650000000000006\n",
      "  vram_util_percent0: 0.10660807291666667\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 40.333333333333336\n",
      "  player_1: 23.400000000000002\n",
      "  player_2: 14.266666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 10.568666666666665\n",
      "  player_1: -1.3113333333333335\n",
      "  player_2: -6.257333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -28.266666666666666\n",
      "  player_1: -33.0\n",
      "  player_2: -44.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11474288167120275\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3690369513175491\n",
      "  mean_inference_ms: 2.1075046515466975\n",
      "  mean_raw_obs_processing_ms: 0.2681096668301199\n",
      "time_since_restore: 2556.449203968048\n",
      "time_this_iter_s: 6.259045362472534\n",
      "time_total_s: 2556.449203968048\n",
      "timers:\n",
      "  learn_throughput: 700.913\n",
      "  learn_time_ms: 5706.845\n",
      "  load_throughput: 6347792.66\n",
      "  load_time_ms: 0.63\n",
      "  sample_throughput: 610.833\n",
      "  sample_time_ms: 6548.433\n",
      "  update_time_ms: 4.188\n",
      "timestamp: 1643327634\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1500000\n",
      "training_iteration: 375\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1507968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-54-06\n",
      "done: false\n",
      "episode_len_mean: 141.82\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 31\n",
      "episodes_total: 9350\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4827380629380544\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01579204019887887\n",
      "        policy_loss: -0.07511762075747053\n",
      "        total_loss: 55.36602490107219\n",
      "        vf_explained_var: 0.04509743332862854\n",
      "        vf_loss: 55.43403591791789\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5962558656930923\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01586317785908856\n",
      "        policy_loss: -0.07280744472518563\n",
      "        total_loss: 37.22718194961548\n",
      "        vf_explained_var: 0.3733586955070496\n",
      "        vf_loss: 37.292851311365766\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4841715850432714\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012266269436853055\n",
      "        policy_loss: -0.0478969312676539\n",
      "        total_loss: 44.04900967597961\n",
      "        vf_explained_var: 0.4108034572998683\n",
      "        vf_loss: 44.08862723986308\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1507968\n",
      "  num_agent_steps_trained: 1507968\n",
      "  num_steps_sampled: 1508000\n",
      "  num_steps_trained: 1508000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 377\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.025000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6375\n",
      "  vram_util_percent0: 0.10660807291666667\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 30.666666666666664\n",
      "  player_1: 30.53333333333333\n",
      "  player_2: 20.46666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 11.174666666666669\n",
      "  player_1: -0.6733333333333333\n",
      "  player_2: -7.501333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -28.266666666666666\n",
      "  player_1: -29.666666666666668\n",
      "  player_2: -35.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11457502782255002\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3687660824286521\n",
      "  mean_inference_ms: 2.1066584793457097\n",
      "  mean_raw_obs_processing_ms: 0.26823161945344476\n",
      "time_since_restore: 2569.0197727680206\n",
      "time_this_iter_s: 6.265530109405518\n",
      "time_total_s: 2569.0197727680206\n",
      "timers:\n",
      "  learn_throughput: 705.945\n",
      "  learn_time_ms: 5666.16\n",
      "  load_throughput: 6263894.863\n",
      "  load_time_ms: 0.639\n",
      "  sample_throughput: 614.972\n",
      "  sample_time_ms: 6504.364\n",
      "  update_time_ms: 4.191\n",
      "timestamp: 1643327646\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1508000\n",
      "training_iteration: 377\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1515969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-54-20\n",
      "done: false\n",
      "episode_len_mean: 140.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 9406\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48310750504334765\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015554554075915614\n",
      "        policy_loss: -0.10190254930872471\n",
      "        total_loss: 50.7984741751353\n",
      "        vf_explained_var: 0.28882151265939077\n",
      "        vf_loss: 50.8933771165212\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5677553437153499\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013476569006873646\n",
      "        policy_loss: -0.07240506827831268\n",
      "        total_loss: 33.851730891863504\n",
      "        vf_explained_var: 0.1816864933570226\n",
      "        vf_loss: 33.91807179292043\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5263425086935362\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013879979766290015\n",
      "        policy_loss: -0.058039936864127714\n",
      "        total_loss: 34.302363011042274\n",
      "        vf_explained_var: 0.3654074768225352\n",
      "        vf_loss: 34.35103404680888\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1515969\n",
      "  num_agent_steps_trained: 1515969\n",
      "  num_steps_sampled: 1516000\n",
      "  num_steps_trained: 1516000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 379\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.1875\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6375\n",
      "  vram_util_percent0: 0.041341145833333336\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 30.666666666666664\n",
      "  player_1: 30.53333333333333\n",
      "  player_2: 20.46666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 10.085999999999999\n",
      "  player_1: -1.746\n",
      "  player_2: -5.34\n",
      "policy_reward_min:\n",
      "  player_0: -17.53333333333333\n",
      "  player_1: -29.666666666666668\n",
      "  player_2: -29.2\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11464749030268047\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36873708244322567\n",
      "  mean_inference_ms: 2.106887417258933\n",
      "  mean_raw_obs_processing_ms: 0.26791832291237305\n",
      "time_since_restore: 2582.6855750083923\n",
      "time_this_iter_s: 6.313178539276123\n",
      "time_total_s: 2582.6855750083923\n",
      "timers:\n",
      "  learn_throughput: 699.064\n",
      "  learn_time_ms: 5721.936\n",
      "  load_throughput: 6217468.129\n",
      "  load_time_ms: 0.643\n",
      "  sample_throughput: 609.656\n",
      "  sample_time_ms: 6561.077\n",
      "  update_time_ms: 3.97\n",
      "timestamp: 1643327660\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1516000\n",
      "training_iteration: 379\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1523968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-54-33\n",
      "done: false\n",
      "episode_len_mean: 141.42\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 30\n",
      "episodes_total: 9461\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49521709978580475\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012894298045356151\n",
      "        policy_loss: -0.08159109151922167\n",
      "        total_loss: 42.57045088768005\n",
      "        vf_explained_var: 0.4292307456334432\n",
      "        vf_loss: 42.64623947779337\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.595777285695076\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017164355189881166\n",
      "        policy_loss: -0.09659502126354104\n",
      "        total_loss: 38.10128744443258\n",
      "        vf_explained_var: -0.0004541150728861491\n",
      "        vf_loss: 38.19015833218892\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5238119698564212\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01283886031169762\n",
      "        policy_loss: -0.06911424731214841\n",
      "        total_loss: 45.97695573488871\n",
      "        vf_explained_var: 0.45366306444009147\n",
      "        vf_loss: 46.0374038028717\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1523968\n",
      "  num_agent_steps_trained: 1523968\n",
      "  num_steps_sampled: 1524000\n",
      "  num_steps_trained: 1524000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 381\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.174999999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.6625\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 37.0\n",
      "  player_1: 22.733333333333327\n",
      "  player_2: 23.266666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 8.879333333333335\n",
      "  player_1: -0.5986666666666669\n",
      "  player_2: -5.280666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -25.53333333333333\n",
      "  player_1: -28.666666666666664\n",
      "  player_2: -34.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11444917195670877\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36850929253303716\n",
      "  mean_inference_ms: 2.1041712522626876\n",
      "  mean_raw_obs_processing_ms: 0.26743832929148603\n",
      "time_since_restore: 2595.274304151535\n",
      "time_this_iter_s: 6.245060920715332\n",
      "time_total_s: 2595.274304151535\n",
      "timers:\n",
      "  learn_throughput: 702.487\n",
      "  learn_time_ms: 5694.057\n",
      "  load_throughput: 6329591.791\n",
      "  load_time_ms: 0.632\n",
      "  sample_throughput: 613.257\n",
      "  sample_time_ms: 6522.556\n",
      "  update_time_ms: 4.009\n",
      "timestamp: 1643327673\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1524000\n",
      "training_iteration: 381\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1531969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-54-46\n",
      "done: false\n",
      "episode_len_mean: 144.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 9515\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48020719359318415\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015277035376342004\n",
      "        policy_loss: -0.07395960111015787\n",
      "        total_loss: 47.6216406472524\n",
      "        vf_explained_var: 0.09206423064072927\n",
      "        vf_loss: 47.68872559229533\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5727275019884109\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015024063299734889\n",
      "        policy_loss: -0.07296047490711013\n",
      "        total_loss: 27.040114955902098\n",
      "        vf_explained_var: 0.025888798435529072\n",
      "        vf_loss: 27.106314651171367\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5088878426949183\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013832706198202989\n",
      "        policy_loss: -0.06170317683679362\n",
      "        total_loss: 32.2027587111791\n",
      "        vf_explained_var: 0.2169113097588221\n",
      "        vf_loss: 32.25512481212616\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1531969\n",
      "  num_agent_steps_trained: 1531969\n",
      "  num_steps_sampled: 1532000\n",
      "  num_steps_trained: 1532000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 383\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.8\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.7\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.333333333333336\n",
      "  player_1: 24.53333333333333\n",
      "  player_2: 23.266666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 9.318\n",
      "  player_1: -1.3260000000000003\n",
      "  player_2: -4.991999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -25.53333333333333\n",
      "  player_1: -28.666666666666664\n",
      "  player_2: -41.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11453747058146646\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3684191530636658\n",
      "  mean_inference_ms: 2.1039906495257528\n",
      "  mean_raw_obs_processing_ms: 0.2675241265287952\n",
      "time_since_restore: 2608.410336971283\n",
      "time_this_iter_s: 6.7125937938690186\n",
      "time_total_s: 2608.410336971283\n",
      "timers:\n",
      "  learn_throughput: 700.301\n",
      "  learn_time_ms: 5711.828\n",
      "  load_throughput: 5956549.031\n",
      "  load_time_ms: 0.672\n",
      "  sample_throughput: 615.089\n",
      "  sample_time_ms: 6503.119\n",
      "  update_time_ms: 4.014\n",
      "timestamp: 1643327686\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1532000\n",
      "training_iteration: 383\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1539968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-54-58\n",
      "done: false\n",
      "episode_len_mean: 144.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 9572\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47227113087972006\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014866252365589645\n",
      "        policy_loss: -0.06529110606759786\n",
      "        total_loss: 48.139852371215824\n",
      "        vf_explained_var: 0.10042817533016205\n",
      "        vf_loss: 48.19845388730367\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5859405691425006\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015053633255617265\n",
      "        policy_loss: -0.05561831577060123\n",
      "        total_loss: 30.5831343460083\n",
      "        vf_explained_var: 0.02529298742612203\n",
      "        vf_loss: 30.631978454589845\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47711877087752025\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011225244945174684\n",
      "        policy_loss: -0.07007238529001673\n",
      "        total_loss: 49.579418121973674\n",
      "        vf_explained_var: 0.22081032594045003\n",
      "        vf_loss: 49.64191357294718\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1539968\n",
      "  num_agent_steps_trained: 1539968\n",
      "  num_steps_sampled: 1540000\n",
      "  num_steps_trained: 1540000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 385\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.071428571428573\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.699999999999996\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 24.53333333333333\n",
      "  player_2: 19.266666666666673\n",
      "policy_reward_mean:\n",
      "  player_0: 9.203333333333333\n",
      "  player_1: -1.9986666666666661\n",
      "  player_2: -4.204666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -14.8\n",
      "  player_1: -23.666666666666668\n",
      "  player_2: -41.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11478316231392037\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36831573357319203\n",
      "  mean_inference_ms: 2.105724019052167\n",
      "  mean_raw_obs_processing_ms: 0.26768751627398063\n",
      "time_since_restore: 2621.033589363098\n",
      "time_this_iter_s: 6.294706106185913\n",
      "time_total_s: 2621.033589363098\n",
      "timers:\n",
      "  learn_throughput: 705.065\n",
      "  learn_time_ms: 5673.239\n",
      "  load_throughput: 6042794.986\n",
      "  load_time_ms: 0.662\n",
      "  sample_throughput: 616.86\n",
      "  sample_time_ms: 6484.458\n",
      "  update_time_ms: 4.015\n",
      "timestamp: 1643327698\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1540000\n",
      "training_iteration: 385\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1547968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-55-11\n",
      "done: false\n",
      "episode_len_mean: 143.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 9623\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4755282978216807\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01364215588006834\n",
      "        policy_loss: -0.11605775294747825\n",
      "        total_loss: 41.90113739490509\n",
      "        vf_explained_var: 0.41243216236432395\n",
      "        vf_loss: 42.01105633417765\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6006491178274155\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015401303775123611\n",
      "        policy_loss: -0.05969761597303053\n",
      "        total_loss: 52.952612686157224\n",
      "        vf_explained_var: 0.2734833872318268\n",
      "        vf_loss: 53.00537991523743\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5137339945634206\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012053434995726666\n",
      "        policy_loss: -0.12477916375113031\n",
      "        total_loss: 27.22199131011963\n",
      "        vf_explained_var: 0.27604931195576987\n",
      "        vf_loss: 27.338634545008343\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1547968\n",
      "  num_agent_steps_trained: 1547968\n",
      "  num_steps_sampled: 1548000\n",
      "  num_steps_trained: 1548000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 387\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.12857142857143\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.699999999999996\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 25.933333333333334\n",
      "  player_2: 21.0\n",
      "policy_reward_mean:\n",
      "  player_0: 8.608000000000002\n",
      "  player_1: -0.96\n",
      "  player_2: -4.648000000000001\n",
      "policy_reward_min:\n",
      "  player_0: -14.8\n",
      "  player_1: -26.0\n",
      "  player_2: -34.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11480496902764525\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3682166643602491\n",
      "  mean_inference_ms: 2.1050950065716556\n",
      "  mean_raw_obs_processing_ms: 0.2676629082660339\n",
      "time_since_restore: 2633.967570543289\n",
      "time_this_iter_s: 6.310832977294922\n",
      "time_total_s: 2633.967570543289\n",
      "timers:\n",
      "  learn_throughput: 703.722\n",
      "  learn_time_ms: 5684.063\n",
      "  load_throughput: 6087082.215\n",
      "  load_time_ms: 0.657\n",
      "  sample_throughput: 613.107\n",
      "  sample_time_ms: 6524.147\n",
      "  update_time_ms: 4.041\n",
      "timestamp: 1643327711\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1548000\n",
      "training_iteration: 387\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1555968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-55-24\n",
      "done: false\n",
      "episode_len_mean: 156.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 9674\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47661356021960577\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014344515096781883\n",
      "        policy_loss: -0.06008024042472243\n",
      "        total_loss: 33.38759427388509\n",
      "        vf_explained_var: 0.23876501043637594\n",
      "        vf_loss: 33.44121939023336\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5627417229612668\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014991566196871038\n",
      "        policy_loss: -0.0845014590335389\n",
      "        total_loss: 54.21549607594808\n",
      "        vf_explained_var: 0.22994115789731343\n",
      "        vf_loss: 54.293251565297446\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5290332482258479\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013139705472034014\n",
      "        policy_loss: -0.07804711324938883\n",
      "        total_loss: 40.15343427658081\n",
      "        vf_explained_var: 0.17807346761226653\n",
      "        vf_loss: 40.22261178334554\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1555968\n",
      "  num_agent_steps_trained: 1555968\n",
      "  num_steps_sampled: 1556000\n",
      "  num_steps_trained: 1556000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 389\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.1125\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.7\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 31.0\n",
      "  player_2: 21.0\n",
      "policy_reward_mean:\n",
      "  player_0: 7.554666666666666\n",
      "  player_1: 1.0186666666666668\n",
      "  player_2: -5.573333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -19.866666666666664\n",
      "  player_1: -29.333333333333332\n",
      "  player_2: -30.2\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11464118357995122\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3679222882016444\n",
      "  mean_inference_ms: 2.104050271014691\n",
      "  mean_raw_obs_processing_ms: 0.26754218880841596\n",
      "time_since_restore: 2646.7572474479675\n",
      "time_this_iter_s: 6.390590190887451\n",
      "time_total_s: 2646.7572474479675\n",
      "timers:\n",
      "  learn_throughput: 713.718\n",
      "  learn_time_ms: 5604.458\n",
      "  load_throughput: 6067270.36\n",
      "  load_time_ms: 0.659\n",
      "  sample_throughput: 622.195\n",
      "  sample_time_ms: 6428.854\n",
      "  update_time_ms: 3.983\n",
      "timestamp: 1643327724\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1556000\n",
      "training_iteration: 389\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1563970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-55-37\n",
      "done: false\n",
      "episode_len_mean: 152.62\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 31\n",
      "episodes_total: 9731\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.451652610997359\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012886098490756316\n",
      "        policy_loss: -0.089192423671484\n",
      "        total_loss: 37.96573218981425\n",
      "        vf_explained_var: -0.03157782554626465\n",
      "        vf_loss: 38.04912581125895\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5476853724320729\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015853964124980847\n",
      "        policy_loss: -0.07939432381652295\n",
      "        total_loss: 30.458962701161703\n",
      "        vf_explained_var: 0.3173255062103271\n",
      "        vf_loss: 30.531223004659015\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45956142216920853\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010006937692596694\n",
      "        policy_loss: -0.040023124466339745\n",
      "        total_loss: 38.80049417495727\n",
      "        vf_explained_var: 0.09262160579363506\n",
      "        vf_loss: 38.833762729962665\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1563970\n",
      "  num_agent_steps_trained: 1563970\n",
      "  num_steps_sampled: 1564000\n",
      "  num_steps_trained: 1564000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 391\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 23.387500000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.7\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.799999999999997\n",
      "  player_1: 31.0\n",
      "  player_2: 19.0\n",
      "policy_reward_mean:\n",
      "  player_0: 9.093333333333334\n",
      "  player_1: 0.3133333333333336\n",
      "  player_2: -6.406666666666669\n",
      "policy_reward_min:\n",
      "  player_0: -19.866666666666664\n",
      "  player_1: -31.599999999999994\n",
      "  player_2: -34.46666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11472562853646139\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3678121511715837\n",
      "  mean_inference_ms: 2.103916837081443\n",
      "  mean_raw_obs_processing_ms: 0.26748404381491947\n",
      "time_since_restore: 2659.539554834366\n",
      "time_this_iter_s: 6.245957851409912\n",
      "time_total_s: 2659.539554834366\n",
      "timers:\n",
      "  learn_throughput: 713.871\n",
      "  learn_time_ms: 5603.252\n",
      "  load_throughput: 6056319.399\n",
      "  load_time_ms: 0.66\n",
      "  sample_throughput: 619.349\n",
      "  sample_time_ms: 6458.399\n",
      "  update_time_ms: 4.055\n",
      "timestamp: 1643327737\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1564000\n",
      "training_iteration: 391\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1571968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-55-50\n",
      "done: false\n",
      "episode_len_mean: 151.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 9778\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4635580073793729\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01614584494420948\n",
      "        policy_loss: -0.06524191764183343\n",
      "        total_loss: 34.51890173276266\n",
      "        vf_explained_var: 0.03815256496270498\n",
      "        vf_loss: 34.576877756118776\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.579436586399873\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01332057805587187\n",
      "        policy_loss: -0.042107266755774614\n",
      "        total_loss: 30.12757824897766\n",
      "        vf_explained_var: 0.18418898383776347\n",
      "        vf_loss: 30.16369119644165\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5189172482490539\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013201575628086869\n",
      "        policy_loss: -0.06254149163762729\n",
      "        total_loss: 33.16787305355072\n",
      "        vf_explained_var: 0.21863432884216308\n",
      "        vf_loss: 33.221503460407256\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1571968\n",
      "  num_agent_steps_trained: 1571968\n",
      "  num_steps_sampled: 1572000\n",
      "  num_steps_trained: 1572000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 393\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 22.974999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.7\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 21.53333333333333\n",
      "  player_2: 19.0\n",
      "policy_reward_mean:\n",
      "  player_0: 10.208000000000004\n",
      "  player_1: -1.5679999999999992\n",
      "  player_2: -5.639999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -14.599999999999998\n",
      "  player_1: -31.599999999999994\n",
      "  player_2: -34.46666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11479789367269898\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3680337855022269\n",
      "  mean_inference_ms: 2.1045247055644345\n",
      "  mean_raw_obs_processing_ms: 0.2676051753398951\n",
      "time_since_restore: 2672.3709921836853\n",
      "time_this_iter_s: 6.368275165557861\n",
      "time_total_s: 2672.3709921836853\n",
      "timers:\n",
      "  learn_throughput: 718.344\n",
      "  learn_time_ms: 5568.36\n",
      "  load_throughput: 6473940.189\n",
      "  load_time_ms: 0.618\n",
      "  sample_throughput: 618.961\n",
      "  sample_time_ms: 6462.439\n",
      "  update_time_ms: 4.008\n",
      "timestamp: 1643327750\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1572000\n",
      "training_iteration: 393\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1579968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-56-03\n",
      "done: false\n",
      "episode_len_mean: 157.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 28\n",
      "episodes_total: 9832\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46857113152742386\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0157028498541816\n",
      "        policy_loss: -0.09924843843405445\n",
      "        total_loss: 32.69054721991221\n",
      "        vf_explained_var: 0.2807465883096059\n",
      "        vf_loss: 32.782729258537294\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5557691378394762\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014751674963711896\n",
      "        policy_loss: -0.02445096816246708\n",
      "        total_loss: 29.150411535898844\n",
      "        vf_explained_var: 0.04371321737766266\n",
      "        vf_loss: 29.16822418530782\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5402500560879707\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013706077871118333\n",
      "        policy_loss: -0.08201466036960482\n",
      "        total_loss: 52.99043649832408\n",
      "        vf_explained_var: 0.26300123453140256\n",
      "        vf_loss: 53.06319936275482\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1579968\n",
      "  num_agent_steps_trained: 1579968\n",
      "  num_steps_sampled: 1580000\n",
      "  num_steps_trained: 1580000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 395\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.666666666666668\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.7\n",
      "  vram_util_percent0: 0.02882667824074074\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.333333333333336\n",
      "  player_1: 26.400000000000002\n",
      "  player_2: 14.0\n",
      "policy_reward_mean:\n",
      "  player_0: 8.652000000000001\n",
      "  player_1: 0.97\n",
      "  player_2: -6.621999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -15.0\n",
      "  player_1: -25.0\n",
      "  player_2: -31.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11442892168498948\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36783094671587935\n",
      "  mean_inference_ms: 2.1040260713820222\n",
      "  mean_raw_obs_processing_ms: 0.2676553145303185\n",
      "time_since_restore: 2685.360296726227\n",
      "time_this_iter_s: 6.554563760757446\n",
      "time_total_s: 2685.360296726227\n",
      "timers:\n",
      "  learn_throughput: 714.062\n",
      "  learn_time_ms: 5601.758\n",
      "  load_throughput: 6527845.609\n",
      "  load_time_ms: 0.613\n",
      "  sample_throughput: 621.371\n",
      "  sample_time_ms: 6437.382\n",
      "  update_time_ms: 4.095\n",
      "timestamp: 1643327763\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1580000\n",
      "training_iteration: 395\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1587968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-56-16\n",
      "done: false\n",
      "episode_len_mean: 154.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 9881\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.459963216483593\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013211580995011967\n",
      "        policy_loss: -0.07438333354890346\n",
      "        total_loss: 35.006750192642215\n",
      "        vf_explained_var: -0.19193908492724102\n",
      "        vf_loss: 35.07518824895223\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5308818091948827\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014187279126978562\n",
      "        policy_loss: -0.09278780853996674\n",
      "        total_loss: 20.559325321515402\n",
      "        vf_explained_var: -0.0239869624376297\n",
      "        vf_loss: 20.645728775660196\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5088875353336334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013107434920614953\n",
      "        policy_loss: -0.06834012367917845\n",
      "        total_loss: 17.701530141830446\n",
      "        vf_explained_var: 0.2208037128051122\n",
      "        vf_loss: 17.761022709210714\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1587968\n",
      "  num_agent_steps_trained: 1587968\n",
      "  num_steps_sampled: 1588000\n",
      "  num_steps_trained: 1588000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 397\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.671428571428567\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.699999999999996\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.333333333333336\n",
      "  player_1: 30.46666666666667\n",
      "  player_2: 14.199999999999996\n",
      "policy_reward_mean:\n",
      "  player_0: 8.248000000000001\n",
      "  player_1: 3.05\n",
      "  player_2: -8.298\n",
      "policy_reward_min:\n",
      "  player_0: -24.93333333333333\n",
      "  player_1: -20.333333333333336\n",
      "  player_2: -31.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11450137766095819\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36755504066491945\n",
      "  mean_inference_ms: 2.1037927981852804\n",
      "  mean_raw_obs_processing_ms: 0.2678787252737338\n",
      "time_since_restore: 2698.1511149406433\n",
      "time_this_iter_s: 6.307804107666016\n",
      "time_total_s: 2698.1511149406433\n",
      "timers:\n",
      "  learn_throughput: 714.584\n",
      "  learn_time_ms: 5597.661\n",
      "  load_throughput: 6456748.768\n",
      "  load_time_ms: 0.62\n",
      "  sample_throughput: 621.022\n",
      "  sample_time_ms: 6441.0\n",
      "  update_time_ms: 4.094\n",
      "timestamp: 1643327776\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1588000\n",
      "training_iteration: 397\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1595971\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-56-29\n",
      "done: false\n",
      "episode_len_mean: 154.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 9934\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4572444416085879\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011010655433996463\n",
      "        policy_loss: -0.06997250787292918\n",
      "        total_loss: 41.28832518100739\n",
      "        vf_explained_var: 0.24870356500148774\n",
      "        vf_loss: 41.35334310611089\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5633671552936236\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015692776604143244\n",
      "        policy_loss: -0.07773981660604477\n",
      "        total_loss: 23.866088352998098\n",
      "        vf_explained_var: 0.010955350796381632\n",
      "        vf_loss: 23.93676644563675\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5353659254312515\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011891830093548682\n",
      "        policy_loss: -0.07684375500927369\n",
      "        total_loss: 33.347076621055606\n",
      "        vf_explained_var: 0.18711305201053618\n",
      "        vf_loss: 33.41589362780253\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1595971\n",
      "  num_agent_steps_trained: 1595971\n",
      "  num_steps_sampled: 1596000\n",
      "  num_steps_trained: 1596000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 399\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 23.014285714285712\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.699999999999996\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 30.46666666666667\n",
      "  player_2: 14.8\n",
      "policy_reward_mean:\n",
      "  player_0: 9.399333333333333\n",
      "  player_1: 0.01333333333333318\n",
      "  player_2: -6.412666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -26.133333333333333\n",
      "  player_1: -24.0\n",
      "  player_2: -27.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11467615613166716\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3676711044877723\n",
      "  mean_inference_ms: 2.1037181874273214\n",
      "  mean_raw_obs_processing_ms: 0.26781460339798707\n",
      "time_since_restore: 2710.8619492053986\n",
      "time_this_iter_s: 6.283252716064453\n",
      "time_total_s: 2710.8619492053986\n",
      "timers:\n",
      "  learn_throughput: 714.832\n",
      "  learn_time_ms: 5595.723\n",
      "  load_throughput: 6368273.297\n",
      "  load_time_ms: 0.628\n",
      "  sample_throughput: 620.574\n",
      "  sample_time_ms: 6445.646\n",
      "  update_time_ms: 4.211\n",
      "timestamp: 1643327789\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1596000\n",
      "training_iteration: 399\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1603968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-56-42\n",
      "done: false\n",
      "episode_len_mean: 167.28\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 9979\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46649157027403515\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014909154566351694\n",
      "        policy_loss: -0.05494600423146039\n",
      "        total_loss: 39.69413441499074\n",
      "        vf_explained_var: 0.3341243688265483\n",
      "        vf_loss: 39.74237096309662\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5451043603817621\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017967665391787797\n",
      "        policy_loss: -0.1018684566517671\n",
      "        total_loss: 36.34255737622579\n",
      "        vf_explained_var: 0.25086582601070406\n",
      "        vf_loss: 36.43634019374847\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5251117150982221\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014169297186805731\n",
      "        policy_loss: -0.07524065814291438\n",
      "        total_loss: 27.61143420378367\n",
      "        vf_explained_var: -0.17897141277790068\n",
      "        vf_loss: 27.6771107117335\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1603968\n",
      "  num_agent_steps_trained: 1603968\n",
      "  num_steps_sampled: 1604000\n",
      "  num_steps_trained: 1604000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 401\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 20.6625\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.712500000000006\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 35.0\n",
      "  player_1: 29.400000000000002\n",
      "  player_2: 14.8\n",
      "policy_reward_mean:\n",
      "  player_0: 9.789333333333333\n",
      "  player_1: -0.8306666666666668\n",
      "  player_2: -5.958666666666668\n",
      "policy_reward_min:\n",
      "  player_0: -26.133333333333333\n",
      "  player_1: -27.800000000000004\n",
      "  player_2: -35.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1145042134079091\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3674551634101059\n",
      "  mean_inference_ms: 2.102431552318686\n",
      "  mean_raw_obs_processing_ms: 0.26752031609027843\n",
      "time_since_restore: 2723.4884819984436\n",
      "time_this_iter_s: 6.2908713817596436\n",
      "time_total_s: 2723.4884819984436\n",
      "timers:\n",
      "  learn_throughput: 714.367\n",
      "  learn_time_ms: 5599.364\n",
      "  load_throughput: 6448806.888\n",
      "  load_time_ms: 0.62\n",
      "  sample_throughput: 621.649\n",
      "  sample_time_ms: 6434.499\n",
      "  update_time_ms: 4.279\n",
      "timestamp: 1643327802\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1604000\n",
      "training_iteration: 401\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1611968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-56-54\n",
      "done: false\n",
      "episode_len_mean: 166.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 10030\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.449584475060304\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015222688711391234\n",
      "        policy_loss: -0.08859672685464223\n",
      "        total_loss: 40.505153013865154\n",
      "        vf_explained_var: 0.3618024679025014\n",
      "        vf_loss: 40.58689963340759\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5628167476256688\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014816712662771655\n",
      "        policy_loss: -0.06690389366354793\n",
      "        total_loss: 34.03888831138611\n",
      "        vf_explained_var: 0.1970239935318629\n",
      "        vf_loss: 34.09912443796794\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5100473699967066\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013327167052630724\n",
      "        policy_loss: -0.06708506206671397\n",
      "        total_loss: 28.562438192367555\n",
      "        vf_explained_var: 0.08319162944952647\n",
      "        vf_loss: 28.620527197519937\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1611968\n",
      "  num_agent_steps_trained: 1611968\n",
      "  num_steps_sampled: 1612000\n",
      "  num_steps_trained: 1612000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 403\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.549999999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.712500000000006\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 35.0\n",
      "  player_1: 29.400000000000002\n",
      "  player_2: 14.333333333333332\n",
      "policy_reward_mean:\n",
      "  player_0: 9.424000000000001\n",
      "  player_1: 0.35\n",
      "  player_2: -6.773999999999998\n",
      "policy_reward_min:\n",
      "  player_0: -16.53333333333333\n",
      "  player_1: -27.800000000000004\n",
      "  player_2: -35.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11449643044448964\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3673919114244025\n",
      "  mean_inference_ms: 2.101254145913318\n",
      "  mean_raw_obs_processing_ms: 0.26723687203092383\n",
      "time_since_restore: 2736.152631998062\n",
      "time_this_iter_s: 6.305792331695557\n",
      "time_total_s: 2736.152631998062\n",
      "timers:\n",
      "  learn_throughput: 716.072\n",
      "  learn_time_ms: 5586.032\n",
      "  load_throughput: 6355969.086\n",
      "  load_time_ms: 0.629\n",
      "  sample_throughput: 622.224\n",
      "  sample_time_ms: 6428.548\n",
      "  update_time_ms: 4.267\n",
      "timestamp: 1643327814\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1612000\n",
      "training_iteration: 403\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1619968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-57-07\n",
      "done: false\n",
      "episode_len_mean: 154.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 10080\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4640718020002047\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014627930333469218\n",
      "        policy_loss: -0.12302796985954047\n",
      "        total_loss: 42.87668708960215\n",
      "        vf_explained_var: 0.12786569158236186\n",
      "        vf_loss: 42.99313269297282\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5237064199646314\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015286475132215855\n",
      "        policy_loss: -0.05029340026279291\n",
      "        total_loss: 41.923173348108925\n",
      "        vf_explained_var: 0.20595762848854066\n",
      "        vf_loss: 41.9665880393982\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47587135593096413\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012356456019012209\n",
      "        policy_loss: -0.03735881372665366\n",
      "        total_loss: 37.93865441004435\n",
      "        vf_explained_var: 0.4439020323753357\n",
      "        vf_loss: 37.96767238299052\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1619968\n",
      "  num_agent_steps_trained: 1619968\n",
      "  num_steps_sampled: 1620000\n",
      "  num_steps_trained: 1620000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 405\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 21.75\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.7\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.33333333333333\n",
      "  player_1: 30.333333333333332\n",
      "  player_2: 18.199999999999996\n",
      "policy_reward_mean:\n",
      "  player_0: 9.819333333333335\n",
      "  player_1: -1.7146666666666668\n",
      "  player_2: -5.104666666666668\n",
      "policy_reward_min:\n",
      "  player_0: -15.0\n",
      "  player_1: -31.066666666666663\n",
      "  player_2: -34.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11460210000938906\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3672563929977815\n",
      "  mean_inference_ms: 2.1017948609476362\n",
      "  mean_raw_obs_processing_ms: 0.26702682518793674\n",
      "time_since_restore: 2748.873587846756\n",
      "time_this_iter_s: 6.2805397510528564\n",
      "time_total_s: 2748.873587846756\n",
      "timers:\n",
      "  learn_throughput: 720.755\n",
      "  learn_time_ms: 5549.733\n",
      "  load_throughput: 6301775.157\n",
      "  load_time_ms: 0.635\n",
      "  sample_throughput: 620.146\n",
      "  sample_time_ms: 6450.091\n",
      "  update_time_ms: 4.237\n",
      "timestamp: 1643327827\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1620000\n",
      "training_iteration: 405\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1627968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-57-20\n",
      "done: false\n",
      "episode_len_mean: 150.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 10135\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4537436647216479\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013487563529512652\n",
      "        policy_loss: -0.007320216111838818\n",
      "        total_loss: 40.68097103118897\n",
      "        vf_explained_var: 0.02657637615998586\n",
      "        vf_loss: 40.68222185293833\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5669181934992472\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014627625010398333\n",
      "        policy_loss: -0.07726039110062023\n",
      "        total_loss: 37.947784150441485\n",
      "        vf_explained_var: 0.18853920976320904\n",
      "        vf_loss: 38.018462098439535\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49366147359212237\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011322777592383773\n",
      "        policy_loss: -0.09944531021018822\n",
      "        total_loss: 35.07995454470316\n",
      "        vf_explained_var: 0.17157274822394053\n",
      "        vf_loss: 35.1717568620046\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1627968\n",
      "  num_agent_steps_trained: 1627968\n",
      "  num_steps_sampled: 1628000\n",
      "  num_steps_trained: 1628000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 407\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.262500000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.7\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.0\n",
      "  player_1: 30.333333333333332\n",
      "  player_2: 18.199999999999996\n",
      "policy_reward_mean:\n",
      "  player_0: 9.869333333333334\n",
      "  player_1: -1.9166666666666672\n",
      "  player_2: -4.9526666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -18.133333333333333\n",
      "  player_1: -31.066666666666663\n",
      "  player_2: -34.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11436383537223559\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36682342225517567\n",
      "  mean_inference_ms: 2.100702533705464\n",
      "  mean_raw_obs_processing_ms: 0.26702459013596774\n",
      "time_since_restore: 2761.492678642273\n",
      "time_this_iter_s: 6.23535680770874\n",
      "time_total_s: 2761.492678642273\n",
      "timers:\n",
      "  learn_throughput: 720.964\n",
      "  learn_time_ms: 5548.129\n",
      "  load_throughput: 6304143.088\n",
      "  load_time_ms: 0.635\n",
      "  sample_throughput: 623.282\n",
      "  sample_time_ms: 6417.639\n",
      "  update_time_ms: 4.234\n",
      "timestamp: 1643327840\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1628000\n",
      "training_iteration: 407\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1635970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-57-33\n",
      "done: false\n",
      "episode_len_mean: 150.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 10188\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4467787320415179\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01367542272607049\n",
      "        policy_loss: -0.0017672170326113702\n",
      "        total_loss: 43.7982772509257\n",
      "        vf_explained_var: 0.3079778434832891\n",
      "        vf_loss: 43.79389037926992\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5701151870687803\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013975757171545714\n",
      "        policy_loss: -0.12115447292570025\n",
      "        total_loss: 24.753238201141357\n",
      "        vf_explained_var: -0.1711059691508611\n",
      "        vf_loss: 24.86810339609782\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5387083157896996\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013220153576187386\n",
      "        policy_loss: -0.06752082141737144\n",
      "        total_loss: 43.544933032989505\n",
      "        vf_explained_var: 0.3182574361562729\n",
      "        vf_loss: 43.603530184427896\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1635970\n",
      "  num_agent_steps_trained: 1635970\n",
      "  num_steps_sampled: 1636000\n",
      "  num_steps_trained: 1636000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 409\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.400000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.8\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 40.0\n",
      "  player_1: 25.6\n",
      "  player_2: 18.866666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 9.920666666666666\n",
      "  player_1: -1.9253333333333336\n",
      "  player_2: -4.995333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -19.666666666666668\n",
      "  player_1: -33.333333333333336\n",
      "  player_2: -30.666666666666668\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11412700178129175\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3664006831531175\n",
      "  mean_inference_ms: 2.0985420774741685\n",
      "  mean_raw_obs_processing_ms: 0.2669338902382909\n",
      "time_since_restore: 2773.9509205818176\n",
      "time_this_iter_s: 6.195357799530029\n",
      "time_total_s: 2773.9509205818176\n",
      "timers:\n",
      "  learn_throughput: 724.602\n",
      "  learn_time_ms: 5520.274\n",
      "  load_throughput: 6427559.574\n",
      "  load_time_ms: 0.622\n",
      "  sample_throughput: 625.814\n",
      "  sample_time_ms: 6391.673\n",
      "  update_time_ms: 4.17\n",
      "timestamp: 1643327853\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1636000\n",
      "training_iteration: 409\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1643968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-57-45\n",
      "done: false\n",
      "episode_len_mean: 150.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 10240\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4108574711283048\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01281930411747453\n",
      "        policy_loss: -0.08968329585002115\n",
      "        total_loss: 28.46068547328313\n",
      "        vf_explained_var: 0.010542129675547282\n",
      "        vf_loss: 28.544600025018056\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5264429517587026\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015695568927597683\n",
      "        policy_loss: -0.04603683392051607\n",
      "        total_loss: 27.130043989817302\n",
      "        vf_explained_var: 0.1591422571738561\n",
      "        vf_loss: 27.16901777267456\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5215418082475662\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013061105199158192\n",
      "        policy_loss: -0.08653565228606264\n",
      "        total_loss: 33.03779457569122\n",
      "        vf_explained_var: 0.26538688600063326\n",
      "        vf_loss: 33.115514159202576\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1643968\n",
      "  num_agent_steps_trained: 1643968\n",
      "  num_steps_sampled: 1644000\n",
      "  num_steps_trained: 1644000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 411\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.9625\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.7\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 45.666666666666664\n",
      "  player_1: 26.666666666666664\n",
      "  player_2: 18.866666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 9.440666666666665\n",
      "  player_1: -1.7273333333333338\n",
      "  player_2: -4.713333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -19.666666666666668\n",
      "  player_1: -24.333333333333336\n",
      "  player_2: -30.666666666666668\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11421244141285769\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3664648128716471\n",
      "  mean_inference_ms: 2.098628154490731\n",
      "  mean_raw_obs_processing_ms: 0.2671068817299731\n",
      "time_since_restore: 2786.656469106674\n",
      "time_this_iter_s: 6.34075927734375\n",
      "time_total_s: 2786.656469106674\n",
      "timers:\n",
      "  learn_throughput: 725.151\n",
      "  learn_time_ms: 5516.094\n",
      "  load_throughput: 6366823.27\n",
      "  load_time_ms: 0.628\n",
      "  sample_throughput: 628.322\n",
      "  sample_time_ms: 6366.162\n",
      "  update_time_ms: 3.973\n",
      "timestamp: 1643327865\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1644000\n",
      "training_iteration: 411\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1651968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-57-58\n",
      "done: false\n",
      "episode_len_mean: 152.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 28\n",
      "episodes_total: 10294\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46784752438465754\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015194103532970378\n",
      "        policy_loss: -0.06385056400050719\n",
      "        total_loss: 50.70812019348145\n",
      "        vf_explained_var: 0.26975081861019135\n",
      "        vf_loss: 50.76513333638509\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5577936215202014\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014265105901713124\n",
      "        policy_loss: -0.05243123490363359\n",
      "        total_loss: 40.431696049372356\n",
      "        vf_explained_var: 0.36876714368661245\n",
      "        vf_loss: 40.477708180745445\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5208537242809932\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011624584953594119\n",
      "        policy_loss: -0.07647076055873185\n",
      "        total_loss: 42.3395485273997\n",
      "        vf_explained_var: 0.23809738755226134\n",
      "        vf_loss: 42.40817247390747\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1651968\n",
      "  num_agent_steps_trained: 1651968\n",
      "  num_steps_sampled: 1652000\n",
      "  num_steps_trained: 1652000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 413\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.199999999999996\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.8\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 45.666666666666664\n",
      "  player_1: 34.06666666666666\n",
      "  player_2: 22.266666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 8.216000000000001\n",
      "  player_1: -0.1880000000000002\n",
      "  player_2: -5.028\n",
      "policy_reward_min:\n",
      "  player_0: -42.4\n",
      "  player_1: -29.666666666666664\n",
      "  player_2: -32.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11441414220442107\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36654580569572154\n",
      "  mean_inference_ms: 2.099565650229171\n",
      "  mean_raw_obs_processing_ms: 0.2672036736569513\n",
      "time_since_restore: 2799.669822692871\n",
      "time_this_iter_s: 6.436511993408203\n",
      "time_total_s: 2799.669822692871\n",
      "timers:\n",
      "  learn_throughput: 722.649\n",
      "  learn_time_ms: 5535.194\n",
      "  load_throughput: 6381596.044\n",
      "  load_time_ms: 0.627\n",
      "  sample_throughput: 625.196\n",
      "  sample_time_ms: 6397.99\n",
      "  update_time_ms: 4.072\n",
      "timestamp: 1643327878\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1652000\n",
      "training_iteration: 413\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1659968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-58-11\n",
      "done: false\n",
      "episode_len_mean: 144.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 28\n",
      "episodes_total: 10350\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4540109075109164\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012769721595874823\n",
      "        policy_loss: -0.10834815298517544\n",
      "        total_loss: 36.71346957842509\n",
      "        vf_explained_var: 0.10686228036880493\n",
      "        vf_loss: 36.816071084340415\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5498093302051227\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01725444338818003\n",
      "        policy_loss: -0.06732420310378075\n",
      "        total_loss: 47.7811518573761\n",
      "        vf_explained_var: 0.20988386789957683\n",
      "        vf_loss: 47.84071160316467\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.507348493138949\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011501089705231303\n",
      "        policy_loss: -0.03109229529897372\n",
      "        total_loss: 53.34782499631246\n",
      "        vf_explained_var: 0.3965618803103765\n",
      "        vf_loss: 53.37115372657776\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1659968\n",
      "  num_agent_steps_trained: 1659968\n",
      "  num_steps_sampled: 1660000\n",
      "  num_steps_trained: 1660000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 415\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.271428571428572\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.75714285714287\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 39.666666666666664\n",
      "  player_1: 34.06666666666666\n",
      "  player_2: 22.266666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 7.842666666666665\n",
      "  player_1: -0.9873333333333335\n",
      "  player_2: -3.855333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -42.4\n",
      "  player_1: -36.0\n",
      "  player_2: -36.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11419611480313716\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3663726218401175\n",
      "  mean_inference_ms: 2.0988157172848934\n",
      "  mean_raw_obs_processing_ms: 0.26699183671177973\n",
      "time_since_restore: 2812.51779794693\n",
      "time_this_iter_s: 6.318291902542114\n",
      "time_total_s: 2812.51779794693\n",
      "timers:\n",
      "  learn_throughput: 719.49\n",
      "  learn_time_ms: 5559.494\n",
      "  load_throughput: 6342033.719\n",
      "  load_time_ms: 0.631\n",
      "  sample_throughput: 626.298\n",
      "  sample_time_ms: 6386.74\n",
      "  update_time_ms: 3.975\n",
      "timestamp: 1643327891\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1660000\n",
      "training_iteration: 415\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1667970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-58-24\n",
      "done: false\n",
      "episode_len_mean: 146.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 10408\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46779650608698525\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01301183386851335\n",
      "        policy_loss: -0.09889847803938513\n",
      "        total_loss: 32.276261784235636\n",
      "        vf_explained_var: -0.005970733165740967\n",
      "        vf_loss: 32.36930504639943\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5748410566647848\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015658100513586154\n",
      "        policy_loss: -0.12370462995022535\n",
      "        total_loss: 44.23403594017029\n",
      "        vf_explained_var: 0.10631369094053904\n",
      "        vf_loss: 44.35069418907165\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.516329458753268\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014047243999211787\n",
      "        policy_loss: -0.06289457381237298\n",
      "        total_loss: 40.71530562082926\n",
      "        vf_explained_var: 0.20537839730580648\n",
      "        vf_loss: 40.768718544642134\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1667970\n",
      "  num_agent_steps_trained: 1667970\n",
      "  num_steps_sampled: 1668000\n",
      "  num_steps_trained: 1668000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 417\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.942857142857143\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.800000000000004\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 42.666666666666664\n",
      "  player_1: 27.8\n",
      "  player_2: 21.8\n",
      "policy_reward_mean:\n",
      "  player_0: 9.260666666666667\n",
      "  player_1: -1.4313333333333336\n",
      "  player_2: -4.829333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -20.333333333333336\n",
      "  player_1: -31.6\n",
      "  player_2: -47.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11416683176753768\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3664026453652633\n",
      "  mean_inference_ms: 2.0989199259252205\n",
      "  mean_raw_obs_processing_ms: 0.2671855283886735\n",
      "time_since_restore: 2825.2052726745605\n",
      "time_this_iter_s: 6.367654323577881\n",
      "time_total_s: 2825.2052726745605\n",
      "timers:\n",
      "  learn_throughput: 718.615\n",
      "  learn_time_ms: 5566.263\n",
      "  load_throughput: 6342992.817\n",
      "  load_time_ms: 0.631\n",
      "  sample_throughput: 626.474\n",
      "  sample_time_ms: 6384.939\n",
      "  update_time_ms: 3.965\n",
      "timestamp: 1643327904\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1668000\n",
      "training_iteration: 417\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1675970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-58-37\n",
      "done: false\n",
      "episode_len_mean: 146.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 28\n",
      "episodes_total: 10462\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47077033122380574\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015287911631236212\n",
      "        policy_loss: -0.062359634737173715\n",
      "        total_loss: 46.56789123217265\n",
      "        vf_explained_var: 0.29806772371133167\n",
      "        vf_loss: 46.623371575673424\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5529236854116122\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018203397361961228\n",
      "        policy_loss: -0.05667743797414005\n",
      "        total_loss: 34.32224738279979\n",
      "        vf_explained_var: 0.06387897213300069\n",
      "        vf_loss: 34.3707332277298\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5051309800148011\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011997406168432116\n",
      "        policy_loss: -0.1135634239235272\n",
      "        total_loss: 44.80628872871399\n",
      "        vf_explained_var: 0.42124516288439434\n",
      "        vf_loss: 44.91175358136495\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1675970\n",
      "  num_agent_steps_trained: 1675970\n",
      "  num_steps_sampled: 1676000\n",
      "  num_steps_trained: 1676000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 419\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.1125\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.875\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 42.666666666666664\n",
      "  player_1: 27.4\n",
      "  player_2: 21.8\n",
      "policy_reward_mean:\n",
      "  player_0: 10.923333333333332\n",
      "  player_1: -1.6686666666666665\n",
      "  player_2: -6.254666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -16.133333333333333\n",
      "  player_1: -31.6\n",
      "  player_2: -47.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1143385464123719\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3665500939948157\n",
      "  mean_inference_ms: 2.0989351084131855\n",
      "  mean_raw_obs_processing_ms: 0.2672589870707049\n",
      "time_since_restore: 2837.9324712753296\n",
      "time_this_iter_s: 6.326364278793335\n",
      "time_total_s: 2837.9324712753296\n",
      "timers:\n",
      "  learn_throughput: 715.083\n",
      "  learn_time_ms: 5593.759\n",
      "  load_throughput: 6325773.32\n",
      "  load_time_ms: 0.632\n",
      "  sample_throughput: 623.625\n",
      "  sample_time_ms: 6414.112\n",
      "  update_time_ms: 3.926\n",
      "timestamp: 1643327917\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1676000\n",
      "training_iteration: 419\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1683968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-58-50\n",
      "done: false\n",
      "episode_len_mean: 146.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 10509\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46888678322235744\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013845474580678759\n",
      "        policy_loss: -0.09473199039697647\n",
      "        total_loss: 30.412601877848306\n",
      "        vf_explained_var: -0.12894264360268912\n",
      "        vf_loss: 30.501103474299114\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5084098569552103\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015118381816677795\n",
      "        policy_loss: -0.06073766549428304\n",
      "        total_loss: 29.83464179197947\n",
      "        vf_explained_var: 0.14391955276330312\n",
      "        vf_loss: 29.8885763835907\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5181455099582672\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011975437644626557\n",
      "        policy_loss: -0.08844243473062913\n",
      "        total_loss: 23.863474245071412\n",
      "        vf_explained_var: -0.0031323623657226562\n",
      "        vf_loss: 23.94383314450582\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1683968\n",
      "  num_agent_steps_trained: 1683968\n",
      "  num_steps_sampled: 1684000\n",
      "  num_steps_trained: 1684000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 421\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.675000000000004\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.8\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.666666666666664\n",
      "  player_1: 24.0\n",
      "  player_2: 20.0\n",
      "policy_reward_mean:\n",
      "  player_0: 10.192666666666666\n",
      "  player_1: -2.1093333333333333\n",
      "  player_2: -5.083333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -22.0\n",
      "  player_1: -27.666666666666668\n",
      "  player_2: -34.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1143699581340766\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36643040508117136\n",
      "  mean_inference_ms: 2.0994758876572766\n",
      "  mean_raw_obs_processing_ms: 0.26725247895386384\n",
      "time_since_restore: 2851.0085945129395\n",
      "time_this_iter_s: 6.600414037704468\n",
      "time_total_s: 2851.0085945129395\n",
      "timers:\n",
      "  learn_throughput: 714.147\n",
      "  learn_time_ms: 5601.091\n",
      "  load_throughput: 6412083.317\n",
      "  load_time_ms: 0.624\n",
      "  sample_throughput: 618.751\n",
      "  sample_time_ms: 6464.633\n",
      "  update_time_ms: 3.94\n",
      "timestamp: 1643327930\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1684000\n",
      "training_iteration: 421\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1691969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-59-03\n",
      "done: false\n",
      "episode_len_mean: 151.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 31\n",
      "episodes_total: 10569\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4904245736201604\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014968551764638202\n",
      "        policy_loss: -0.028616220504045487\n",
      "        total_loss: 59.25143347422282\n",
      "        vf_explained_var: -0.029918047587076824\n",
      "        vf_loss: 59.27331418991089\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5613186978300413\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014491472314287528\n",
      "        policy_loss: -0.11550410725176334\n",
      "        total_loss: 39.015666805903116\n",
      "        vf_explained_var: 0.278378572066625\n",
      "        vf_loss: 39.12464971224467\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48772233704725904\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011402708398054528\n",
      "        policy_loss: -0.07626319299141567\n",
      "        total_loss: 42.63231506983439\n",
      "        vf_explained_var: 0.21017712593078614\n",
      "        vf_loss: 42.700881792704266\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1691969\n",
      "  num_agent_steps_trained: 1691969\n",
      "  num_steps_sampled: 1692000\n",
      "  num_steps_trained: 1692000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 423\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.275\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.9\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666668\n",
      "  player_1: 24.266666666666666\n",
      "  player_2: 18.266666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 9.375333333333332\n",
      "  player_1: -1.318666666666667\n",
      "  player_2: -5.056666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -14.466666666666665\n",
      "  player_1: -21.333333333333336\n",
      "  player_2: -35.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11444615542939665\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36604251679619665\n",
      "  mean_inference_ms: 2.099034825955386\n",
      "  mean_raw_obs_processing_ms: 0.267421731160682\n",
      "time_since_restore: 2863.5434391498566\n",
      "time_this_iter_s: 6.2693562507629395\n",
      "time_total_s: 2863.5434391498566\n",
      "timers:\n",
      "  learn_throughput: 717.391\n",
      "  learn_time_ms: 5575.76\n",
      "  load_throughput: 6395462.204\n",
      "  load_time_ms: 0.625\n",
      "  sample_throughput: 622.641\n",
      "  sample_time_ms: 6424.251\n",
      "  update_time_ms: 3.896\n",
      "timestamp: 1643327943\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1692000\n",
      "training_iteration: 423\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1699968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-59-15\n",
      "done: false\n",
      "episode_len_mean: 148.62\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 28\n",
      "episodes_total: 10620\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46572196344534555\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01597214810034567\n",
      "        policy_loss: -0.0718962163788577\n",
      "        total_loss: 47.48924973169962\n",
      "        vf_explained_var: 0.1026298447450002\n",
      "        vf_loss: 47.553958371480306\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5486498510837555\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013643298578099348\n",
      "        policy_loss: -0.10473615158659716\n",
      "        total_loss: 48.26969859441122\n",
      "        vf_explained_var: 0.22377962072690327\n",
      "        vf_loss: 48.36829537073771\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5346091370781263\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014195417488915174\n",
      "        policy_loss: -0.06381541185701887\n",
      "        total_loss: 41.83635359605153\n",
      "        vf_explained_var: 0.26383116622765856\n",
      "        vf_loss: 41.89058702150981\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1699968\n",
      "  num_agent_steps_trained: 1699968\n",
      "  num_steps_sampled: 1700000\n",
      "  num_steps_trained: 1700000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 425\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 23.833333333333336\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.8\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 25.266666666666666\n",
      "  player_2: 18.266666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 9.257333333333333\n",
      "  player_1: -1.3506666666666671\n",
      "  player_2: -4.906666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -19.866666666666664\n",
      "  player_1: -36.666666666666664\n",
      "  player_2: -35.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11427407075559567\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3657559901320566\n",
      "  mean_inference_ms: 2.097389224803901\n",
      "  mean_raw_obs_processing_ms: 0.2672166709575656\n",
      "time_since_restore: 2876.436154127121\n",
      "time_this_iter_s: 6.400022268295288\n",
      "time_total_s: 2876.436154127121\n",
      "timers:\n",
      "  learn_throughput: 718.986\n",
      "  learn_time_ms: 5563.391\n",
      "  load_throughput: 6458985.948\n",
      "  load_time_ms: 0.619\n",
      "  sample_throughput: 623.517\n",
      "  sample_time_ms: 6415.224\n",
      "  update_time_ms: 3.887\n",
      "timestamp: 1643327955\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1700000\n",
      "training_iteration: 425\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1707968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-59-28\n",
      "done: false\n",
      "episode_len_mean: 151.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 10670\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.460271061261495\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013916363037812213\n",
      "        policy_loss: -0.06878207755585511\n",
      "        total_loss: 43.12595846176148\n",
      "        vf_explained_var: 0.17916528801123302\n",
      "        vf_loss: 43.18847809155782\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.519751405219237\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015043124639742018\n",
      "        policy_loss: -0.0643173109437339\n",
      "        total_loss: 34.29058184305827\n",
      "        vf_explained_var: 0.027660070061683653\n",
      "        vf_loss: 34.348129733403525\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5348546851674716\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012828224549099284\n",
      "        policy_loss: -0.04341731925650189\n",
      "        total_loss: 42.06110086917877\n",
      "        vf_explained_var: 0.15457291662693023\n",
      "        vf_loss: 42.09585910876592\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1707968\n",
      "  num_agent_steps_trained: 1707968\n",
      "  num_steps_sampled: 1708000\n",
      "  num_steps_trained: 1708000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 427\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 23.833333333333332\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.8\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 25.266666666666666\n",
      "  player_2: 24.133333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 8.101333333333333\n",
      "  player_1: -1.5306666666666666\n",
      "  player_2: -3.5706666666666673\n",
      "policy_reward_min:\n",
      "  player_0: -22.866666666666664\n",
      "  player_1: -36.666666666666664\n",
      "  player_2: -30.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11434535581699987\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36585883350836157\n",
      "  mean_inference_ms: 2.097804390203969\n",
      "  mean_raw_obs_processing_ms: 0.2672102425873579\n",
      "time_since_restore: 2889.226496696472\n",
      "time_this_iter_s: 6.393482208251953\n",
      "time_total_s: 2889.226496696472\n",
      "timers:\n",
      "  learn_throughput: 718.216\n",
      "  learn_time_ms: 5569.353\n",
      "  load_throughput: 6422146.685\n",
      "  load_time_ms: 0.623\n",
      "  sample_throughput: 622.434\n",
      "  sample_time_ms: 6426.385\n",
      "  update_time_ms: 5.245\n",
      "timestamp: 1643327968\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1708000\n",
      "training_iteration: 427\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1715968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-59-41\n",
      "done: false\n",
      "episode_len_mean: 154.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 10724\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4767575414975484\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015448913074045171\n",
      "        policy_loss: -0.09327174821868539\n",
      "        total_loss: 40.43253650665283\n",
      "        vf_explained_var: -0.013299416700998943\n",
      "        vf_loss: 40.518856035868325\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5429624285300573\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015591526991574184\n",
      "        policy_loss: -0.07333737428920964\n",
      "        total_loss: 29.907974398930868\n",
      "        vf_explained_var: 0.30867818315823875\n",
      "        vf_loss: 29.974295797348024\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48083083987236025\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012215357884222773\n",
      "        policy_loss: -0.01673273665830493\n",
      "        total_loss: 29.029778051376343\n",
      "        vf_explained_var: 0.21773042798042297\n",
      "        vf_loss: 29.03826526006063\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1715968\n",
      "  num_agent_steps_trained: 1715968\n",
      "  num_steps_sampled: 1716000\n",
      "  num_steps_trained: 1716000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 429\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.8625\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.8125\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.0\n",
      "  player_1: 20.599999999999998\n",
      "  player_2: 24.133333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 8.264\n",
      "  player_1: -2.246\n",
      "  player_2: -3.0180000000000002\n",
      "policy_reward_min:\n",
      "  player_0: -22.866666666666664\n",
      "  player_1: -28.93333333333333\n",
      "  player_2: -26.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11436142128265699\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3659455365754261\n",
      "  mean_inference_ms: 2.098250494966801\n",
      "  mean_raw_obs_processing_ms: 0.26736517158997264\n",
      "time_since_restore: 2902.018369436264\n",
      "time_this_iter_s: 6.2508111000061035\n",
      "time_total_s: 2902.018369436264\n",
      "timers:\n",
      "  learn_throughput: 718.833\n",
      "  learn_time_ms: 5564.573\n",
      "  load_throughput: 6389373.143\n",
      "  load_time_ms: 0.626\n",
      "  sample_throughput: 620.373\n",
      "  sample_time_ms: 6447.737\n",
      "  update_time_ms: 5.23\n",
      "timestamp: 1643327981\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1716000\n",
      "training_iteration: 429\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1723968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_00-59-54\n",
      "done: false\n",
      "episode_len_mean: 150.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 10774\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49239931176106133\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016948627745443742\n",
      "        policy_loss: -0.047536107500394185\n",
      "        total_loss: 21.51173330148061\n",
      "        vf_explained_var: 0.26440199156602223\n",
      "        vf_loss: 21.55164246082306\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5651667658487955\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016548132601309464\n",
      "        policy_loss: -0.09021996528531115\n",
      "        total_loss: 37.374733238220216\n",
      "        vf_explained_var: -0.08437771519025167\n",
      "        vf_loss: 37.45750620206197\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5118191382288932\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012774111828356543\n",
      "        policy_loss: -0.11302490844701728\n",
      "        total_loss: 27.11487825234731\n",
      "        vf_explained_var: 0.014248560865720112\n",
      "        vf_loss: 27.219280967712404\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1723968\n",
      "  num_agent_steps_trained: 1723968\n",
      "  num_steps_sampled: 1724000\n",
      "  num_steps_trained: 1724000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 431\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.387500000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.8\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 35.0\n",
      "  player_1: 23.800000000000004\n",
      "  player_2: 20.4\n",
      "policy_reward_mean:\n",
      "  player_0: 8.574666666666667\n",
      "  player_1: -2.2873333333333328\n",
      "  player_2: -3.2873333333333337\n",
      "policy_reward_min:\n",
      "  player_0: -14.733333333333334\n",
      "  player_1: -28.666666666666664\n",
      "  player_2: -30.066666666666666\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1142780970892075\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3657596977745745\n",
      "  mean_inference_ms: 2.0971949232742926\n",
      "  mean_raw_obs_processing_ms: 0.267433240324574\n",
      "time_since_restore: 2914.74435710907\n",
      "time_this_iter_s: 6.30413293838501\n",
      "time_total_s: 2914.74435710907\n",
      "timers:\n",
      "  learn_throughput: 718.287\n",
      "  learn_time_ms: 5568.801\n",
      "  load_throughput: 6363442.443\n",
      "  load_time_ms: 0.629\n",
      "  sample_throughput: 624.03\n",
      "  sample_time_ms: 6409.944\n",
      "  update_time_ms: 5.257\n",
      "timestamp: 1643327994\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1724000\n",
      "training_iteration: 431\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1731968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-00-07\n",
      "done: false\n",
      "episode_len_mean: 149.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 10830\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46211285402377444\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015002338522729891\n",
      "        policy_loss: -0.03537551589310169\n",
      "        total_loss: 42.69837160746256\n",
      "        vf_explained_var: 0.27205381174882254\n",
      "        vf_loss: 42.72699557622274\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5388360102971395\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014957688574995156\n",
      "        policy_loss: -0.08741564027033746\n",
      "        total_loss: 59.7640255133311\n",
      "        vf_explained_var: 0.20879732569058737\n",
      "        vf_loss: 59.84471030235291\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49335246642430625\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0137075693398765\n",
      "        policy_loss: -0.09032733044276635\n",
      "        total_loss: 33.85221416473389\n",
      "        vf_explained_var: 0.25143146216869355\n",
      "        vf_loss: 33.9332888062795\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1731968\n",
      "  num_agent_steps_trained: 1731968\n",
      "  num_steps_sampled: 1732000\n",
      "  num_steps_trained: 1732000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 433\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.400000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.89999999999999\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 35.0\n",
      "  player_1: 24.666666666666664\n",
      "  player_2: 23.2\n",
      "policy_reward_mean:\n",
      "  player_0: 10.792666666666667\n",
      "  player_1: -3.4673333333333343\n",
      "  player_2: -4.325333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -15.8\n",
      "  player_1: -30.866666666666667\n",
      "  player_2: -36.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11421514245959959\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36535727952160585\n",
      "  mean_inference_ms: 2.0970190192618303\n",
      "  mean_raw_obs_processing_ms: 0.267138906810351\n",
      "time_since_restore: 2927.3941292762756\n",
      "time_this_iter_s: 6.200405836105347\n",
      "time_total_s: 2927.3941292762756\n",
      "timers:\n",
      "  learn_throughput: 718.504\n",
      "  learn_time_ms: 5567.123\n",
      "  load_throughput: 6324104.188\n",
      "  load_time_ms: 0.633\n",
      "  sample_throughput: 617.788\n",
      "  sample_time_ms: 6474.709\n",
      "  update_time_ms: 5.298\n",
      "timestamp: 1643328007\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1732000\n",
      "training_iteration: 433\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1739970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-00-20\n",
      "done: false\n",
      "episode_len_mean: 146.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 10885\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4946760453780492\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014806001662191193\n",
      "        policy_loss: -0.13531655705844362\n",
      "        total_loss: 30.23667033513387\n",
      "        vf_explained_var: 0.11149224877357483\n",
      "        vf_loss: 30.365324141184487\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.540310626924038\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015648297248223267\n",
      "        policy_loss: -0.07593087007602056\n",
      "        total_loss: 53.3846467812856\n",
      "        vf_explained_var: 0.06905634880065918\n",
      "        vf_loss: 53.45353591601054\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49165928463141123\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01357236106678632\n",
      "        policy_loss: -0.02905538897961378\n",
      "        total_loss: 46.23651660760244\n",
      "        vf_explained_var: 0.18062953372796375\n",
      "        vf_loss: 46.25641085942586\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1739970\n",
      "  num_agent_steps_trained: 1739970\n",
      "  num_steps_sampled: 1740000\n",
      "  num_steps_trained: 1740000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 435\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.385714285714283\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.88571428571428\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.0\n",
      "  player_1: 25.0\n",
      "  player_2: 23.2\n",
      "policy_reward_mean:\n",
      "  player_0: 11.39\n",
      "  player_1: -3.2200000000000006\n",
      "  player_2: -5.17\n",
      "policy_reward_min:\n",
      "  player_0: -15.8\n",
      "  player_1: -30.866666666666667\n",
      "  player_2: -47.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1140024924546928\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36517498901439294\n",
      "  mean_inference_ms: 2.0959452834847805\n",
      "  mean_raw_obs_processing_ms: 0.2670840530331722\n",
      "time_since_restore: 2939.9215388298035\n",
      "time_this_iter_s: 6.2165610790252686\n",
      "time_total_s: 2939.9215388298035\n",
      "timers:\n",
      "  learn_throughput: 720.728\n",
      "  learn_time_ms: 5549.943\n",
      "  load_throughput: 6223695.515\n",
      "  load_time_ms: 0.643\n",
      "  sample_throughput: 620.628\n",
      "  sample_time_ms: 6445.083\n",
      "  update_time_ms: 5.325\n",
      "timestamp: 1643328020\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1740000\n",
      "training_iteration: 435\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1747968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-00-33\n",
      "done: false\n",
      "episode_len_mean: 155.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 10935\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48192471702893575\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016176237353450536\n",
      "        policy_loss: -0.02492536217905581\n",
      "        total_loss: 37.63427525520325\n",
      "        vf_explained_var: 0.2135441360870997\n",
      "        vf_loss: 37.65192172050476\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5471529640754064\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015206528177101669\n",
      "        policy_loss: -0.08873240377909193\n",
      "        total_loss: 45.759697087605794\n",
      "        vf_explained_var: 0.08096153676509857\n",
      "        vf_loss: 45.84158700942993\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4680341017246246\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011758989493064292\n",
      "        policy_loss: -0.1034869488577048\n",
      "        total_loss: 33.39648653825124\n",
      "        vf_explained_var: 0.2581576653321584\n",
      "        vf_loss: 33.49203605651856\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1747968\n",
      "  num_agent_steps_trained: 1747968\n",
      "  num_steps_sampled: 1748000\n",
      "  num_steps_trained: 1748000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 437\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.312500000000004\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.8\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.666666666666664\n",
      "  player_1: 25.4\n",
      "  player_2: 26.866666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 9.146\n",
      "  player_1: -1.9539999999999995\n",
      "  player_2: -4.192\n",
      "policy_reward_min:\n",
      "  player_0: -17.2\n",
      "  player_1: -26.333333333333332\n",
      "  player_2: -47.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.114218374071383\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3652628781554758\n",
      "  mean_inference_ms: 2.0959613525710252\n",
      "  mean_raw_obs_processing_ms: 0.26713793178298006\n",
      "time_since_restore: 2952.7183504104614\n",
      "time_this_iter_s: 6.347181558609009\n",
      "time_total_s: 2952.7183504104614\n",
      "timers:\n",
      "  learn_throughput: 721.785\n",
      "  learn_time_ms: 5541.82\n",
      "  load_throughput: 6193139.904\n",
      "  load_time_ms: 0.646\n",
      "  sample_throughput: 621.438\n",
      "  sample_time_ms: 6436.683\n",
      "  update_time_ms: 3.959\n",
      "timestamp: 1643328033\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1748000\n",
      "training_iteration: 437\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1755969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-00-46\n",
      "done: false\n",
      "episode_len_mean: 164.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 10982\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48686260481675464\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015456460269119248\n",
      "        policy_loss: -0.061809261739253996\n",
      "        total_loss: 28.21004467646281\n",
      "        vf_explained_var: 0.40184921443462374\n",
      "        vf_loss: 28.264898454348245\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5511164131760597\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0172141504050281\n",
      "        policy_loss: -0.07082436102132003\n",
      "        total_loss: 27.112256032625833\n",
      "        vf_explained_var: 0.2545350752274195\n",
      "        vf_loss: 27.175333870251972\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4729193607966105\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010930873418310512\n",
      "        policy_loss: -0.09992788931975762\n",
      "        total_loss: 37.22942662080129\n",
      "        vf_explained_var: 0.09743573208649953\n",
      "        vf_loss: 37.321976437568665\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1755969\n",
      "  num_agent_steps_trained: 1755969\n",
      "  num_steps_sampled: 1756000\n",
      "  num_steps_trained: 1756000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 439\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.011111111111106\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.9\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.666666666666664\n",
      "  player_1: 25.4\n",
      "  player_2: 26.866666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 8.434666666666667\n",
      "  player_1: -1.869333333333333\n",
      "  player_2: -3.5653333333333337\n",
      "policy_reward_min:\n",
      "  player_0: -17.2\n",
      "  player_1: -25.333333333333336\n",
      "  player_2: -33.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11383421522562324\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36495616753314747\n",
      "  mean_inference_ms: 2.0947004881009135\n",
      "  mean_raw_obs_processing_ms: 0.26693290177999346\n",
      "time_since_restore: 2966.3395228385925\n",
      "time_this_iter_s: 7.080407381057739\n",
      "time_total_s: 2966.3395228385925\n",
      "timers:\n",
      "  learn_throughput: 710.095\n",
      "  learn_time_ms: 5633.05\n",
      "  load_throughput: 5935895.839\n",
      "  load_time_ms: 0.674\n",
      "  sample_throughput: 621.791\n",
      "  sample_time_ms: 6433.028\n",
      "  update_time_ms: 4.023\n",
      "timestamp: 1643328046\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1756000\n",
      "training_iteration: 439\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1763970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-00-59\n",
      "done: false\n",
      "episode_len_mean: 153.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 11038\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48502684275309244\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014395158836072574\n",
      "        policy_loss: -0.010270319481690725\n",
      "        total_loss: 50.80897081375122\n",
      "        vf_explained_var: 0.20229767521222433\n",
      "        vf_loss: 50.8127632522583\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5746379095315933\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015097407337598269\n",
      "        policy_loss: -0.02554883484651024\n",
      "        total_loss: 32.56310338973999\n",
      "        vf_explained_var: -0.2566240984201431\n",
      "        vf_loss: 32.58185812950134\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48452807436386747\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013054251396864251\n",
      "        policy_loss: -0.14671364215823512\n",
      "        total_loss: 50.86310985247294\n",
      "        vf_explained_var: 0.20415904680887859\n",
      "        vf_loss: 51.001011238098144\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1763970\n",
      "  num_agent_steps_trained: 1763970\n",
      "  num_steps_sampled: 1764000\n",
      "  num_steps_trained: 1764000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 441\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.211111111111112\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.8\n",
      "  vram_util_percent0: 0.019694010416666668\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.0\n",
      "  player_1: 16.733333333333327\n",
      "  player_2: 18.46666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 9.913333333333334\n",
      "  player_1: -3.094666666666667\n",
      "  player_2: -3.8186666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -16.666666666666668\n",
      "  player_1: -26.8\n",
      "  player_2: -33.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11389387408585055\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36465133397050964\n",
      "  mean_inference_ms: 2.094726547228636\n",
      "  mean_raw_obs_processing_ms: 0.2669314505788663\n",
      "time_since_restore: 2979.211948156357\n",
      "time_this_iter_s: 6.419894456863403\n",
      "time_total_s: 2979.211948156357\n",
      "timers:\n",
      "  learn_throughput: 708.184\n",
      "  learn_time_ms: 5648.251\n",
      "  load_throughput: 5902897.755\n",
      "  load_time_ms: 0.678\n",
      "  sample_throughput: 614.113\n",
      "  sample_time_ms: 6513.46\n",
      "  update_time_ms: 4.041\n",
      "timestamp: 1643328059\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1764000\n",
      "training_iteration: 441\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1771968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-01-15\n",
      "done: false\n",
      "episode_len_mean: 149.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 11088\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49704979757467904\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015468834151445966\n",
      "        policy_loss: -0.08763501773277918\n",
      "        total_loss: 29.863920685450235\n",
      "        vf_explained_var: 0.20914320131142936\n",
      "        vf_loss: 29.944594659805297\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5573174223303795\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014566331688311038\n",
      "        policy_loss: -0.09591712141875178\n",
      "        total_loss: 27.218812624613445\n",
      "        vf_explained_var: -0.18036798497041068\n",
      "        vf_loss: 27.308174937566122\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49057544191678365\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014455113361194283\n",
      "        policy_loss: -0.0477561254799366\n",
      "        total_loss: 29.14108397801717\n",
      "        vf_explained_var: 0.2805125703414281\n",
      "        vf_loss: 29.179083102544148\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1771968\n",
      "  num_agent_steps_trained: 1771968\n",
      "  num_steps_sampled: 1772000\n",
      "  num_steps_trained: 1772000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 443\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.75\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.8\n",
      "  vram_util_percent0: 0.07737630208333335\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 20.133333333333333\n",
      "  player_2: 21.8\n",
      "policy_reward_mean:\n",
      "  player_0: 9.758\n",
      "  player_1: -2.6520000000000006\n",
      "  player_2: -4.106\n",
      "policy_reward_min:\n",
      "  player_0: -15.666666666666668\n",
      "  player_1: -27.333333333333336\n",
      "  player_2: -32.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11406579629089265\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3650669638389135\n",
      "  mean_inference_ms: 2.096660521884325\n",
      "  mean_raw_obs_processing_ms: 0.26697908301333745\n",
      "time_since_restore: 2994.5425143241882\n",
      "time_this_iter_s: 8.300931453704834\n",
      "time_total_s: 2994.5425143241882\n",
      "timers:\n",
      "  learn_throughput: 681.857\n",
      "  learn_time_ms: 5866.337\n",
      "  load_throughput: 5984595.848\n",
      "  load_time_ms: 0.668\n",
      "  sample_throughput: 607.871\n",
      "  sample_time_ms: 6580.34\n",
      "  update_time_ms: 4.13\n",
      "timestamp: 1643328075\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1772000\n",
      "training_iteration: 443\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1779968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-01-29\n",
      "done: false\n",
      "episode_len_mean: 151.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 11143\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5028610741098721\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014242919778286402\n",
      "        policy_loss: -0.10514391620953878\n",
      "        total_loss: 27.050149358113607\n",
      "        vf_explained_var: 0.11689024885495504\n",
      "        vf_loss: 27.14888389587402\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5469694260756175\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012906872344004417\n",
      "        policy_loss: -0.03487279145978391\n",
      "        total_loss: 32.78366646607717\n",
      "        vf_explained_var: 0.31449274798234306\n",
      "        vf_loss: 32.812731218338016\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5016210406025251\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012189429255934858\n",
      "        policy_loss: -0.09814916534038881\n",
      "        total_loss: 23.58273892879486\n",
      "        vf_explained_var: 0.1867896968126297\n",
      "        vf_loss: 23.67266021410624\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1779968\n",
      "  num_agent_steps_trained: 1779968\n",
      "  num_steps_sampled: 1780000\n",
      "  num_steps_trained: 1780000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 445\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.3125\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.8125\n",
      "  vram_util_percent0: 0.0986328125\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 20.133333333333333\n",
      "  player_2: 21.8\n",
      "policy_reward_mean:\n",
      "  player_0: 10.029333333333332\n",
      "  player_1: -3.3846666666666665\n",
      "  player_2: -3.644666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -16.2\n",
      "  player_1: -39.666666666666664\n",
      "  player_2: -25.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11423884154417785\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3652225880243064\n",
      "  mean_inference_ms: 2.098303013092768\n",
      "  mean_raw_obs_processing_ms: 0.26715593809651106\n",
      "time_since_restore: 3008.716602087021\n",
      "time_this_iter_s: 6.39612078666687\n",
      "time_total_s: 3008.716602087021\n",
      "timers:\n",
      "  learn_throughput: 666.77\n",
      "  learn_time_ms: 5999.073\n",
      "  load_throughput: 6070343.73\n",
      "  load_time_ms: 0.659\n",
      "  sample_throughput: 580.087\n",
      "  sample_time_ms: 6895.522\n",
      "  update_time_ms: 4.02\n",
      "timestamp: 1643328089\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1780000\n",
      "training_iteration: 445\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1787969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-01-42\n",
      "done: false\n",
      "episode_len_mean: 147.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 28\n",
      "episodes_total: 11198\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4644274472196897\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011802761323197046\n",
      "        policy_loss: -0.10660078323135773\n",
      "        total_loss: 42.22236848036448\n",
      "        vf_explained_var: 0.25697437405586243\n",
      "        vf_loss: 42.32365782101949\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5432540485262871\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015448700262095372\n",
      "        policy_loss: -0.052530406814378997\n",
      "        total_loss: 35.12865882078807\n",
      "        vf_explained_var: 0.04000340938568115\n",
      "        vf_loss: 35.17423720836639\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4577494885524114\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0110151430781904\n",
      "        policy_loss: -0.05169030893594027\n",
      "        total_loss: 40.05375988960266\n",
      "        vf_explained_var: 0.19283492823441822\n",
      "        vf_loss: 40.0980149491628\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1787969\n",
      "  num_agent_steps_trained: 1787969\n",
      "  num_steps_sampled: 1788000\n",
      "  num_steps_trained: 1788000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 447\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.287499999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.9\n",
      "  vram_util_percent0: 0.0986328125\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 37.666666666666664\n",
      "  player_1: 16.866666666666667\n",
      "  player_2: 23.133333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 10.109333333333332\n",
      "  player_1: -3.6946666666666657\n",
      "  player_2: -3.414666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -13.066666666666666\n",
      "  player_1: -39.666666666666664\n",
      "  player_2: -24.666666666666668\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1142950138990761\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3653617770482745\n",
      "  mean_inference_ms: 2.0999871397561707\n",
      "  mean_raw_obs_processing_ms: 0.267424492241495\n",
      "time_since_restore: 3021.6957733631134\n",
      "time_this_iter_s: 6.535141468048096\n",
      "time_total_s: 3021.6957733631134\n",
      "timers:\n",
      "  learn_throughput: 665.302\n",
      "  learn_time_ms: 6012.305\n",
      "  load_throughput: 6163788.53\n",
      "  load_time_ms: 0.649\n",
      "  sample_throughput: 579.005\n",
      "  sample_time_ms: 6908.406\n",
      "  update_time_ms: 4.064\n",
      "timestamp: 1643328102\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1788000\n",
      "training_iteration: 447\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1795970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-01-55\n",
      "done: false\n",
      "episode_len_mean: 153.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 28\n",
      "episodes_total: 11250\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5124832645058632\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018374691799650726\n",
      "        policy_loss: -0.10053590333710115\n",
      "        total_loss: 40.77010340690613\n",
      "        vf_explained_var: 0.28171176731586456\n",
      "        vf_loss: 40.862370670636494\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5695557147264481\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0170538682667393\n",
      "        policy_loss: -0.06461507851878802\n",
      "        total_loss: 38.026378043492635\n",
      "        vf_explained_var: 0.09771639843781789\n",
      "        vf_loss: 38.083319101333615\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45351650337378185\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012202195508355848\n",
      "        policy_loss: -0.06476584480454524\n",
      "        total_loss: 28.06068974018097\n",
      "        vf_explained_var: 0.2330487992366155\n",
      "        vf_loss: 28.117219109535217\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1795970\n",
      "  num_agent_steps_trained: 1795970\n",
      "  num_steps_sampled: 1796000\n",
      "  num_steps_trained: 1796000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 449\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.9\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 56.0\n",
      "  vram_util_percent0: 0.10367838541666667\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 37.666666666666664\n",
      "  player_1: 19.666666666666668\n",
      "  player_2: 23.133333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 10.522\n",
      "  player_1: -3.5079999999999996\n",
      "  player_2: -4.013999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -18.333333333333332\n",
      "  player_1: -27.93333333333333\n",
      "  player_2: -24.666666666666668\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11437943415429612\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36525113660872294\n",
      "  mean_inference_ms: 2.1002819375591226\n",
      "  mean_raw_obs_processing_ms: 0.26713142439374404\n",
      "time_since_restore: 3034.9270391464233\n",
      "time_this_iter_s: 6.312956809997559\n",
      "time_total_s: 3034.9270391464233\n",
      "timers:\n",
      "  learn_throughput: 671.939\n",
      "  learn_time_ms: 5952.922\n",
      "  load_throughput: 6509356.716\n",
      "  load_time_ms: 0.615\n",
      "  sample_throughput: 574.719\n",
      "  sample_time_ms: 6959.919\n",
      "  update_time_ms: 3.921\n",
      "timestamp: 1643328115\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1796000\n",
      "training_iteration: 449\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1803969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-02-08\n",
      "done: false\n",
      "episode_len_mean: 154.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 11301\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4901167555650075\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016065732474004713\n",
      "        policy_loss: -0.06405246049476167\n",
      "        total_loss: 32.594811334609986\n",
      "        vf_explained_var: 0.18676258782545727\n",
      "        vf_loss: 32.651634302139286\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.536586432158947\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013765309607744408\n",
      "        policy_loss: -0.07151843042578548\n",
      "        total_loss: 46.78799064000447\n",
      "        vf_explained_var: 0.2699297058582306\n",
      "        vf_loss: 46.85331515312195\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4836570409933726\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011452790756883621\n",
      "        policy_loss: -0.029160943906754257\n",
      "        total_loss: 47.82982063611348\n",
      "        vf_explained_var: 0.45727815667788185\n",
      "        vf_loss: 47.85125083605448\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1803969\n",
      "  num_agent_steps_trained: 1803969\n",
      "  num_steps_sampled: 1804000\n",
      "  num_steps_trained: 1804000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 451\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.2625\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.9\n",
      "  vram_util_percent0: 0.10286458333333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 24.0\n",
      "  player_2: 24.333333333333332\n",
      "policy_reward_mean:\n",
      "  player_0: 7.728\n",
      "  player_1: -1.6499999999999997\n",
      "  player_2: -3.0779999999999994\n",
      "policy_reward_min:\n",
      "  player_0: -19.666666666666668\n",
      "  player_1: -29.400000000000002\n",
      "  player_2: -34.733333333333334\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11445055467914925\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36527670382662764\n",
      "  mean_inference_ms: 2.100625200537568\n",
      "  mean_raw_obs_processing_ms: 0.26742607555546394\n",
      "time_since_restore: 3047.9213452339172\n",
      "time_this_iter_s: 6.32107138633728\n",
      "time_total_s: 3047.9213452339172\n",
      "timers:\n",
      "  learn_throughput: 670.816\n",
      "  learn_time_ms: 5962.887\n",
      "  load_throughput: 6441379.099\n",
      "  load_time_ms: 0.621\n",
      "  sample_throughput: 578.818\n",
      "  sample_time_ms: 6910.634\n",
      "  update_time_ms: 3.825\n",
      "timestamp: 1643328128\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1804000\n",
      "training_iteration: 451\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1811971\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-02-21\n",
      "done: false\n",
      "episode_len_mean: 141.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 30\n",
      "episodes_total: 11358\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4647321731845538\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014193561757559413\n",
      "        policy_loss: -0.06536351284012198\n",
      "        total_loss: 43.847575847307844\n",
      "        vf_explained_var: 0.0980601845184962\n",
      "        vf_loss: 43.90655256430308\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5585592638452848\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013699955489729139\n",
      "        policy_loss: -0.07705458397278563\n",
      "        total_loss: 37.34028612295786\n",
      "        vf_explained_var: 0.25179894030094147\n",
      "        vf_loss: 37.41117604573568\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4901906751592954\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011912259021046338\n",
      "        policy_loss: -0.04738116118436058\n",
      "        total_loss: 32.82120989322662\n",
      "        vf_explained_var: 0.21461403210957844\n",
      "        vf_loss: 32.86055032571157\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1811971\n",
      "  num_agent_steps_trained: 1811971\n",
      "  num_steps_sampled: 1812000\n",
      "  num_steps_trained: 1812000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 453\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.875\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.9\n",
      "  vram_util_percent0: 0.10286458333333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.333333333333336\n",
      "  player_1: 24.0\n",
      "  player_2: 24.333333333333332\n",
      "policy_reward_mean:\n",
      "  player_0: 8.248000000000001\n",
      "  player_1: -3.22\n",
      "  player_2: -2.0280000000000005\n",
      "policy_reward_min:\n",
      "  player_0: -23.866666666666664\n",
      "  player_1: -29.666666666666668\n",
      "  player_2: -34.733333333333334\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1139877433006021\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36503407252171427\n",
      "  mean_inference_ms: 2.09872025716593\n",
      "  mean_raw_obs_processing_ms: 0.2673643395220514\n",
      "time_since_restore: 3060.9879286289215\n",
      "time_this_iter_s: 6.317183971405029\n",
      "time_total_s: 3060.9879286289215\n",
      "timers:\n",
      "  learn_throughput: 690.526\n",
      "  learn_time_ms: 5792.687\n",
      "  load_throughput: 6320530.44\n",
      "  load_time_ms: 0.633\n",
      "  sample_throughput: 586.072\n",
      "  sample_time_ms: 6825.096\n",
      "  update_time_ms: 3.74\n",
      "timestamp: 1643328141\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1812000\n",
      "training_iteration: 453\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1819972\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-02-35\n",
      "done: false\n",
      "episode_len_mean: 134.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 34\n",
      "episodes_total: 11420\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4601129150390625\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014484966203890509\n",
      "        policy_loss: -0.09869856469333171\n",
      "        total_loss: 54.27204806645711\n",
      "        vf_explained_var: 0.10255912443002065\n",
      "        vf_loss: 54.36422864596049\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5638594643274943\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016630259016114146\n",
      "        policy_loss: -0.06961600627750159\n",
      "        total_loss: 29.08676015218099\n",
      "        vf_explained_var: 0.3237011454502742\n",
      "        vf_loss: 29.148892601331074\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5100839739044507\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012778546122547899\n",
      "        policy_loss: -0.060266191338499385\n",
      "        total_loss: 43.69661803881327\n",
      "        vf_explained_var: 0.1560007752974828\n",
      "        vf_loss: 43.74825875282288\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1819972\n",
      "  num_agent_steps_trained: 1819972\n",
      "  num_steps_sampled: 1820000\n",
      "  num_steps_trained: 1820000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 455\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.233333333333334\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.9\n",
      "  vram_util_percent0: 0.10286458333333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 28.333333333333336\n",
      "  player_2: 18.933333333333334\n",
      "policy_reward_mean:\n",
      "  player_0: 7.5986666666666665\n",
      "  player_1: -1.3133333333333332\n",
      "  player_2: -3.285333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -23.866666666666664\n",
      "  player_1: -29.666666666666668\n",
      "  player_2: -39.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11405301606793639\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3651631545564218\n",
      "  mean_inference_ms: 2.099395742889942\n",
      "  mean_raw_obs_processing_ms: 0.26738183831944584\n",
      "time_since_restore: 3074.534179210663\n",
      "time_this_iter_s: 6.767481088638306\n",
      "time_total_s: 3074.534179210663\n",
      "timers:\n",
      "  learn_throughput: 696.85\n",
      "  learn_time_ms: 5740.118\n",
      "  load_throughput: 5724644.624\n",
      "  load_time_ms: 0.699\n",
      "  sample_throughput: 608.386\n",
      "  sample_time_ms: 6574.777\n",
      "  update_time_ms: 3.813\n",
      "timestamp: 1643328155\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1820000\n",
      "training_iteration: 455\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1827970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-02-48\n",
      "done: false\n",
      "episode_len_mean: 136.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 11473\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.468488656381766\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01579189215840112\n",
      "        policy_loss: -0.0788701914747556\n",
      "        total_loss: 36.817323390642805\n",
      "        vf_explained_var: 0.24218614339828493\n",
      "        vf_loss: 36.88908712863922\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5748033598065376\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015609739491300042\n",
      "        policy_loss: -0.11755261195202668\n",
      "        total_loss: 40.09037338574728\n",
      "        vf_explained_var: -0.029886099894841513\n",
      "        vf_loss: 40.200901692708335\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5057187050580978\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014540778036850194\n",
      "        policy_loss: -0.027100365925580263\n",
      "        total_loss: 42.65299251874288\n",
      "        vf_explained_var: 0.28958378354708353\n",
      "        vf_loss: 42.67027795791626\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1827970\n",
      "  num_agent_steps_trained: 1827970\n",
      "  num_steps_sampled: 1828000\n",
      "  num_steps_trained: 1828000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 457\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.8\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 56.0\n",
      "  vram_util_percent0: 0.10286458333333333\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.666666666666664\n",
      "  player_1: 30.0\n",
      "  player_2: 16.133333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 9.81\n",
      "  player_1: -0.43599999999999994\n",
      "  player_2: -6.374\n",
      "policy_reward_min:\n",
      "  player_0: -21.199999999999996\n",
      "  player_1: -27.333333333333332\n",
      "  player_2: -39.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11431023059987444\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.365196970971529\n",
      "  mean_inference_ms: 2.100882852898595\n",
      "  mean_raw_obs_processing_ms: 0.26778206217643435\n",
      "time_since_restore: 3087.497859477997\n",
      "time_this_iter_s: 6.26926064491272\n",
      "time_total_s: 3087.497859477997\n",
      "timers:\n",
      "  learn_throughput: 696.614\n",
      "  learn_time_ms: 5742.061\n",
      "  load_throughput: 5631261.034\n",
      "  load_time_ms: 0.71\n",
      "  sample_throughput: 602.722\n",
      "  sample_time_ms: 6636.561\n",
      "  update_time_ms: 3.713\n",
      "timestamp: 1643328168\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1828000\n",
      "training_iteration: 457\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1835968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-03-02\n",
      "done: false\n",
      "episode_len_mean: 150.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 11525\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47294864694277444\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014147016214252895\n",
      "        policy_loss: -0.08180416836558531\n",
      "        total_loss: 24.298069696426392\n",
      "        vf_explained_var: 0.10751628875732422\n",
      "        vf_loss: 24.373507590293883\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5712749982873598\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014545688628168137\n",
      "        policy_loss: -0.07844574672169984\n",
      "        total_loss: 19.52403510093689\n",
      "        vf_explained_var: -0.043946678042411806\n",
      "        vf_loss: 19.5959353240331\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4868953975041707\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012958352469468082\n",
      "        policy_loss: -0.10408792560299238\n",
      "        total_loss: 22.435403378804526\n",
      "        vf_explained_var: 0.35083847443262733\n",
      "        vf_loss: 22.530744439760845\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1835968\n",
      "  num_agent_steps_trained: 1835968\n",
      "  num_steps_sampled: 1836000\n",
      "  num_steps_trained: 1836000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 459\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.533333333333335\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.722222222222214\n",
      "  vram_util_percent0: 0.09966362847222224\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.666666666666664\n",
      "  player_1: 30.0\n",
      "  player_2: 19.8\n",
      "policy_reward_mean:\n",
      "  player_0: 10.083999999999998\n",
      "  player_1: -1.3460000000000003\n",
      "  player_2: -5.7379999999999995\n",
      "policy_reward_min:\n",
      "  player_0: -20.0\n",
      "  player_1: -27.333333333333332\n",
      "  player_2: -31.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1145330747153019\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36520349582608225\n",
      "  mean_inference_ms: 2.102552004227342\n",
      "  mean_raw_obs_processing_ms: 0.2678500499124654\n",
      "time_since_restore: 3101.6890721321106\n",
      "time_this_iter_s: 7.199265718460083\n",
      "time_total_s: 3101.6890721321106\n",
      "timers:\n",
      "  learn_throughput: 683.124\n",
      "  learn_time_ms: 5855.455\n",
      "  load_throughput: 5517369.113\n",
      "  load_time_ms: 0.725\n",
      "  sample_throughput: 603.96\n",
      "  sample_time_ms: 6622.96\n",
      "  update_time_ms: 3.889\n",
      "timestamp: 1643328182\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1836000\n",
      "training_iteration: 459\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1843968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-03-15\n",
      "done: false\n",
      "episode_len_mean: 149.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 31\n",
      "episodes_total: 11582\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4840211124221484\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014917042927615209\n",
      "        policy_loss: -0.1213246240032216\n",
      "        total_loss: 26.611232113838195\n",
      "        vf_explained_var: 0.3186496561765671\n",
      "        vf_loss: 26.725844073295594\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6002809834480286\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014637193432438356\n",
      "        policy_loss: -0.04065032309386879\n",
      "        total_loss: 44.151232957839966\n",
      "        vf_explained_var: 0.19099975446859996\n",
      "        vf_loss: 44.18529640833537\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4627266974250476\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010286600129227696\n",
      "        policy_loss: -0.09296433996409177\n",
      "        total_loss: 35.139695274035134\n",
      "        vf_explained_var: 0.12834950268268586\n",
      "        vf_loss: 35.22571600119273\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1843968\n",
      "  num_agent_steps_trained: 1843968\n",
      "  num_steps_sampled: 1844000\n",
      "  num_steps_trained: 1844000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 461\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.462500000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 56.1\n",
      "  vram_util_percent0: 0.10172526041666667\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 28.0\n",
      "  player_1: 27.866666666666667\n",
      "  player_2: 19.8\n",
      "policy_reward_mean:\n",
      "  player_0: 9.366666666666667\n",
      "  player_1: -2.7153333333333336\n",
      "  player_2: -3.651333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -21.933333333333334\n",
      "  player_1: -31.0\n",
      "  player_2: -27.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11437732117967425\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3652416106034525\n",
      "  mean_inference_ms: 2.102322026820621\n",
      "  mean_raw_obs_processing_ms: 0.2677697016765921\n",
      "time_since_restore: 3114.8613514900208\n",
      "time_this_iter_s: 6.35565972328186\n",
      "time_total_s: 3114.8613514900208\n",
      "timers:\n",
      "  learn_throughput: 681.835\n",
      "  learn_time_ms: 5866.522\n",
      "  load_throughput: 5413926.232\n",
      "  load_time_ms: 0.739\n",
      "  sample_throughput: 595.553\n",
      "  sample_time_ms: 6716.452\n",
      "  update_time_ms: 4.072\n",
      "timestamp: 1643328195\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1844000\n",
      "training_iteration: 461\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1851968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-03-30\n",
      "done: false\n",
      "episode_len_mean: 141.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 11638\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47694352666536965\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015025190008545906\n",
      "        policy_loss: -0.06763593318872153\n",
      "        total_loss: 43.8948545106252\n",
      "        vf_explained_var: 0.25958778738975524\n",
      "        vf_loss: 43.95572925726572\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5572732051213583\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016270899910252108\n",
      "        policy_loss: -0.08539852550874154\n",
      "        total_loss: 55.148072465260825\n",
      "        vf_explained_var: 0.16202356815338134\n",
      "        vf_loss: 55.22614906628927\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46667632579803464\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014047267711539461\n",
      "        policy_loss: -0.056675088312476876\n",
      "        total_loss: 43.08831681887309\n",
      "        vf_explained_var: 0.294954949816068\n",
      "        vf_loss: 43.135510002772016\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1851968\n",
      "  num_agent_steps_trained: 1851968\n",
      "  num_steps_sampled: 1852000\n",
      "  num_steps_trained: 1852000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 463\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.71999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.910000000000004\n",
      "  vram_util_percent0: 0.099609375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.333333333333336\n",
      "  player_1: 27.866666666666667\n",
      "  player_2: 17.733333333333334\n",
      "policy_reward_mean:\n",
      "  player_0: 9.588\n",
      "  player_1: -2.0799999999999996\n",
      "  player_2: -4.508\n",
      "policy_reward_min:\n",
      "  player_0: -14.866666666666664\n",
      "  player_1: -31.0\n",
      "  player_2: -27.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11420895434176628\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36498040548004973\n",
      "  mean_inference_ms: 2.100939683350009\n",
      "  mean_raw_obs_processing_ms: 0.26750125986134765\n",
      "time_since_restore: 3129.6717641353607\n",
      "time_this_iter_s: 7.956878662109375\n",
      "time_total_s: 3129.6717641353607\n",
      "timers:\n",
      "  learn_throughput: 663.696\n",
      "  learn_time_ms: 6026.86\n",
      "  load_throughput: 5354487.601\n",
      "  load_time_ms: 0.747\n",
      "  sample_throughput: 592.792\n",
      "  sample_time_ms: 6747.724\n",
      "  update_time_ms: 4.067\n",
      "timestamp: 1643328210\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1852000\n",
      "training_iteration: 463\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1859968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-03-46\n",
      "done: false\n",
      "episode_len_mean: 141.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 11695\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4555423746506373\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014199564370649871\n",
      "        policy_loss: -0.08174718042835594\n",
      "        total_loss: 41.77902907212575\n",
      "        vf_explained_var: 0.15016521334648134\n",
      "        vf_loss: 41.85438640276591\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5493006852269172\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014226120758406373\n",
      "        policy_loss: -0.08119679098327955\n",
      "        total_loss: 29.912065874735514\n",
      "        vf_explained_var: 0.19764342546463012\n",
      "        vf_loss: 29.986861001650492\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5283163917064667\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01214550596026432\n",
      "        policy_loss: -0.09622111856937408\n",
      "        total_loss: 38.51590910752614\n",
      "        vf_explained_var: 0.25016382535298665\n",
      "        vf_loss: 38.603932147026065\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1859968\n",
      "  num_agent_steps_trained: 1859968\n",
      "  num_steps_sampled: 1860000\n",
      "  num_steps_trained: 1860000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 465\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.21\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 56.1\n",
      "  vram_util_percent0: 0.099609375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.333333333333336\n",
      "  player_1: 25.266666666666666\n",
      "  player_2: 19.266666666666666\n",
      "policy_reward_mean:\n",
      "  player_0: 10.386666666666667\n",
      "  player_1: -2.047333333333333\n",
      "  player_2: -5.339333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -15.400000000000002\n",
      "  player_1: -27.0\n",
      "  player_2: -32.733333333333334\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11401680727701712\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36489387684193525\n",
      "  mean_inference_ms: 2.1008086528130465\n",
      "  mean_raw_obs_processing_ms: 0.2675457032961841\n",
      "time_since_restore: 3145.616156101227\n",
      "time_this_iter_s: 7.684312343597412\n",
      "time_total_s: 3145.616156101227\n",
      "timers:\n",
      "  learn_throughput: 644.566\n",
      "  learn_time_ms: 6205.73\n",
      "  load_throughput: 5613174.077\n",
      "  load_time_ms: 0.713\n",
      "  sample_throughput: 565.745\n",
      "  sample_time_ms: 7070.322\n",
      "  update_time_ms: 4.159\n",
      "timestamp: 1643328226\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1860000\n",
      "training_iteration: 465\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1867969\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-04-03\n",
      "done: false\n",
      "episode_len_mean: 146.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 11744\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48162996957699455\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016954117438647928\n",
      "        policy_loss: -0.0700058777599285\n",
      "        total_loss: 38.3560179122289\n",
      "        vf_explained_var: 0.2941419897476832\n",
      "        vf_loss: 38.418394459088645\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5338546928763389\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015466736625642928\n",
      "        policy_loss: -0.08072065811759482\n",
      "        total_loss: 25.231065018971762\n",
      "        vf_explained_var: 0.2467153239250183\n",
      "        vf_loss: 25.304825633366903\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48123228321472805\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012473212551412871\n",
      "        policy_loss: -0.05461375700309873\n",
      "        total_loss: 33.282566471099855\n",
      "        vf_explained_var: 0.4058189775546392\n",
      "        vf_loss: 33.32876104990641\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1867969\n",
      "  num_agent_steps_trained: 1867969\n",
      "  num_steps_sampled: 1868000\n",
      "  num_steps_trained: 1868000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 467\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.919999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.8\n",
      "  vram_util_percent0: 0.10029296875\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 27.333333333333336\n",
      "  player_1: 25.266666666666666\n",
      "  player_2: 19.933333333333334\n",
      "policy_reward_mean:\n",
      "  player_0: 9.426666666666668\n",
      "  player_1: -2.527333333333333\n",
      "  player_2: -3.8993333333333338\n",
      "policy_reward_min:\n",
      "  player_0: -20.333333333333336\n",
      "  player_1: -26.0\n",
      "  player_2: -32.733333333333334\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11449038662660298\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36565318351201237\n",
      "  mean_inference_ms: 2.1071817207716776\n",
      "  mean_raw_obs_processing_ms: 0.2678910412602528\n",
      "time_since_restore: 3162.0399584770203\n",
      "time_this_iter_s: 8.142821788787842\n",
      "time_total_s: 3162.0399584770203\n",
      "timers:\n",
      "  learn_throughput: 618.206\n",
      "  learn_time_ms: 6470.332\n",
      "  load_throughput: 5502891.629\n",
      "  load_time_ms: 0.727\n",
      "  sample_throughput: 544.875\n",
      "  sample_time_ms: 7341.138\n",
      "  update_time_ms: 4.304\n",
      "timestamp: 1643328243\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1868000\n",
      "training_iteration: 467\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1875972\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-04-19\n",
      "done: false\n",
      "episode_len_mean: 155.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 11798\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4923451712727547\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0162544615403876\n",
      "        policy_loss: -0.08403460477478802\n",
      "        total_loss: 53.912085603078204\n",
      "        vf_explained_var: 0.22871281782786052\n",
      "        vf_loss: 53.988806031545\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5929110455513\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016244771353588778\n",
      "        policy_loss: -0.07200152268167585\n",
      "        total_loss: 32.371725505193076\n",
      "        vf_explained_var: 0.036888533035914106\n",
      "        vf_loss: 32.43641688664754\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5014003085096678\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013553840592901375\n",
      "        policy_loss: -0.06152747182485958\n",
      "        total_loss: 45.1070720799764\n",
      "        vf_explained_var: -0.2437110004822413\n",
      "        vf_loss: 45.15945033709208\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1875972\n",
      "  num_agent_steps_trained: 1875972\n",
      "  num_steps_sampled: 1876000\n",
      "  num_steps_trained: 1876000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 469\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.76666666666667\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.8\n",
      "  vram_util_percent0: 0.10026041666666667\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666668\n",
      "  player_1: 21.666666666666664\n",
      "  player_2: 19.933333333333334\n",
      "policy_reward_mean:\n",
      "  player_0: 7.834666666666667\n",
      "  player_1: -1.6353333333333335\n",
      "  player_2: -3.199333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -20.333333333333336\n",
      "  player_1: -29.333333333333332\n",
      "  player_2: -30.666666666666668\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11506066168517101\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3665182804352327\n",
      "  mean_inference_ms: 2.1137287500797632\n",
      "  mean_raw_obs_processing_ms: 0.2686119129716647\n",
      "time_since_restore: 3178.1082425117493\n",
      "time_this_iter_s: 7.830778360366821\n",
      "time_total_s: 3178.1082425117493\n",
      "timers:\n",
      "  learn_throughput: 607.375\n",
      "  learn_time_ms: 6585.713\n",
      "  load_throughput: 5433388.173\n",
      "  load_time_ms: 0.736\n",
      "  sample_throughput: 523.511\n",
      "  sample_time_ms: 7640.717\n",
      "  update_time_ms: 6.256\n",
      "timestamp: 1643328259\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1876000\n",
      "training_iteration: 469\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1883972\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-04-36\n",
      "done: false\n",
      "episode_len_mean: 158.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 11848\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44984427909056346\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01314112507006636\n",
      "        policy_loss: -0.03393577781816324\n",
      "        total_loss: 35.99064542134603\n",
      "        vf_explained_var: -0.035467515985171\n",
      "        vf_loss: 36.0186678981781\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5592842039465904\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014685225025362646\n",
      "        policy_loss: -0.12090906090413531\n",
      "        total_loss: 27.352075839042662\n",
      "        vf_explained_var: 0.2908413114150365\n",
      "        vf_loss: 27.466376527150473\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4971258885661761\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012678542590632181\n",
      "        policy_loss: -0.1115085664515694\n",
      "        total_loss: 38.64011049906413\n",
      "        vf_explained_var: 0.2892101244131724\n",
      "        vf_loss: 38.74306092580159\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1883972\n",
      "  num_agent_steps_trained: 1883972\n",
      "  num_steps_sampled: 1884000\n",
      "  num_steps_trained: 1884000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 471\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.71\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.64000000000001\n",
      "  vram_util_percent0: 0.099755859375\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.0\n",
      "  player_1: 21.133333333333336\n",
      "  player_2: 17.199999999999996\n",
      "policy_reward_mean:\n",
      "  player_0: 8.567999999999998\n",
      "  player_1: -2.2600000000000002\n",
      "  player_2: -3.3079999999999994\n",
      "policy_reward_min:\n",
      "  player_0: -14.666666666666666\n",
      "  player_1: -29.333333333333332\n",
      "  player_2: -30.666666666666668\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11506361197652995\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36666880601682394\n",
      "  mean_inference_ms: 2.1152453828045075\n",
      "  mean_raw_obs_processing_ms: 0.2690630644595345\n",
      "time_since_restore: 3194.7614121437073\n",
      "time_this_iter_s: 8.278500318527222\n",
      "time_total_s: 3194.7614121437073\n",
      "timers:\n",
      "  learn_throughput: 582.051\n",
      "  learn_time_ms: 6872.245\n",
      "  load_throughput: 5266083.681\n",
      "  load_time_ms: 0.76\n",
      "  sample_throughput: 508.421\n",
      "  sample_time_ms: 7867.491\n",
      "  update_time_ms: 6.277\n",
      "timestamp: 1643328276\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1884000\n",
      "training_iteration: 471\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1891968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-04-53\n",
      "done: false\n",
      "episode_len_mean: 150.6\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 11905\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4678685189286868\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014951383241117238\n",
      "        policy_loss: -0.07833679679781198\n",
      "        total_loss: 75.73007764180501\n",
      "        vf_explained_var: 0.2662464221318563\n",
      "        vf_loss: 75.80168637593587\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.570172431965669\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014563807971741578\n",
      "        policy_loss: -0.06240097635115186\n",
      "        total_loss: 33.223168001174926\n",
      "        vf_explained_var: 0.3977443222204844\n",
      "        vf_loss: 33.27901518503825\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4917950393756231\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01199959686643524\n",
      "        policy_loss: -0.04043559273084005\n",
      "        total_loss: 31.487557406425477\n",
      "        vf_explained_var: 0.5237058264017105\n",
      "        vf_loss: 31.51989314874013\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1891968\n",
      "  num_agent_steps_trained: 1891968\n",
      "  num_steps_sampled: 1892000\n",
      "  num_steps_trained: 1892000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 473\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.890000000000004\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 56.1\n",
      "  vram_util_percent0: 0.09749348958333334\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 18.266666666666666\n",
      "  player_2: 22.066666666666663\n",
      "policy_reward_mean:\n",
      "  player_0: 9.592000000000002\n",
      "  player_1: -2.2479999999999993\n",
      "  player_2: -4.343999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -14.666666666666666\n",
      "  player_1: -24.666666666666668\n",
      "  player_2: -36.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11528905858092965\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3671525250858907\n",
      "  mean_inference_ms: 2.121116858540449\n",
      "  mean_raw_obs_processing_ms: 0.2694619174759409\n",
      "time_since_restore: 3211.785296678543\n",
      "time_this_iter_s: 7.972270488739014\n",
      "time_total_s: 3211.785296678543\n",
      "timers:\n",
      "  learn_throughput: 570.351\n",
      "  learn_time_ms: 7013.225\n",
      "  load_throughput: 4841490.203\n",
      "  load_time_ms: 0.826\n",
      "  sample_throughput: 483.088\n",
      "  sample_time_ms: 8280.063\n",
      "  update_time_ms: 6.399\n",
      "timestamp: 1643328293\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1892000\n",
      "training_iteration: 473\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1899970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-05-09\n",
      "done: false\n",
      "episode_len_mean: 147.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 11959\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46808978150288266\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01679550802349695\n",
      "        policy_loss: -0.10821515071516236\n",
      "        total_loss: 36.49728771209717\n",
      "        vf_explained_var: -0.02413712819417318\n",
      "        vf_loss: 36.59794495900472\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.54218492547671\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015297249899376766\n",
      "        policy_loss: -0.018949476691583794\n",
      "        total_loss: 31.233405939737956\n",
      "        vf_explained_var: -0.03942072550455729\n",
      "        vf_loss: 31.245471585591634\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.486636886994044\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012315101178343562\n",
      "        policy_loss: -0.12006121669740727\n",
      "        total_loss: 34.3874857433637\n",
      "        vf_explained_var: 0.15931939860184988\n",
      "        vf_loss: 34.499234189987185\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1899970\n",
      "  num_agent_steps_trained: 1899970\n",
      "  num_steps_sampled: 1900000\n",
      "  num_steps_trained: 1900000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 475\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.909999999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 56.3\n",
      "  vram_util_percent0: 0.09749348958333334\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 26.266666666666666\n",
      "  player_2: 22.066666666666663\n",
      "policy_reward_mean:\n",
      "  player_0: 9.208\n",
      "  player_1: -1.274\n",
      "  player_2: -4.934\n",
      "policy_reward_min:\n",
      "  player_0: -16.799999999999997\n",
      "  player_1: -28.133333333333333\n",
      "  player_2: -36.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11553334823459674\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36741155458023206\n",
      "  mean_inference_ms: 2.12545810640761\n",
      "  mean_raw_obs_processing_ms: 0.2698827726637987\n",
      "time_since_restore: 3227.6706557273865\n",
      "time_this_iter_s: 7.783990859985352\n",
      "time_total_s: 3227.6706557273865\n",
      "timers:\n",
      "  learn_throughput: 570.427\n",
      "  learn_time_ms: 7012.291\n",
      "  load_throughput: 4654519.628\n",
      "  load_time_ms: 0.859\n",
      "  sample_throughput: 485.533\n",
      "  sample_time_ms: 8238.369\n",
      "  update_time_ms: 6.354\n",
      "timestamp: 1643328309\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1900000\n",
      "training_iteration: 475\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1907968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-05-25\n",
      "done: false\n",
      "episode_len_mean: 143.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 12016\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45592939953009287\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015796680688399647\n",
      "        policy_loss: -0.06821027057555815\n",
      "        total_loss: 51.9103687842687\n",
      "        vf_explained_var: 0.3046101397275925\n",
      "        vf_loss: 51.97146999518077\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5646300652623176\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01660134595867324\n",
      "        policy_loss: -0.08622140581409136\n",
      "        total_loss: 21.377989637056988\n",
      "        vf_explained_var: -0.008888423840204874\n",
      "        vf_loss: 21.456740345954895\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4911636031667391\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01238347219560108\n",
      "        policy_loss: -0.08094343566025297\n",
      "        total_loss: 26.372181617418924\n",
      "        vf_explained_var: 0.4334210826953252\n",
      "        vf_loss: 26.444766250451405\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1907968\n",
      "  num_agent_steps_trained: 1907968\n",
      "  num_steps_sampled: 1908000\n",
      "  num_steps_trained: 1908000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 477\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.200000000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 56.6\n",
      "  vram_util_percent0: 0.09765625\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 26.266666666666666\n",
      "  player_2: 19.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 8.521333333333333\n",
      "  player_1: -1.5086666666666668\n",
      "  player_2: -4.012666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -16.799999999999997\n",
      "  player_1: -31.666666666666664\n",
      "  player_2: -36.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11576297924175791\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3679470813910989\n",
      "  mean_inference_ms: 2.129854808385021\n",
      "  mean_raw_obs_processing_ms: 0.270451028045337\n",
      "time_since_restore: 3243.942040205002\n",
      "time_this_iter_s: 7.8350934982299805\n",
      "time_total_s: 3243.942040205002\n",
      "timers:\n",
      "  learn_throughput: 571.595\n",
      "  learn_time_ms: 6997.963\n",
      "  load_throughput: 4499964.059\n",
      "  load_time_ms: 0.889\n",
      "  sample_throughput: 484.671\n",
      "  sample_time_ms: 8253.028\n",
      "  update_time_ms: 6.552\n",
      "timestamp: 1643328325\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1908000\n",
      "training_iteration: 477\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1915968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-05-41\n",
      "done: false\n",
      "episode_len_mean: 140.62\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 12069\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46620354741811754\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01672501685347015\n",
      "        policy_loss: -0.0966201999038458\n",
      "        total_loss: 34.50582035382589\n",
      "        vf_explained_var: 0.29927970548470817\n",
      "        vf_loss: 34.59491421699524\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5588806641101837\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015544613808657837\n",
      "        policy_loss: -0.08246122477653747\n",
      "        total_loss: 35.267604405085244\n",
      "        vf_explained_var: 0.2495916481812795\n",
      "        vf_loss: 35.34307044665019\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.522640547255675\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012975738543949168\n",
      "        policy_loss: -0.0690906810007679\n",
      "        total_loss: 45.39020113309225\n",
      "        vf_explained_var: 0.4819851517677307\n",
      "        vf_loss: 45.45053348859151\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1915968\n",
      "  num_agent_steps_trained: 1915968\n",
      "  num_steps_sampled: 1916000\n",
      "  num_steps_trained: 1916000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 479\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.170000000000005\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 56.9\n",
      "  vram_util_percent0: 0.09765625\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.33333333333333\n",
      "  player_1: 21.333333333333332\n",
      "  player_2: 24.53333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 9.602\n",
      "  player_1: -2.724\n",
      "  player_2: -3.878\n",
      "policy_reward_min:\n",
      "  player_0: -18.266666666666662\n",
      "  player_1: -31.666666666666664\n",
      "  player_2: -36.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11604005497698919\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36848590869304376\n",
      "  mean_inference_ms: 2.133873483110355\n",
      "  mean_raw_obs_processing_ms: 0.27069891088549525\n",
      "time_since_restore: 3259.573076725006\n",
      "time_this_iter_s: 7.708884000778198\n",
      "time_total_s: 3259.573076725006\n",
      "timers:\n",
      "  learn_throughput: 574.384\n",
      "  learn_time_ms: 6963.983\n",
      "  load_throughput: 4360209.99\n",
      "  load_time_ms: 0.917\n",
      "  sample_throughput: 487.732\n",
      "  sample_time_ms: 8201.233\n",
      "  update_time_ms: 4.801\n",
      "timestamp: 1643328341\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1916000\n",
      "training_iteration: 479\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1923968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-05-54\n",
      "done: false\n",
      "episode_len_mean: 145.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 12122\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44740287760893505\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013160181141273219\n",
      "        policy_loss: -0.08812512539094314\n",
      "        total_loss: 25.49194102605184\n",
      "        vf_explained_var: 0.022870417833328247\n",
      "        vf_loss: 25.57414413134257\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5687449590365092\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015945545428645194\n",
      "        policy_loss: -0.07083404665191968\n",
      "        total_loss: 43.87821658611298\n",
      "        vf_explained_var: 0.09630213419596355\n",
      "        vf_loss: 43.94187486012777\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4716077747941017\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012330891135437318\n",
      "        policy_loss: -0.061233588047325614\n",
      "        total_loss: 26.946016817092897\n",
      "        vf_explained_var: 0.3352089097102483\n",
      "        vf_loss: 26.998927214940387\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1923968\n",
      "  num_agent_steps_trained: 1923968\n",
      "  num_steps_sampled: 1924000\n",
      "  num_steps_trained: 1924000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 481\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.137500000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 57.2\n",
      "  vram_util_percent0: 0.09765625\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.33333333333333\n",
      "  player_1: 26.4\n",
      "  player_2: 24.53333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 10.455333333333336\n",
      "  player_1: -2.500666666666666\n",
      "  player_2: -4.954666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -17.333333333333332\n",
      "  player_1: -35.33333333333333\n",
      "  player_2: -33.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11601777962504219\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36837331904453974\n",
      "  mean_inference_ms: 2.1353468393949595\n",
      "  mean_raw_obs_processing_ms: 0.27091370080647903\n",
      "time_since_restore: 3272.266350030899\n",
      "time_this_iter_s: 6.294790506362915\n",
      "time_total_s: 3272.266350030899\n",
      "timers:\n",
      "  learn_throughput: 602.59\n",
      "  learn_time_ms: 6638.014\n",
      "  load_throughput: 4595364.431\n",
      "  load_time_ms: 0.87\n",
      "  sample_throughput: 502.515\n",
      "  sample_time_ms: 7959.958\n",
      "  update_time_ms: 4.567\n",
      "timestamp: 1643328354\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1924000\n",
      "training_iteration: 481\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1931968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-06-06\n",
      "done: false\n",
      "episode_len_mean: 151.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 12173\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44136541614929836\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013121971917873907\n",
      "        policy_loss: -0.1173578568858405\n",
      "        total_loss: 36.64151944001516\n",
      "        vf_explained_var: -0.06698134919007619\n",
      "        vf_loss: 36.75297226746877\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4996153595050176\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014161202118593792\n",
      "        policy_loss: -0.05305456895846874\n",
      "        total_loss: 39.37495781898499\n",
      "        vf_explained_var: 0.3594531909624736\n",
      "        vf_loss: 39.42164013385773\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46855908731619517\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01337423589730113\n",
      "        policy_loss: -0.08257176247735819\n",
      "        total_loss: 43.86542174339294\n",
      "        vf_explained_var: 0.1635326381524404\n",
      "        vf_loss: 43.93896627267202\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1931968\n",
      "  num_agent_steps_trained: 1931968\n",
      "  num_steps_sampled: 1932000\n",
      "  num_steps_trained: 1932000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 483\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.049999999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 57.5\n",
      "  vram_util_percent0: 0.09765625\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.0\n",
      "  player_1: 26.4\n",
      "  player_2: 24.6\n",
      "policy_reward_mean:\n",
      "  player_0: 9.442666666666666\n",
      "  player_1: -3.5453333333333332\n",
      "  player_2: -2.8973333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -29.199999999999996\n",
      "  player_1: -35.33333333333333\n",
      "  player_2: -32.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11584385719542531\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3682293755586518\n",
      "  mean_inference_ms: 2.134594153244493\n",
      "  mean_raw_obs_processing_ms: 0.27115022027546765\n",
      "time_since_restore: 3284.8963322639465\n",
      "time_this_iter_s: 6.2297656536102295\n",
      "time_total_s: 3284.8963322639465\n",
      "timers:\n",
      "  learn_throughput: 634.993\n",
      "  learn_time_ms: 6299.285\n",
      "  load_throughput: 5074010.585\n",
      "  load_time_ms: 0.788\n",
      "  sample_throughput: 534.845\n",
      "  sample_time_ms: 7478.804\n",
      "  update_time_ms: 4.32\n",
      "timestamp: 1643328366\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1932000\n",
      "training_iteration: 483\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1939968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-06-19\n",
      "done: false\n",
      "episode_len_mean: 154.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 12229\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44556146492560705\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013637049428895504\n",
      "        policy_loss: -0.15756028332437078\n",
      "        total_loss: 36.933913060824075\n",
      "        vf_explained_var: 0.1963293081521988\n",
      "        vf_loss: 37.085336716969806\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5558162605762482\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017939286004196524\n",
      "        policy_loss: -0.023363217537601788\n",
      "        total_loss: 41.249866712888085\n",
      "        vf_explained_var: 0.28695710361003873\n",
      "        vf_loss: 41.265157492955524\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47391858210166293\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01356098239733574\n",
      "        policy_loss: -0.0610763239270697\n",
      "        total_loss: 37.057425044377645\n",
      "        vf_explained_var: 0.13469434181849163\n",
      "        vf_loss: 37.10934783935547\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1939968\n",
      "  num_agent_steps_trained: 1939968\n",
      "  num_steps_sampled: 1940000\n",
      "  num_steps_trained: 1940000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 485\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.262500000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 56.1\n",
      "  vram_util_percent0: 0.09765625\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 32.0\n",
      "  player_1: 25.333333333333332\n",
      "  player_2: 24.6\n",
      "policy_reward_mean:\n",
      "  player_0: 10.19\n",
      "  player_1: -4.3679999999999986\n",
      "  player_2: -2.821999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -29.199999999999996\n",
      "  player_1: -32.800000000000004\n",
      "  player_2: -35.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11586290033766698\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3681044332142838\n",
      "  mean_inference_ms: 2.133340934735943\n",
      "  mean_raw_obs_processing_ms: 0.2708174615666118\n",
      "time_since_restore: 3297.5988879203796\n",
      "time_this_iter_s: 6.251180648803711\n",
      "time_total_s: 3297.5988879203796\n",
      "timers:\n",
      "  learn_throughput: 659.773\n",
      "  learn_time_ms: 6062.694\n",
      "  load_throughput: 5537583.259\n",
      "  load_time_ms: 0.722\n",
      "  sample_throughput: 559.532\n",
      "  sample_time_ms: 7148.827\n",
      "  update_time_ms: 4.26\n",
      "timestamp: 1643328379\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1940000\n",
      "training_iteration: 485\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1947968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-06-32\n",
      "done: false\n",
      "episode_len_mean: 156.54\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000213\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 19\n",
      "episodes_total: 12275\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4502653460701307\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016545456982264936\n",
      "        policy_loss: -0.08474415505925814\n",
      "        total_loss: 25.697214374542238\n",
      "        vf_explained_var: -0.03872737010320028\n",
      "        vf_loss: 25.774512885411582\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5408151595791181\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014596268261969575\n",
      "        policy_loss: -0.07551691881070534\n",
      "        total_loss: 36.06456312815348\n",
      "        vf_explained_var: 0.38943210025628405\n",
      "        vf_loss: 36.13351195017497\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.472878495156765\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01318787025044306\n",
      "        policy_loss: -0.03112796747436126\n",
      "        total_loss: 23.95078610420227\n",
      "        vf_explained_var: 0.14671901285648345\n",
      "        vf_loss: 23.97301234404246\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1947968\n",
      "  num_agent_steps_trained: 1947968\n",
      "  num_steps_sampled: 1948000\n",
      "  num_steps_trained: 1948000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 487\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.05\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 56.25\n",
      "  vram_util_percent0: 0.09765625\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 24.333333333333332\n",
      "  player_2: 19.4\n",
      "policy_reward_mean:\n",
      "  player_0: 11.031333333333334\n",
      "  player_1: -3.7646666666666664\n",
      "  player_2: -4.266666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -24.06666666666667\n",
      "  player_1: -47.333333333333336\n",
      "  player_2: -35.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11609101561306602\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3682327829157051\n",
      "  mean_inference_ms: 2.1348100395129266\n",
      "  mean_raw_obs_processing_ms: 0.2708249523010371\n",
      "time_since_restore: 3310.3852319717407\n",
      "time_this_iter_s: 6.31453537940979\n",
      "time_total_s: 3310.3852319717407\n",
      "timers:\n",
      "  learn_throughput: 688.55\n",
      "  learn_time_ms: 5809.31\n",
      "  load_throughput: 6000649.523\n",
      "  load_time_ms: 0.667\n",
      "  sample_throughput: 588.897\n",
      "  sample_time_ms: 6792.356\n",
      "  update_time_ms: 3.98\n",
      "timestamp: 1643328392\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1948000\n",
      "training_iteration: 487\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1955970\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-06-45\n",
      "done: false\n",
      "episode_len_mean: 157.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 28\n",
      "episodes_total: 12330\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.41686399847269057\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013029564294423788\n",
      "        policy_loss: -0.042434989400208\n",
      "        total_loss: 35.1745817788442\n",
      "        vf_explained_var: 0.30123096565405527\n",
      "        vf_loss: 35.21115360577901\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5521644777059556\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013549308286880356\n",
      "        policy_loss: -0.13254882898181677\n",
      "        total_loss: 35.084465902646386\n",
      "        vf_explained_var: 0.09204974790414175\n",
      "        vf_loss: 35.21091743946076\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4860645802815755\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01241964773022725\n",
      "        policy_loss: -0.012232180604090294\n",
      "        total_loss: 20.01674869855245\n",
      "        vf_explained_var: 0.32235198775927226\n",
      "        vf_loss: 20.020597664515176\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1955970\n",
      "  num_agent_steps_trained: 1955970\n",
      "  num_steps_sampled: 1956000\n",
      "  num_steps_trained: 1956000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 489\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.574999999999996\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 56.1\n",
      "  vram_util_percent0: 0.09765625\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 24.333333333333332\n",
      "  player_2: 18.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 10.432\n",
      "  player_1: -4.78\n",
      "  player_2: -2.6519999999999997\n",
      "policy_reward_min:\n",
      "  player_0: -31.66666666666667\n",
      "  player_1: -47.333333333333336\n",
      "  player_2: -29.333333333333332\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11588621117210655\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36785326870582596\n",
      "  mean_inference_ms: 2.1330931924904553\n",
      "  mean_raw_obs_processing_ms: 0.2706834517240517\n",
      "time_since_restore: 3323.145763397217\n",
      "time_this_iter_s: 6.274805545806885\n",
      "time_total_s: 3323.145763397217\n",
      "timers:\n",
      "  learn_throughput: 714.868\n",
      "  learn_time_ms: 5595.441\n",
      "  load_throughput: 6450542.505\n",
      "  load_time_ms: 0.62\n",
      "  sample_throughput: 616.194\n",
      "  sample_time_ms: 6491.467\n",
      "  update_time_ms: 3.675\n",
      "timestamp: 1643328405\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1956000\n",
      "training_iteration: 489\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1963968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-06-58\n",
      "done: false\n",
      "episode_len_mean: 148.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 12386\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4683918014168739\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015881031566483822\n",
      "        policy_loss: -0.0740567858144641\n",
      "        total_loss: 39.319340289433796\n",
      "        vf_explained_var: 0.494400727947553\n",
      "        vf_loss: 39.386251033147175\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5499899844328563\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015319287266029278\n",
      "        policy_loss: -0.07583343661079804\n",
      "        total_loss: 38.75356184005737\n",
      "        vf_explained_var: 0.0224396151304245\n",
      "        vf_loss: 38.82250133832296\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48384614020586014\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013083264579084547\n",
      "        policy_loss: -0.10158341869711876\n",
      "        total_loss: 36.383924894332885\n",
      "        vf_explained_var: 0.0961128439505895\n",
      "        vf_loss: 36.476676902771\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1963968\n",
      "  num_agent_steps_trained: 1963968\n",
      "  num_steps_sampled: 1964000\n",
      "  num_steps_trained: 1964000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 491\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.21111111111111\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.86666666666667\n",
      "  vram_util_percent0: 0.09765625\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.333333333333336\n",
      "  player_1: 25.266666666666666\n",
      "  player_2: 18.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 11.238666666666665\n",
      "  player_1: -4.035333333333334\n",
      "  player_2: -4.203333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -31.66666666666667\n",
      "  player_1: -32.66666666666667\n",
      "  player_2: -30.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11575929621179697\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36763640367393985\n",
      "  mean_inference_ms: 2.131323485192354\n",
      "  mean_raw_obs_processing_ms: 0.27039535173577023\n",
      "time_since_restore: 3335.869247674942\n",
      "time_this_iter_s: 6.3081583976745605\n",
      "time_total_s: 3335.869247674942\n",
      "timers:\n",
      "  learn_throughput: 713.813\n",
      "  learn_time_ms: 5603.709\n",
      "  load_throughput: 6329353.001\n",
      "  load_time_ms: 0.632\n",
      "  sample_throughput: 626.336\n",
      "  sample_time_ms: 6386.345\n",
      "  update_time_ms: 3.716\n",
      "timestamp: 1643328418\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1964000\n",
      "training_iteration: 491\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1971968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-07-10\n",
      "done: false\n",
      "episode_len_mean: 142.28\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 12439\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44978776186704633\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013823091654776362\n",
      "        policy_loss: -0.1232790626399219\n",
      "        total_loss: 43.48658598105113\n",
      "        vf_explained_var: 0.14333064317703248\n",
      "        vf_loss: 43.60364492813746\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5535023858149847\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016980435997432628\n",
      "        policy_loss: -0.10870731523260474\n",
      "        total_loss: 26.873466583887737\n",
      "        vf_explained_var: 0.2965905326604843\n",
      "        vf_loss: 26.97453266700109\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4768355734149615\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012253666844635613\n",
      "        policy_loss: -0.05483232712994019\n",
      "        total_loss: 38.90400812149048\n",
      "        vf_explained_var: 0.11293410400549571\n",
      "        vf_loss: 38.950568990707396\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1971968\n",
      "  num_agent_steps_trained: 1971968\n",
      "  num_steps_sampled: 1972000\n",
      "  num_steps_trained: 1972000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 493\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.5625\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 56.1\n",
      "  vram_util_percent0: 0.09765625\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 36.333333333333336\n",
      "  player_1: 25.266666666666666\n",
      "  player_2: 28.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 10.222000000000001\n",
      "  player_1: -1.7559999999999996\n",
      "  player_2: -5.466\n",
      "policy_reward_min:\n",
      "  player_0: -20.2\n",
      "  player_1: -26.133333333333336\n",
      "  player_2: -41.73333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1157526351682243\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3673552647793485\n",
      "  mean_inference_ms: 2.1310913584634146\n",
      "  mean_raw_obs_processing_ms: 0.2705695623305917\n",
      "time_since_restore: 3348.6308608055115\n",
      "time_this_iter_s: 6.31564998626709\n",
      "time_total_s: 3348.6308608055115\n",
      "timers:\n",
      "  learn_throughput: 712.948\n",
      "  learn_time_ms: 5610.504\n",
      "  load_throughput: 6264596.542\n",
      "  load_time_ms: 0.639\n",
      "  sample_throughput: 625.273\n",
      "  sample_time_ms: 6397.205\n",
      "  update_time_ms: 4.055\n",
      "timestamp: 1643328430\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1972000\n",
      "training_iteration: 493\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1979968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-07-23\n",
      "done: false\n",
      "episode_len_mean: 153.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999787\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 12490\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45514785041411715\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015950470127815305\n",
      "        policy_loss: -0.07739647078017393\n",
      "        total_loss: 44.554518272082014\n",
      "        vf_explained_var: 0.15261273284753163\n",
      "        vf_loss: 44.624737044970196\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5192506039142608\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015422613403711422\n",
      "        policy_loss: -0.05815789015032351\n",
      "        total_loss: 46.53574546178182\n",
      "        vf_explained_var: 0.21203761359055837\n",
      "        vf_loss: 46.58696325937907\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49425340404113133\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011778492573815101\n",
      "        policy_loss: -0.0932364670621852\n",
      "        total_loss: 28.52866803010305\n",
      "        vf_explained_var: 0.1398333670695623\n",
      "        vf_loss: 28.6139538192749\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1979968\n",
      "  num_agent_steps_trained: 1979968\n",
      "  num_steps_sampled: 1980000\n",
      "  num_steps_trained: 1980000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 495\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.9125\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.9\n",
      "  vram_util_percent0: 0.09765625\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 41.333333333333336\n",
      "  player_1: 22.133333333333336\n",
      "  player_2: 16.46666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 11.727333333333334\n",
      "  player_1: -3.758666666666666\n",
      "  player_2: -4.968666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -20.2\n",
      "  player_1: -35.66666666666667\n",
      "  player_2: -41.73333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11566510186603775\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3670870935760726\n",
      "  mean_inference_ms: 2.129712868561448\n",
      "  mean_raw_obs_processing_ms: 0.27030966203125567\n",
      "time_since_restore: 3361.4732728004456\n",
      "time_this_iter_s: 6.332565784454346\n",
      "time_total_s: 3361.4732728004456\n",
      "timers:\n",
      "  learn_throughput: 712.006\n",
      "  learn_time_ms: 5617.932\n",
      "  load_throughput: 6271387.56\n",
      "  load_time_ms: 0.638\n",
      "  sample_throughput: 623.885\n",
      "  sample_time_ms: 6411.436\n",
      "  update_time_ms: 4.041\n",
      "timestamp: 1643328443\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1980000\n",
      "training_iteration: 495\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1987968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-07-36\n",
      "done: false\n",
      "episode_len_mean: 159.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 12537\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48634900291760763\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015687005777387335\n",
      "        policy_loss: -0.008826925625714162\n",
      "        total_loss: 36.694337754249574\n",
      "        vf_explained_var: 0.2522803952296575\n",
      "        vf_loss: 36.696105292638144\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49296533440550167\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01575366960018679\n",
      "        policy_loss: -0.08256699086477359\n",
      "        total_loss: 28.91122975985209\n",
      "        vf_explained_var: 0.1207132895787557\n",
      "        vf_loss: 28.986707614262897\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4873576451341311\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011922992516192608\n",
      "        policy_loss: -0.07783281779848039\n",
      "        total_loss: 22.408202664057413\n",
      "        vf_explained_var: 0.26831086734930676\n",
      "        vf_loss: 22.477987421353657\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1987968\n",
      "  num_agent_steps_trained: 1987968\n",
      "  num_steps_sampled: 1988000\n",
      "  num_steps_trained: 1988000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 497\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.175000000000004\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 56.1125\n",
      "  vram_util_percent0: 0.09765625\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 41.333333333333336\n",
      "  player_1: 22.0\n",
      "  player_2: 21.666666666666668\n",
      "policy_reward_mean:\n",
      "  player_0: 10.791333333333332\n",
      "  player_1: -4.306666666666667\n",
      "  player_2: -3.484666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -20.599999999999998\n",
      "  player_1: -35.66666666666667\n",
      "  player_2: -31.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11544023819561895\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36716246907762395\n",
      "  mean_inference_ms: 2.1284293684396953\n",
      "  mean_raw_obs_processing_ms: 0.27021520616975375\n",
      "time_since_restore: 3374.3295209407806\n",
      "time_this_iter_s: 6.394427061080933\n",
      "time_total_s: 3374.3295209407806\n",
      "timers:\n",
      "  learn_throughput: 712.547\n",
      "  learn_time_ms: 5613.665\n",
      "  load_throughput: 6208265.246\n",
      "  load_time_ms: 0.644\n",
      "  sample_throughput: 622.647\n",
      "  sample_time_ms: 6424.185\n",
      "  update_time_ms: 4.018\n",
      "timestamp: 1643328456\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1988000\n",
      "training_iteration: 497\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1995968\n",
      "custom_metrics: {}\n",
      "date: 2022-01-28_01-07-49\n",
      "done: false\n",
      "episode_len_mean: 157.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 12590\n",
      "experiment_id: 1cf34fbceba648faabb88c0799df1a9f\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4681877258419991\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015905899312516947\n",
      "        policy_loss: -0.08620145224034786\n",
      "        total_loss: 33.99145140647888\n",
      "        vf_explained_var: 0.48056730012098947\n",
      "        vf_loss: 34.070494947433474\n",
      "      model: {}\n",
      "    player_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.4500000000000001\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.55279979377985\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015957197325151357\n",
      "        policy_loss: -0.06254519135846447\n",
      "        total_loss: 48.87976019223531\n",
      "        vf_explained_var: 0.17262917121251425\n",
      "        vf_loss: 48.93512427330017\n",
      "      model: {}\n",
      "    player_2:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.6749999999999999\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45047992542386056\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012060855379340865\n",
      "        policy_loss: -0.09224332110956311\n",
      "        total_loss: 48.62959127426147\n",
      "        vf_explained_var: 0.14479718069235484\n",
      "        vf_loss: 48.71369305928548\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1995968\n",
      "  num_agent_steps_trained: 1995968\n",
      "  num_steps_sampled: 1996000\n",
      "  num_steps_trained: 1996000\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 499\n",
      "node_ip: 172.17.87.73\n",
      "num_healthy_workers: 16\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.0875\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 55.875\n",
      "  vram_util_percent0: 0.09765625\n",
      "pid: 2156\n",
      "policy_reward_max:\n",
      "  player_0: 33.666666666666664\n",
      "  player_1: 22.0\n",
      "  player_2: 23.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 10.677333333333333\n",
      "  player_1: -3.868666666666667\n",
      "  player_2: -3.8086666666666673\n",
      "policy_reward_min:\n",
      "  player_0: -20.599999999999998\n",
      "  player_1: -36.333333333333336\n",
      "  player_2: -32.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11574620246198958\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36725982896861853\n",
      "  mean_inference_ms: 2.1305789974250264\n",
      "  mean_raw_obs_processing_ms: 0.27058273409620065\n",
      "time_since_restore: 3386.959851026535\n",
      "time_this_iter_s: 6.296028137207031\n",
      "time_total_s: 3386.959851026535\n",
      "timers:\n",
      "  learn_throughput: 713.896\n",
      "  learn_time_ms: 5603.059\n",
      "  load_throughput: 6191083.066\n",
      "  load_time_ms: 0.646\n",
      "  sample_throughput: 624.281\n",
      "  sample_time_ms: 6407.369\n",
      "  update_time_ms: 4.118\n",
      "timestamp: 1643328469\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1996000\n",
      "training_iteration: 499\n",
      "trial_id: default\n",
      "\n",
      "training done, because max_iters 500 reached\n",
      "Finished training. Running manual test/inference loop.\n"
     ]
    }
   ],
   "source": [
    "# run manual training loop and print results after each iteration\n",
    "max_steps = 1e7\n",
    "max_iters = 500\n",
    "for iters in range(max_iters):\n",
    "    result = trainer.train()\n",
    "    if iters % 2 ==0:\n",
    "        print(pretty_print(result))\n",
    "    # stop training if the target train steps or reward are reached\n",
    "    if result[\"timesteps_total\"] >= max_steps:\n",
    "        print(f\"training done, because max_steps {max_steps} {result['timesteps_total']} reached\")\n",
    "        break\n",
    "else:\n",
    "    print(f\"training done, because max_iters {max_iters} reached\")\n",
    "# manual test loop\n",
    "print(\"Finished training. Running manual test/inference loop.\")\n",
    "# prepare environment with max 10 steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t12\t0\tx]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t10\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [5\tx\tx\tx]\n",
      " [x\t7\tx\tx]\n",
      " [x\tx\tx\tx]  \n",
      "\n",
      "agent  player_0  action  draw from discard pile\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 11 \n",
      "discard pile top: empty \n",
      "======= Player 0 ========== \n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t12\t0\tx]  \n",
      "======= Player 1 ========== \n",
      " [x\tx\tx\tx]\n",
      " [x\tx\tx\tx]\n",
      " [x\t10\t1\tx]  \n",
      "======= Player 2 ========== \n",
      " [5\tx\tx\tx]\n",
      " [x\t7\tx\tx]\n",
      " [x\tx\tx\tx]  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "  warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "/home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "  warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n"
     ]
    }
   ],
   "source": [
    "env = PettingZooEnv(env_creator())\n",
    "obs = env.reset()\n",
    "done = {'__all__': False}\n",
    "# run one iteration until done\n",
    "for i in range(1):\n",
    "    if done[\"__all__\"]:\n",
    "        print(\"game done\")\n",
    "        break\n",
    "    # get agent from current observation\n",
    "    agent = list(obs.keys())[0]\n",
    "\n",
    "    # format observation dict\n",
    "    obs = obs[agent]\n",
    "    env.render()\n",
    "    \n",
    "    # get deterministic action\n",
    "    # trainer.compute_single_action(obs, policy_id=agent)\n",
    "    policy = trainer.get_policy(policy_id=agent)\n",
    "    action_exploration_policy, _, action_info = policy.compute_single_action(obs)\n",
    "    logits = action_info['action_dist_inputs']\n",
    "    action = logits.argmax()\n",
    "    # \n",
    "    print(\"agent \", agent, \" action \", env.env.env.table.render_action_explainer(action                                                           ))\n",
    "    next_obs, reward, done, obs = env.step({agent: action})\n",
    "    # observations contain original observations and the action mask\n",
    "    # print(f\"Obs: {obs}, Action: {action}, done: {done}\")\n",
    "    \n",
    "env.render()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Observation ({} dtype={}) outside given space ({})!', {'observation': array([15, 15,  7,  4, 15, -2, 15,  6, 12, 15, 15,  4,  9,  6,  3,  0,  1,\n        0,  1,  0,  2,  1,  1,  2,  0,  1,  1,  0,  1, -2,  4], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)}, None, Dict(action_mask:Box([0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], (26,), int8), observations:Box([-24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24\n -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24], [127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127\n 127 127 127 127 127 127 127 127 127 127 127 127 127], (31,), int8)))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m obs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m15\u001b[39m,  \u001b[38;5;241m7\u001b[39m,  \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m15\u001b[39m,  \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m15\u001b[39m,  \u001b[38;5;241m4\u001b[39m,  \u001b[38;5;241m9\u001b[39m,  \u001b[38;5;241m6\u001b[39m,  \u001b[38;5;241m3\u001b[39m,  \u001b[38;5;241m0\u001b[39m,  \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;241m0\u001b[39m,  \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;241m0\u001b[39m,  \u001b[38;5;241m2\u001b[39m,  \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;241m2\u001b[39m,  \u001b[38;5;241m0\u001b[39m,  \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;241m0\u001b[39m,  \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,  \u001b[38;5;241m4\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint8), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint8)}\n\u001b[0;32m----> 3\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_single_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/skybo/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:1234\u001b[0m, in \u001b[0;36mTrainer.compute_single_action\u001b[0;34m(self, observation, state, prev_action, prev_reward, info, input_dict, policy_id, full_fetch, explore, timestep, episode, unsquash_action, clip_action, unsquash_actions, clip_actions, **kwargs)\u001b[0m\n\u001b[1;32m   1232\u001b[0m pp \u001b[38;5;241m=\u001b[39m local_worker\u001b[38;5;241m.\u001b[39mpreprocessors[policy_id]\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pp \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(pp)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNoPreprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1234\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1235\u001b[0m observation \u001b[38;5;241m=\u001b[39m local_worker\u001b[38;5;241m.\u001b[39mfilters[policy_id](\n\u001b[1;32m   1236\u001b[0m     observation, update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;66;03m# Input-dict.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/skybo/lib/python3.9/site-packages/ray/rllib/models/preprocessors.py:275\u001b[0m, in \u001b[0;36mDictFlatteningPreprocessor.transform\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;129m@override\u001b[39m(Preprocessor)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: TensorType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(observation, array, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/skybo/lib/python3.9/site-packages/ray/rllib/models/preprocessors.py:72\u001b[0m, in \u001b[0;36mPreprocessor.check_shape\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_space\u001b[38;5;241m.\u001b[39mcontains(observation):\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     73\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservation (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dtype=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) outside given space (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)!\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     74\u001b[0m             observation, observation\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m     75\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_space,\n\u001b[1;32m     76\u001b[0m                 gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mBox) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_space)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservation for a Box/MultiBinary/MultiDiscrete space \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould be an np.array, not a Python list.\u001b[39m\u001b[38;5;124m\"\u001b[39m, observation)\n",
      "\u001b[0;31mValueError\u001b[0m: ('Observation ({} dtype={}) outside given space ({})!', {'observation': array([15, 15,  7,  4, 15, -2, 15,  6, 12, 15, 15,  4,  9,  6,  3,  0,  1,\n        0,  1,  0,  2,  1,  1,  2,  0,  1,  1,  0,  1, -2,  4], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)}, None, Dict(action_mask:Box([0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], (26,), int8), observations:Box([-24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24\n -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24], [127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127\n 127 127 127 127 127 127 127 127 127 127 127 127 127], (31,), int8)))"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0128 01:17:45.539222787   21306 backup_poller.cc:134]       Run client channel backup poller: {\"created\":\"@1643329065.539180366\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1643329065.539174615\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n"
     ]
    }
   ],
   "source": [
    "obs = {'observation': np.array([15, 15,  7,  4, 15, -2, 15,  6, 12, 15, 15,  4,  9,  6,  3,  0,  1,\n",
    "        0,  1,  0,  2,  1,  1,  2,  0,  1,  1,  0,  1, -2,  4], dtype=np.int8), 'action_mask': np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=np.int8)}\n",
    "action = trainer.compute_single_action(obs, policy_id=agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try creating with rllib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
