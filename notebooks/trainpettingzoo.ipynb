{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We create an instance of a skyjo_env environment, call reset() to initialize the game and list the available agents (players):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rlskyjo.environment.skyjo_env' from '/home/michi/skybo_rl/rlskyjo/environment/skyjo_env.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from rlskyjo.environment import skyjo_env\n",
    "from importlib import reload\n",
    "reload(skyjo_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "skyjo_env_cfg = {\"num_players\": 3}\n",
    "env_pettingzoo = skyjo_env.env(**skyjo_env_cfg)\n",
    "env_pettingzoo.reset()\n",
    "\n",
    "env_pettingzoo.agents, env_pettingzoo.agent_selection\n",
    "\n",
    "def sample_place():\n",
    "    return np.random.randint(0,23)\n",
    "        \n",
    "def sample_draw():\n",
    "    return np.random.randint(24,25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_0 draw\n",
      "player_0 place\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n"
     ]
    }
   ],
   "source": [
    "print(env_pettingzoo.agent_selection, \"draw\")\n",
    "env_pettingzoo.step(sample_draw())\n",
    "print(env_pettingzoo.agent_selection, \"place\")\n",
    "env_pettingzoo.step(\n",
    "    sample_place()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_admissible_policy(observation, action_mask):\n",
    "    \"\"\"picks randomly an admissible action from the action mask\"\"\"\n",
    "    return np.random.choice(\n",
    "        np.arange(len(action_mask)),\n",
    "        p= action_mask/np.sum(action_mask)\n",
    "    )\n",
    "\n",
    "assert 1 not in [random_admissible_policy(None,[1,0,1]) for _ in range(1000)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training fct: {'observations': array([-2, 10,  1,  1,  2,  0,  0,  0,  0,  1,  0,  0,  0,  1,  0,  1,  0,\n",
      "        0, 15, 15, 15, 11, 15, 15, 15, 15, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 15,  5, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "       -2,  0, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t u\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t 0]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([-2, 10,  1,  1,  2,  0,  0,  0,  0,  1,  0,  0,  0,  1,  0,  1,  0,\n",
      "        0,  3, 15, 15, 11, 15, 15, 15, 15, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 15,  5, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "       -2,  0, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 3 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t u\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t 0]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 8\n",
      "training fct: {'observations': array([-2,  9,  1,  1,  2,  0,  0,  1,  0,  1,  0,  0,  0,  1,  0,  2,  0,\n",
      "       11, 15, 15, 15, 11, 15, 15, 15, 15, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 15,  5, 15,  3, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "       -2,  0, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t u\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t 0]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([-2,  9,  1,  1,  2,  0,  0,  1,  0,  1,  0,  0,  0,  1,  0,  2,  0,\n",
      "       11, 11, 15, 15, 11, 15, 15, 15, 15, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 15,  5, 15,  3, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "       -2,  0, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 11 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t u\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t 0]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 16\n",
      "training fct: {'observations': array([ 1,  9,  1,  1,  2,  0,  0,  2,  0,  1,  0,  0,  0,  1,  0,  3,  0,\n",
      "       11, 15, 15, 15, 11, 15, 15, 15, 15, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 15,  5, 15,  3, 15, 15, 15, 15, 15, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t u\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t 0]\n",
      " [u\t 3\t u\t u]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([ 1,  9,  1,  1,  2,  0,  0,  2,  0,  1,  0,  0,  0,  1,  0,  3,  0,\n",
      "       11,  9, 15, 15, 11, 15, 15, 15, 15, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 15,  5, 15,  3, 15, 15, 15, 15, 15, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 9 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t u\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t 0]\n",
      " [u\t 3\t u\t u]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 5\n",
      "training fct: {'observations': array([ 1,  9,  1,  1,  2,  0,  1,  2,  0,  1,  0,  0,  0,  2,  0,  3,  0,\n",
      "        2, 15, 15, 15, 11, 15, 15,  9, 15, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 15,  5, 15,  3, 15, 15, 15, 15, 15, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 9\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t 0]\n",
      " [u\t 3\t u\t u]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([ 1,  9,  1,  1,  2,  0,  1,  2,  0,  1,  0,  0,  0,  2,  0,  3,  0,\n",
      "        2, 12, 15, 15, 11, 15, 15,  9, 15, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 15,  5, 15,  3, 15, 15, 15, 15, 15, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 12 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 9\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t 0]\n",
      " [u\t 3\t u\t u]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 5\n",
      "training fct: {'observations': array([ 1,  8,  1,  1,  2,  0,  1,  2,  0,  1,  0,  0,  0,  2,  0,  4,  1,\n",
      "       11, 15, 15, 15, 11, 15, 15,  9, 15, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 12,  5, 15,  3, 15, 15, 15, 15, 15, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 9\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 12\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t 0]\n",
      " [u\t 3\t u\t u]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([ 1,  8,  1,  1,  2,  0,  1,  2,  0,  1,  0,  0,  0,  2,  0,  4,  1,\n",
      "       11, -1, 15, 15, 11, 15, 15,  9, 15, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 12,  5, 15,  3, 15, 15, 15, 15, 15, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: -1 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 9\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 12\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t 0]\n",
      " [u\t 3\t u\t u]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 12\n",
      "training fct: {'observations': array([ 2,  8,  1,  2,  2,  1,  1,  2,  0,  1,  0,  0,  0,  2,  0,  4,  1,\n",
      "       -1, 15, 15, 15, 11, 15, 15,  9, 15, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 12,  5, 15,  3, 15, 15, 15,  1, 15, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 9\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 12\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [u\t 3\t u\t u]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([ 2,  8,  1,  1,  2,  1,  1,  2,  0,  1,  0,  0,  0,  2,  0,  4,  1,\n",
      "       11, -1, 15, 15, 11, 15, 15,  9, 15, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 12,  5, 15,  3, 15, 15, 15,  1, 15, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: -1 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 9\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 12\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [u\t 3\t u\t u]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 6\n",
      "training fct: {'observations': array([ 2,  8,  1,  2,  2,  1,  1,  2,  0,  1,  1,  0,  0,  2,  0,  4,  1,\n",
      "        6, 15, 15, 15, 11, 15, 15,  9, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 12,  5, 15,  3, 15, 15, 15,  1, 15, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 9\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 12\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [u\t 3\t u\t u]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([ 2,  8,  1,  2,  2,  1,  1,  2,  0,  1,  0,  0,  0,  2,  0,  4,  1,\n",
      "       11,  6, 15, 15, 11, 15, 15,  9, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 12,  5, 15,  3, 15, 15, 15,  1, 15, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 6 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 9\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 12\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [u\t 3\t u\t u]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 7\n",
      "training fct: {'observations': array([ 2,  7,  1,  2,  2,  1,  2,  2,  0,  1,  1,  0,  0,  2,  0,  4,  1,\n",
      "        2, 15, 15, 15, 11, 15, 15,  9, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 12,  5,  6,  3, 15, 15, 15,  1, 15, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 9\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 12\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [u\t 3\t u\t u]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([ 2,  7,  1,  2,  2,  1,  1,  2,  0,  1,  1,  0,  0,  2,  0,  4,  1,\n",
      "       11,  2, 15, 15, 11, 15, 15,  9, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 12,  5,  6,  3, 15, 15, 15,  1, 15, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 2 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 9\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 12\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [u\t 3\t u\t u]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 22\n",
      "training fct: {'observations': array([14,  7,  1,  2,  2,  1,  2,  2,  0,  1,  1,  0,  0,  2,  0,  4,  2,\n",
      "        2, 15, 15, 15, 11, 15, 15,  9, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 12,  5,  6,  3, 15, 15, 15,  1, 15, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 12, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 9\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 12\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [u\t 3\t u\t 12]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([14,  7,  1,  2,  2,  1,  2,  2,  0,  1,  1,  0,  0,  2,  0,  4,  2,\n",
      "        2,  9, 15, 15, 11, 15, 15,  9, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 12,  5,  6,  3, 15, 15, 15,  1, 15, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 12, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 9 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 9\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 12\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [u\t 3\t u\t 12]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 5\n",
      "training fct: {'observations': array([14,  7,  1,  2,  2,  1,  2,  2,  0,  1,  1,  0,  0,  3,  0,  4,  2,\n",
      "        9, 15, 15, 15, 11, 15, 15,  9, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 12,  5,  6,  3, 15, 15, 15,  1, 15, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 12, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 9\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 12\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [u\t 3\t u\t 12]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([14,  7,  1,  2,  2,  1,  2,  2,  0,  1,  1,  0,  0,  3,  0,  4,  2,\n",
      "        9,  4, 15, 15, 11, 15, 15,  9, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 12,  5,  6,  3, 15, 15, 15,  1, 15, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 12, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 4 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 9\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 12\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [u\t 3\t u\t 12]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 5\n",
      "training fct: {'observations': array([14,  7,  1,  2,  2,  1,  2,  2,  1,  1,  1,  0,  0,  3,  0,  4,  2,\n",
      "       12, 15, 15, 15, 11, 15, 15,  9, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15,  4,  5,  6,  3, 15, 15, 15,  1, 15, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 12, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 9\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 4\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [u\t 3\t u\t 12]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([14,  7,  1,  2,  2,  1,  2,  2,  1,  1,  1,  0,  0,  3,  0,  4,  1,\n",
      "        9, 12, 15, 15, 11, 15, 15,  9, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15,  4,  5,  6,  3, 15, 15, 15,  1, 15, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 12, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 12 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 9\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 4\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [u\t 3\t u\t 12]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 1\n",
      "training fct: {'observations': array([18,  6,  1,  2,  2,  1,  2,  2,  1,  1,  1,  0,  0,  3,  0,  4,  3,\n",
      "       12, 15, 15, 15, 11, 15, 15,  9, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15,  4,  5,  6,  3, 15, 15, 15,  1, 12, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 12, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 9\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 4\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [12\t 3\t u\t 12]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([18,  6,  1,  2,  2,  1,  2,  2,  1,  1,  1,  0,  0,  3,  0,  4,  2,\n",
      "        9, 12, 15, 15, 11, 15, 15,  9, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15,  4,  5,  6,  3, 15, 15, 15,  1, 12, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 12, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 12 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 9\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 4\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [12\t 3\t u\t 12]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 5\n",
      "training fct: {'observations': array([21,  6,  1,  2,  2,  1,  2,  2,  1,  1,  1,  0,  0,  3,  0,  4,  3,\n",
      "        9, 15, 15, 15, 11, 15, 15, 12, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15,  4,  5,  6,  3, 15, 15, 15,  1, 12, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 12, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 4\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [12\t 3\t u\t 12]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([21,  6,  1,  2,  2,  1,  2,  2,  1,  1,  1,  0,  0,  3,  0,  4,  3,\n",
      "        9,  6, 15, 15, 11, 15, 15, 12, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15,  4,  5,  6,  3, 15, 15, 15,  1, 12, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 12, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 6 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t u]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 4\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [12\t 3\t u\t 12]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 21\n",
      "training fct: {'observations': array([21,  6,  1,  2,  2,  1,  2,  3,  1,  1,  2,  0,  0,  3,  0,  4,  3,\n",
      "        6, 15, 15, 15, 11, 15, 15, 12, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15,  4,  5,  6,  3,  3, 15, 15,  1, 12, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 12, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 4\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [12\t 3\t u\t 12]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([21,  6,  1,  2,  2,  1,  2,  3,  1,  1,  2,  0,  0,  3,  0,  4,  3,\n",
      "        6, 10, 15, 15, 11, 15, 15, 12, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15,  4,  5,  6,  3,  3, 15, 15,  1, 12, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 12, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 10 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 4\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [12\t 3\t u\t 12]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 10\n",
      "training fct: {'observations': array([21,  6,  1,  2,  2,  1,  2,  3,  1,  1,  2,  0,  0,  3,  1,  4,  3,\n",
      "       12, 15, 15, 15, 11, 15, 15, 12, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15,  4,  5,  6,  3,  3, 15, 15,  1, 12, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 4\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([21,  6,  1,  2,  2,  1,  2,  3,  1,  1,  2,  0,  0,  3,  1,  4,  2,\n",
      "        6, 12, 15, 15, 11, 15, 15, 12, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15,  4,  5,  6,  3,  3, 15, 15,  1, 12, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 12 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [u\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 4\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 1\n",
      "training fct: {'observations': array([24,  6,  1,  2,  2,  1,  2,  3,  1,  1,  2,  0,  0,  3,  2,  4,  3,\n",
      "       10, 15, 15, 12, 11, 15, 15, 12, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15,  4,  5,  6,  3,  3, 15, 15,  1, 12, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 4\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([24,  6,  1,  2,  2,  1,  2,  3,  1,  1,  2,  0,  0,  3,  1,  4,  3,\n",
      "        6, 10, 15, 12, 11, 15, 15, 12, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15,  4,  5,  6,  3,  3, 15, 15,  1, 12, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 10 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 4\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 5\n",
      "training fct: {'observations': array([24,  6,  1,  2,  2,  1,  2,  3,  1,  1,  2,  0,  0,  3,  2,  4,  3,\n",
      "        4, 15, 15, 12, 11, 15, 15, 12, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 10,  5,  6,  3,  3, 15, 15,  1, 12, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([24,  6,  1,  2,  2,  1,  2,  3,  1,  1,  2,  0,  0,  3,  2,  4,  3,\n",
      "        4, -1, 15, 12, 11, 15, 15, 12, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 10,  5,  6,  3,  3, 15, 15,  1, 12, 15, 15,  3, 15, 15, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: -1 \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t u\t u\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 3\n",
      "training fct: {'observations': array([23,  5,  1,  3,  3,  1,  2,  3,  1,  1,  2,  0,  0,  3,  2,  4,  3,\n",
      "        0, 15, 15, 12, 11, 15, 15, 12, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 10,  5,  6,  3,  3, 15, 15,  1, 12, 15, -1,  3, 15, 15, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t u\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([23,  5,  1,  3,  3,  1,  2,  3,  1,  1,  2,  0,  0,  3,  2,  4,  3,\n",
      "        0, -2, 15, 12, 11, 15, 15, 12, -1, 15, -1, 15, 15, 15, 15, 15, 15,\n",
      "        9, 15, 10,  5,  6,  3,  3, 15, 15,  1, 12, 15, -1,  3, 15, 15, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: -2 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t u]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t u\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 21\n",
      "training fct: {'observations': array([23,  5,  2,  3,  3,  1,  3,  3,  1,  1,  2,  0,  0,  3,  2,  4,  3,\n",
      "       -2, 15, 15, 12, 11, 15, 15, 12, -1, 15, -1,  2, 15, 15, 15, 15, 15,\n",
      "        9, 15, 10,  5,  6,  3,  3, 15, 15,  1, 12, 15, -1,  3, 15, 15, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t u\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([23,  5,  1,  3,  3,  1,  3,  3,  1,  1,  2,  0,  0,  3,  2,  4,  3,\n",
      "        0, -2, 15, 12, 11, 15, 15, 12, -1, 15, -1,  2, 15, 15, 15, 15, 15,\n",
      "        9, 15, 10,  5,  6,  3,  3, 15, 15,  1, 12, 15, -1,  3, 15, 15, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: -2 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t u\t 6\t u]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t u\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 4\n",
      "training fct: {'observations': array([23,  5,  2,  3,  3,  1,  3,  3,  1,  1,  2,  0,  0,  4,  2,  4,  3,\n",
      "        9, 15, 15, 12, 11, 15, 15, 12, -1, 15, -1,  2, 15, 15, 15, 15, 15,\n",
      "        9, -2, 10,  5,  6,  3,  3, 15, 15,  1, 12, 15, -1,  3, 15, 15, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t -2\t 6\t u]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t u\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([23,  5,  2,  3,  3,  1,  3,  3,  1,  1,  2,  0,  0,  4,  2,  4,  3,\n",
      "        9, 11, 15, 12, 11, 15, 15, 12, -1, 15, -1,  2, 15, 15, 15, 15, 15,\n",
      "        9, -2, 10,  5,  6,  3,  3, 15, 15,  1, 12, 15, -1,  3, 15, 15, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 11 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t -2\t 6\t u]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t u\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 6\n",
      "training fct: {'observations': array([34,  4,  2,  3,  4,  1,  3,  3,  1,  1,  2,  0,  0,  4,  2,  5,  3,\n",
      "        0, 15, 15, 12, 11, 15, 15, 12, -1, 15, -1,  2, 15, 15, 15, 15, 15,\n",
      "        9, -2, 10,  5,  6,  3,  3, 15, 15,  1, 12, 15, -1,  3, 15, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t -2\t 6\t u]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([34,  4,  2,  3,  3,  1,  3,  3,  1,  1,  2,  0,  0,  4,  2,  5,  3,\n",
      "        9,  0, 15, 12, 11, 15, 15, 12, -1, 15, -1,  2, 15, 15, 15, 15, 15,\n",
      "        9, -2, 10,  5,  6,  3,  3, 15, 15,  1, 12, 15, -1,  3, 15, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 0 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t -2\t 6\t u]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 0\n",
      "training fct: {'observations': array([34,  4,  2,  3,  4,  1,  3,  3,  1,  1,  2,  1,  0,  4,  2,  5,  3,\n",
      "        7, 15,  0, 12, 11, 15, 15, 12, -1, 15, -1,  2, 15, 15, 15, 15, 15,\n",
      "        9, -2, 10,  5,  6,  3,  3, 15, 15,  1, 12, 15, -1,  3, 15, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 7 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t -2\t 6\t u]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([34,  4,  2,  3,  4,  1,  3,  3,  1,  1,  2,  0,  0,  4,  2,  5,  3,\n",
      "        9,  7,  0, 12, 11, 15, 15, 12, -1, 15, -1,  2, 15, 15, 15, 15, 15,\n",
      "        9, -2, 10,  5,  6,  3,  3, 15, 15,  1, 12, 15, -1,  3, 15, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 7 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t -2\t 6\t u]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 10\n",
      "training fct: {'observations': array([34,  4,  2,  3,  4,  1,  3,  3,  1,  1,  2,  1,  1,  4,  2,  5,  3,\n",
      "        8, 15,  0, 12, 11, 15, 15, 12, -1, 15, -1,  2, 15, 15, 15, 15, 15,\n",
      "        9, -2, 10,  5,  6,  3,  3,  7, 15,  1, 12, 15, -1,  3, 15, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t -2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([34,  4,  2,  3,  4,  1,  3,  3,  1,  1,  2,  1,  1,  4,  2,  5,  3,\n",
      "        8,  9,  0, 12, 11, 15, 15, 12, -1, 15, -1,  2, 15, 15, 15, 15, 15,\n",
      "        9, -2, 10,  5,  6,  3,  3,  7, 15,  1, 12, 15, -1,  3, 15, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 9 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t -2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t u\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 17\n",
      "training fct: {'observations': array([35,  3,  2,  3,  4,  1,  3,  3,  1,  1,  2,  1,  2,  5,  2,  5,  3,\n",
      "        9, 15,  0, 12, 11, 15, 15, 12, -1, 15, -1,  2, 15, 15, 15, 15, 15,\n",
      "        9, -2, 10,  5,  6,  3,  3,  7, 15,  1, 12, 15, -1,  3,  8, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t -2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t 8\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([35,  3,  2,  3,  4,  1,  3,  3,  1,  1,  2,  1,  2,  4,  2,  5,  3,\n",
      "        8,  9,  0, 12, 11, 15, 15, 12, -1, 15, -1,  2, 15, 15, 15, 15, 15,\n",
      "        9, -2, 10,  5,  6,  3,  3,  7, 15,  1, 12, 15, -1,  3,  8, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 9 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t u]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t -2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t 8\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 11\n",
      "training fct: {'observations': array([41,  3,  2,  3,  4,  1,  3,  3,  2,  1,  2,  1,  2,  5,  2,  5,  3,\n",
      "        4, 15,  0, 12, 11, 15, 15, 12, -1, 15, -1,  2, 15,  9, 15, 15, 15,\n",
      "        9, -2, 10,  5,  6,  3,  3,  7, 15,  1, 12, 15, -1,  3,  8, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t -2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t 8\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([41,  3,  2,  3,  4,  1,  3,  3,  2,  1,  2,  1,  2,  5,  2,  5,  3,\n",
      "        4,  2,  0, 12, 11, 15, 15, 12, -1, 15, -1,  2, 15,  9, 15, 15, 15,\n",
      "        9, -2, 10,  5,  6,  3,  3,  7, 15,  1, 12, 15, -1,  3,  8, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 2 \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t -2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t 8\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 4\n",
      "training fct: {'observations': array([42,  3,  2,  3,  4,  1,  4,  3,  2,  1,  2,  1,  2,  5,  2,  5,  3,\n",
      "       -2, 15,  0, 12, 11, 15, 15, 12, -1, 15, -1,  2, 15,  9, 15, 15, 15,\n",
      "        9,  2, 10,  5,  6,  3,  3,  7, 15,  1, 12, 15, -1,  3,  8, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t 2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t 8\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([42,  3,  2,  3,  4,  1,  4,  3,  2,  1,  2,  1,  2,  5,  2,  5,  3,\n",
      "       -2,  7,  0, 12, 11, 15, 15, 12, -1, 15, -1,  2, 15,  9, 15, 15, 15,\n",
      "        9,  2, 10,  5,  6,  3,  3,  7, 15,  1, 12, 15, -1,  3,  8, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 7 \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t 2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t 8\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 5\n",
      "training fct: {'observations': array([41,  3,  2,  3,  4,  1,  4,  3,  2,  1,  2,  2,  2,  5,  2,  5,  3,\n",
      "        8, 15,  0, 12, 11, 15, 15, 12, -1, 15, -1,  2, 15,  9, 15, 15, 15,\n",
      "        9,  2, 10,  5,  6,  3,  3,  7, 15,  1, 12, 15, -1,  3,  7, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t 2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([41,  3,  2,  3,  4,  1,  4,  3,  2,  1,  2,  2,  1,  5,  2,  5,  3,\n",
      "       -2,  8,  0, 12, 11, 15, 15, 12, -1, 15, -1,  2, 15,  9, 15, 15, 15,\n",
      "        9,  2, 10,  5,  6,  3,  3,  7, 15,  1, 12, 15, -1,  3,  7, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 8 \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 12\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t 2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 5\n",
      "training fct: {'observations': array([40,  3,  2,  3,  4,  1,  4,  3,  2,  1,  2,  2,  2,  5,  2,  5,  3,\n",
      "       12, 15,  0, 12, 11, 15, 15,  8, -1, 15, -1,  2, 15,  9, 15, 15, 15,\n",
      "        9,  2, 10,  5,  6,  3,  3,  7, 15,  1, 12, 15, -1,  3,  7, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 8\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t 2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([40,  3,  2,  3,  4,  1,  4,  3,  2,  1,  2,  2,  2,  5,  2,  5,  3,\n",
      "       12, 10,  0, 12, 11, 15, 15,  8, -1, 15, -1,  2, 15,  9, 15, 15, 15,\n",
      "        9,  2, 10,  5,  6,  3,  3,  7, 15,  1, 12, 15, -1,  3,  7, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 10 \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 8\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [u\t 2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 13\n",
      "training fct: {'observations': array([40,  3,  2,  3,  4,  1,  4,  4,  2,  1,  2,  2,  2,  5,  3,  5,  3,\n",
      "       10, 15,  0, 12, 11, 15, 15,  8, -1, 15, -1,  2, 15,  9, 15,  3, 15,\n",
      "        9,  2, 10,  5,  6,  3,  3,  7, 15,  1, 12, 15, -1,  3,  7, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 8\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [3\t 2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([40,  3,  2,  3,  4,  1,  4,  4,  2,  1,  2,  2,  2,  5,  2,  5,  3,\n",
      "       12, 10,  0, 12, 11, 15, 15,  8, -1, 15, -1,  2, 15,  9, 15,  3, 15,\n",
      "        9,  2, 10,  5,  6,  3,  3,  7, 15,  1, 12, 15, -1,  3,  7, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 10 \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 8\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [3\t 2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [12\t 3\t u\t 10]\n",
      " [u\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 1\n",
      "training fct: {'observations': array([39,  3,  2,  3,  4,  1,  4,  4,  2,  1,  2,  2,  2,  5,  3,  5,  3,\n",
      "       12, 15,  0, 12, 11, 15, 15,  8, -1, 15, -1,  2, 15,  9, 15,  3, 15,\n",
      "        9,  2, 10,  5,  6,  3,  3,  7, 15,  1, 10, 15, -1,  3,  7, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 8\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [3\t 2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [10\t 3\t u\t 10]\n",
      " [u\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([39,  3,  2,  3,  4,  1,  4,  4,  2,  1,  2,  2,  2,  5,  3,  5,  3,\n",
      "       12,  4,  0, 12, 11, 15, 15,  8, -1, 15, -1,  2, 15,  9, 15,  3, 15,\n",
      "        9,  2, 10,  5,  6,  3,  3,  7, 15,  1, 10, 15, -1,  3,  7, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 4 \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [12\t u\t u\t u]\n",
      " [11\t 8\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [3\t 2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [10\t 3\t u\t 10]\n",
      " [u\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 1\n",
      "training fct: {'observations': array([32,  3,  2,  3,  4,  1,  4,  4,  3,  1,  2,  2,  2,  5,  3,  5,  3,\n",
      "       12, 15,  0,  4, 11, 15, 15,  8, -1, 15, -1,  2, 15,  9, 15,  3, 15,\n",
      "        9,  2, 10,  5,  6,  3,  3,  7, 15,  1, 10, 15, -1,  3,  7, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [11\t 8\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [3\t 2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [10\t 3\t u\t 10]\n",
      " [u\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([32,  3,  2,  3,  4,  1,  4,  4,  3,  1,  2,  2,  2,  5,  3,  5,  3,\n",
      "       12,  6,  0,  4, 11, 15, 15,  8, -1, 15, -1,  2, 15,  9, 15,  3, 15,\n",
      "        9,  2, 10,  5,  6,  3,  3,  7, 15,  1, 10, 15, -1,  3,  7, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 6 \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [11\t 8\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 3]\n",
      " [3\t 2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [10\t 3\t u\t 10]\n",
      " [u\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 9\n",
      "training fct: {'observations': array([32,  3,  2,  3,  4,  1,  4,  4,  3,  1,  3,  2,  2,  5,  3,  5,  3,\n",
      "        3, 15,  0,  4, 11, 15, 15,  8, -1, 15, -1,  2, 15,  9, 15,  3, 15,\n",
      "        9,  2, 10,  5,  6,  3,  6,  7, 15,  1, 10, 15, -1,  3,  7, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 3 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [11\t 8\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 6]\n",
      " [3\t 2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [10\t 3\t u\t 10]\n",
      " [u\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([32,  3,  2,  3,  4,  1,  4,  3,  3,  1,  3,  2,  2,  5,  3,  5,  3,\n",
      "       12,  3,  0,  4, 11, 15, 15,  8, -1, 15, -1,  2, 15,  9, 15,  3, 15,\n",
      "        9,  2, 10,  5,  6,  3,  6,  7, 15,  1, 10, 15, -1,  3,  7, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 3 \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [11\t 8\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 6]\n",
      " [3\t 2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [10\t 3\t u\t 10]\n",
      " [u\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 14\n",
      "training fct: {'observations': array([32,  2,  2,  3,  4,  1,  4,  4,  3,  1,  3,  2,  2,  5,  3,  5,  4,\n",
      "        3, 15,  0,  4, 11, 15, 15,  8, -1, 15, -1,  2, 15,  9, 15,  3, 15,\n",
      "        9,  2, 10,  5,  6,  3,  6,  7, 15,  1, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 3 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [11\t 8\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 6]\n",
      " [3\t 2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([32,  2,  2,  3,  4,  1,  4,  4,  3,  1,  3,  2,  2,  5,  3,  5,  4,\n",
      "        3, -1,  0,  4, 11, 15, 15,  8, -1, 15, -1,  2, 15,  9, 15,  3, 15,\n",
      "        9,  2, 10,  5,  6,  3,  6,  7, 15,  1, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: -1 \n",
      "discard pile top: 3 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [11\t 8\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 6]\n",
      " [3\t 2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 5\n",
      "training fct: {'observations': array([23,  2,  2,  4,  4,  1,  4,  4,  3,  1,  3,  2,  2,  5,  3,  5,  4,\n",
      "        8, 15,  0,  4, 11, 15, 15, -1, -1, 15, -1,  2, 15,  9, 15,  3, 15,\n",
      "        9,  2, 10,  5,  6,  3,  6,  7, 15,  1, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [11\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 6]\n",
      " [3\t 2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([23,  2,  2,  4,  4,  1,  4,  4,  3,  1,  3,  2,  1,  5,  3,  5,  4,\n",
      "        3,  8,  0,  4, 11, 15, 15, -1, -1, 15, -1,  2, 15,  9, 15,  3, 15,\n",
      "        9,  2, 10,  5,  6,  3,  6,  7, 15,  1, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 8 \n",
      "discard pile top: 3 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [11\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 6]\n",
      " [3\t 2\t 6\t 7]\n",
      " [u\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 14\n",
      "training fct: {'observations': array([23,  2,  2,  4,  4,  1,  4,  5,  3,  1,  3,  2,  2,  5,  3,  5,  4,\n",
      "        8, 15,  0,  4, 11, 15, 15, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "        9,  2, 10,  5,  6,  3,  6,  7, 15,  1, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [11\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 6]\n",
      " [3\t 2\t 6\t 7]\n",
      " [3\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([23,  2,  2,  4,  4,  1,  4,  5,  3,  1,  3,  2,  2,  5,  3,  5,  4,\n",
      "        8,  8,  0,  4, 11, 15, 15, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "        9,  2, 10,  5,  6,  3,  6,  7, 15,  1, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2,  0, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 8 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [11\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 6]\n",
      " [3\t 2\t 6\t 7]\n",
      " [3\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 0]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 9\n",
      "training fct: {'observations': array([23,  2,  2,  4,  4,  1,  4,  5,  3,  1,  3,  2,  3,  5,  3,  5,  4,\n",
      "        0, 15,  0,  4, 11, 15, 15, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "        9,  2, 10,  5,  6,  3,  6,  7, 15,  1, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2,  8, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [11\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 6]\n",
      " [3\t 2\t 6\t 7]\n",
      " [3\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 8]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([23,  2,  2,  4,  4,  1,  4,  5,  3,  1,  3,  2,  3,  5,  3,  5,  4,\n",
      "        0,  3,  0,  4, 11, 15, 15, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "        9,  2, 10,  5,  6,  3,  6,  7, 15,  1, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2,  8, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 3 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[0\t u\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [11\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 6]\n",
      " [3\t 2\t 6\t 7]\n",
      " [3\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 8]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 3\n",
      "training fct: {'observations': array([26,  2,  2,  4,  4,  1,  4,  6,  3,  1,  3,  2,  3,  5,  4,  5,  4,\n",
      "       10, 15,  0,  4, 11,  3, 15, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "        9,  2, 10,  5,  6,  3,  6,  7, 15,  1, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2,  8, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [11\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 6]\n",
      " [3\t 2\t 6\t 7]\n",
      " [3\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 8]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([26,  2,  2,  4,  4,  1,  4,  6,  3,  1,  3,  2,  3,  5,  3,  5,  4,\n",
      "        0, 10,  0,  4, 11,  3, 15, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "        9,  2, 10,  5,  6,  3,  6,  7, 15,  1, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2,  8, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 10 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [11\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 6]\n",
      " [3\t 2\t 6\t 7]\n",
      " [3\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 8]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 7\n",
      "training fct: {'observations': array([26,  2,  2,  4,  4,  1,  4,  6,  3,  1,  3,  2,  3,  5,  4,  5,  4,\n",
      "        6, 15,  0,  4, 11,  3, 15, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "        9,  2, 10,  5, 10,  3,  6,  7, 15,  1, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2,  8, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [11\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 8]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([26,  2,  2,  4,  4,  1,  4,  6,  3,  1,  2,  2,  3,  5,  4,  5,  4,\n",
      "        0,  6,  0,  4, 11,  3, 15, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "        9,  2, 10,  5, 10,  3,  6,  7, 15,  1, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2,  8, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 6 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [11\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t -1\t 11\t 8]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 0\n",
      "training fct: {'observations': array([26,  2,  2,  4,  4,  1,  4,  6,  3,  1,  3,  2,  3,  5,  4,  5,  4,\n",
      "        1, 15,  0,  4, 11,  3, 15, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "        9,  2, 10,  5, 10,  3,  6,  7, 15,  6, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2,  8, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [11\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 8]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([26,  2,  2,  4,  4,  1,  4,  6,  3,  1,  3,  2,  3,  5,  4,  5,  4,\n",
      "        1,  0,  0,  4, 11,  3, 15, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "        9,  2, 10,  5, 10,  3,  6,  7, 15,  6, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2,  8, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 0 \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [11\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 8]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 2\n",
      "training fct: {'observations': array([15,  2,  2,  4,  5,  1,  4,  6,  3,  1,  3,  2,  3,  5,  4,  5,  4,\n",
      "       11, 15,  0,  4,  0,  3, 15, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "        9,  2, 10,  5, 10,  3,  6,  7, 15,  6, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2,  8, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 8]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([15,  2,  2,  4,  5,  1,  4,  6,  3,  1,  3,  2,  3,  5,  4,  4,  4,\n",
      "        1, 11,  0,  4,  0,  3, 15, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "        9,  2, 10,  5, 10,  3,  6,  7, 15,  6, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2,  8, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 11 \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 9\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 8]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 3\n",
      "training fct: {'observations': array([15,  2,  2,  4,  5,  1,  4,  6,  3,  1,  3,  2,  3,  5,  4,  5,  4,\n",
      "        9, 15,  0,  4,  0,  3, 15, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "       11,  2, 10,  5, 10,  3,  6,  7, 15,  6, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2,  8, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 11\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 8]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([15,  2,  2,  4,  5,  1,  4,  6,  3,  1,  3,  2,  3,  5,  4,  5,  4,\n",
      "        9, 11,  0,  4,  0,  3, 15, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "       11,  2, 10,  5, 10,  3,  6,  7, 15,  6, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2,  8, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 11 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 11\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 8]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 9\n",
      "training fct: {'observations': array([15,  2,  2,  4,  5,  1,  4,  6,  3,  1,  3,  2,  3,  5,  4,  6,  4,\n",
      "        8, 15,  0,  4,  0,  3, 15, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "       11,  2, 10,  5, 10,  3,  6,  7, 15,  6, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 11\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 11]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([15,  2,  2,  4,  5,  1,  4,  6,  3,  1,  3,  2,  2,  5,  4,  6,  4,\n",
      "        9,  8,  0,  4,  0,  3, 15, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "       11,  2, 10,  5, 10,  3,  6,  7, 15,  6, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 8 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 2]\n",
      " [4\t u\t u\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 11\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 11]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 4\n",
      "training fct: {'observations': array([23,  2,  2,  4,  5,  1,  4,  6,  3,  1,  3,  2,  3,  5,  4,  6,  5,\n",
      "       12, 15,  0,  4,  0,  3,  8, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "       11,  2, 10,  5, 10,  3,  6,  7, 15,  6, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 2]\n",
      " [4\t 8\t u\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 11\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 11]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([23,  2,  2,  4,  5,  1,  4,  6,  3,  1,  3,  2,  3,  5,  4,  6,  5,\n",
      "       12,  4,  0,  4,  0,  3,  8, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "       11,  2, 10,  5, 10,  3,  6,  7, 15,  6, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 4 \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 2]\n",
      " [4\t 8\t u\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 11\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t u]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 11]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 11\n",
      "training fct: {'observations': array([23,  1,  2,  4,  5,  1,  4,  6,  5,  1,  3,  2,  3,  5,  4,  6,  5,\n",
      "        4, 15,  0,  4,  0,  3,  8, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "       11,  2, 10,  5, 10,  3,  6,  7,  4,  6, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 2]\n",
      " [4\t 8\t u\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 11\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 11]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([23,  1,  2,  4,  5,  1,  4,  6,  5,  1,  3,  2,  3,  5,  4,  6,  5,\n",
      "        4, 12,  0,  4,  0,  3,  8, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "       11,  2, 10,  5, 10,  3,  6,  7,  4,  6, 10, 12, -1,  3,  7, 11, 15,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 12 \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 2]\n",
      " [4\t 8\t u\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 11\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 11]\n",
      " [10\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 1\n",
      "training fct: {'observations': array([23,  1,  2,  4,  5,  1,  4,  6,  5,  1,  3,  2,  3,  5,  4,  6,  6,\n",
      "       10, 15,  0,  4,  0,  3,  8, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "       11,  2, 10,  5, 10,  3,  6,  7,  4,  6, 12, 12, -1,  3,  7, 11, 15,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 2]\n",
      " [4\t 8\t u\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 11\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 11]\n",
      " [12\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([23,  1,  2,  4,  5,  1,  4,  6,  5,  1,  3,  2,  3,  5,  3,  6,  6,\n",
      "        4, 10,  0,  4,  0,  3,  8, -1, -1, 15, -1,  2, 15,  9, 15,  3,  3,\n",
      "       11,  2, 10,  5, 10,  3,  6,  7,  4,  6, 12, 12, -1,  3,  7, 11, 15,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 10 \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 2]\n",
      " [4\t 8\t u\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 11\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 11]\n",
      " [12\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 9\n",
      "training fct: {'observations': array([31,  1,  2,  4,  5,  1,  4,  6,  5,  1,  3,  2,  3,  5,  4,  6,  6,\n",
      "        2, 15,  0,  4,  0,  3,  8, -1, -1, 15, -1, 10, 15,  9, 15,  3,  3,\n",
      "       11,  2, 10,  5, 10,  3,  6,  7,  4,  6, 12, 12, -1,  3,  7, 11, 15,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t u\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 11\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 11]\n",
      " [12\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([31,  1,  2,  4,  5,  1,  4,  6,  5,  1,  3,  2,  3,  5,  4,  6,  6,\n",
      "        2,  6,  0,  4,  0,  3,  8, -1, -1, 15, -1, 10, 15,  9, 15,  3,  3,\n",
      "       11,  2, 10,  5, 10,  3,  6,  7,  4,  6, 12, 12, -1,  3,  7, 11, 15,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 6 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t u\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 11\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 11]\n",
      " [12\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 3\n",
      "training fct: {'observations': array([31,  1,  2,  4,  5,  1,  4,  6,  5,  1,  4,  2,  3,  5,  4,  6,  6,\n",
      "       11, 15,  0,  4,  0,  3,  8, -1, -1, 15, -1, 10, 15,  9, 15,  3,  3,\n",
      "        6,  2, 10,  5, 10,  3,  6,  7,  4,  6, 12, 12, -1,  3,  7, 11, 15,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t u\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 6\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 11]\n",
      " [12\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([31,  1,  2,  4,  5,  1,  4,  6,  5,  1,  4,  2,  3,  5,  4,  6,  6,\n",
      "       11, 11,  0,  4,  0,  3,  8, -1, -1, 15, -1, 10, 15,  9, 15,  3,  3,\n",
      "        6,  2, 10,  5, 10,  3,  6,  7,  4,  6, 12, 12, -1,  3,  7, 11, 15,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 11 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t u\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 6\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 11]\n",
      " [12\t 3\t u\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 19\n",
      "training fct: {'observations': array([31,  1,  2,  4,  5,  1,  4,  6,  5,  1,  4,  2,  3,  5,  4,  8,  6,\n",
      "       11, 15,  0,  4,  0,  3,  8, -1, -1, 15, -1, 10, 15,  9, 15,  3,  3,\n",
      "        6,  2, 10,  5, 10,  3,  6,  7,  4,  6, 12, 12, -1,  3,  7, 11, 11,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t u\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 6\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 11]\n",
      " [12\t 3\t 11\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([31,  1,  2,  4,  5,  1,  4,  6,  5,  1,  4,  2,  3,  5,  4,  7,  6,\n",
      "       11, 11,  0,  4,  0,  3,  8, -1, -1, 15, -1, 10, 15,  9, 15,  3,  3,\n",
      "        6,  2, 10,  5, 10,  3,  6,  7,  4,  6, 12, 12, -1,  3,  7, 11, 11,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 11 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t u\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 6\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 11]\n",
      " [12\t 3\t 11\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 19\n",
      "training fct: {'observations': array([37,  1,  2,  4,  5,  1,  4,  6,  5,  1,  5,  2,  3,  5,  4,  8,  6,\n",
      "       11, 15,  0,  4,  0,  3,  8, -1, -1,  6, -1, 10, 15,  9, 15,  3,  3,\n",
      "        6,  2, 10,  5, 10,  3,  6,  7,  4,  6, 12, 12, -1,  3,  7, 11, 11,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t 6\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 6\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 11]\n",
      " [12\t 3\t 11\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([37,  1,  2,  4,  5,  1,  4,  6,  5,  1,  5,  2,  3,  5,  4,  8,  6,\n",
      "       11,  4,  0,  4,  0,  3,  8, -1, -1,  6, -1, 10, 15,  9, 15,  3,  3,\n",
      "        6,  2, 10,  5, 10,  3,  6,  7,  4,  6, 12, 12, -1,  3,  7, 11, 11,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 4 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t 6\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 6\t 5\t 6]\n",
      " [3\t 2\t 10\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 11]\n",
      " [12\t 3\t 11\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 7\n",
      "training fct: {'observations': array([37,  1,  2,  4,  5,  1,  4,  6,  6,  1,  5,  2,  3,  5,  4,  8,  6,\n",
      "       10, 15,  0,  4,  0,  3,  8, -1, -1,  6, -1, 10, 15,  9, 15,  3,  3,\n",
      "        6,  2, 10,  5,  4,  3,  6,  7,  4,  6, 12, 12, -1,  3,  7, 11, 11,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t 6\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 6\t 5\t 6]\n",
      " [3\t 2\t 4\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 11]\n",
      " [12\t 3\t 11\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([37,  1,  2,  4,  5,  1,  4,  6,  6,  1,  5,  2,  3,  5,  3,  8,  6,\n",
      "       11, 10,  0,  4,  0,  3,  8, -1, -1,  6, -1, 10, 15,  9, 15,  3,  3,\n",
      "        6,  2, 10,  5,  4,  3,  6,  7,  4,  6, 12, 12, -1,  3,  7, 11, 11,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 10 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t 6\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 6\t 5\t 6]\n",
      " [3\t 2\t 4\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t -1\t 11\t 11]\n",
      " [12\t 3\t 11\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 3\n",
      "training fct: {'observations': array([37,  1,  2,  4,  5,  1,  4,  6,  6,  1,  5,  2,  3,  5,  4,  8,  6,\n",
      "       -1, 15,  0,  4,  0,  3,  8, -1, -1,  6, -1, 10, 15,  9, 15,  3,  3,\n",
      "        6,  2, 10,  5,  4,  3,  6,  7,  4,  6, 12, 12, 10,  3,  7, 11, 11,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t 6\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 6\t 5\t 6]\n",
      " [3\t 2\t 4\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t 10\t 11\t 11]\n",
      " [12\t 3\t 11\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([37,  1,  2,  3,  5,  1,  4,  6,  6,  1,  5,  2,  3,  5,  4,  8,  6,\n",
      "       11, -1,  0,  4,  0,  3,  8, -1, -1,  6, -1, 10, 15,  9, 15,  3,  3,\n",
      "        6,  2, 10,  5,  4,  3,  6,  7,  4,  6, 12, 12, 10,  3,  7, 11, 11,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: -1 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t 6\t u]\n",
      " [0\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 6\t 5\t 6]\n",
      " [3\t 2\t 4\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t 10\t 11\t 11]\n",
      " [12\t 3\t 11\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 2\n",
      "training fct: {'observations': array([36,  1,  2,  4,  5,  1,  4,  6,  6,  1,  5,  2,  3,  5,  4,  8,  6,\n",
      "        0, 15,  0,  4, -1,  3,  8, -1, -1,  6, -1, 10, 15,  9, 15,  3,  3,\n",
      "        6,  2, 10,  5,  4,  3,  6,  7,  4,  6, 12, 12, 10,  3,  7, 11, 11,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t 6\t u]\n",
      " [-1\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 6\t 5\t 6]\n",
      " [3\t 2\t 4\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t 10\t 11\t 11]\n",
      " [12\t 3\t 11\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([36,  1,  2,  4,  5,  1,  4,  6,  6,  1,  5,  2,  3,  5,  4,  8,  6,\n",
      "        0,  6,  0,  4, -1,  3,  8, -1, -1,  6, -1, 10, 15,  9, 15,  3,  3,\n",
      "        6,  2, 10,  5,  4,  3,  6,  7,  4,  6, 12, 12, 10,  3,  7, 11, 11,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 6 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t 6\t u]\n",
      " [-1\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[u\t 6\t 5\t 6]\n",
      " [3\t 2\t 4\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t 10\t 11\t 11]\n",
      " [12\t 3\t 11\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 0\n",
      "training fct: {'observations': array([36,  0,  2,  4,  5,  1,  4,  6,  6,  2,  6,  2,  3,  5,  4,  8,  6,\n",
      "        5, 15,  0,  4, -1,  3,  8, -1, -1,  6, -1, 10, 15,  9,  6,  3,  3,\n",
      "        6,  2, 10,  5,  4,  3,  6,  7,  4,  6, 12, 12, 10,  3,  7, 11, 11,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t 6\t u]\n",
      " [-1\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[6\t 6\t 5\t 6]\n",
      " [3\t 2\t 4\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t 10\t 11\t 11]\n",
      " [12\t 3\t 11\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([36,  0,  2,  4,  5,  1,  4,  6,  6,  1,  6,  2,  3,  5,  4,  8,  6,\n",
      "        0,  5,  0,  4, -1,  3,  8, -1, -1,  6, -1, 10, 15,  9,  6,  3,  3,\n",
      "        6,  2, 10,  5,  4,  3,  6,  7,  4,  6, 12, 12, 10,  3,  7, 11, 11,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 5 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t 6\t u]\n",
      " [-1\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[6\t 6\t 5\t 6]\n",
      " [3\t 2\t 4\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t 10\t 11\t 11]\n",
      " [12\t 3\t 11\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_2: 3\n",
      "training fct: {'observations': array([36,  0,  2,  4,  5,  1,  4,  6,  6,  2,  6,  2,  3,  5,  4,  8,  6,\n",
      "       10, 15,  0,  4, -1,  3,  8, -1, -1,  6, -1, 10, 15,  9,  6,  3,  3,\n",
      "        6,  2, 10,  5,  4,  3,  6,  7,  4,  6, 12, 12,  5,  3,  7, 11, 11,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t 6\t u]\n",
      " [-1\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[6\t 6\t 5\t 6]\n",
      " [3\t 2\t 4\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t 5\t 11\t 11]\n",
      " [12\t 3\t 11\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([36,  0,  2,  4,  5,  1,  4,  6,  6,  2,  6,  2,  3,  5,  3,  8,  6,\n",
      "        0, 10,  0,  4, -1,  3,  8, -1, -1,  6, -1, 10, 15,  9,  6,  3,  3,\n",
      "        6,  2, 10,  5,  4,  3,  6,  7,  4,  6, 12, 12,  5,  3,  7, 11, 11,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 10 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t 6\t u]\n",
      " [-1\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[6\t 6\t 5\t 6]\n",
      " [3\t 2\t 4\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t 5\t 11\t 11]\n",
      " [12\t 3\t 11\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_0: 10\n",
      "training fct: {'observations': array([46,  0,  2,  5,  5,  1,  4,  6,  6,  2,  6,  2,  3,  5,  4,  8,  6,\n",
      "       -1, 15,  0,  4, -1,  3,  8, -1, -1,  6, -1, 10, 10,  9,  6,  3,  3,\n",
      "        6,  2, 10,  5,  4,  3,  6,  7,  4,  6, 12, 12,  5,  3,  7, 11, 11,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t 6\t 10]\n",
      " [-1\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[6\t 6\t 5\t 6]\n",
      " [3\t 2\t 4\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t 5\t 11\t 11]\n",
      " [12\t 3\t 11\t 10]\n",
      " [12\t 7\t -2\t u]]\n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([46,  0,  2,  5,  5,  1,  4,  6,  6,  2,  6,  2,  3,  5,  4,  8,  6,\n",
      "       -1, 15,  0,  4, -1,  3,  8, -1, -1,  6, -1, 10, 10,  9,  6,  3,  3,\n",
      "        6,  2, 10,  5,  4,  3,  6,  7,  4,  6, 12, 12,  5,  3,  7, 11, 11,\n",
      "       -2, 11, 10, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 38.66666666666667 True {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: -1 \n",
      "======= GAME DONE ======== \n",
      "Results: {0: 46.0, 1: 118.0, 2: 87.0} \n",
      "======= Player 0 ========== \n",
      "[[0\t 3\t -1\t 10]\n",
      " [4\t 8\t 6\t 10]\n",
      " [-1\t -1\t -1\t 9]]\n",
      "======= Player 1 ========== \n",
      "[[6\t 6\t 5\t 6]\n",
      " [3\t 2\t 4\t 7]\n",
      " [3\t 10\t 3\t 4]]\n",
      "======= Player 2 ========== \n",
      "[[6\t 5\t 11\t 11]\n",
      " [12\t 3\t 11\t 10]\n",
      " [12\t 7\t -2\t u1]]\n",
      "\n",
      "done 38.66666666666667\n",
      "{'player_1': -33.33333333333333, 'player_2': -2.3333333333333286}\n"
     ]
    }
   ],
   "source": [
    "i_episode = 1  \n",
    "while i_episode <= 1:\n",
    "    i_episode  += 1\n",
    "    env_pettingzoo.reset()\n",
    "    for agent in env_pettingzoo.agent_iter(max_iter=600):        \n",
    "        # get observation (state) for current agent:\n",
    "        obs, reward, done, info = env_pettingzoo.last()\n",
    "        \n",
    "        print(\"training fct:\", obs, reward, done, info)\n",
    "        # perform q-learning with update_Q_value()\n",
    "        # your code here\n",
    "        \n",
    "        env_pettingzoo.render()\n",
    "        \n",
    "        # store current state            \n",
    "        if not done: \n",
    "            # choose action using epsilon_greedy_policy()\n",
    "            # your code here    \n",
    "            observation = obs[\"observations\"]\n",
    "            action_mask = obs[\"action_mask\"]\n",
    "            action = random_admissible_policy(observation, action_mask)\n",
    "        \n",
    "            print(f\"sampled action {agent}: {action}\")\n",
    "            env_pettingzoo.step(action)\n",
    "        else: \n",
    "            # agent is done\n",
    "            env_pettingzoo.step(None)\n",
    "            print('done', reward)\n",
    "            break\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(env_pettingzoo._cumulative_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more envs test with rllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.20.107.113',\n",
       " 'raylet_ip_address': '172.20.107.113',\n",
       " 'redis_address': '172.20.107.113:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2022-01-30_10-49-02_206427_299/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-01-30_10-49-02_206427_299/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-01-30_10-49-02_206427_299',\n",
       " 'metrics_export_port': 48711,\n",
       " 'node_id': '82e969e898a4593bfe1a4fdfecff14ef0ec554f194ae696ef63ce318'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import init\n",
    "init(num_cpus=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "from rlskyjo.environment import skyjo_env\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "import ray.rllib.agents.dqn as dqn\n",
    "from ray.rllib.agents import ppo\n",
    "from copy import deepcopy\n",
    "from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from rlskyjo.models.action_mask_model import TorchActionMaskModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.agents.dqn import DQNTrainer, DQNTorchPolicy\n",
    "from ray.tune.logger import pretty_print\n",
    "import os\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "  warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "/home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "  warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env_name  = \"pettingzoo_skyjo\"\n",
    "def env_creator():\n",
    "        env = skyjo_env.env(**skyjo_env_cfg)\n",
    "        return env\n",
    "\n",
    "register_env(env_name,\n",
    "                lambda config: PettingZooEnv(env_creator()))\n",
    "ModelCatalog.register_custom_model(\n",
    "        \"pa_model2\", TorchActionMaskModel\n",
    "    )\n",
    "env = PettingZooEnv(env_creator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observations': array([ 4, 10,  1,  0,  0,  1,  0,  1,  0,  0,  1,  0,  0,  1,  0,  1,  1,\n",
       "        11, 15, 15, 15, 15, 15, 15,  6, 15, -2, 15, 15, 15, 15, 15, 15, 15,\n",
       "         9,  3, 15, 15, 15, 15, 15, 15, 15, 15, 12, 15, 15,  1, 15, 15, 15,\n",
       "        15, 15, 15, 15], dtype=int8),\n",
       " 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1], dtype=int8)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.observe(env.env.agent_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with multiagent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 10:49:51,590\tWARNING ppo.py:143 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=15 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 266.\n",
      "2022-01-30 10:49:51,591\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-01-30 10:49:51,591\tINFO trainer.py:743 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=595)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=595)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=595)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=595)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=598)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=598)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=598)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=598)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=601)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=601)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=601)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=601)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=592)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=592)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=592)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=592)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=591)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=591)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=591)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=591)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=602)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=602)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=602)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=602)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=599)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=599)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=599)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=599)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=589)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=589)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=589)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=589)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=597)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=597)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=597)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=597)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=596)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=596)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=596)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=596)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=590)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=590)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=590)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=590)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=600)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=600)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=600)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=600)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=593)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=593)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=593)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=593)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=603)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=603)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=603)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=603)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=588)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=588)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=588)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=588)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "2022-01-30 10:50:17,566\tINFO trainable.py:124 -- Trainable.setup took 25.977 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "custom_config={\n",
    "    \"env\":env_name,\n",
    "    \"model\": {\n",
    "        \"custom_model\": \"pa_model2\",\n",
    "    },\n",
    "    \"framework\": \"torch\",\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "    \"num_gpus\": int(torch.cuda.device_count()),\n",
    "    \"num_workers\": os.cpu_count() - 1,\n",
    "    \"multiagent\":{\n",
    "            \"policies\": {\n",
    "                name: (None, env.observation_space, env.action_space, {}) for name in env.agents\n",
    "            },\n",
    "            \"policy_mapping_fn\": lambda agent_id: agent_id\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "ppo_config = ppo.DEFAULT_CONFIG.copy()\n",
    "ppo_config.update(custom_config)\n",
    "\n",
    "trainer = ppo.PPOTrainer(config=ppo_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=598)\u001b[0m 2022-01-30 10:50:17,655\tWARNING deprecation.py:45 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=598)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/numpy/core/_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=598)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=598)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/numpy/core/_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=598)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 7950\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_10-50-35\n",
      "done: false\n",
      "episode_len_mean: 102.12676056338029\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000018\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 71\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.7233023464679718\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010768052894116928\n",
      "        policy_loss: -0.0023573344200849534\n",
      "        total_loss: 673.5334084828695\n",
      "        vf_explained_var: 0.002090949813524882\n",
      "        vf_loss: 673.5336132303873\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.733982622027397\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009977537440420037\n",
      "        policy_loss: -0.027513244980946185\n",
      "        total_loss: 539.0426714833577\n",
      "        vf_explained_var: 0.0021769368648529054\n",
      "        vf_loss: 539.0681878026327\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.7251206332445144\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011904101538475874\n",
      "        policy_loss: -0.09328900863416494\n",
      "        total_loss: 491.97131457646685\n",
      "        vf_explained_var: 0.00815347929795583\n",
      "        vf_loss: 492.0622229639689\n",
      "  num_agent_steps_sampled: 7950\n",
      "  num_agent_steps_trained: 7950\n",
      "  num_steps_sampled: 7980\n",
      "  num_steps_trained: 7980\n",
      "iterations_since_restore: 1\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.577272727272728\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.709090909090904\n",
      "  vram_util_percent0: 0.39525627367424243\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 46.0\n",
      "  player_1: 65.33333333333333\n",
      "  player_2: 54.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: -8.676056338028168\n",
      "  player_1: 7.422535211267606\n",
      "  player_2: 4.253521126760563\n",
      "policy_reward_min:\n",
      "  player_0: -81.66666666666667\n",
      "  player_1: -65.33333333333333\n",
      "  player_2: -68.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09861484603492736\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 5.903267285204072\n",
      "  mean_inference_ms: 1.8156482909832874\n",
      "  mean_raw_obs_processing_ms: 0.24585834382512342\n",
      "time_since_restore: 17.564250946044922\n",
      "time_this_iter_s: 17.564250946044922\n",
      "time_total_s: 17.564250946044922\n",
      "timers:\n",
      "  learn_throughput: 606.38\n",
      "  learn_time_ms: 13160.056\n",
      "  load_throughput: 1166343.03\n",
      "  load_time_ms: 6.842\n",
      "  sample_throughput: 1788.379\n",
      "  sample_time_ms: 4462.141\n",
      "  update_time_ms: 4.912\n",
      "timestamp: 1643536235\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7980\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 23910\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_10-51-08\n",
      "done: false\n",
      "episode_len_mean: 109.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 217\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6994233739376068\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010256346383612993\n",
      "        policy_loss: -0.08627845511461298\n",
      "        total_loss: 624.7530986181895\n",
      "        vf_explained_var: 0.0497225679953893\n",
      "        vf_loss: 624.8373245366414\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6785005140304565\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011845888281085291\n",
      "        policy_loss: -0.06432993776009728\n",
      "        total_loss: 558.5531546147664\n",
      "        vf_explained_var: 0.006128572920958201\n",
      "        vf_loss: 558.615113957723\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6875611867507299\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009919067485504677\n",
      "        policy_loss: -0.04915718118349711\n",
      "        total_loss: 612.5865832392375\n",
      "        vf_explained_var: 0.0621675451596578\n",
      "        vf_loss: 612.6337553151449\n",
      "  num_agent_steps_sampled: 23910\n",
      "  num_agent_steps_trained: 23910\n",
      "  num_steps_sampled: 23940\n",
      "  num_steps_trained: 23940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 3\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.863636363636362\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.59999999999998\n",
      "  vram_util_percent0: 0.39713541666666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 56.0\n",
      "  player_1: 49.0\n",
      "  player_2: 54.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.1766666666666667\n",
      "  player_1: -2.793333333333332\n",
      "  player_2: 3.6166666666666663\n",
      "policy_reward_min:\n",
      "  player_0: -81.66666666666667\n",
      "  player_1: -87.66666666666667\n",
      "  player_2: -81.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09376753276125495\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 2.446807816671032\n",
      "  mean_inference_ms: 1.7321489904658034\n",
      "  mean_raw_obs_processing_ms: 0.23318681848519784\n",
      "time_since_restore: 50.465391397476196\n",
      "time_this_iter_s: 17.972360849380493\n",
      "time_total_s: 50.465391397476196\n",
      "timers:\n",
      "  learn_throughput: 551.371\n",
      "  learn_time_ms: 14473.007\n",
      "  load_throughput: 824302.935\n",
      "  load_time_ms: 9.681\n",
      "  sample_throughput: 705.224\n",
      "  sample_time_ms: 11315.552\n",
      "  update_time_ms: 5.237\n",
      "timestamp: 1643536268\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 23940\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 39871\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_10-51-43\n",
      "done: false\n",
      "episode_len_mean: 115.12\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 73\n",
      "episodes_total: 357\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6366658866405488\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012679561969502894\n",
      "        policy_loss: -0.07415324546086291\n",
      "        total_loss: 532.2611072794597\n",
      "        vf_explained_var: 0.07138492316007614\n",
      "        vf_loss: 532.3327276357015\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6493675816059112\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011387253743799827\n",
      "        policy_loss: -0.09462582044303418\n",
      "        total_loss: 462.6684444618225\n",
      "        vf_explained_var: -0.008883411586284638\n",
      "        vf_loss: 462.7607954057058\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6351747532685599\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012338538302841092\n",
      "        policy_loss: -0.05813884633632067\n",
      "        total_loss: 497.4824031321208\n",
      "        vf_explained_var: 0.0413505091269811\n",
      "        vf_loss: 497.5380749766032\n",
      "  num_agent_steps_sampled: 39871\n",
      "  num_agent_steps_trained: 39871\n",
      "  num_steps_sampled: 39900\n",
      "  num_steps_trained: 39900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 5\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.45217391304348\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.59999999999998\n",
      "  vram_util_percent0: 0.3971354166666667\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 55.0\n",
      "  player_1: 51.33333333333333\n",
      "  player_2: 52.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 2.4999999999999996\n",
      "  player_1: 3.7199999999999993\n",
      "  player_2: -3.2199999999999993\n",
      "policy_reward_min:\n",
      "  player_0: -86.66666666666667\n",
      "  player_1: -64.0\n",
      "  player_2: -76.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09400168436302274\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 1.5089478633692108\n",
      "  mean_inference_ms: 1.7515953492361094\n",
      "  mean_raw_obs_processing_ms: 0.2343059151358334\n",
      "time_since_restore: 85.54128193855286\n",
      "time_this_iter_s: 18.517515182495117\n",
      "time_total_s: 85.54128193855286\n",
      "timers:\n",
      "  learn_throughput: 527.033\n",
      "  learn_time_ms: 15141.356\n",
      "  load_throughput: 924381.112\n",
      "  load_time_ms: 8.633\n",
      "  sample_throughput: 581.688\n",
      "  sample_time_ms: 13718.704\n",
      "  update_time_ms: 5.538\n",
      "timestamp: 1643536303\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 39900\n",
      "training_iteration: 5\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 55830\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_10-52-24\n",
      "done: false\n",
      "episode_len_mean: 117.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 66\n",
      "episodes_total: 491\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5914639172951381\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013670374564947754\n",
      "        policy_loss: -0.09432859371726711\n",
      "        total_loss: 422.06704874038695\n",
      "        vf_explained_var: -0.01733822743097941\n",
      "        vf_loss: 422.1586420122782\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.614007617632548\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013120459248257787\n",
      "        policy_loss: -0.07342447037342936\n",
      "        total_loss: 440.2869093672434\n",
      "        vf_explained_var: 0.07517364005247752\n",
      "        vf_loss: 440.35771052837373\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.59779279311498\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012406538660529935\n",
      "        policy_loss: -0.08490137354160349\n",
      "        total_loss: 624.781702448527\n",
      "        vf_explained_var: -0.004169207314650218\n",
      "        vf_loss: 624.8641237449646\n",
      "  num_agent_steps_sampled: 55830\n",
      "  num_agent_steps_trained: 55830\n",
      "  num_steps_sampled: 55860\n",
      "  num_steps_trained: 55860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 7\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.860714285714286\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.59999999999999\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 49.33333333333333\n",
      "  player_1: 58.66666666666667\n",
      "  player_2: 48.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 3.6133333333333337\n",
      "  player_1: 1.0733333333333333\n",
      "  player_2: -1.6866666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -82.66666666666667\n",
      "  player_1: -81.33333333333333\n",
      "  player_2: -97.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09399926614220824\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 1.1600278914999456\n",
      "  mean_inference_ms: 1.7544930286574365\n",
      "  mean_raw_obs_processing_ms: 0.23452428601964215\n",
      "time_since_restore: 126.11209893226624\n",
      "time_this_iter_s: 23.8162624835968\n",
      "time_total_s: 126.11209893226624\n",
      "timers:\n",
      "  learn_throughput: 492.171\n",
      "  learn_time_ms: 16213.864\n",
      "  load_throughput: 652020.598\n",
      "  load_time_ms: 12.239\n",
      "  sample_throughput: 537.305\n",
      "  sample_time_ms: 14851.902\n",
      "  update_time_ms: 9.285\n",
      "timestamp: 1643536344\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 55860\n",
      "training_iteration: 7\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 71790\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_10-53-07\n",
      "done: false\n",
      "episode_len_mean: 124.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 64\n",
      "episodes_total: 623\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.556938475171725\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012399751896619774\n",
      "        policy_loss: -0.061970246496299904\n",
      "        total_loss: 368.99082703908283\n",
      "        vf_explained_var: 0.08529407560825347\n",
      "        vf_loss: 369.0503173319499\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5709783871968588\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01355642096719091\n",
      "        policy_loss: -0.13549769463638464\n",
      "        total_loss: 386.41937416712443\n",
      "        vf_explained_var: 0.04761102279027303\n",
      "        vf_loss: 386.5521607526143\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5689137595891953\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013111955587681579\n",
      "        policy_loss: -0.04938678205013275\n",
      "        total_loss: 441.80529134114585\n",
      "        vf_explained_var: -0.015122055013974508\n",
      "        vf_loss: 441.8520557085673\n",
      "  num_agent_steps_sampled: 71790\n",
      "  num_agent_steps_trained: 71790\n",
      "  num_steps_sampled: 71820\n",
      "  num_steps_trained: 71820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 9\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.596153846153847\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.64999999999999\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 56.33333333333333\n",
      "  player_1: 52.66666666666667\n",
      "  player_2: 49.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 1.8933333333333335\n",
      "  player_1: 0.263333333333334\n",
      "  player_2: 0.843333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -86.66666666666667\n",
      "  player_1: -85.66666666666667\n",
      "  player_2: -85.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09915282736780034\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.9826771517376094\n",
      "  mean_inference_ms: 1.8568318720765797\n",
      "  mean_raw_obs_processing_ms: 0.2479493662157469\n",
      "time_since_restore: 169.48989534378052\n",
      "time_this_iter_s: 21.77919864654541\n",
      "time_total_s: 169.48989534378052\n",
      "timers:\n",
      "  learn_throughput: 468.431\n",
      "  learn_time_ms: 17035.602\n",
      "  load_throughput: 722107.084\n",
      "  load_time_ms: 11.051\n",
      "  sample_throughput: 479.754\n",
      "  sample_time_ms: 16633.518\n",
      "  update_time_ms: 8.529\n",
      "timestamp: 1643536387\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 71820\n",
      "training_iteration: 9\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 87750\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_10-53-51\n",
      "done: false\n",
      "episode_len_mean: 127.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 60\n",
      "episodes_total: 747\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5203477082649868\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01437673569096404\n",
      "        policy_loss: -0.04427846044146766\n",
      "        total_loss: 390.2813377380371\n",
      "        vf_explained_var: 0.1231875902414322\n",
      "        vf_loss: 390.3227432632446\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5281340835491817\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01277125316273062\n",
      "        policy_loss: -0.1197267339254419\n",
      "        total_loss: 378.6748738161723\n",
      "        vf_explained_var: 0.014477274417877196\n",
      "        vf_loss: 378.79204573313393\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.509756212234497\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013999711491530035\n",
      "        policy_loss: -0.08983622123487293\n",
      "        total_loss: 377.72875228722893\n",
      "        vf_explained_var: 0.027250055074691772\n",
      "        vf_loss: 377.8157879861196\n",
      "  num_agent_steps_sampled: 87750\n",
      "  num_agent_steps_trained: 87750\n",
      "  num_steps_sampled: 87780\n",
      "  num_steps_trained: 87780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 11\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.962962962962964\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.67037037037038\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 53.333333333333336\n",
      "  player_1: 48.66666666666667\n",
      "  player_2: 53.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 2.0400000000000005\n",
      "  player_1: -0.6999999999999997\n",
      "  player_2: 1.6600000000000006\n",
      "policy_reward_min:\n",
      "  player_0: -87.33333333333333\n",
      "  player_1: -91.66666666666667\n",
      "  player_2: -74.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10165658165564243\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.8731095905238578\n",
      "  mean_inference_ms: 1.9117643024367135\n",
      "  mean_raw_obs_processing_ms: 0.2560601378006925\n",
      "time_since_restore: 213.21795535087585\n",
      "time_this_iter_s: 22.467636585235596\n",
      "time_total_s: 213.21795535087585\n",
      "timers:\n",
      "  learn_throughput: 442.118\n",
      "  learn_time_ms: 18049.482\n",
      "  load_throughput: 736472.644\n",
      "  load_time_ms: 10.835\n",
      "  sample_throughput: 423.537\n",
      "  sample_time_ms: 18841.322\n",
      "  update_time_ms: 8.515\n",
      "timestamp: 1643536431\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 87780\n",
      "training_iteration: 11\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 103711\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_10-54-23\n",
      "done: false\n",
      "episode_len_mean: 131.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 59\n",
      "episodes_total: 874\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.472951247692108\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012973241758336371\n",
      "        policy_loss: -0.10243783087469638\n",
      "        total_loss: 536.7738394737244\n",
      "        vf_explained_var: -0.011289125084877014\n",
      "        vf_loss: 536.873683166504\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4836988562345506\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016548485392006568\n",
      "        policy_loss: -0.06401602160806458\n",
      "        total_loss: 374.78723987579343\n",
      "        vf_explained_var: 0.044964748919010165\n",
      "        vf_loss: 374.847947546641\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.491519453128179\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01423106924372405\n",
      "        policy_loss: -0.08341028923168778\n",
      "        total_loss: 586.8899717585246\n",
      "        vf_explained_var: -0.04578392297029495\n",
      "        vf_loss: 586.9705310821533\n",
      "  num_agent_steps_sampled: 103711\n",
      "  num_agent_steps_trained: 103711\n",
      "  num_steps_sampled: 103740\n",
      "  num_steps_trained: 103740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 13\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.152631578947364\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.72631578947367\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 56.0\n",
      "  player_1: 58.66666666666667\n",
      "  player_2: 59.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.8833333333333333\n",
      "  player_1: 2.773333333333333\n",
      "  player_2: -2.6566666666666663\n",
      "policy_reward_min:\n",
      "  player_0: -80.0\n",
      "  player_1: -105.0\n",
      "  player_2: -87.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10268536670065732\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7923671097561406\n",
      "  mean_inference_ms: 1.930738830009069\n",
      "  mean_raw_obs_processing_ms: 0.2586418870780387\n",
      "time_since_restore: 245.42848563194275\n",
      "time_this_iter_s: 15.939855813980103\n",
      "time_total_s: 245.42848563194275\n",
      "timers:\n",
      "  learn_throughput: 444.759\n",
      "  learn_time_ms: 17942.31\n",
      "  load_throughput: 769903.319\n",
      "  load_time_ms: 10.365\n",
      "  sample_throughput: 404.431\n",
      "  sample_time_ms: 19731.43\n",
      "  update_time_ms: 8.548\n",
      "timestamp: 1643536463\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 103740\n",
      "training_iteration: 13\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 119670\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_10-54-55\n",
      "done: false\n",
      "episode_len_mean: 129.42\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 61\n",
      "episodes_total: 994\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4470042953888576\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014834335349524691\n",
      "        policy_loss: -0.055284362708528836\n",
      "        total_loss: 358.1921843910217\n",
      "        vf_explained_var: 0.16278747538725535\n",
      "        vf_loss: 358.2445009295146\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4386708843708038\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01393038332503238\n",
      "        policy_loss: -0.11863132343782733\n",
      "        total_loss: 364.6544397290548\n",
      "        vf_explained_var: 0.07500788241624833\n",
      "        vf_loss: 364.7702843093872\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4608504062891006\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01706987844508215\n",
      "        policy_loss: -0.07411470013981064\n",
      "        total_loss: 511.8411809412638\n",
      "        vf_explained_var: 0.028805292149384817\n",
      "        vf_loss: 511.9118828900655\n",
      "  num_agent_steps_sampled: 119670\n",
      "  num_agent_steps_trained: 119670\n",
      "  num_steps_sampled: 119700\n",
      "  num_steps_trained: 119700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 15\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.594999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.56\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 54.0\n",
      "  player_1: 64.0\n",
      "  player_2: 61.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.636666666666666\n",
      "  player_1: 6.336666666666666\n",
      "  player_2: -6.973333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -82.0\n",
      "  player_1: -96.0\n",
      "  player_2: -86.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10113054210287756\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7258605404719017\n",
      "  mean_inference_ms: 1.9014116316081293\n",
      "  mean_raw_obs_processing_ms: 0.25471392915347624\n",
      "time_since_restore: 277.66547679901123\n",
      "time_this_iter_s: 16.46042561531067\n",
      "time_total_s: 277.66547679901123\n",
      "timers:\n",
      "  learn_throughput: 451.534\n",
      "  learn_time_ms: 17673.107\n",
      "  load_throughput: 768374.478\n",
      "  load_time_ms: 10.386\n",
      "  sample_throughput: 410.433\n",
      "  sample_time_ms: 19442.905\n",
      "  update_time_ms: 8.463\n",
      "timestamp: 1643536495\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 119700\n",
      "training_iteration: 15\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 135631\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_10-55-27\n",
      "done: false\n",
      "episode_len_mean: 135.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 55\n",
      "episodes_total: 1109\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4222554882367453\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01532965781420053\n",
      "        policy_loss: -0.12486338580648104\n",
      "        total_loss: 416.71659622510276\n",
      "        vf_explained_var: 0.011572185357411703\n",
      "        vf_loss: 416.83839380900065\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4129819122950236\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014659634587660548\n",
      "        policy_loss: -0.06897792969519893\n",
      "        total_loss: 388.5120932896932\n",
      "        vf_explained_var: -0.11435291975736618\n",
      "        vf_loss: 388.57813870747884\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4246607571840286\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014531289452994922\n",
      "        policy_loss: -0.05931910338345915\n",
      "        total_loss: 420.9049143473307\n",
      "        vf_explained_var: 0.012936891118685404\n",
      "        vf_loss: 420.9613284937541\n",
      "  num_agent_steps_sampled: 135631\n",
      "  num_agent_steps_trained: 135631\n",
      "  num_steps_sampled: 135660\n",
      "  num_steps_trained: 135660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 17\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.705000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.59999999999999\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 64.66666666666667\n",
      "  player_1: 65.33333333333333\n",
      "  player_2: 56.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 1.2366666666666668\n",
      "  player_1: 4.666666666666666\n",
      "  player_2: -2.903333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -77.0\n",
      "  player_1: -95.33333333333333\n",
      "  player_2: -80.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09990152654103905\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.6767443696100142\n",
      "  mean_inference_ms: 1.8796601229102652\n",
      "  mean_raw_obs_processing_ms: 0.2513630190500411\n",
      "time_since_restore: 309.10139513015747\n",
      "time_this_iter_s: 16.157912731170654\n",
      "time_total_s: 309.10139513015747\n",
      "timers:\n",
      "  learn_throughput: 475.748\n",
      "  learn_time_ms: 16773.588\n",
      "  load_throughput: 996636.62\n",
      "  load_time_ms: 8.007\n",
      "  sample_throughput: 418.132\n",
      "  sample_time_ms: 19084.867\n",
      "  update_time_ms: 5.771\n",
      "timestamp: 1643536527\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 135660\n",
      "training_iteration: 17\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 151590\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_10-55-58\n",
      "done: false\n",
      "episode_len_mean: 141.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 59\n",
      "episodes_total: 1223\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.397604775627454\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015124761728755374\n",
      "        policy_loss: -0.10568152158831556\n",
      "        total_loss: 273.7575640360514\n",
      "        vf_explained_var: 0.020445097784201303\n",
      "        vf_loss: 273.8602196820577\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.381224543452263\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017122184491089598\n",
      "        policy_loss: -0.05435485870422174\n",
      "        total_loss: 325.09974027633666\n",
      "        vf_explained_var: 0.014189207355181376\n",
      "        vf_loss: 325.1506703313192\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.379422733783722\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019477175154875775\n",
      "        policy_loss: -0.10816291237870852\n",
      "        total_loss: 495.8037222290039\n",
      "        vf_explained_var: 0.008012769023577372\n",
      "        vf_loss: 495.90798993428547\n",
      "  num_agent_steps_sampled: 151590\n",
      "  num_agent_steps_trained: 151590\n",
      "  num_steps_sampled: 151620\n",
      "  num_steps_trained: 151620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 19\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.857894736842105\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.599999999999994\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 62.0\n",
      "  player_1: 46.0\n",
      "  player_2: 48.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 5.193333333333335\n",
      "  player_1: 2.923333333333334\n",
      "  player_2: -5.116666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -66.66666666666667\n",
      "  player_1: -74.0\n",
      "  player_2: -83.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09892675998479694\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.6366529588602795\n",
      "  mean_inference_ms: 1.8604102993514118\n",
      "  mean_raw_obs_processing_ms: 0.24873573309399213\n",
      "time_since_restore: 340.10064148902893\n",
      "time_this_iter_s: 15.22931432723999\n",
      "time_total_s: 340.10064148902893\n",
      "timers:\n",
      "  learn_throughput: 510.68\n",
      "  learn_time_ms: 15626.228\n",
      "  load_throughput: 991147.216\n",
      "  load_time_ms: 8.051\n",
      "  sample_throughput: 450.576\n",
      "  sample_time_ms: 17710.663\n",
      "  update_time_ms: 5.672\n",
      "timestamp: 1643536558\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 151620\n",
      "training_iteration: 19\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 167550\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_10-56-28\n",
      "done: false\n",
      "episode_len_mean: 153.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 51\n",
      "episodes_total: 1324\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.331826523343722\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01653720489963727\n",
      "        policy_loss: -0.06690506614744664\n",
      "        total_loss: 263.9844722398122\n",
      "        vf_explained_var: 0.08894425531228384\n",
      "        vf_loss: 264.04806973139443\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3272353148460387\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01725129408487798\n",
      "        policy_loss: -0.13013049403205515\n",
      "        total_loss: 267.59682192802427\n",
      "        vf_explained_var: 0.034926900764306386\n",
      "        vf_loss: 267.723501812617\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3756696675221125\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015908781311084703\n",
      "        policy_loss: -0.07577041913988068\n",
      "        total_loss: 480.31548676172895\n",
      "        vf_explained_var: 0.06920343101024627\n",
      "        vf_loss: 480.3880775674184\n",
      "  num_agent_steps_sampled: 167550\n",
      "  num_agent_steps_trained: 167550\n",
      "  num_steps_sampled: 167580\n",
      "  num_steps_trained: 167580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 21\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.121052631578948\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.599999999999994\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 59.66666666666667\n",
      "  player_1: 48.66666666666667\n",
      "  player_2: 42.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 6.726666666666667\n",
      "  player_1: 4.896666666666668\n",
      "  player_2: -8.623333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -63.66666666666667\n",
      "  player_1: -85.0\n",
      "  player_2: -105.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09824947868323289\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.6060482058307832\n",
      "  mean_inference_ms: 1.8465319490678562\n",
      "  mean_raw_obs_processing_ms: 0.24648154302492883\n",
      "time_since_restore: 370.5622327327728\n",
      "time_this_iter_s: 15.266823768615723\n",
      "time_total_s: 370.5622327327728\n",
      "timers:\n",
      "  learn_throughput: 555.162\n",
      "  learn_time_ms: 14374.191\n",
      "  load_throughput: 957442.036\n",
      "  load_time_ms: 8.335\n",
      "  sample_throughput: 485.439\n",
      "  sample_time_ms: 16438.742\n",
      "  update_time_ms: 5.496\n",
      "timestamp: 1643536588\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 167580\n",
      "training_iteration: 21\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 183510\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_10-57-07\n",
      "done: false\n",
      "episode_len_mean: 159.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 49\n",
      "episodes_total: 1423\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.326730915904045\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01572356470691375\n",
      "        policy_loss: -0.12365098558676739\n",
      "        total_loss: 399.1742883268992\n",
      "        vf_explained_var: 0.09480079958836238\n",
      "        vf_loss: 399.2947947947184\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3006099780400595\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02058256675900187\n",
      "        policy_loss: -0.10860893324948848\n",
      "        total_loss: 187.8967852083842\n",
      "        vf_explained_var: -0.014110037783781688\n",
      "        vf_loss: 188.0012787246704\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2963345837593079\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018103617114774716\n",
      "        policy_loss: -0.05002189465022335\n",
      "        total_loss: 340.9475006421407\n",
      "        vf_explained_var: -0.08755972852309545\n",
      "        vf_loss: 340.9939022699992\n",
      "  num_agent_steps_sampled: 183510\n",
      "  num_agent_steps_trained: 183510\n",
      "  num_steps_sampled: 183540\n",
      "  num_steps_trained: 183540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 23\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.64\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.6\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 46.0\n",
      "  player_1: 60.0\n",
      "  player_2: 69.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: -0.8366666666666663\n",
      "  player_1: 6.133333333333334\n",
      "  player_2: -2.2966666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -90.0\n",
      "  player_1: -63.0\n",
      "  player_2: -78.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09858424623913424\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5823117336851718\n",
      "  mean_inference_ms: 1.8529896308793374\n",
      "  mean_raw_obs_processing_ms: 0.24657309215049478\n",
      "time_since_restore: 408.75235986709595\n",
      "time_this_iter_s: 21.265011310577393\n",
      "time_total_s: 408.75235986709595\n",
      "timers:\n",
      "  learn_throughput: 534.133\n",
      "  learn_time_ms: 14940.106\n",
      "  load_throughput: 832387.371\n",
      "  load_time_ms: 9.587\n",
      "  sample_throughput: 502.056\n",
      "  sample_time_ms: 15894.639\n",
      "  update_time_ms: 5.726\n",
      "timestamp: 1643536627\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 183540\n",
      "training_iteration: 23\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 199472\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_10-57-43\n",
      "done: false\n",
      "episode_len_mean: 158.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 53\n",
      "episodes_total: 1528\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.287905944188436\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018591498126401547\n",
      "        policy_loss: -0.09264131407563884\n",
      "        total_loss: 304.9710652224223\n",
      "        vf_explained_var: -0.03155047595500946\n",
      "        vf_loss: 305.0599878120422\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2690958178043366\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019393260321125847\n",
      "        policy_loss: -0.09820438767783343\n",
      "        total_loss: 304.40994950612384\n",
      "        vf_explained_var: -0.06514518111944198\n",
      "        vf_loss: 304.5023364384969\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2806709470351536\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015780546841802804\n",
      "        policy_loss: -0.09278414787569393\n",
      "        total_loss: 252.7607207139333\n",
      "        vf_explained_var: -0.0841374859213829\n",
      "        vf_loss: 252.85035027186075\n",
      "  num_agent_steps_sampled: 199472\n",
      "  num_agent_steps_trained: 199472\n",
      "  num_steps_sampled: 199500\n",
      "  num_steps_trained: 199500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 25\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.878947368421056\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.673684210526304\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 56.33333333333333\n",
      "  player_1: 48.66666666666667\n",
      "  player_2: 46.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.7766666666666668\n",
      "  player_1: 0.8466666666666666\n",
      "  player_2: 1.3766666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -63.0\n",
      "  player_1: -81.66666666666667\n",
      "  player_2: -82.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10099358606876267\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5668468949813363\n",
      "  mean_inference_ms: 1.8989709188341541\n",
      "  mean_raw_obs_processing_ms: 0.25105230132001394\n",
      "time_since_restore: 444.74014353752136\n",
      "time_this_iter_s: 15.582190752029419\n",
      "time_total_s: 444.74014353752136\n",
      "timers:\n",
      "  learn_throughput: 523.335\n",
      "  learn_time_ms: 15248.369\n",
      "  load_throughput: 718543.485\n",
      "  load_time_ms: 11.106\n",
      "  sample_throughput: 474.368\n",
      "  sample_time_ms: 16822.367\n",
      "  update_time_ms: 5.818\n",
      "timestamp: 1643536663\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 199500\n",
      "training_iteration: 25\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 215430\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_10-58-14\n",
      "done: false\n",
      "episode_len_mean: 169.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 48\n",
      "episodes_total: 1619\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.241743853886922\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015639447645603467\n",
      "        policy_loss: -0.07387168154120445\n",
      "        total_loss: 259.2311087703705\n",
      "        vf_explained_var: 0.056826900045077004\n",
      "        vf_loss: 259.3018519258499\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2309198687473932\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01654956202953476\n",
      "        policy_loss: -0.1169723474762092\n",
      "        total_loss: 223.77769360860188\n",
      "        vf_explained_var: -0.11517615795135498\n",
      "        vf_loss: 223.88970063368478\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.232509693900744\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020227255054651748\n",
      "        policy_loss: -0.07640223982200647\n",
      "        total_loss: 350.2518720181783\n",
      "        vf_explained_var: 0.16832675337791442\n",
      "        vf_loss: 350.3242293612162\n",
      "  num_agent_steps_sampled: 215430\n",
      "  num_agent_steps_trained: 215430\n",
      "  num_steps_sampled: 215460\n",
      "  num_steps_trained: 215460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 27\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.872222222222222\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.70000000000001\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 51.0\n",
      "  player_1: 54.0\n",
      "  player_2: 47.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 4.69\n",
      "  player_1: 3.2500000000000004\n",
      "  player_2: -4.94\n",
      "policy_reward_min:\n",
      "  player_0: -62.66666666666667\n",
      "  player_1: -52.33333333333333\n",
      "  player_2: -84.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10060114371093785\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5499281141682568\n",
      "  mean_inference_ms: 1.8902752790006108\n",
      "  mean_raw_obs_processing_ms: 0.24932528675507482\n",
      "time_since_restore: 475.79943656921387\n",
      "time_this_iter_s: 15.183989763259888\n",
      "time_total_s: 475.79943656921387\n",
      "timers:\n",
      "  learn_throughput: 525.784\n",
      "  learn_time_ms: 15177.323\n",
      "  load_throughput: 755397.856\n",
      "  load_time_ms: 10.564\n",
      "  sample_throughput: 474.77\n",
      "  sample_time_ms: 16808.15\n",
      "  update_time_ms: 5.912\n",
      "timestamp: 1643536694\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 215460\n",
      "training_iteration: 27\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 231390\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_10-58-46\n",
      "done: false\n",
      "episode_len_mean: 176.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 45\n",
      "episodes_total: 1712\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.217271319925785\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017283009001269722\n",
      "        policy_loss: -0.12652008506158988\n",
      "        total_loss: 214.6715980529785\n",
      "        vf_explained_var: -0.1620386838912964\n",
      "        vf_loss: 214.79466145833334\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1729041010141372\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018126286954123428\n",
      "        policy_loss: -0.057007001613577205\n",
      "        total_loss: 230.27960105578106\n",
      "        vf_explained_var: -0.056400859852631886\n",
      "        vf_loss: 230.331170018514\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1936386019984881\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016027350886455308\n",
      "        policy_loss: -0.06887229233980179\n",
      "        total_loss: 223.06022104899088\n",
      "        vf_explained_var: 0.013001104096571604\n",
      "        vf_loss: 223.12428508758546\n",
      "  num_agent_steps_sampled: 231390\n",
      "  num_agent_steps_trained: 231390\n",
      "  num_steps_sampled: 231420\n",
      "  num_steps_trained: 231420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 29\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.544999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.665000000000006\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 56.0\n",
      "  player_1: 64.0\n",
      "  player_2: 36.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.333333333333334\n",
      "  player_1: -0.4966666666666666\n",
      "  player_2: 1.163333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -78.0\n",
      "  player_1: -69.0\n",
      "  player_2: -87.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10033926999046937\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.533819985885707\n",
      "  mean_inference_ms: 1.8832583230272788\n",
      "  mean_raw_obs_processing_ms: 0.24839134178063546\n",
      "time_since_restore: 508.0646421909332\n",
      "time_this_iter_s: 16.329861402511597\n",
      "time_total_s: 508.0646421909332\n",
      "timers:\n",
      "  learn_throughput: 521.681\n",
      "  learn_time_ms: 15296.693\n",
      "  load_throughput: 754476.652\n",
      "  load_time_ms: 10.577\n",
      "  sample_throughput: 477.369\n",
      "  sample_time_ms: 16716.618\n",
      "  update_time_ms: 5.845\n",
      "timestamp: 1643536726\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 231420\n",
      "training_iteration: 29\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 247350\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_10-59-20\n",
      "done: false\n",
      "episode_len_mean: 172.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 43\n",
      "episodes_total: 1802\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.198139215906461\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019952724713376238\n",
      "        policy_loss: -0.05820015856064856\n",
      "        total_loss: 335.22302612940473\n",
      "        vf_explained_var: 0.08583136280377705\n",
      "        vf_loss: 335.277235399882\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1565985667705536\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016636496240680573\n",
      "        policy_loss: -0.11958411044751605\n",
      "        total_loss: 234.6489349937439\n",
      "        vf_explained_var: 0.01925685117642085\n",
      "        vf_loss: 234.763528175354\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.153045928478241\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014534300410038364\n",
      "        policy_loss: -0.10454215194409092\n",
      "        total_loss: 188.61281957626343\n",
      "        vf_explained_var: 0.12316071033477784\n",
      "        vf_loss: 188.7130003007253\n",
      "  num_agent_steps_sampled: 247350\n",
      "  num_agent_steps_trained: 247350\n",
      "  num_steps_sampled: 247380\n",
      "  num_steps_trained: 247380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 31\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.971428571428573\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.70000000000001\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.66666666666667\n",
      "  player_1: 49.33333333333333\n",
      "  player_2: 55.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: -3.3566666666666674\n",
      "  player_1: 2.4133333333333336\n",
      "  player_2: 3.943333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -85.66666666666667\n",
      "  player_1: -59.0\n",
      "  player_2: -63.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09975536473815012\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5194883214950333\n",
      "  mean_inference_ms: 1.8857528940696158\n",
      "  mean_raw_obs_processing_ms: 0.25078836688817296\n",
      "time_since_restore: 542.2668178081512\n",
      "time_this_iter_s: 17.54810857772827\n",
      "time_total_s: 542.2668178081512\n",
      "timers:\n",
      "  learn_throughput: 511.698\n",
      "  learn_time_ms: 15595.126\n",
      "  load_throughput: 728201.542\n",
      "  load_time_ms: 10.959\n",
      "  sample_throughput: 468.122\n",
      "  sample_time_ms: 17046.843\n",
      "  update_time_ms: 6.435\n",
      "timestamp: 1643536760\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 247380\n",
      "training_iteration: 31\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 263310\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_10-59-53\n",
      "done: false\n",
      "episode_len_mean: 170.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 47\n",
      "episodes_total: 1899\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1478302946686745\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018383365350400707\n",
      "        policy_loss: -0.0639843319170177\n",
      "        total_loss: 238.7558607451121\n",
      "        vf_explained_var: 0.07751794546842575\n",
      "        vf_loss: 238.81616878509521\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1587257965405782\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018420728262163663\n",
      "        policy_loss: -0.07574910554569214\n",
      "        total_loss: 260.7680217107137\n",
      "        vf_explained_var: 0.07261553208033243\n",
      "        vf_loss: 260.83824494361875\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1516549361745516\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015323723424365502\n",
      "        policy_loss: -0.11048708651525278\n",
      "        total_loss: 305.85434145291646\n",
      "        vf_explained_var: -0.019645152886708577\n",
      "        vf_loss: 305.9602310244242\n",
      "  num_agent_steps_sampled: 263310\n",
      "  num_agent_steps_trained: 263310\n",
      "  num_steps_sampled: 263340\n",
      "  num_steps_trained: 263340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 33\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.51\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.70000000000001\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 55.66666666666667\n",
      "  player_1: 64.33333333333333\n",
      "  player_2: 61.0\n",
      "policy_reward_mean:\n",
      "  player_0: -5.07\n",
      "  player_1: 6.5\n",
      "  player_2: 1.5700000000000003\n",
      "policy_reward_min:\n",
      "  player_0: -106.66666666666667\n",
      "  player_1: -71.33333333333334\n",
      "  player_2: -83.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09950272893943446\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5065608726187963\n",
      "  mean_inference_ms: 1.8978920864417\n",
      "  mean_raw_obs_processing_ms: 0.2528330534122059\n",
      "time_since_restore: 574.8676207065582\n",
      "time_this_iter_s: 16.471075773239136\n",
      "time_total_s: 574.8676207065582\n",
      "timers:\n",
      "  learn_throughput: 528.591\n",
      "  learn_time_ms: 15096.737\n",
      "  load_throughput: 799226.001\n",
      "  load_time_ms: 9.985\n",
      "  sample_throughput: 468.096\n",
      "  sample_time_ms: 17047.796\n",
      "  update_time_ms: 6.178\n",
      "timestamp: 1643536793\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 263340\n",
      "training_iteration: 33\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 279270\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-00-25\n",
      "done: false\n",
      "episode_len_mean: 184.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 45\n",
      "episodes_total: 1982\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.168316921989123\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021217686514498648\n",
      "        policy_loss: -0.060801374011983475\n",
      "        total_loss: 180.42843714078268\n",
      "        vf_explained_var: -0.00697257916132609\n",
      "        vf_loss: 180.48499546368916\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1305143131812414\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01655513765139612\n",
      "        policy_loss: -0.05348993500694633\n",
      "        total_loss: 139.98841386477153\n",
      "        vf_explained_var: 0.025836507081985472\n",
      "        vf_loss: 140.03693654855093\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1054748329520225\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015222918208174481\n",
      "        policy_loss: -0.14323047983149687\n",
      "        total_loss: 208.9489241409302\n",
      "        vf_explained_var: 0.14853775252898535\n",
      "        vf_loss: 209.08758714358012\n",
      "  num_agent_steps_sampled: 279270\n",
      "  num_agent_steps_trained: 279270\n",
      "  num_steps_sampled: 279300\n",
      "  num_steps_trained: 279300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 35\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.62\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.70000000000001\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 47.0\n",
      "  player_1: 52.33333333333333\n",
      "  player_2: 47.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -1.2366666666666664\n",
      "  player_1: 3.4933333333333336\n",
      "  player_2: 0.7433333333333338\n",
      "policy_reward_min:\n",
      "  player_0: -59.66666666666667\n",
      "  player_1: -66.0\n",
      "  player_2: -60.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09898969043464717\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4962074485529868\n",
      "  mean_inference_ms: 1.8871688997310685\n",
      "  mean_raw_obs_processing_ms: 0.25223248712149937\n",
      "time_since_restore: 607.1103515625\n",
      "time_this_iter_s: 16.344955444335938\n",
      "time_total_s: 607.1103515625\n",
      "timers:\n",
      "  learn_throughput: 539.606\n",
      "  learn_time_ms: 14788.571\n",
      "  load_throughput: 889150.152\n",
      "  load_time_ms: 8.975\n",
      "  sample_throughput: 492.992\n",
      "  sample_time_ms: 16186.878\n",
      "  update_time_ms: 6.034\n",
      "timestamp: 1643536825\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 279300\n",
      "training_iteration: 35\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 295230\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-00-57\n",
      "done: false\n",
      "episode_len_mean: 179.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 44\n",
      "episodes_total: 2069\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1211872842907906\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018796811542509034\n",
      "        policy_loss: -0.07414685032640894\n",
      "        total_loss: 208.13473733901978\n",
      "        vf_explained_var: 0.17047704587380091\n",
      "        vf_loss: 208.20324449221292\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0590724565585454\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015167054338472473\n",
      "        policy_loss: -0.0759008733741939\n",
      "        total_loss: 136.51600752671558\n",
      "        vf_explained_var: 0.07676086614529291\n",
      "        vf_loss: 136.58735789140064\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.113760271370411\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018314951379519093\n",
      "        policy_loss: -0.10074652544688434\n",
      "        total_loss: 176.8818055820465\n",
      "        vf_explained_var: 0.19932352870702744\n",
      "        vf_loss: 176.97705742835998\n",
      "  num_agent_steps_sampled: 295230\n",
      "  num_agent_steps_trained: 295230\n",
      "  num_steps_sampled: 295260\n",
      "  num_steps_trained: 295260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 37\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.55\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.70000000000001\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 55.33333333333333\n",
      "  player_1: 55.33333333333333\n",
      "  player_2: 50.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.2533333333333336\n",
      "  player_1: 2.813333333333334\n",
      "  player_2: -1.0666666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -82.66666666666667\n",
      "  player_1: -73.66666666666667\n",
      "  player_2: -66.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09847775109607325\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.48552264802209477\n",
      "  mean_inference_ms: 1.87839093756815\n",
      "  mean_raw_obs_processing_ms: 0.24984763384618425\n",
      "time_since_restore: 638.5836169719696\n",
      "time_this_iter_s: 16.21145510673523\n",
      "time_total_s: 638.5836169719696\n",
      "timers:\n",
      "  learn_throughput: 536.881\n",
      "  learn_time_ms: 14863.641\n",
      "  load_throughput: 817691.005\n",
      "  load_time_ms: 9.759\n",
      "  sample_throughput: 492.98\n",
      "  sample_time_ms: 16187.265\n",
      "  update_time_ms: 6.035\n",
      "timestamp: 1643536857\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 295260\n",
      "training_iteration: 37\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 311190\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-01-28\n",
      "done: false\n",
      "episode_len_mean: 190.84\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 34\n",
      "episodes_total: 2150\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0856222461660703\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018095884822347703\n",
      "        policy_loss: -0.07390277021254102\n",
      "        total_loss: 229.6587280782064\n",
      "        vf_explained_var: 0.14712995072205862\n",
      "        vf_loss: 229.72720207214354\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0629149697224298\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01624895889218654\n",
      "        policy_loss: -0.11891305423652132\n",
      "        total_loss: 223.38469150225322\n",
      "        vf_explained_var: 0.14922211557626724\n",
      "        vf_loss: 223.49873112042744\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.055421559115251\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018433103723221697\n",
      "        policy_loss: -0.06270247647849223\n",
      "        total_loss: 111.06351765314739\n",
      "        vf_explained_var: -0.09864743808905284\n",
      "        vf_loss: 111.12069041570028\n",
      "  num_agent_steps_sampled: 311190\n",
      "  num_agent_steps_trained: 311190\n",
      "  num_steps_sampled: 311220\n",
      "  num_steps_trained: 311220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 39\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.719999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.730000000000004\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.33333333333333\n",
      "  player_1: 44.66666666666667\n",
      "  player_2: 50.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: -5.193333333333333\n",
      "  player_1: 1.7466666666666675\n",
      "  player_2: 6.446666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -70.33333333333333\n",
      "  player_1: -76.66666666666667\n",
      "  player_2: -53.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09819491933596244\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.47748074118503375\n",
      "  mean_inference_ms: 1.8696080709485197\n",
      "  mean_raw_obs_processing_ms: 0.24952084969471935\n",
      "time_since_restore: 669.9357385635376\n",
      "time_this_iter_s: 15.821304082870483\n",
      "time_total_s: 669.9357385635376\n",
      "timers:\n",
      "  learn_throughput: 539.92\n",
      "  learn_time_ms: 14779.975\n",
      "  load_throughput: 790206.648\n",
      "  load_time_ms: 10.099\n",
      "  sample_throughput: 490.79\n",
      "  sample_time_ms: 16259.495\n",
      "  update_time_ms: 6.786\n",
      "timestamp: 1643536888\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 311220\n",
      "training_iteration: 39\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 327150\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-01-59\n",
      "done: false\n",
      "episode_len_mean: 211.28\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 34\n",
      "episodes_total: 2221\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.023246898651123\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018196139822203486\n",
      "        policy_loss: -0.08384745419801523\n",
      "        total_loss: 169.46687028566996\n",
      "        vf_explained_var: 0.14436178733905156\n",
      "        vf_loss: 169.54525950749715\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0314255861441295\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01859558248559021\n",
      "        policy_loss: -0.11040724790344636\n",
      "        total_loss: 139.61358304977418\n",
      "        vf_explained_var: 0.21940924674272538\n",
      "        vf_loss: 139.71841126441956\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0809723703066507\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016678933190656456\n",
      "        policy_loss: -0.08263972179032862\n",
      "        total_loss: 275.47697542826336\n",
      "        vf_explained_var: 0.08714216142892837\n",
      "        vf_loss: 275.5546121374766\n",
      "  num_agent_steps_sampled: 327150\n",
      "  num_agent_steps_trained: 327150\n",
      "  num_steps_sampled: 327180\n",
      "  num_steps_trained: 327180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 41\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.042105263157895\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.70000000000001\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 52.0\n",
      "  player_1: 51.0\n",
      "  player_2: 51.0\n",
      "policy_reward_mean:\n",
      "  player_0: -3.5299999999999985\n",
      "  player_1: 4.3900000000000015\n",
      "  player_2: 2.140000000000001\n",
      "policy_reward_min:\n",
      "  player_0: -73.0\n",
      "  player_1: -70.33333333333333\n",
      "  player_2: -81.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09788051411848658\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4693932586621931\n",
      "  mean_inference_ms: 1.8628917704094925\n",
      "  mean_raw_obs_processing_ms: 0.24704544813555707\n",
      "time_since_restore: 701.0848009586334\n",
      "time_this_iter_s: 15.311973333358765\n",
      "time_total_s: 701.0848009586334\n",
      "timers:\n",
      "  learn_throughput: 548.372\n",
      "  learn_time_ms: 14552.159\n",
      "  load_throughput: 856810.881\n",
      "  load_time_ms: 9.314\n",
      "  sample_throughput: 496.885\n",
      "  sample_time_ms: 16060.054\n",
      "  update_time_ms: 6.171\n",
      "timestamp: 1643536919\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 327180\n",
      "training_iteration: 41\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 343110\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-02-31\n",
      "done: false\n",
      "episode_len_mean: 209.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 34\n",
      "episodes_total: 2299\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0480040516455968\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01607628164382201\n",
      "        policy_loss: -0.11873480494599789\n",
      "        total_loss: 133.99063932418824\n",
      "        vf_explained_var: 0.2096849811077118\n",
      "        vf_loss: 134.10455200831095\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9721846706668535\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017136624306006498\n",
      "        policy_loss: -0.06654903162891666\n",
      "        total_loss: 171.51901855945587\n",
      "        vf_explained_var: 0.03849127560853958\n",
      "        vf_loss: 171.58042651176453\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.077122246325016\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017705232318174068\n",
      "        policy_loss: -0.07638218437631925\n",
      "        total_loss: 170.4587084054947\n",
      "        vf_explained_var: 0.17524644653002422\n",
      "        vf_loss: 170.52977977752687\n",
      "  num_agent_steps_sampled: 343110\n",
      "  num_agent_steps_trained: 343110\n",
      "  num_steps_sampled: 343140\n",
      "  num_steps_trained: 343140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 43\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.665000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.70000000000001\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 43.66666666666667\n",
      "  player_1: 51.0\n",
      "  player_2: 48.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 1.55\n",
      "  player_1: 2.610000000000001\n",
      "  player_2: -1.1599999999999988\n",
      "policy_reward_min:\n",
      "  player_0: -71.33333333333333\n",
      "  player_1: -88.33333333333333\n",
      "  player_2: -81.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0976079370313003\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4613577243051591\n",
      "  mean_inference_ms: 1.8554724978115076\n",
      "  mean_raw_obs_processing_ms: 0.24576365034205552\n",
      "time_since_restore: 732.4030156135559\n",
      "time_this_iter_s: 16.104538917541504\n",
      "time_total_s: 732.4030156135559\n",
      "timers:\n",
      "  learn_throughput: 552.97\n",
      "  learn_time_ms: 14431.175\n",
      "  load_throughput: 911730.705\n",
      "  load_time_ms: 8.753\n",
      "  sample_throughput: 504.347\n",
      "  sample_time_ms: 15822.436\n",
      "  update_time_ms: 6.199\n",
      "timestamp: 1643536951\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 343140\n",
      "training_iteration: 43\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 359070\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-03-03\n",
      "done: false\n",
      "episode_len_mean: 231.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 33\n",
      "episodes_total: 2364\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.022873271405697\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01731198811172362\n",
      "        policy_loss: -0.09037590968422592\n",
      "        total_loss: 145.9917927821477\n",
      "        vf_explained_var: -0.08076204836368561\n",
      "        vf_loss: 146.07697477022808\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9406873711943626\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014738122062678655\n",
      "        policy_loss: -0.08438897011995626\n",
      "        total_loss: 152.61666262308756\n",
      "        vf_explained_var: 0.010019859671592713\n",
      "        vf_loss: 152.6966298778852\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0276853401462238\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01588994066113912\n",
      "        policy_loss: -0.07406633937886606\n",
      "        total_loss: 123.37282399654389\n",
      "        vf_explained_var: -0.1650768835345904\n",
      "        vf_loss: 123.44212267239888\n",
      "  num_agent_steps_sampled: 359070\n",
      "  num_agent_steps_trained: 359070\n",
      "  num_steps_sampled: 359100\n",
      "  num_steps_trained: 359100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 45\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.105263157894736\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.70000000000001\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 52.33333333333333\n",
      "  player_1: 54.33333333333333\n",
      "  player_2: 55.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.2766666666666664\n",
      "  player_1: -0.5433333333333337\n",
      "  player_2: 2.2666666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -57.0\n",
      "  player_1: -88.33333333333333\n",
      "  player_2: -66.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09735418736994866\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.45561640006912413\n",
      "  mean_inference_ms: 1.851424400195126\n",
      "  mean_raw_obs_processing_ms: 0.2449371782685303\n",
      "time_since_restore: 764.2645514011383\n",
      "time_this_iter_s: 15.704211235046387\n",
      "time_total_s: 764.2645514011383\n",
      "timers:\n",
      "  learn_throughput: 554.33\n",
      "  learn_time_ms: 14395.762\n",
      "  load_throughput: 937387.51\n",
      "  load_time_ms: 8.513\n",
      "  sample_throughput: 504.771\n",
      "  sample_time_ms: 15809.159\n",
      "  update_time_ms: 6.196\n",
      "timestamp: 1643536983\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 359100\n",
      "training_iteration: 45\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 375030\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-03-35\n",
      "done: false\n",
      "episode_len_mean: 246.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 36\n",
      "episodes_total: 2433\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0470606610178947\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017804889826769758\n",
      "        policy_loss: -0.13027131755991528\n",
      "        total_loss: 146.8287603123983\n",
      "        vf_explained_var: 0.19630067457755407\n",
      "        vf_loss: 146.95369058291118\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9269858504335086\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01956137864644518\n",
      "        policy_loss: -0.07382433975115418\n",
      "        total_loss: 189.28967207749685\n",
      "        vf_explained_var: -0.11700475563605627\n",
      "        vf_loss: 189.35762638250986\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.996942583223184\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018530094400437586\n",
      "        policy_loss: -0.06589167099601279\n",
      "        total_loss: 153.23257495403288\n",
      "        vf_explained_var: 0.3213720761736234\n",
      "        vf_loss: 153.29290766239166\n",
      "  num_agent_steps_sampled: 375030\n",
      "  num_agent_steps_trained: 375030\n",
      "  num_steps_sampled: 375060\n",
      "  num_steps_trained: 375060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 47\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.460000000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.70000000000001\n",
      "  vram_util_percent0: 0.3974609375\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 47.33333333333333\n",
      "  player_1: 54.33333333333333\n",
      "  player_2: 55.0\n",
      "policy_reward_mean:\n",
      "  player_0: -5.443333333333333\n",
      "  player_1: 3.8066666666666675\n",
      "  player_2: 4.636666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -62.0\n",
      "  player_1: -63.0\n",
      "  player_2: -67.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09705627763680919\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.44871290390982066\n",
      "  mean_inference_ms: 1.8445716780917103\n",
      "  mean_raw_obs_processing_ms: 0.24383402235485177\n",
      "time_since_restore: 796.6705074310303\n",
      "time_this_iter_s: 16.137408018112183\n",
      "time_total_s: 796.6705074310303\n",
      "timers:\n",
      "  learn_throughput: 550.958\n",
      "  learn_time_ms: 14483.866\n",
      "  load_throughput: 990135.662\n",
      "  load_time_ms: 8.06\n",
      "  sample_throughput: 503.543\n",
      "  sample_time_ms: 15847.688\n",
      "  update_time_ms: 6.187\n",
      "timestamp: 1643537015\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 375060\n",
      "training_iteration: 47\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 390990\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-04-07\n",
      "done: false\n",
      "episode_len_mean: 241.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 2488\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9919464398423831\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020057502479722492\n",
      "        policy_loss: -0.08551763429655694\n",
      "        total_loss: 140.52927629152933\n",
      "        vf_explained_var: -0.20580690284570058\n",
      "        vf_loss: 140.60877692699432\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8499946541587512\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016705310794530893\n",
      "        policy_loss: -0.09694346958072857\n",
      "        total_loss: 154.64705226421356\n",
      "        vf_explained_var: -0.14265290270249048\n",
      "        vf_loss: 154.7389843527476\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9104511645436287\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015479782977860927\n",
      "        policy_loss: -0.04778482521573702\n",
      "        total_loss: 126.36106195767721\n",
      "        vf_explained_var: 0.13569331953922908\n",
      "        vf_loss: 126.40420317490896\n",
      "  num_agent_steps_sampled: 390990\n",
      "  num_agent_steps_trained: 390990\n",
      "  num_steps_sampled: 391020\n",
      "  num_steps_trained: 391020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 49\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.679999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.70000000000001\n",
      "  vram_util_percent0: 0.4021484374999999\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 47.66666666666667\n",
      "  player_1: 52.0\n",
      "  player_2: 53.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: -2.3666666666666667\n",
      "  player_1: 1.093333333333333\n",
      "  player_2: 4.273333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -72.66666666666667\n",
      "  player_1: -72.0\n",
      "  player_2: -65.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09693754758821785\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.44393398213467655\n",
      "  mean_inference_ms: 1.8376935827544094\n",
      "  mean_raw_obs_processing_ms: 0.24356212473618055\n",
      "time_since_restore: 828.7380044460297\n",
      "time_this_iter_s: 16.20512890815735\n",
      "time_total_s: 828.7380044460297\n",
      "timers:\n",
      "  learn_throughput: 548.307\n",
      "  learn_time_ms: 14553.894\n",
      "  load_throughput: 989678.941\n",
      "  load_time_ms: 8.063\n",
      "  sample_throughput: 502.607\n",
      "  sample_time_ms: 15877.211\n",
      "  update_time_ms: 5.717\n",
      "timestamp: 1643537047\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 391020\n",
      "training_iteration: 49\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 406950\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-04-40\n",
      "done: false\n",
      "episode_len_mean: 271.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 2542\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9978089511394501\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016239064823967055\n",
      "        policy_loss: -0.09783524655581763\n",
      "        total_loss: 205.37357100804647\n",
      "        vf_explained_var: 0.10499002496401469\n",
      "        vf_loss: 205.46409863154094\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8920066684484482\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021168847116005717\n",
      "        policy_loss: -0.09703227651305496\n",
      "        total_loss: 90.74243847370148\n",
      "        vf_explained_var: -0.17589215079943338\n",
      "        vf_loss: 90.833120191892\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9178771362702052\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017391949985546187\n",
      "        policy_loss: -0.08141146740255256\n",
      "        total_loss: 96.94299051125844\n",
      "        vf_explained_var: 0.04770630041758219\n",
      "        vf_loss: 97.01918396313985\n",
      "  num_agent_steps_sampled: 406950\n",
      "  num_agent_steps_trained: 406950\n",
      "  num_steps_sampled: 406980\n",
      "  num_steps_trained: 406980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 51\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.710000000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.705000000000005\n",
      "  vram_util_percent0: 0.3930338541666667\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 47.66666666666667\n",
      "  player_1: 43.33333333333333\n",
      "  player_2: 44.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: -2.3000000000000007\n",
      "  player_1: 2.01\n",
      "  player_2: 3.29\n",
      "policy_reward_min:\n",
      "  player_0: -72.66666666666667\n",
      "  player_1: -72.0\n",
      "  player_2: -58.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09662972496122042\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4385220566471849\n",
      "  mean_inference_ms: 1.832651651761442\n",
      "  mean_raw_obs_processing_ms: 0.24174097478439419\n",
      "time_since_restore: 860.9090278148651\n",
      "time_this_iter_s: 16.399696111679077\n",
      "time_total_s: 860.9090278148651\n",
      "timers:\n",
      "  learn_throughput: 544.391\n",
      "  learn_time_ms: 14658.575\n",
      "  load_throughput: 975237.641\n",
      "  load_time_ms: 8.183\n",
      "  sample_throughput: 501.644\n",
      "  sample_time_ms: 15907.709\n",
      "  update_time_ms: 5.841\n",
      "timestamp: 1643537080\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 406980\n",
      "training_iteration: 51\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 422910\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-05-13\n",
      "done: false\n",
      "episode_len_mean: 269.82\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 33\n",
      "episodes_total: 2605\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9432793377836546\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017069711515311305\n",
      "        policy_loss: -0.07481196466833354\n",
      "        total_loss: 163.56058325449627\n",
      "        vf_explained_var: 0.13893733769655228\n",
      "        vf_loss: 163.62771484533945\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9157710827390353\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016589012901714096\n",
      "        policy_loss: -0.09556382076038669\n",
      "        total_loss: 115.5700942325592\n",
      "        vf_explained_var: 0.20403002699216208\n",
      "        vf_loss: 115.65819245974222\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9427334535121917\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017149934153864402\n",
      "        policy_loss: -0.08477967623310785\n",
      "        total_loss: 128.38343731562296\n",
      "        vf_explained_var: 0.0016284420092900593\n",
      "        vf_loss: 128.46307191212972\n",
      "  num_agent_steps_sampled: 422910\n",
      "  num_agent_steps_trained: 422910\n",
      "  num_steps_sampled: 422940\n",
      "  num_steps_trained: 422940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 53\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.329999999999995\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.794999999999995\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 43.0\n",
      "  player_1: 43.33333333333333\n",
      "  player_2: 43.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: -1.7733333333333337\n",
      "  player_1: -0.07333333333333361\n",
      "  player_2: 4.846666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -66.33333333333334\n",
      "  player_1: -51.66666666666667\n",
      "  player_2: -81.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0965247300224284\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.43337954837321596\n",
      "  mean_inference_ms: 1.830904263084409\n",
      "  mean_raw_obs_processing_ms: 0.24061313835674053\n",
      "time_since_restore: 894.1478972434998\n",
      "time_this_iter_s: 15.975404977798462\n",
      "time_total_s: 894.1478972434998\n",
      "timers:\n",
      "  learn_throughput: 537.746\n",
      "  learn_time_ms: 14839.708\n",
      "  load_throughput: 979923.057\n",
      "  load_time_ms: 8.143\n",
      "  sample_throughput: 491.983\n",
      "  sample_time_ms: 16220.063\n",
      "  update_time_ms: 5.847\n",
      "timestamp: 1643537113\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 422940\n",
      "training_iteration: 53\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 438870\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-05-44\n",
      "done: false\n",
      "episode_len_mean: 284.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 2656\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9655386555194855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015888567068478826\n",
      "        policy_loss: -0.07269924697155754\n",
      "        total_loss: 88.26872993866603\n",
      "        vf_explained_var: -0.2486901730298996\n",
      "        vf_loss: 88.33427921692531\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8566241643826167\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015549557628176747\n",
      "        policy_loss: -0.05235237518170228\n",
      "        total_loss: 66.1595254913966\n",
      "        vf_explained_var: -0.30549612830082573\n",
      "        vf_loss: 66.20488044579824\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8636799342433612\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014614638504838335\n",
      "        policy_loss: -0.07716043915910026\n",
      "        total_loss: 87.68953309138615\n",
      "        vf_explained_var: -0.14812400033076603\n",
      "        vf_loss: 87.7601167233785\n",
      "  num_agent_steps_sampled: 438870\n",
      "  num_agent_steps_trained: 438870\n",
      "  num_steps_sampled: 438900\n",
      "  num_steps_trained: 438900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 55\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.142105263157895\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.79999999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 43.0\n",
      "  player_1: 48.0\n",
      "  player_2: 41.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.34666666666666673\n",
      "  player_1: 0.8533333333333336\n",
      "  player_2: 2.4933333333333336\n",
      "policy_reward_min:\n",
      "  player_0: -66.0\n",
      "  player_1: -50.0\n",
      "  player_2: -81.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09649939207460939\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4291586654207946\n",
      "  mean_inference_ms: 1.8255324815161527\n",
      "  mean_raw_obs_processing_ms: 0.24070992981856587\n",
      "time_since_restore: 924.7932069301605\n",
      "time_this_iter_s: 15.384494543075562\n",
      "time_total_s: 924.7932069301605\n",
      "timers:\n",
      "  learn_throughput: 542.312\n",
      "  learn_time_ms: 14714.771\n",
      "  load_throughput: 945709.367\n",
      "  load_time_ms: 8.438\n",
      "  sample_throughput: 495.124\n",
      "  sample_time_ms: 16117.167\n",
      "  update_time_ms: 6.098\n",
      "timestamp: 1643537144\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 438900\n",
      "training_iteration: 55\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 454830\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-06-16\n",
      "done: false\n",
      "episode_len_mean: 286.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 2714\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9121967262029648\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016314751831256823\n",
      "        policy_loss: -0.08197757803524534\n",
      "        total_loss: 51.18436873912811\n",
      "        vf_explained_var: 0.005788762966791788\n",
      "        vf_loss: 51.25900471051534\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9008131758371989\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015643686656385162\n",
      "        policy_loss: -0.08390470925718546\n",
      "        total_loss: 73.28687427043914\n",
      "        vf_explained_var: -0.06770627856254578\n",
      "        vf_loss: 73.36373928626378\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8983106449246406\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01491709405772743\n",
      "        policy_loss: -0.08133015728866061\n",
      "        total_loss: 92.90969022115071\n",
      "        vf_explained_var: 0.21288812736670176\n",
      "        vf_loss: 92.98430737813314\n",
      "  num_agent_steps_sampled: 454830\n",
      "  num_agent_steps_trained: 454830\n",
      "  num_steps_sampled: 454860\n",
      "  num_steps_trained: 454860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 57\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.747368421052634\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.79999999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.333333333333336\n",
      "  player_1: 48.0\n",
      "  player_2: 44.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: -3.2533333333333347\n",
      "  player_1: 2.996666666666667\n",
      "  player_2: 3.256666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -75.0\n",
      "  player_1: -46.0\n",
      "  player_2: -62.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09619204832381431\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4245579020933637\n",
      "  mean_inference_ms: 1.8209680129526993\n",
      "  mean_raw_obs_processing_ms: 0.239414795608071\n",
      "time_since_restore: 957.3145775794983\n",
      "time_this_iter_s: 15.34669280052185\n",
      "time_total_s: 957.3145775794983\n",
      "timers:\n",
      "  learn_throughput: 541.526\n",
      "  learn_time_ms: 14736.14\n",
      "  load_throughput: 971774.742\n",
      "  load_time_ms: 8.212\n",
      "  sample_throughput: 493.248\n",
      "  sample_time_ms: 16178.481\n",
      "  update_time_ms: 6.124\n",
      "timestamp: 1643537176\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 454860\n",
      "training_iteration: 57\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 470790\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-06-48\n",
      "done: false\n",
      "episode_len_mean: 272.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 28\n",
      "episodes_total: 2771\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8924355878432592\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016462445855479756\n",
      "        policy_loss: -0.11715205727455516\n",
      "        total_loss: 99.15718466599782\n",
      "        vf_explained_var: 0.03660540093978246\n",
      "        vf_loss: 99.26692879041036\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8550041472911835\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01625712981898289\n",
      "        policy_loss: -0.09580438207524518\n",
      "        total_loss: 59.53776339054107\n",
      "        vf_explained_var: 0.03689209481080373\n",
      "        vf_loss: 59.62625213940938\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.879898223678271\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01616930680584763\n",
      "        policy_loss: -0.05412912107693652\n",
      "        total_loss: 113.9153852446874\n",
      "        vf_explained_var: -0.11837369610865911\n",
      "        vf_loss: 113.9622378007571\n",
      "  num_agent_steps_sampled: 470790\n",
      "  num_agent_steps_trained: 470790\n",
      "  num_steps_sampled: 470820\n",
      "  num_steps_trained: 470820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 59\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.668421052631581\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.79999999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 42.333333333333336\n",
      "  player_1: 45.33333333333333\n",
      "  player_2: 48.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: -0.37000000000000033\n",
      "  player_1: 3.7799999999999994\n",
      "  player_2: -0.4100000000000006\n",
      "policy_reward_min:\n",
      "  player_0: -56.33333333333333\n",
      "  player_1: -73.66666666666667\n",
      "  player_2: -62.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09591891399664682\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.42050804834273053\n",
      "  mean_inference_ms: 1.8153400915851072\n",
      "  mean_raw_obs_processing_ms: 0.23838647512369227\n",
      "time_since_restore: 988.7641594409943\n",
      "time_this_iter_s: 15.491600275039673\n",
      "time_total_s: 988.7641594409943\n",
      "timers:\n",
      "  learn_throughput: 543.559\n",
      "  learn_time_ms: 14681.008\n",
      "  load_throughput: 961830.463\n",
      "  load_time_ms: 8.297\n",
      "  sample_throughput: 495.433\n",
      "  sample_time_ms: 16107.128\n",
      "  update_time_ms: 6.026\n",
      "timestamp: 1643537208\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 470820\n",
      "training_iteration: 59\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 486750\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-07-20\n",
      "done: false\n",
      "episode_len_mean: 255.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 28\n",
      "episodes_total: 2833\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8833504865566889\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015477541497252455\n",
      "        policy_loss: -0.06203020316082984\n",
      "        total_loss: 155.67650267283122\n",
      "        vf_explained_var: 0.070647318760554\n",
      "        vf_loss: 155.73156789620717\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7901972207427025\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014886984407948679\n",
      "        policy_loss: -0.06550232032313943\n",
      "        total_loss: 87.82581523974737\n",
      "        vf_explained_var: 0.060646910270055136\n",
      "        vf_loss: 87.88461881717046\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8401145564516386\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014820678252816227\n",
      "        policy_loss: -0.11295961890680095\n",
      "        total_loss: 139.63569115638734\n",
      "        vf_explained_var: 0.028200808564821878\n",
      "        vf_loss: 139.7419817384084\n",
      "  num_agent_steps_sampled: 486750\n",
      "  num_agent_steps_trained: 486750\n",
      "  num_steps_sampled: 486780\n",
      "  num_steps_trained: 486780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 61\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.419999999999996\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.794999999999995\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 58.66666666666667\n",
      "  player_1: 45.33333333333333\n",
      "  player_2: 46.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -3.4233333333333325\n",
      "  player_1: 7.026666666666666\n",
      "  player_2: -0.6033333333333327\n",
      "policy_reward_min:\n",
      "  player_0: -59.66666666666667\n",
      "  player_1: -42.0\n",
      "  player_2: -81.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09575347062232029\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4162828753327352\n",
      "  mean_inference_ms: 1.8083582416468564\n",
      "  mean_raw_obs_processing_ms: 0.23848212567617622\n",
      "time_since_restore: 1021.1591112613678\n",
      "time_this_iter_s: 16.158808946609497\n",
      "time_total_s: 1021.1591112613678\n",
      "timers:\n",
      "  learn_throughput: 542.839\n",
      "  learn_time_ms: 14700.488\n",
      "  load_throughput: 926348.495\n",
      "  load_time_ms: 8.614\n",
      "  sample_throughput: 496.348\n",
      "  sample_time_ms: 16077.434\n",
      "  update_time_ms: 5.948\n",
      "timestamp: 1643537240\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 486780\n",
      "training_iteration: 61\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 502710\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-07-52\n",
      "done: false\n",
      "episode_len_mean: 283.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 2886\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8506861434380213\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01538564583715337\n",
      "        policy_loss: -0.12059362897028526\n",
      "        total_loss: 60.11985223054886\n",
      "        vf_explained_var: -0.03356383283933004\n",
      "        vf_loss: 60.23352204799652\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7744608936707179\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015786192043569826\n",
      "        policy_loss: -0.08391979727738848\n",
      "        total_loss: 55.714347734451295\n",
      "        vf_explained_var: 0.08764985899130504\n",
      "        vf_loss: 55.791163806915286\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.802341991464297\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016332723328538398\n",
      "        policy_loss: -0.048569117373165986\n",
      "        total_loss: 104.03005424340566\n",
      "        vf_explained_var: -0.058754746119181314\n",
      "        vf_loss: 104.07127392450968\n",
      "  num_agent_steps_sampled: 502710\n",
      "  num_agent_steps_trained: 502710\n",
      "  num_steps_sampled: 502740\n",
      "  num_steps_trained: 502740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 63\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.189999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.789999999999985\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 58.66666666666667\n",
      "  player_1: 43.0\n",
      "  player_2: 46.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.813333333333334\n",
      "  player_1: -1.4666666666666663\n",
      "  player_2: 1.6533333333333342\n",
      "policy_reward_min:\n",
      "  player_0: -56.33333333333333\n",
      "  player_1: -55.333333333333336\n",
      "  player_2: -81.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09549531945653532\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4130448321413134\n",
      "  mean_inference_ms: 1.8060633707138936\n",
      "  mean_raw_obs_processing_ms: 0.23720734051066622\n",
      "time_since_restore: 1053.0259130001068\n",
      "time_this_iter_s: 16.36428165435791\n",
      "time_total_s: 1053.0259130001068\n",
      "timers:\n",
      "  learn_throughput: 547.342\n",
      "  learn_time_ms: 14579.545\n",
      "  load_throughput: 907427.279\n",
      "  load_time_ms: 8.794\n",
      "  sample_throughput: 502.571\n",
      "  sample_time_ms: 15878.354\n",
      "  update_time_ms: 5.971\n",
      "timestamp: 1643537272\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 502740\n",
      "training_iteration: 63\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 518672\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-08-25\n",
      "done: false\n",
      "episode_len_mean: 320.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 2934\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.900986979007721\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01741129645302493\n",
      "        policy_loss: -0.06755253133364021\n",
      "        total_loss: 135.83559931914013\n",
      "        vf_explained_var: -0.1493140795826912\n",
      "        vf_loss: 135.8953171602885\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7647993955016136\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01408527957472567\n",
      "        policy_loss: -0.06657973937069377\n",
      "        total_loss: 100.82072826067606\n",
      "        vf_explained_var: 0.02783918191989263\n",
      "        vf_loss: 100.8809691063563\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7833912894129753\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01694222708203758\n",
      "        policy_loss: -0.1141472692725559\n",
      "        total_loss: 70.1014077536265\n",
      "        vf_explained_var: 0.03421530326207479\n",
      "        vf_loss: 70.20793116807937\n",
      "  num_agent_steps_sampled: 518672\n",
      "  num_agent_steps_trained: 518672\n",
      "  num_steps_sampled: 518700\n",
      "  num_steps_trained: 518700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 65\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.395\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.79999999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 49.33333333333333\n",
      "  player_1: 33.0\n",
      "  player_2: 45.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 0.2566666666666666\n",
      "  player_1: -3.163333333333333\n",
      "  player_2: 5.906666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -62.66666666666667\n",
      "  player_1: -75.66666666666667\n",
      "  player_2: -67.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09536276054299188\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40995835481268594\n",
      "  mean_inference_ms: 1.8035380415178843\n",
      "  mean_raw_obs_processing_ms: 0.23599327852243132\n",
      "time_since_restore: 1085.4814836978912\n",
      "time_this_iter_s: 16.115659952163696\n",
      "time_total_s: 1085.4814836978912\n",
      "timers:\n",
      "  learn_throughput: 540.504\n",
      "  learn_time_ms: 14763.999\n",
      "  load_throughput: 887801.115\n",
      "  load_time_ms: 8.988\n",
      "  sample_throughput: 497.77\n",
      "  sample_time_ms: 16031.489\n",
      "  update_time_ms: 7.982\n",
      "timestamp: 1643537305\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 518700\n",
      "training_iteration: 65\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 534630\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-08-55\n",
      "done: false\n",
      "episode_len_mean: 311.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 2991\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8415574717521668\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01609573145577694\n",
      "        policy_loss: -0.08375639695674181\n",
      "        total_loss: 111.67678661664327\n",
      "        vf_explained_var: 0.0974518229564031\n",
      "        vf_loss: 111.753299314181\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8187740847468377\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014365220300445762\n",
      "        policy_loss: -0.06508211420228084\n",
      "        total_loss: 180.10914510567983\n",
      "        vf_explained_var: -0.04725861618916194\n",
      "        vf_loss: 180.16776179949443\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7953579962253571\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015934204383634097\n",
      "        policy_loss: -0.07764982055562238\n",
      "        total_loss: 90.75272875944773\n",
      "        vf_explained_var: -0.005726362466812134\n",
      "        vf_loss: 90.82320870399475\n",
      "  num_agent_steps_sampled: 534630\n",
      "  num_agent_steps_trained: 534630\n",
      "  num_steps_sampled: 534660\n",
      "  num_steps_trained: 534660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 67\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.457894736842107\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.79999999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 57.33333333333333\n",
      "  player_1: 37.0\n",
      "  player_2: 48.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: -0.0633333333333335\n",
      "  player_1: -5.643333333333334\n",
      "  player_2: 8.706666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -62.66666666666667\n",
      "  player_1: -102.66666666666667\n",
      "  player_2: -50.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09504959980042331\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40656827766721215\n",
      "  mean_inference_ms: 1.8001554769426433\n",
      "  mean_raw_obs_processing_ms: 0.23532524952471268\n",
      "time_since_restore: 1116.0349168777466\n",
      "time_this_iter_s: 15.291150331497192\n",
      "time_total_s: 1116.0349168777466\n",
      "timers:\n",
      "  learn_throughput: 547.992\n",
      "  learn_time_ms: 14562.262\n",
      "  load_throughput: 921112.194\n",
      "  load_time_ms: 8.663\n",
      "  sample_throughput: 501.462\n",
      "  sample_time_ms: 15913.476\n",
      "  update_time_ms: 8.003\n",
      "timestamp: 1643537335\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 534660\n",
      "training_iteration: 67\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 550590\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-09-27\n",
      "done: false\n",
      "episode_len_mean: 280.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 28\n",
      "episodes_total: 3051\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8674282446503639\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01684101013955877\n",
      "        policy_loss: -0.07102072005470594\n",
      "        total_loss: 146.91494889418283\n",
      "        vf_explained_var: -0.03707981338103612\n",
      "        vf_loss: 146.97839132944742\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8428713790575664\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017468220990594953\n",
      "        policy_loss: -0.11543069714680314\n",
      "        total_loss: 105.22363955974579\n",
      "        vf_explained_var: -0.021451799074808757\n",
      "        vf_loss: 105.33120914141337\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7923916713396708\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01616017044962367\n",
      "        policy_loss: -0.09532087407074868\n",
      "        total_loss: 79.03212005615234\n",
      "        vf_explained_var: 0.09705882211526234\n",
      "        vf_loss: 79.12016883214315\n",
      "  num_agent_steps_sampled: 550590\n",
      "  num_agent_steps_trained: 550590\n",
      "  num_steps_sampled: 550620\n",
      "  num_steps_trained: 550620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 69\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.370000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.79999999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 57.33333333333333\n",
      "  player_1: 66.33333333333333\n",
      "  player_2: 63.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.9499999999999994\n",
      "  player_1: -3.5000000000000004\n",
      "  player_2: 5.55\n",
      "policy_reward_min:\n",
      "  player_0: -52.66666666666667\n",
      "  player_1: -102.66666666666667\n",
      "  player_2: -96.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09508369405959324\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.40391660275000374\n",
      "  mean_inference_ms: 1.7956598435888547\n",
      "  mean_raw_obs_processing_ms: 0.23572456204833236\n",
      "time_since_restore: 1148.034277677536\n",
      "time_this_iter_s: 16.067190170288086\n",
      "time_total_s: 1148.034277677536\n",
      "timers:\n",
      "  learn_throughput: 545.95\n",
      "  learn_time_ms: 14616.723\n",
      "  load_throughput: 918912.092\n",
      "  load_time_ms: 8.684\n",
      "  sample_throughput: 501.922\n",
      "  sample_time_ms: 15898.87\n",
      "  update_time_ms: 7.986\n",
      "timestamp: 1643537367\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 550620\n",
      "training_iteration: 69\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 566550\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-09-59\n",
      "done: false\n",
      "episode_len_mean: 291.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 17\n",
      "episodes_total: 3095\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8280205776294073\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015716452337458834\n",
      "        policy_loss: -0.0527111914070944\n",
      "        total_loss: 67.31746812184652\n",
      "        vf_explained_var: -0.0006302286187807719\n",
      "        vf_loss: 67.36310719966889\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7530747784177462\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01580232787165694\n",
      "        policy_loss: -0.07100433057174087\n",
      "        total_loss: 82.47741162776947\n",
      "        vf_explained_var: -0.06977403660615285\n",
      "        vf_loss: 82.54130474249521\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7798859472076098\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016575753782602003\n",
      "        policy_loss: -0.1144859761185944\n",
      "        total_loss: 40.04426932970683\n",
      "        vf_explained_var: 0.189247857828935\n",
      "        vf_loss: 40.15129598935445\n",
      "  num_agent_steps_sampled: 566550\n",
      "  num_agent_steps_trained: 566550\n",
      "  num_steps_sampled: 566580\n",
      "  num_steps_trained: 566580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 71\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.68421052631579\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.79999999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 42.33333333333333\n",
      "  player_1: 66.33333333333333\n",
      "  player_2: 63.0\n",
      "policy_reward_mean:\n",
      "  player_0: -4.133333333333334\n",
      "  player_1: 1.9266666666666663\n",
      "  player_2: 5.206666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -59.333333333333336\n",
      "  player_1: -66.33333333333334\n",
      "  player_2: -96.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09517394367720143\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.401575438941175\n",
      "  mean_inference_ms: 1.793850821158474\n",
      "  mean_raw_obs_processing_ms: 0.2350758799476435\n",
      "time_since_restore: 1179.6392827033997\n",
      "time_this_iter_s: 15.18178391456604\n",
      "time_total_s: 1179.6392827033997\n",
      "timers:\n",
      "  learn_throughput: 548.743\n",
      "  learn_time_ms: 14542.325\n",
      "  load_throughput: 926756.321\n",
      "  load_time_ms: 8.611\n",
      "  sample_throughput: 499.41\n",
      "  sample_time_ms: 15978.845\n",
      "  update_time_ms: 7.921\n",
      "timestamp: 1643537399\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 566580\n",
      "training_iteration: 71\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 582512\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-10-30\n",
      "done: false\n",
      "episode_len_mean: 330.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 28\n",
      "episodes_total: 3148\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7832085298498471\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01724564932126668\n",
      "        policy_loss: -0.0857361154925699\n",
      "        total_loss: 126.82770180384318\n",
      "        vf_explained_var: -0.11470391641060511\n",
      "        vf_loss: 126.90567738215128\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.828249835918347\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014755426331927689\n",
      "        policy_loss: -0.07633312567137182\n",
      "        total_loss: 93.99056573867797\n",
      "        vf_explained_var: 0.18752249975999197\n",
      "        vf_loss: 94.06025843540827\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7334809069832166\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014216509471197299\n",
      "        policy_loss: -0.09110209765844048\n",
      "        total_loss: 114.29050438880921\n",
      "        vf_explained_var: 0.043409497539202375\n",
      "        vf_loss: 114.37520899136861\n",
      "  num_agent_steps_sampled: 582512\n",
      "  num_agent_steps_trained: 582512\n",
      "  num_steps_sampled: 582540\n",
      "  num_steps_trained: 582540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 73\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.536842105263156\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.79999999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 42.33333333333333\n",
      "  player_1: 43.33333333333333\n",
      "  player_2: 45.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -3.0766666666666667\n",
      "  player_1: 1.6833333333333338\n",
      "  player_2: 4.393333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -59.666666666666664\n",
      "  player_1: -56.66666666666667\n",
      "  player_2: -52.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09493459697606837\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.39840134036106734\n",
      "  mean_inference_ms: 1.7901762476588992\n",
      "  mean_raw_obs_processing_ms: 0.2336399278188875\n",
      "time_since_restore: 1210.7606160640717\n",
      "time_this_iter_s: 15.389323949813843\n",
      "time_total_s: 1210.7606160640717\n",
      "timers:\n",
      "  learn_throughput: 551.406\n",
      "  learn_time_ms: 14472.104\n",
      "  load_throughput: 941275.86\n",
      "  load_time_ms: 8.478\n",
      "  sample_throughput: 501.787\n",
      "  sample_time_ms: 15903.168\n",
      "  update_time_ms: 7.932\n",
      "timestamp: 1643537430\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 582540\n",
      "training_iteration: 73\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 598470\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-11-03\n",
      "done: false\n",
      "episode_len_mean: 302.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3198\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8070174107948939\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016861461528568118\n",
      "        policy_loss: -0.09048370181272428\n",
      "        total_loss: 93.73229308287303\n",
      "        vf_explained_var: -0.19139635264873506\n",
      "        vf_loss: 93.81518965562185\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8003952782352766\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018148142440983957\n",
      "        policy_loss: -0.07215935725951567\n",
      "        total_loss: 68.31584909439087\n",
      "        vf_explained_var: -0.014239281415939331\n",
      "        vf_loss: 68.37984197616578\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7735079192121823\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016974136250844603\n",
      "        policy_loss: -0.0736674843604366\n",
      "        total_loss: 69.50237819989522\n",
      "        vf_explained_var: -0.057921513815720874\n",
      "        vf_loss: 69.56840747356415\n",
      "  num_agent_steps_sampled: 598470\n",
      "  num_agent_steps_trained: 598470\n",
      "  num_steps_sampled: 598500\n",
      "  num_steps_trained: 598500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 75\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.415000000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.79999999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 52.66666666666667\n",
      "  player_1: 43.33333333333333\n",
      "  player_2: 45.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 0.3799999999999998\n",
      "  player_1: 0.020000000000000035\n",
      "  player_2: 2.6\n",
      "policy_reward_min:\n",
      "  player_0: -60.666666666666664\n",
      "  player_1: -75.33333333333333\n",
      "  player_2: -64.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09474139981806111\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.39551328942075037\n",
      "  mean_inference_ms: 1.7856559211391265\n",
      "  mean_raw_obs_processing_ms: 0.23267301847380353\n",
      "time_since_restore: 1243.1255974769592\n",
      "time_this_iter_s: 16.28611993789673\n",
      "time_total_s: 1243.1255974769592\n",
      "timers:\n",
      "  learn_throughput: 551.642\n",
      "  learn_time_ms: 14465.914\n",
      "  load_throughput: 1033424.29\n",
      "  load_time_ms: 7.722\n",
      "  sample_throughput: 505.884\n",
      "  sample_time_ms: 15774.356\n",
      "  update_time_ms: 5.809\n",
      "timestamp: 1643537463\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 598500\n",
      "training_iteration: 75\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 614430\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-11-34\n",
      "done: false\n",
      "episode_len_mean: 327.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 3242\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8051680968205134\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017323601579131112\n",
      "        policy_loss: -0.0648514368198812\n",
      "        total_loss: 120.88277290185293\n",
      "        vf_explained_var: -0.0436276317636172\n",
      "        vf_loss: 120.93982896010081\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7693647426366806\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017012768446372017\n",
      "        policy_loss: -0.09031298962732157\n",
      "        total_loss: 51.77016286611557\n",
      "        vf_explained_var: -0.00990575651327769\n",
      "        vf_loss: 51.852819998264316\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7193228351076444\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015079690345149145\n",
      "        policy_loss: -0.11550890910439193\n",
      "        total_loss: 69.4409564669927\n",
      "        vf_explained_var: 0.1340609019001325\n",
      "        vf_loss: 69.5496798435847\n",
      "  num_agent_steps_sampled: 614430\n",
      "  num_agent_steps_trained: 614430\n",
      "  num_steps_sampled: 614460\n",
      "  num_steps_trained: 614460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 77\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.23\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.870000000000005\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 52.66666666666667\n",
      "  player_1: 43.33333333333333\n",
      "  player_2: 45.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -1.9500000000000002\n",
      "  player_1: 1.4599999999999997\n",
      "  player_2: 3.489999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -74.33333333333333\n",
      "  player_1: -75.33333333333333\n",
      "  player_2: -64.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09470913499948287\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.39347993679114857\n",
      "  mean_inference_ms: 1.7820238909112853\n",
      "  mean_raw_obs_processing_ms: 0.23259360390976164\n",
      "time_since_restore: 1274.8591449260712\n",
      "time_this_iter_s: 16.157438278198242\n",
      "time_total_s: 1274.8591449260712\n",
      "timers:\n",
      "  learn_throughput: 547.018\n",
      "  learn_time_ms: 14588.193\n",
      "  load_throughput: 896362.811\n",
      "  load_time_ms: 8.903\n",
      "  sample_throughput: 504.46\n",
      "  sample_time_ms: 15818.898\n",
      "  update_time_ms: 5.702\n",
      "timestamp: 1643537494\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 614460\n",
      "training_iteration: 77\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 630392\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-12-05\n",
      "done: false\n",
      "episode_len_mean: 345.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 3293\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8003039318323135\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01573351886716845\n",
      "        policy_loss: -0.05441017413822313\n",
      "        total_loss: 98.45422197977702\n",
      "        vf_explained_var: 0.06301850775877635\n",
      "        vf_loss: 98.50155184984207\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7858426433801651\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016295967228940167\n",
      "        policy_loss: -0.03908823909858863\n",
      "        total_loss: 40.58181923151016\n",
      "        vf_explained_var: 0.039264038503170014\n",
      "        vf_loss: 40.613574244181315\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7350856226185958\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01524683386437213\n",
      "        policy_loss: -0.1200014978616188\n",
      "        total_loss: 85.11963960409165\n",
      "        vf_explained_var: 0.04605106920003891\n",
      "        vf_loss: 85.23277933518092\n",
      "  num_agent_steps_sampled: 630392\n",
      "  num_agent_steps_trained: 630392\n",
      "  num_steps_sampled: 630420\n",
      "  num_steps_trained: 630420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 79\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.78421052631579\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.857894736842105\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 45.66666666666667\n",
      "  player_1: 40.66666666666667\n",
      "  player_2: 38.0\n",
      "policy_reward_mean:\n",
      "  player_0: -3.3233333333333337\n",
      "  player_1: 2.8166666666666664\n",
      "  player_2: 3.5066666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -74.33333333333333\n",
      "  player_1: -59.0\n",
      "  player_2: -63.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09440806864133164\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.39102121292634423\n",
      "  mean_inference_ms: 1.7786773007180114\n",
      "  mean_raw_obs_processing_ms: 0.2320985801460985\n",
      "time_since_restore: 1305.085695028305\n",
      "time_this_iter_s: 15.569160223007202\n",
      "time_total_s: 1305.085695028305\n",
      "timers:\n",
      "  learn_throughput: 553.517\n",
      "  learn_time_ms: 14416.911\n",
      "  load_throughput: 953960.022\n",
      "  load_time_ms: 8.365\n",
      "  sample_throughput: 505.712\n",
      "  sample_time_ms: 15779.748\n",
      "  update_time_ms: 5.578\n",
      "timestamp: 1643537525\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 630420\n",
      "training_iteration: 79\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 646350\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-12-37\n",
      "done: false\n",
      "episode_len_mean: 329.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 3340\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8270352831482888\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016442438044704202\n",
      "        policy_loss: -0.11161398310835163\n",
      "        total_loss: 64.31969960848491\n",
      "        vf_explained_var: 0.044066611329714456\n",
      "        vf_loss: 64.42391422828038\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7708403431375821\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014647860512880773\n",
      "        policy_loss: -0.06411203904698293\n",
      "        total_loss: 73.69127103090287\n",
      "        vf_explained_var: 0.061462853650252024\n",
      "        vf_loss: 73.7487915468216\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.722516800314188\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01388398204654384\n",
      "        policy_loss: -0.0673650686815381\n",
      "        total_loss: 84.75749539295832\n",
      "        vf_explained_var: -0.09966757913430532\n",
      "        vf_loss: 84.81861267646154\n",
      "  num_agent_steps_sampled: 646350\n",
      "  num_agent_steps_trained: 646350\n",
      "  num_steps_sampled: 646380\n",
      "  num_steps_trained: 646380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 81\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.144444444444446\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.900000000000006\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 45.66666666666667\n",
      "  player_1: 39.0\n",
      "  player_2: 37.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 0.09333333333333275\n",
      "  player_1: 0.2733333333333327\n",
      "  player_2: 2.6333333333333324\n",
      "policy_reward_min:\n",
      "  player_0: -58.333333333333336\n",
      "  player_1: -59.0\n",
      "  player_2: -63.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09425499586965223\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3889507191752978\n",
      "  mean_inference_ms: 1.7753486137523016\n",
      "  mean_raw_obs_processing_ms: 0.23194550469923123\n",
      "time_since_restore: 1337.1773550510406\n",
      "time_this_iter_s: 14.732397317886353\n",
      "time_total_s: 1337.1773550510406\n",
      "timers:\n",
      "  learn_throughput: 551.634\n",
      "  learn_time_ms: 14466.11\n",
      "  load_throughput: 913956.406\n",
      "  load_time_ms: 8.731\n",
      "  sample_throughput: 504.4\n",
      "  sample_time_ms: 15820.771\n",
      "  update_time_ms: 5.534\n",
      "timestamp: 1643537557\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 646380\n",
      "training_iteration: 81\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 662310\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-13-06\n",
      "done: false\n",
      "episode_len_mean: 332.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 30\n",
      "episodes_total: 3388\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7999127338329951\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01485564454431369\n",
      "        policy_loss: -0.062300746797894435\n",
      "        total_loss: 104.65857041358947\n",
      "        vf_explained_var: 0.12306769490242005\n",
      "        vf_loss: 104.71418554623922\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7651866819461187\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014485648770293267\n",
      "        policy_loss: -0.06963002433379491\n",
      "        total_loss: 128.69009948174158\n",
      "        vf_explained_var: -0.17855968157450358\n",
      "        vf_loss: 128.75321124951046\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7055061480402947\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015047766440419158\n",
      "        policy_loss: -0.1005403449619189\n",
      "        total_loss: 97.98803030808767\n",
      "        vf_explained_var: 0.16561295360326767\n",
      "        vf_loss: 98.0817989929517\n",
      "  num_agent_steps_sampled: 662310\n",
      "  num_agent_steps_trained: 662310\n",
      "  num_steps_sampled: 662340\n",
      "  num_steps_trained: 662340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 83\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.82222222222222\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.900000000000006\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 42.0\n",
      "  player_1: 43.0\n",
      "  player_2: 41.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -0.5666666666666664\n",
      "  player_1: -2.666666666666666\n",
      "  player_2: 6.233333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -74.0\n",
      "  player_1: -76.0\n",
      "  player_2: -60.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09419064964134534\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.38678198062844593\n",
      "  mean_inference_ms: 1.7725306360879318\n",
      "  mean_raw_obs_processing_ms: 0.23127010857334893\n",
      "time_since_restore: 1366.8883228302002\n",
      "time_this_iter_s: 14.81496548652649\n",
      "time_total_s: 1366.8883228302002\n",
      "timers:\n",
      "  learn_throughput: 557.094\n",
      "  learn_time_ms: 14324.346\n",
      "  load_throughput: 881685.526\n",
      "  load_time_ms: 9.051\n",
      "  sample_throughput: 508.421\n",
      "  sample_time_ms: 15695.649\n",
      "  update_time_ms: 5.55\n",
      "timestamp: 1643537586\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 662340\n",
      "training_iteration: 83\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 678270\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-13-38\n",
      "done: false\n",
      "episode_len_mean: 340.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 3428\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7638672479987144\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017611131273389827\n",
      "        policy_loss: -0.0736677249148488\n",
      "        total_loss: 114.03410879770915\n",
      "        vf_explained_var: 0.02621732880671819\n",
      "        vf_loss: 114.09985147953033\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7419816689689954\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01944645131154857\n",
      "        policy_loss: -0.09010174449222784\n",
      "        total_loss: 54.59831216017405\n",
      "        vf_explained_var: 0.17226116061210633\n",
      "        vf_loss: 54.67966307878494\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7296963707109292\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015970199684832095\n",
      "        policy_loss: -0.06496182872603337\n",
      "        total_loss: 76.72411266009013\n",
      "        vf_explained_var: 0.09680421014626821\n",
      "        vf_loss: 76.78188830296199\n",
      "  num_agent_steps_sampled: 678270\n",
      "  num_agent_steps_trained: 678270\n",
      "  num_steps_sampled: 678300\n",
      "  num_steps_trained: 678300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 85\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.915789473684208\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.894736842105274\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 42.0\n",
      "  player_1: 43.0\n",
      "  player_2: 41.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -1.9966666666666664\n",
      "  player_1: -5.426666666666666\n",
      "  player_2: 10.423333333333336\n",
      "policy_reward_min:\n",
      "  player_0: -74.0\n",
      "  player_1: -76.0\n",
      "  player_2: -60.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09406457112371473\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3846557214944349\n",
      "  mean_inference_ms: 1.7686255880775632\n",
      "  mean_raw_obs_processing_ms: 0.23051800873617545\n",
      "time_since_restore: 1398.823032617569\n",
      "time_this_iter_s: 15.325947284698486\n",
      "time_total_s: 1398.823032617569\n",
      "timers:\n",
      "  learn_throughput: 558.871\n",
      "  learn_time_ms: 14278.799\n",
      "  load_throughput: 846892.482\n",
      "  load_time_ms: 9.423\n",
      "  sample_throughput: 508.594\n",
      "  sample_time_ms: 15690.32\n",
      "  update_time_ms: 5.526\n",
      "timestamp: 1643537618\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 678300\n",
      "training_iteration: 85\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 694230\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-14-10\n",
      "done: false\n",
      "episode_len_mean: 341.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 3480\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.806919431189696\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016709363862384332\n",
      "        policy_loss: -0.07124474436665575\n",
      "        total_loss: 114.534260216554\n",
      "        vf_explained_var: 0.08948875516653061\n",
      "        vf_loss: 114.59798615137736\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.747816710670789\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01822651376248141\n",
      "        policy_loss: -0.07533327516478797\n",
      "        total_loss: 75.67996648311615\n",
      "        vf_explained_var: 0.056780571738878884\n",
      "        vf_loss: 75.74709782600402\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.737872429639101\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01572503802160346\n",
      "        policy_loss: -0.08493921430626263\n",
      "        total_loss: 99.32197538852692\n",
      "        vf_explained_var: 0.026336075166861216\n",
      "        vf_loss: 99.39983736038208\n",
      "  num_agent_steps_sampled: 694230\n",
      "  num_agent_steps_trained: 694230\n",
      "  num_steps_sampled: 694260\n",
      "  num_steps_trained: 694260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 87\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.364999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.90000000000001\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.66666666666667\n",
      "  player_1: 45.0\n",
      "  player_2: 42.0\n",
      "policy_reward_mean:\n",
      "  player_0: -2.463333333333334\n",
      "  player_1: 0.006666666666666643\n",
      "  player_2: 5.456666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -64.66666666666667\n",
      "  player_1: -58.666666666666664\n",
      "  player_2: -45.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09403091964486826\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.38260894516195987\n",
      "  mean_inference_ms: 1.7672349717615743\n",
      "  mean_raw_obs_processing_ms: 0.23008956927478885\n",
      "time_since_restore: 1430.0870883464813\n",
      "time_this_iter_s: 16.11825942993164\n",
      "time_total_s: 1430.0870883464813\n",
      "timers:\n",
      "  learn_throughput: 560.712\n",
      "  learn_time_ms: 14231.897\n",
      "  load_throughput: 886343.487\n",
      "  load_time_ms: 9.003\n",
      "  sample_throughput: 513.045\n",
      "  sample_time_ms: 15554.183\n",
      "  update_time_ms: 5.564\n",
      "timestamp: 1643537650\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 694260\n",
      "training_iteration: 87\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 710190\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-14-40\n",
      "done: false\n",
      "episode_len_mean: 316.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 3526\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7818982132275899\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015503861145503445\n",
      "        policy_loss: -0.07194154601776973\n",
      "        total_loss: 82.88972541650136\n",
      "        vf_explained_var: -0.17617854118347168\n",
      "        vf_loss: 82.95469002882639\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7567652969559033\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017079438770467734\n",
      "        policy_loss: -0.0978294670364509\n",
      "        total_loss: 57.34676820039749\n",
      "        vf_explained_var: -0.0524293252825737\n",
      "        vf_loss: 57.43691168864568\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7282043225566546\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01565000669725729\n",
      "        policy_loss: -0.08594109144993127\n",
      "        total_loss: 61.31156249682108\n",
      "        vf_explained_var: -0.12868252744277317\n",
      "        vf_loss: 61.39046088298162\n",
      "  num_agent_steps_sampled: 710190\n",
      "  num_agent_steps_trained: 710190\n",
      "  num_steps_sampled: 710220\n",
      "  num_steps_trained: 710220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 89\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.668421052631578\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.900000000000006\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.66666666666667\n",
      "  player_1: 45.0\n",
      "  player_2: 42.0\n",
      "policy_reward_mean:\n",
      "  player_0: -2.4499999999999993\n",
      "  player_1: 3.1299999999999994\n",
      "  player_2: 2.3200000000000007\n",
      "policy_reward_min:\n",
      "  player_0: -64.66666666666667\n",
      "  player_1: -67.0\n",
      "  player_2: -50.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09389749740315038\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.38112234324773914\n",
      "  mean_inference_ms: 1.7661464392297272\n",
      "  mean_raw_obs_processing_ms: 0.2297854348875444\n",
      "time_since_restore: 1460.5659432411194\n",
      "time_this_iter_s: 14.909033298492432\n",
      "time_total_s: 1460.5659432411194\n",
      "timers:\n",
      "  learn_throughput: 559.932\n",
      "  learn_time_ms: 14251.725\n",
      "  load_throughput: 865744.443\n",
      "  load_time_ms: 9.218\n",
      "  sample_throughput: 510.246\n",
      "  sample_time_ms: 15639.505\n",
      "  update_time_ms: 5.739\n",
      "timestamp: 1643537680\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 710220\n",
      "training_iteration: 89\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 726150\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-15-10\n",
      "done: false\n",
      "episode_len_mean: 343.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 17\n",
      "episodes_total: 3561\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7361061531802019\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014662220598254786\n",
      "        policy_loss: -0.1015258304681629\n",
      "        total_loss: 45.61009922266007\n",
      "        vf_explained_var: -0.0926509823401769\n",
      "        vf_loss: 45.70502690394719\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6666981387138367\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01717357459375042\n",
      "        policy_loss: -0.04985461480915546\n",
      "        total_loss: 18.25286696990331\n",
      "        vf_explained_var: 0.10306536883115769\n",
      "        vf_loss: 18.294993420441944\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6550651839375496\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016448872949727803\n",
      "        policy_loss: -0.05385043927157918\n",
      "        total_loss: 28.165146810213724\n",
      "        vf_explained_var: 0.03564873884121577\n",
      "        vf_loss: 28.211595319112142\n",
      "  num_agent_steps_sampled: 726150\n",
      "  num_agent_steps_trained: 726150\n",
      "  num_steps_sampled: 726180\n",
      "  num_steps_trained: 726180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 91\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.757894736842104\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.900000000000006\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 43.33333333333333\n",
      "  player_1: 45.0\n",
      "  player_2: 42.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -2.926666666666666\n",
      "  player_1: 0.9133333333333334\n",
      "  player_2: 5.013333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -62.0\n",
      "  player_1: -67.0\n",
      "  player_2: -50.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09360608092126574\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3792982733332851\n",
      "  mean_inference_ms: 1.7622829990083917\n",
      "  mean_raw_obs_processing_ms: 0.2294539963904043\n",
      "time_since_restore: 1490.5926735401154\n",
      "time_this_iter_s: 15.109583139419556\n",
      "time_total_s: 1490.5926735401154\n",
      "timers:\n",
      "  learn_throughput: 567.998\n",
      "  learn_time_ms: 14049.342\n",
      "  load_throughput: 949311.666\n",
      "  load_time_ms: 8.406\n",
      "  sample_throughput: 520.548\n",
      "  sample_time_ms: 15330.01\n",
      "  update_time_ms: 5.779\n",
      "timestamp: 1643537710\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 726180\n",
      "training_iteration: 91\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 742110\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-15-41\n",
      "done: false\n",
      "episode_len_mean: 399.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 3605\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7455219823122025\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017397008653841795\n",
      "        policy_loss: -0.09904102094005793\n",
      "        total_loss: 110.24371627171834\n",
      "        vf_explained_var: 0.29215136299530664\n",
      "        vf_loss: 110.33492899974188\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7336225907007853\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015038334773204648\n",
      "        policy_loss: -0.08402636216410125\n",
      "        total_loss: 92.59472803592682\n",
      "        vf_explained_var: 0.030377093851566315\n",
      "        vf_loss: 92.6719867491722\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7346498803794383\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017527746540745282\n",
      "        policy_loss: -0.07688408895783748\n",
      "        total_loss: 90.86566176891327\n",
      "        vf_explained_var: 0.10032377421855926\n",
      "        vf_loss: 90.93465864658356\n",
      "  num_agent_steps_sampled: 742110\n",
      "  num_agent_steps_trained: 742110\n",
      "  num_steps_sampled: 742140\n",
      "  num_steps_trained: 742140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 93\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.429999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.90000000000001\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 43.666666666666664\n",
      "  player_1: 37.333333333333336\n",
      "  player_2: 45.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 2.17\n",
      "  player_1: -3.9199999999999995\n",
      "  player_2: 4.749999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -52.66666666666667\n",
      "  player_1: -72.0\n",
      "  player_2: -49.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0936497811672015\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3775225021679096\n",
      "  mean_inference_ms: 1.7592323381064077\n",
      "  mean_raw_obs_processing_ms: 0.22885136110061008\n",
      "time_since_restore: 1521.2376358509064\n",
      "time_this_iter_s: 15.78493046760559\n",
      "time_total_s: 1521.2376358509064\n",
      "timers:\n",
      "  learn_throughput: 564.068\n",
      "  learn_time_ms: 14147.237\n",
      "  load_throughput: 949982.571\n",
      "  load_time_ms: 8.4\n",
      "  sample_throughput: 519.461\n",
      "  sample_time_ms: 15362.076\n",
      "  update_time_ms: 5.713\n",
      "timestamp: 1643537741\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 742140\n",
      "training_iteration: 93\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 758070\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-16-13\n",
      "done: false\n",
      "episode_len_mean: 404.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 17\n",
      "episodes_total: 3644\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6776394840578238\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017484916409514804\n",
      "        policy_loss: -0.07734813932950298\n",
      "        total_loss: 38.01734323819478\n",
      "        vf_explained_var: 0.012817174295584361\n",
      "        vf_loss: 38.086823159853616\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6650347097714742\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016767773850582066\n",
      "        policy_loss: -0.04487461098780235\n",
      "        total_loss: 41.0014535621802\n",
      "        vf_explained_var: -0.23023441851139068\n",
      "        vf_loss: 41.03878234704336\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6039584744970004\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015705349452397616\n",
      "        policy_loss: -0.08728109464670221\n",
      "        total_loss: 55.76389842907588\n",
      "        vf_explained_var: 0.20750855932633083\n",
      "        vf_loss: 55.844111772378284\n",
      "  num_agent_steps_sampled: 758070\n",
      "  num_agent_steps_trained: 758070\n",
      "  num_steps_sampled: 758100\n",
      "  num_steps_trained: 758100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 95\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.626315789473686\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.900000000000006\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 43.666666666666664\n",
      "  player_1: 51.0\n",
      "  player_2: 45.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 0.726666666666666\n",
      "  player_1: -1.5233333333333337\n",
      "  player_2: 3.7966666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -72.0\n",
      "  player_1: -72.0\n",
      "  player_2: -54.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0934573388358459\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3761231731525674\n",
      "  mean_inference_ms: 1.7563488660671052\n",
      "  mean_raw_obs_processing_ms: 0.2284903449194224\n",
      "time_since_restore: 1552.8884534835815\n",
      "time_this_iter_s: 15.17304277420044\n",
      "time_total_s: 1552.8884534835815\n",
      "timers:\n",
      "  learn_throughput: 564.981\n",
      "  learn_time_ms: 14124.359\n",
      "  load_throughput: 998444.216\n",
      "  load_time_ms: 7.992\n",
      "  sample_throughput: 516.456\n",
      "  sample_time_ms: 15451.448\n",
      "  update_time_ms: 5.842\n",
      "timestamp: 1643537773\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 758100\n",
      "training_iteration: 95\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 774030\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-16-43\n",
      "done: false\n",
      "episode_len_mean: 378.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 3691\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7512540101011594\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01688572721381206\n",
      "        policy_loss: -0.06988404740889867\n",
      "        total_loss: 97.85742058038711\n",
      "        vf_explained_var: 0.2827242536346118\n",
      "        vf_loss: 97.91970603624979\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7324021631479263\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014775500430105998\n",
      "        policy_loss: -0.09081789345946163\n",
      "        total_loss: 66.27257013003032\n",
      "        vf_explained_var: 0.21195253779490789\n",
      "        vf_loss: 66.35673949718475\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6802630863587061\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017245268952468393\n",
      "        policy_loss: -0.08106584006377185\n",
      "        total_loss: 98.83131801923116\n",
      "        vf_explained_var: -0.12349646131197611\n",
      "        vf_loss: 98.9046239344279\n",
      "  num_agent_steps_sampled: 774030\n",
      "  num_agent_steps_trained: 774030\n",
      "  num_steps_sampled: 774060\n",
      "  num_steps_trained: 774060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 97\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.731578947368419\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.900000000000006\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 43.666666666666664\n",
      "  player_1: 51.0\n",
      "  player_2: 54.0\n",
      "policy_reward_mean:\n",
      "  player_0: -1.9333333333333333\n",
      "  player_1: -0.5133333333333338\n",
      "  player_2: 5.446666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -72.0\n",
      "  player_1: -71.33333333333334\n",
      "  player_2: -54.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09322154764859439\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3742728660139613\n",
      "  mean_inference_ms: 1.7531995391755981\n",
      "  mean_raw_obs_processing_ms: 0.2280398302639103\n",
      "time_since_restore: 1583.242097377777\n",
      "time_this_iter_s: 15.19002079963684\n",
      "time_total_s: 1583.242097377777\n",
      "timers:\n",
      "  learn_throughput: 568.536\n",
      "  learn_time_ms: 14036.061\n",
      "  load_throughput: 1010404.122\n",
      "  load_time_ms: 7.898\n",
      "  sample_throughput: 516.946\n",
      "  sample_time_ms: 15436.801\n",
      "  update_time_ms: 5.841\n",
      "timestamp: 1643537803\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 774060\n",
      "training_iteration: 97\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 789990\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-17-15\n",
      "done: false\n",
      "episode_len_mean: 386.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 3731\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7735920935869217\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015922869303098962\n",
      "        policy_loss: -0.08024200056058665\n",
      "        total_loss: 121.20063888470332\n",
      "        vf_explained_var: -0.04733749349912008\n",
      "        vf_loss: 121.27371527910232\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6783598453303178\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017078960952832557\n",
      "        policy_loss: -0.026574969526069858\n",
      "        total_loss: 84.0222869682312\n",
      "        vf_explained_var: 0.0733260198434194\n",
      "        vf_loss: 84.04117641766867\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6821038916210334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014482492880924838\n",
      "        policy_loss: -0.11277172040815155\n",
      "        total_loss: 72.37511141379674\n",
      "        vf_explained_var: 0.013513224124908447\n",
      "        vf_loss: 72.48136610031128\n",
      "  num_agent_steps_sampled: 789990\n",
      "  num_agent_steps_trained: 789990\n",
      "  num_steps_sampled: 790020\n",
      "  num_steps_trained: 790020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 99\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.420000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.90000000000001\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.0\n",
      "  player_1: 48.66666666666667\n",
      "  player_2: 54.0\n",
      "policy_reward_mean:\n",
      "  player_0: -4.426666666666666\n",
      "  player_1: 0.44333333333333313\n",
      "  player_2: 6.983333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -61.666666666666664\n",
      "  player_1: -62.0\n",
      "  player_2: -53.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09293530139445107\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.37260725514151083\n",
      "  mean_inference_ms: 1.7512224841393331\n",
      "  mean_raw_obs_processing_ms: 0.22703348605175236\n",
      "time_since_restore: 1615.0610282421112\n",
      "time_this_iter_s: 16.454978942871094\n",
      "time_total_s: 1615.0610282421112\n",
      "timers:\n",
      "  learn_throughput: 563.073\n",
      "  learn_time_ms: 14172.232\n",
      "  load_throughput: 976868.582\n",
      "  load_time_ms: 8.169\n",
      "  sample_throughput: 520.612\n",
      "  sample_time_ms: 15328.114\n",
      "  update_time_ms: 5.705\n",
      "timestamp: 1643537835\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 790020\n",
      "training_iteration: 99\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 805950\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-17-47\n",
      "done: false\n",
      "episode_len_mean: 407.84\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 19\n",
      "episodes_total: 3774\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7193001932402452\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01769000321140235\n",
      "        policy_loss: -0.06092866886407137\n",
      "        total_loss: 60.286690783500674\n",
      "        vf_explained_var: -0.23324995279312133\n",
      "        vf_loss: 60.33965897401174\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6990711937348048\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017713197180846692\n",
      "        policy_loss: -0.08388482148448627\n",
      "        total_loss: 56.08210774342219\n",
      "        vf_explained_var: -0.21945791850487392\n",
      "        vf_loss: 56.158021977742514\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6882347579797109\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018401429910757088\n",
      "        policy_loss: -0.1014458359281222\n",
      "        total_loss: 22.704437269369762\n",
      "        vf_explained_var: -0.10953445762395858\n",
      "        vf_loss: 22.797602355480194\n",
      "  num_agent_steps_sampled: 805950\n",
      "  num_agent_steps_trained: 805950\n",
      "  num_steps_sampled: 805980\n",
      "  num_steps_trained: 805980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 101\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.149999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.900000000000006\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 47.333333333333336\n",
      "  player_2: 54.0\n",
      "policy_reward_mean:\n",
      "  player_0: -2.923333333333334\n",
      "  player_1: -3.813333333333333\n",
      "  player_2: 9.736666666666668\n",
      "policy_reward_min:\n",
      "  player_0: -61.666666666666664\n",
      "  player_1: -62.0\n",
      "  player_2: -46.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09274173532148514\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3712820768181959\n",
      "  mean_inference_ms: 1.749390954328513\n",
      "  mean_raw_obs_processing_ms: 0.22669489908602697\n",
      "time_since_restore: 1646.8573009967804\n",
      "time_this_iter_s: 14.675701141357422\n",
      "time_total_s: 1646.8573009967804\n",
      "timers:\n",
      "  learn_throughput: 556.151\n",
      "  learn_time_ms: 14348.625\n",
      "  load_throughput: 974808.752\n",
      "  load_time_ms: 8.186\n",
      "  sample_throughput: 508.196\n",
      "  sample_time_ms: 15702.607\n",
      "  update_time_ms: 5.83\n",
      "timestamp: 1643537867\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 805980\n",
      "training_iteration: 101\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 821910\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-18-21\n",
      "done: false\n",
      "episode_len_mean: 366.82\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3817\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7405956847469012\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018361521830506527\n",
      "        policy_loss: -0.04587687706264357\n",
      "        total_loss: 74.58762840747833\n",
      "        vf_explained_var: -0.11668127983808517\n",
      "        vf_loss: 74.62524227062862\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6846897698442141\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01849403287110884\n",
      "        policy_loss: -0.08208105720890065\n",
      "        total_loss: 37.33064416368802\n",
      "        vf_explained_var: -0.15200634737809499\n",
      "        vf_loss: 37.404402892589566\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6104684098561605\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014794509545041119\n",
      "        policy_loss: -0.09516590133154144\n",
      "        total_loss: 56.68079671462377\n",
      "        vf_explained_var: 0.10819273094336192\n",
      "        vf_loss: 56.76930524190267\n",
      "  num_agent_steps_sampled: 821910\n",
      "  num_agent_steps_trained: 821910\n",
      "  num_steps_sampled: 821940\n",
      "  num_steps_trained: 821940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 103\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.355\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 63.980000000000004\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.0\n",
      "  player_1: 47.333333333333336\n",
      "  player_2: 50.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 0.22000000000000078\n",
      "  player_1: -3.2899999999999987\n",
      "  player_2: 6.07\n",
      "policy_reward_min:\n",
      "  player_0: -61.666666666666664\n",
      "  player_1: -62.0\n",
      "  player_2: -50.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0928730213741045\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3701565820300255\n",
      "  mean_inference_ms: 1.7480834185385594\n",
      "  mean_raw_obs_processing_ms: 0.2266961198471993\n",
      "time_since_restore: 1680.5015394687653\n",
      "time_this_iter_s: 16.13473892211914\n",
      "time_total_s: 1680.5015394687653\n",
      "timers:\n",
      "  learn_throughput: 544.909\n",
      "  learn_time_ms: 14644.648\n",
      "  load_throughput: 1010474.282\n",
      "  load_time_ms: 7.897\n",
      "  sample_throughput: 501.07\n",
      "  sample_time_ms: 15925.932\n",
      "  update_time_ms: 5.765\n",
      "timestamp: 1643537901\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 821940\n",
      "training_iteration: 103\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 837872\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-18-53\n",
      "done: false\n",
      "episode_len_mean: 384.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 19\n",
      "episodes_total: 3852\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6546119705835978\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017152455489437366\n",
      "        policy_loss: -0.07729280130782475\n",
      "        total_loss: 52.80818289756775\n",
      "        vf_explained_var: 0.011457220713297526\n",
      "        vf_loss: 52.87775701125463\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6356163547436396\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01518258092655742\n",
      "        policy_loss: -0.08225215370417573\n",
      "        total_loss: 88.59852009455363\n",
      "        vf_explained_var: -0.012020609279473623\n",
      "        vf_loss: 88.67393962144851\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5469630857805411\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01618521875946044\n",
      "        policy_loss: -0.05970864638375739\n",
      "        total_loss: 79.11013471047083\n",
      "        vf_explained_var: 0.121309412419796\n",
      "        vf_loss: 79.16256043672561\n",
      "  num_agent_steps_sampled: 837872\n",
      "  num_agent_steps_trained: 837872\n",
      "  num_steps_sampled: 837900\n",
      "  num_steps_trained: 837900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 105\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.242105263157896\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.0\n",
      "  player_1: 34.666666666666664\n",
      "  player_2: 45.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 0.6233333333333335\n",
      "  player_1: -3.1266666666666665\n",
      "  player_2: 5.503333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -55.333333333333336\n",
      "  player_1: -70.33333333333333\n",
      "  player_2: -58.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09308464870257568\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3689615369271553\n",
      "  mean_inference_ms: 1.7473530731482672\n",
      "  mean_raw_obs_processing_ms: 0.22628652401725308\n",
      "time_since_restore: 1712.6720170974731\n",
      "time_this_iter_s: 15.2872474193573\n",
      "time_total_s: 1712.6720170974731\n",
      "timers:\n",
      "  learn_throughput: 543.762\n",
      "  learn_time_ms: 14675.547\n",
      "  load_throughput: 901896.347\n",
      "  load_time_ms: 8.848\n",
      "  sample_throughput: 498.317\n",
      "  sample_time_ms: 16013.918\n",
      "  update_time_ms: 5.574\n",
      "timestamp: 1643537933\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 837900\n",
      "training_iteration: 105\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 853830\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-19-23\n",
      "done: false\n",
      "episode_len_mean: 421.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 3894\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6569614347815513\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018683302308018217\n",
      "        policy_loss: -0.09394197767289976\n",
      "        total_loss: 71.52410002231598\n",
      "        vf_explained_var: 0.17861347397168478\n",
      "        vf_loss: 71.6096343700091\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7108174328009288\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01618367861905644\n",
      "        policy_loss: -0.06902648207886765\n",
      "        total_loss: 47.3086684735616\n",
      "        vf_explained_var: 0.10179184526205062\n",
      "        vf_loss: 47.37041236877442\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5878382344543934\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014478102542936048\n",
      "        policy_loss: -0.05797678755906721\n",
      "        total_loss: 32.27567838827769\n",
      "        vf_explained_var: 0.2490508865316709\n",
      "        vf_loss: 32.3271398750941\n",
      "  num_agent_steps_sampled: 853830\n",
      "  num_agent_steps_trained: 853830\n",
      "  num_steps_sampled: 853860\n",
      "  num_steps_trained: 853860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 107\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.238888888888887\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.0\n",
      "  player_1: 34.666666666666664\n",
      "  player_2: 45.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: -1.2400000000000007\n",
      "  player_1: -1.3500000000000005\n",
      "  player_2: 5.59\n",
      "policy_reward_min:\n",
      "  player_0: -58.66666666666667\n",
      "  player_1: -70.33333333333333\n",
      "  player_2: -58.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09291444129441896\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36726779660284115\n",
      "  mean_inference_ms: 1.7457262757537793\n",
      "  mean_raw_obs_processing_ms: 0.2252730426115583\n",
      "time_since_restore: 1743.1725318431854\n",
      "time_this_iter_s: 14.605928659439087\n",
      "time_total_s: 1743.1725318431854\n",
      "timers:\n",
      "  learn_throughput: 543.057\n",
      "  learn_time_ms: 14694.592\n",
      "  load_throughput: 945511.673\n",
      "  load_time_ms: 8.44\n",
      "  sample_throughput: 496.391\n",
      "  sample_time_ms: 16076.043\n",
      "  update_time_ms: 5.551\n",
      "timestamp: 1643537963\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 853860\n",
      "training_iteration: 107\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 869790\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-19-54\n",
      "done: false\n",
      "episode_len_mean: 437.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 15\n",
      "episodes_total: 3920\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6712635747094949\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017355157543838307\n",
      "        policy_loss: -0.07386542678965877\n",
      "        total_loss: 40.43332208673159\n",
      "        vf_explained_var: -0.05149735649426778\n",
      "        vf_loss: 40.49937763373057\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6163052954276402\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015079870317931636\n",
      "        policy_loss: -0.053837534884611765\n",
      "        total_loss: 38.8254155878226\n",
      "        vf_explained_var: -0.09134809096654256\n",
      "        vf_loss: 38.87246729811033\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5423003679513931\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014893490097526108\n",
      "        policy_loss: -0.06609960664374133\n",
      "        total_loss: 57.10850183327993\n",
      "        vf_explained_var: 0.07354119400183359\n",
      "        vf_loss: 57.16789946993192\n",
      "  num_agent_steps_sampled: 869790\n",
      "  num_agent_steps_trained: 869790\n",
      "  num_steps_sampled: 869820\n",
      "  num_steps_trained: 869820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 109\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.785000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 53.33333333333333\n",
      "  player_1: 39.666666666666664\n",
      "  player_2: 48.0\n",
      "policy_reward_mean:\n",
      "  player_0: -1.14\n",
      "  player_1: -0.07000000000000042\n",
      "  player_2: 4.209999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -58.66666666666667\n",
      "  player_1: -70.33333333333333\n",
      "  player_2: -58.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09291972652197299\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3661032975551965\n",
      "  mean_inference_ms: 1.7425403301691569\n",
      "  mean_raw_obs_processing_ms: 0.22524370657472317\n",
      "time_since_restore: 1773.813484430313\n",
      "time_this_iter_s: 15.752872228622437\n",
      "time_total_s: 1773.813484430313\n",
      "timers:\n",
      "  learn_throughput: 547.344\n",
      "  learn_time_ms: 14579.501\n",
      "  load_throughput: 972703.879\n",
      "  load_time_ms: 8.204\n",
      "  sample_throughput: 499.643\n",
      "  sample_time_ms: 15971.404\n",
      "  update_time_ms: 5.471\n",
      "timestamp: 1643537994\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 869820\n",
      "training_iteration: 109\n",
      "trial_id: default\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0130 11:20:23.525907997   16901 backup_poller.cc:134]       Run client channel backup poller: {\"created\":\"@1643538023.525883450\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1643538023.525880635\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 885750\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-20-25\n",
      "done: false\n",
      "episode_len_mean: 476.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 21\n",
      "episodes_total: 3957\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6925105930368105\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017548427924666612\n",
      "        policy_loss: -0.08546598446244995\n",
      "        total_loss: 34.05137357791265\n",
      "        vf_explained_var: 0.1613698453704516\n",
      "        vf_loss: 34.12894288857778\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.670968784938256\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018570603022213465\n",
      "        policy_loss: -0.07049478913152901\n",
      "        total_loss: 30.572023891210556\n",
      "        vf_explained_var: -0.06945013751586278\n",
      "        vf_loss: 30.63416175643603\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5386957595249017\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0165714123967579\n",
      "        policy_loss: -0.07385322296991945\n",
      "        total_loss: 47.946828849315644\n",
      "        vf_explained_var: 0.15177884419759113\n",
      "        vf_loss: 48.01322506268819\n",
      "  num_agent_steps_sampled: 885750\n",
      "  num_agent_steps_trained: 885750\n",
      "  num_steps_sampled: 885780\n",
      "  num_steps_trained: 885780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 111\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.763157894736842\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.00526315789473\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 53.33333333333333\n",
      "  player_1: 39.666666666666664\n",
      "  player_2: 48.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.12666666666666562\n",
      "  player_1: 1.0366666666666662\n",
      "  player_2: 1.836666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -58.66666666666667\n",
      "  player_1: -66.66666666666667\n",
      "  player_2: -56.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09276584188656256\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36450555026278986\n",
      "  mean_inference_ms: 1.7393305394991114\n",
      "  mean_raw_obs_processing_ms: 0.22476404330599412\n",
      "time_since_restore: 1804.7616314888\n",
      "time_this_iter_s: 15.494213342666626\n",
      "time_total_s: 1804.7616314888\n",
      "timers:\n",
      "  learn_throughput: 550.606\n",
      "  learn_time_ms: 14493.112\n",
      "  load_throughput: 915977.371\n",
      "  load_time_ms: 8.712\n",
      "  sample_throughput: 507.161\n",
      "  sample_time_ms: 15734.633\n",
      "  update_time_ms: 5.351\n",
      "timestamp: 1643538025\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 885780\n",
      "training_iteration: 111\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 901714\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-20-56\n",
      "done: false\n",
      "episode_len_mean: 464.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3993\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6790590387582779\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016628488114317577\n",
      "        policy_loss: -0.0838548584918802\n",
      "        total_loss: 49.02191618363062\n",
      "        vf_explained_var: 0.17196941435337065\n",
      "        vf_loss: 49.09828829169273\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6584618371725083\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01619966537873912\n",
      "        policy_loss: -0.07880160719777148\n",
      "        total_loss: 46.17868734558424\n",
      "        vf_explained_var: 0.32298646012941995\n",
      "        vf_loss: 46.25019912997882\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6358454890052477\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021035233726606605\n",
      "        policy_loss: -0.08551266107475385\n",
      "        total_loss: 22.00104641358058\n",
      "        vf_explained_var: 0.10422874450683593\n",
      "        vf_loss: 22.07709331870079\n",
      "  num_agent_steps_sampled: 901714\n",
      "  num_agent_steps_trained: 901714\n",
      "  num_steps_sampled: 901740\n",
      "  num_steps_trained: 901740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 113\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.389473684210529\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.01052631578948\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 53.33333333333333\n",
      "  player_1: 39.666666666666664\n",
      "  player_2: 48.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.21\n",
      "  player_1: -0.39999999999999986\n",
      "  player_2: 2.19\n",
      "policy_reward_min:\n",
      "  player_0: -57.0\n",
      "  player_1: -66.66666666666667\n",
      "  player_2: -56.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09286126693454953\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36306574807187053\n",
      "  mean_inference_ms: 1.736329748214911\n",
      "  mean_raw_obs_processing_ms: 0.2244961540971551\n",
      "time_since_restore: 1835.6687314510345\n",
      "time_this_iter_s: 15.55222749710083\n",
      "time_total_s: 1835.6687314510345\n",
      "timers:\n",
      "  learn_throughput: 561.224\n",
      "  learn_time_ms: 14218.932\n",
      "  load_throughput: 880296.511\n",
      "  load_time_ms: 9.065\n",
      "  sample_throughput: 511.669\n",
      "  sample_time_ms: 15596.012\n",
      "  update_time_ms: 5.409\n",
      "timestamp: 1643538056\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 901740\n",
      "training_iteration: 113\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 917672\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-21-28\n",
      "done: false\n",
      "episode_len_mean: 352.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 32\n",
      "episodes_total: 4051\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6974140553673108\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015504898519589243\n",
      "        policy_loss: -0.08268558165679375\n",
      "        total_loss: 39.92323286771774\n",
      "        vf_explained_var: 0.3129445176323255\n",
      "        vf_loss: 39.99545264482498\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6848929921289285\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01657066372855989\n",
      "        policy_loss: -0.05466603983193636\n",
      "        total_loss: 75.3209731523196\n",
      "        vf_explained_var: 0.10608087340990702\n",
      "        vf_loss: 75.36818273623784\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6570177173117796\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014451287028817603\n",
      "        policy_loss: -0.08905705319717526\n",
      "        total_loss: 123.16118449052175\n",
      "        vf_explained_var: 0.2967994182308515\n",
      "        vf_loss: 123.24048703670502\n",
      "  num_agent_steps_sampled: 917672\n",
      "  num_agent_steps_trained: 917672\n",
      "  num_steps_sampled: 917700\n",
      "  num_steps_trained: 917700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 115\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.172222222222224\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.666666666666664\n",
      "  player_1: 42.0\n",
      "  player_2: 49.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.9566666666666671\n",
      "  player_1: 1.1933333333333322\n",
      "  player_2: 2.763333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -49.666666666666664\n",
      "  player_1: -48.0\n",
      "  player_2: -53.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0926307380908316\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.36214167075790443\n",
      "  mean_inference_ms: 1.7366297780524473\n",
      "  mean_raw_obs_processing_ms: 0.22478343748966637\n",
      "time_since_restore: 1866.9755275249481\n",
      "time_this_iter_s: 14.847311019897461\n",
      "time_total_s: 1866.9755275249481\n",
      "timers:\n",
      "  learn_throughput: 563.762\n",
      "  learn_time_ms: 14154.92\n",
      "  load_throughput: 939389.615\n",
      "  load_time_ms: 8.495\n",
      "  sample_throughput: 515.386\n",
      "  sample_time_ms: 15483.53\n",
      "  update_time_ms: 5.518\n",
      "timestamp: 1643538088\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 917700\n",
      "training_iteration: 115\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 933631\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-21-58\n",
      "done: false\n",
      "episode_len_mean: 346.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 18\n",
      "episodes_total: 4085\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.620195080190897\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014869182367146152\n",
      "        policy_loss: -0.05134550571752091\n",
      "        total_loss: 36.46343578735987\n",
      "        vf_explained_var: 0.06518414596716562\n",
      "        vf_loss: 36.50474453767141\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6562265896300474\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017899014647813752\n",
      "        policy_loss: -0.07133826727047562\n",
      "        total_loss: 73.74157139698664\n",
      "        vf_explained_var: -0.0474836602807045\n",
      "        vf_loss: 73.80485564947128\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6044016107420127\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013446345568640937\n",
      "        policy_loss: -0.08154391931990783\n",
      "        total_loss: 65.39684410174688\n",
      "        vf_explained_var: 0.1998975353439649\n",
      "        vf_loss: 65.46931139071782\n",
      "  num_agent_steps_sampled: 933631\n",
      "  num_agent_steps_trained: 933631\n",
      "  num_steps_sampled: 933660\n",
      "  num_steps_trained: 933660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 117\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.216666666666665\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.0\n",
      "  player_1: 42.0\n",
      "  player_2: 49.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.23666666666666636\n",
      "  player_1: 0.056666666666665505\n",
      "  player_2: 2.706666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -49.666666666666664\n",
      "  player_1: -50.66666666666667\n",
      "  player_2: -53.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09237591294757394\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3615246615126148\n",
      "  mean_inference_ms: 1.736248050038882\n",
      "  mean_raw_obs_processing_ms: 0.2245496733718965\n",
      "time_since_restore: 1896.9673035144806\n",
      "time_this_iter_s: 14.752372741699219\n",
      "time_total_s: 1896.9673035144806\n",
      "timers:\n",
      "  learn_throughput: 565.81\n",
      "  learn_time_ms: 14103.676\n",
      "  load_throughput: 938086.355\n",
      "  load_time_ms: 8.507\n",
      "  sample_throughput: 518.348\n",
      "  sample_time_ms: 15395.054\n",
      "  update_time_ms: 5.603\n",
      "timestamp: 1643538118\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 933660\n",
      "training_iteration: 117\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 949590\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-22-27\n",
      "done: false\n",
      "episode_len_mean: 362.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 18\n",
      "episodes_total: 4125\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6847513506313165\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014441053848274805\n",
      "        policy_loss: -0.09527801501875123\n",
      "        total_loss: 114.07386793335279\n",
      "        vf_explained_var: 0.08059690634409587\n",
      "        vf_loss: 114.15939819812775\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6014473983148734\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017034027705185983\n",
      "        policy_loss: -0.06287146202288568\n",
      "        total_loss: 45.94950095017751\n",
      "        vf_explained_var: -0.014068410893281301\n",
      "        vf_loss: 46.00470717032751\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6367470652858416\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014005405780008005\n",
      "        policy_loss: -0.07576430562573175\n",
      "        total_loss: 62.23757182995478\n",
      "        vf_explained_var: 0.042219107449054716\n",
      "        vf_loss: 62.30388252178828\n",
      "  num_agent_steps_sampled: 949590\n",
      "  num_agent_steps_trained: 949590\n",
      "  num_steps_sampled: 949620\n",
      "  num_steps_trained: 949620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 119\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.511111111111111\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.00555555555555\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.666666666666664\n",
      "  player_1: 42.666666666666664\n",
      "  player_2: 63.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: -0.23333333333333364\n",
      "  player_1: 0.956666666666666\n",
      "  player_2: 2.2766666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -53.66666666666667\n",
      "  player_1: -68.0\n",
      "  player_2: -50.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09211527815115286\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3601162284959097\n",
      "  mean_inference_ms: 1.7328363709970813\n",
      "  mean_raw_obs_processing_ms: 0.22377800973856676\n",
      "time_since_restore: 1926.353299856186\n",
      "time_this_iter_s: 14.559978246688843\n",
      "time_total_s: 1926.353299856186\n",
      "timers:\n",
      "  learn_throughput: 570.95\n",
      "  learn_time_ms: 13976.714\n",
      "  load_throughput: 941461.194\n",
      "  load_time_ms: 8.476\n",
      "  sample_throughput: 518.093\n",
      "  sample_time_ms: 15402.642\n",
      "  update_time_ms: 5.692\n",
      "timestamp: 1643538147\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 949620\n",
      "training_iteration: 119\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 965550\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-22-59\n",
      "done: false\n",
      "episode_len_mean: 404.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 4164\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6488188222050667\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013737360256671612\n",
      "        policy_loss: -0.050491876179973284\n",
      "        total_loss: 75.61990219910939\n",
      "        vf_explained_var: 0.11771500200033187\n",
      "        vf_loss: 75.6611207930247\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5966415662070116\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017003032844814924\n",
      "        policy_loss: -0.11428526545874775\n",
      "        total_loss: 47.338994286060334\n",
      "        vf_explained_var: 0.06862157742182413\n",
      "        vf_loss: 47.44562829573949\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6339649014174938\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015375858207519855\n",
      "        policy_loss: -0.07734946346376091\n",
      "        total_loss: 45.41012130022049\n",
      "        vf_explained_var: 0.17360580215851465\n",
      "        vf_loss: 45.47709210395813\n",
      "  num_agent_steps_sampled: 965550\n",
      "  num_agent_steps_trained: 965550\n",
      "  num_steps_sampled: 965580\n",
      "  num_steps_trained: 965580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 121\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.368421052631582\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.666666666666664\n",
      "  player_1: 42.666666666666664\n",
      "  player_2: 63.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: -2.873333333333333\n",
      "  player_1: -0.26333333333333336\n",
      "  player_2: 6.136666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -62.333333333333336\n",
      "  player_1: -68.0\n",
      "  player_2: -50.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09226579868457212\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3589546442106022\n",
      "  mean_inference_ms: 1.7292693668365158\n",
      "  mean_raw_obs_processing_ms: 0.22336926007856905\n",
      "time_since_restore: 1957.8239178657532\n",
      "time_this_iter_s: 15.090101718902588\n",
      "time_total_s: 1957.8239178657532\n",
      "timers:\n",
      "  learn_throughput: 568.692\n",
      "  learn_time_ms: 14032.202\n",
      "  load_throughput: 951059.615\n",
      "  load_time_ms: 8.391\n",
      "  sample_throughput: 519.023\n",
      "  sample_time_ms: 15375.048\n",
      "  update_time_ms: 5.625\n",
      "timestamp: 1643538179\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 965580\n",
      "training_iteration: 121\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 981510\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-23-33\n",
      "done: false\n",
      "episode_len_mean: 370.54\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 4209\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6497357705732186\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01334517082293606\n",
      "        policy_loss: -0.09687248327497704\n",
      "        total_loss: 53.92508779764175\n",
      "        vf_explained_var: 0.06518089294433593\n",
      "        vf_loss: 54.01295214255651\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6659845321873824\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015757491468645905\n",
      "        policy_loss: -0.05070362892001867\n",
      "        total_loss: 38.12172327935696\n",
      "        vf_explained_var: 0.2117006629705429\n",
      "        vf_loss: 38.165336110194524\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.615107391824325\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014727606383208922\n",
      "        policy_loss: -0.04972826794410745\n",
      "        total_loss: 22.824627628723782\n",
      "        vf_explained_var: 0.14654625376065572\n",
      "        vf_loss: 22.86441460331281\n",
      "  num_agent_steps_sampled: 981510\n",
      "  num_agent_steps_trained: 981510\n",
      "  num_steps_sampled: 981540\n",
      "  num_steps_trained: 981540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 123\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.042857142857141\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 44.0\n",
      "  player_1: 42.666666666666664\n",
      "  player_2: 63.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: -4.51\n",
      "  player_1: 1.3599999999999988\n",
      "  player_2: 6.149999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -62.333333333333336\n",
      "  player_1: -48.66666666666667\n",
      "  player_2: -47.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09213059903349696\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3577583335884667\n",
      "  mean_inference_ms: 1.7260402129063095\n",
      "  mean_raw_obs_processing_ms: 0.2232961229522414\n",
      "time_since_restore: 1992.4209463596344\n",
      "time_this_iter_s: 16.94159436225891\n",
      "time_total_s: 1992.4209463596344\n",
      "timers:\n",
      "  learn_throughput: 553.849\n",
      "  learn_time_ms: 14408.263\n",
      "  load_throughput: 943893.162\n",
      "  load_time_ms: 8.454\n",
      "  sample_throughput: 512.672\n",
      "  sample_time_ms: 15565.511\n",
      "  update_time_ms: 5.58\n",
      "timestamp: 1643538213\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 981540\n",
      "training_iteration: 123\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 997470\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-24-03\n",
      "done: false\n",
      "episode_len_mean: 381.12\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 4255\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6724888729055722\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01489626738342243\n",
      "        policy_loss: -0.08775563754762213\n",
      "        total_loss: 57.76366897185643\n",
      "        vf_explained_var: 0.179378937681516\n",
      "        vf_loss: 57.84136944293976\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6255534325540065\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017476692223571035\n",
      "        policy_loss: -0.06590655864061166\n",
      "        total_loss: 74.73654608567556\n",
      "        vf_explained_var: -0.12399109065532685\n",
      "        vf_loss: 74.79458783388138\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6666695569455624\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014805054189611628\n",
      "        policy_loss: -0.08525487001053988\n",
      "        total_loss: 88.60760636965433\n",
      "        vf_explained_var: 0.1063936260342598\n",
      "        vf_loss: 88.6828676144282\n",
      "  num_agent_steps_sampled: 997470\n",
      "  num_agent_steps_trained: 997470\n",
      "  num_steps_sampled: 997500\n",
      "  num_steps_trained: 997500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 125\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.689473684210528\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.14210526315787\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 44.0\n",
      "  player_1: 38.33333333333333\n",
      "  player_2: 47.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: -3.7166666666666663\n",
      "  player_1: 2.043333333333333\n",
      "  player_2: 4.673333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -64.33333333333333\n",
      "  player_1: -51.33333333333333\n",
      "  player_2: -43.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09194357470007143\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.35681650741887255\n",
      "  mean_inference_ms: 1.7245747846350625\n",
      "  mean_raw_obs_processing_ms: 0.22315761823671806\n",
      "time_since_restore: 2022.2527911663055\n",
      "time_this_iter_s: 14.946285963058472\n",
      "time_total_s: 2022.2527911663055\n",
      "timers:\n",
      "  learn_throughput: 559.6\n",
      "  learn_time_ms: 14260.189\n",
      "  load_throughput: 941355.28\n",
      "  load_time_ms: 8.477\n",
      "  sample_throughput: 513.351\n",
      "  sample_time_ms: 15544.915\n",
      "  update_time_ms: 5.665\n",
      "timestamp: 1643538243\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 997500\n",
      "training_iteration: 125\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1013430\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-24-32\n",
      "done: false\n",
      "episode_len_mean: 378.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 18\n",
      "episodes_total: 4296\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7211245554685592\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014187562184912773\n",
      "        policy_loss: -0.07805681699731698\n",
      "        total_loss: 52.28153855164846\n",
      "        vf_explained_var: -0.08178414762020111\n",
      "        vf_loss: 52.35001881281535\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.642608155955871\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015228355594184298\n",
      "        policy_loss: -0.07028839542918529\n",
      "        total_loss: 60.19992753346761\n",
      "        vf_explained_var: -0.029866417050361634\n",
      "        vf_loss: 60.26336340626081\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5658912908534209\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011992665076108817\n",
      "        policy_loss: -0.06614907862618566\n",
      "        total_loss: 47.21506896495819\n",
      "        vf_explained_var: 0.041118274927139285\n",
      "        vf_loss: 47.27312324206034\n",
      "  num_agent_steps_sampled: 1013430\n",
      "  num_agent_steps_trained: 1013430\n",
      "  num_steps_sampled: 1013460\n",
      "  num_steps_trained: 1013460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 127\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.447058823529414\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.1\n",
      "  vram_util_percent0: 0.3741861979166667\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 49.0\n",
      "  player_1: 38.33333333333333\n",
      "  player_2: 51.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -1.7366666666666666\n",
      "  player_1: 0.09333333333333287\n",
      "  player_2: 4.6433333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -64.33333333333333\n",
      "  player_1: -68.0\n",
      "  player_2: -43.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09185802679832543\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.35577898739342706\n",
      "  mean_inference_ms: 1.7234220589200049\n",
      "  mean_raw_obs_processing_ms: 0.22251178593229107\n",
      "time_since_restore: 2051.2756428718567\n",
      "time_this_iter_s: 14.262651443481445\n",
      "time_total_s: 2051.2756428718567\n",
      "timers:\n",
      "  learn_throughput: 563.312\n",
      "  learn_time_ms: 14166.222\n",
      "  load_throughput: 937508.289\n",
      "  load_time_ms: 8.512\n",
      "  sample_throughput: 514.707\n",
      "  sample_time_ms: 15503.978\n",
      "  update_time_ms: 5.586\n",
      "timestamp: 1643538272\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1013460\n",
      "training_iteration: 127\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1029390\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-25-03\n",
      "done: false\n",
      "episode_len_mean: 364.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 16\n",
      "episodes_total: 4329\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6185636527339617\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01301134373570676\n",
      "        policy_loss: -0.08760669443756341\n",
      "        total_loss: 93.32924843152364\n",
      "        vf_explained_var: 0.1118258269627889\n",
      "        vf_loss: 93.40807239532471\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6401078846553961\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01624104120710702\n",
      "        policy_loss: -0.05911186489587029\n",
      "        total_loss: 55.32316716750463\n",
      "        vf_explained_var: 0.11959637929995855\n",
      "        vf_loss: 55.37497053106626\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5893011848131816\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013260963436128748\n",
      "        policy_loss: -0.047779647434751195\n",
      "        total_loss: 41.27182990153631\n",
      "        vf_explained_var: -0.05996511499087016\n",
      "        vf_loss: 41.31065834681193\n",
      "  num_agent_steps_sampled: 1029390\n",
      "  num_agent_steps_trained: 1029390\n",
      "  num_steps_sampled: 1029420\n",
      "  num_steps_trained: 1029420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 129\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.110000000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 49.0\n",
      "  player_1: 48.0\n",
      "  player_2: 51.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 0.7866666666666664\n",
      "  player_1: 0.48666666666666686\n",
      "  player_2: 1.7266666666666663\n",
      "policy_reward_min:\n",
      "  player_0: -57.333333333333336\n",
      "  player_1: -68.0\n",
      "  player_2: -51.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09170104884471911\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3543800359206451\n",
      "  mean_inference_ms: 1.7185786028825962\n",
      "  mean_raw_obs_processing_ms: 0.2219795526255909\n",
      "time_since_restore: 2081.9184560775757\n",
      "time_this_iter_s: 15.9434072971344\n",
      "time_total_s: 2081.9184560775757\n",
      "timers:\n",
      "  learn_throughput: 558.247\n",
      "  learn_time_ms: 14294.74\n",
      "  load_throughput: 951992.864\n",
      "  load_time_ms: 8.382\n",
      "  sample_throughput: 516.74\n",
      "  sample_time_ms: 15442.958\n",
      "  update_time_ms: 5.557\n",
      "timestamp: 1643538303\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1029420\n",
      "training_iteration: 129\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1045350\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-25-33\n",
      "done: false\n",
      "episode_len_mean: 369.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 16\n",
      "episodes_total: 4368\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.67250261982282\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014069426999920817\n",
      "        policy_loss: -0.08402341959998011\n",
      "        total_loss: 50.46166557272275\n",
      "        vf_explained_var: 0.04622377067804337\n",
      "        vf_loss: 50.53619235515595\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5754799588521322\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014439481205281482\n",
      "        policy_loss: -0.08165345676243305\n",
      "        total_loss: 32.9155005689462\n",
      "        vf_explained_var: -0.2144311452905337\n",
      "        vf_loss: 32.98740730921428\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5886811010787885\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012486341249841644\n",
      "        policy_loss: -0.04771931482478976\n",
      "        total_loss: 84.82131211400032\n",
      "        vf_explained_var: 0.09466497113307316\n",
      "        vf_loss: 84.86060352444649\n",
      "  num_agent_steps_sampled: 1045350\n",
      "  num_agent_steps_trained: 1045350\n",
      "  num_steps_sampled: 1045380\n",
      "  num_steps_trained: 1045380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 131\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.478947368421053\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 49.0\n",
      "  player_1: 48.0\n",
      "  player_2: 44.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.5499999999999994\n",
      "  player_1: -0.92\n",
      "  player_2: 0.3700000000000002\n",
      "policy_reward_min:\n",
      "  player_0: -54.333333333333336\n",
      "  player_1: -68.0\n",
      "  player_2: -68.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09162712293968099\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.35355567610277255\n",
      "  mean_inference_ms: 1.7160379868777762\n",
      "  mean_raw_obs_processing_ms: 0.221834087360829\n",
      "time_since_restore: 2111.800268650055\n",
      "time_this_iter_s: 15.205806016921997\n",
      "time_total_s: 2111.800268650055\n",
      "timers:\n",
      "  learn_throughput: 564.425\n",
      "  learn_time_ms: 14138.294\n",
      "  load_throughput: 1013350.023\n",
      "  load_time_ms: 7.875\n",
      "  sample_throughput: 517.892\n",
      "  sample_time_ms: 15408.62\n",
      "  update_time_ms: 5.623\n",
      "timestamp: 1643538333\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1045380\n",
      "training_iteration: 131\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1061312\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-26-05\n",
      "done: false\n",
      "episode_len_mean: 437.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 18\n",
      "episodes_total: 4402\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6244178482890129\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014162612867561014\n",
      "        policy_loss: -0.06027154241533329\n",
      "        total_loss: 35.72835981647174\n",
      "        vf_explained_var: 0.04848863432804743\n",
      "        vf_loss: 35.77907178759575\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6145516366263231\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01559673339316987\n",
      "        policy_loss: -0.07725328148032228\n",
      "        total_loss: 34.25435182889303\n",
      "        vf_explained_var: 0.26924379100402196\n",
      "        vf_loss: 34.321077268123624\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5657874609529973\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013587613535232966\n",
      "        policy_loss: -0.06522841057429711\n",
      "        total_loss: 52.35136262694994\n",
      "        vf_explained_var: 0.0001279368003209432\n",
      "        vf_loss: 52.40741970896721\n",
      "  num_agent_steps_sampled: 1061312\n",
      "  num_agent_steps_trained: 1061312\n",
      "  num_steps_sampled: 1061340\n",
      "  num_steps_trained: 1061340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 133\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.835000000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 46.66666666666667\n",
      "  player_1: 48.0\n",
      "  player_2: 44.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 4.650000000000001\n",
      "  player_1: -0.91\n",
      "  player_2: -0.74\n",
      "policy_reward_min:\n",
      "  player_0: -53.333333333333336\n",
      "  player_1: -39.333333333333336\n",
      "  player_2: -68.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09151275058226506\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.35305315418141325\n",
      "  mean_inference_ms: 1.7159457449966664\n",
      "  mean_raw_obs_processing_ms: 0.22173687551101437\n",
      "time_since_restore: 2143.8518674373627\n",
      "time_this_iter_s: 16.265522003173828\n",
      "time_total_s: 2143.8518674373627\n",
      "timers:\n",
      "  learn_throughput: 574.82\n",
      "  learn_time_ms: 13882.612\n",
      "  load_throughput: 1091454.572\n",
      "  load_time_ms: 7.311\n",
      "  sample_throughput: 523.728\n",
      "  sample_time_ms: 15236.92\n",
      "  update_time_ms: 5.688\n",
      "timestamp: 1643538365\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1061340\n",
      "training_iteration: 133\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1077270\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-26-36\n",
      "done: false\n",
      "episode_len_mean: 441.12\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 4449\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6837568076948325\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014109668501749486\n",
      "        policy_loss: -0.04620298941309253\n",
      "        total_loss: 56.520136256615324\n",
      "        vf_explained_var: 0.018549471894900003\n",
      "        vf_loss: 56.556815367142356\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6154315400123597\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012843813778110114\n",
      "        policy_loss: -0.07257402784967174\n",
      "        total_loss: 27.46504521926244\n",
      "        vf_explained_var: -0.05108154277006785\n",
      "        vf_loss: 27.528949623902637\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6046554778516292\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013869437150344575\n",
      "        policy_loss: -0.10554201379418374\n",
      "        total_loss: 24.64709781249364\n",
      "        vf_explained_var: -0.015267445941766104\n",
      "        vf_loss: 24.743277906576793\n",
      "  num_agent_steps_sampled: 1077270\n",
      "  num_agent_steps_trained: 1077270\n",
      "  num_steps_sampled: 1077300\n",
      "  num_steps_trained: 1077300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 135\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.768421052631581\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 46.66666666666667\n",
      "  player_1: 33.333333333333336\n",
      "  player_2: 44.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.6966666666666668\n",
      "  player_1: -1.143333333333333\n",
      "  player_2: 0.44666666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -49.666666666666664\n",
      "  player_1: -40.666666666666664\n",
      "  player_2: -68.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09156208038293961\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.35226862993437036\n",
      "  mean_inference_ms: 1.7157441389531527\n",
      "  mean_raw_obs_processing_ms: 0.22161472742044006\n",
      "time_since_restore: 2175.2232282161713\n",
      "time_this_iter_s: 15.618163347244263\n",
      "time_total_s: 2175.2232282161713\n",
      "timers:\n",
      "  learn_throughput: 568.381\n",
      "  learn_time_ms: 14039.887\n",
      "  load_throughput: 1078070.968\n",
      "  load_time_ms: 7.402\n",
      "  sample_throughput: 523.225\n",
      "  sample_time_ms: 15251.579\n",
      "  update_time_ms: 5.519\n",
      "timestamp: 1643538396\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1077300\n",
      "training_iteration: 135\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1093231\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-27-08\n",
      "done: false\n",
      "episode_len_mean: 367.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 19\n",
      "episodes_total: 4493\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6544825541973114\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014685843663583\n",
      "        policy_loss: -0.0919691703779002\n",
      "        total_loss: 39.57788893222809\n",
      "        vf_explained_var: 0.11148286153872808\n",
      "        vf_loss: 39.65994527657827\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6259765656292439\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012559131042160297\n",
      "        policy_loss: -0.06957444311430057\n",
      "        total_loss: 71.53430938005448\n",
      "        vf_explained_var: 0.01270470509926478\n",
      "        vf_loss: 71.59540695905686\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6039011255900065\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012661487687237289\n",
      "        policy_loss: -0.051683020892863474\n",
      "        total_loss: 78.5904598681132\n",
      "        vf_explained_var: 0.15966092199087142\n",
      "        vf_loss: 78.63359656771024\n",
      "  num_agent_steps_sampled: 1093231\n",
      "  num_agent_steps_trained: 1093231\n",
      "  num_steps_sampled: 1093260\n",
      "  num_steps_trained: 1093260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 137\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.289473684210526\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.666666666666664\n",
      "  player_1: 31.666666666666664\n",
      "  player_2: 38.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.0800000000000005\n",
      "  player_1: -0.9400000000000006\n",
      "  player_2: 0.8599999999999998\n",
      "policy_reward_min:\n",
      "  player_0: -52.33333333333333\n",
      "  player_1: -57.333333333333336\n",
      "  player_2: -58.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09146374290706322\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.35091941162190465\n",
      "  mean_inference_ms: 1.7131408429610988\n",
      "  mean_raw_obs_processing_ms: 0.22116264243993874\n",
      "time_since_restore: 2207.069526195526\n",
      "time_this_iter_s: 15.323973417282104\n",
      "time_total_s: 2207.069526195526\n",
      "timers:\n",
      "  learn_throughput: 557.252\n",
      "  learn_time_ms: 14320.278\n",
      "  load_throughput: 1060715.073\n",
      "  load_time_ms: 7.523\n",
      "  sample_throughput: 514.893\n",
      "  sample_time_ms: 15498.355\n",
      "  update_time_ms: 5.538\n",
      "timestamp: 1643538428\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1093260\n",
      "training_iteration: 137\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1109191\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-27-38\n",
      "done: false\n",
      "episode_len_mean: 329.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 15\n",
      "episodes_total: 4531\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5628762770692507\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013564780056797949\n",
      "        policy_loss: -0.05964152266271412\n",
      "        total_loss: 57.27250687678655\n",
      "        vf_explained_var: 0.17311745395263037\n",
      "        vf_loss: 57.322992005348205\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.582223002165556\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012450776278698564\n",
      "        policy_loss: -0.05023257582448423\n",
      "        total_loss: 20.431912161509196\n",
      "        vf_explained_var: -0.0036386541525522866\n",
      "        vf_loss: 20.473740422725676\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5374437240759532\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012752617373547158\n",
      "        policy_loss: -0.08952100965815286\n",
      "        total_loss: 37.38744399825732\n",
      "        vf_explained_var: 0.1921652947862943\n",
      "        vf_loss: 37.46835703730583\n",
      "  num_agent_steps_sampled: 1109191\n",
      "  num_agent_steps_trained: 1109191\n",
      "  num_steps_sampled: 1109220\n",
      "  num_steps_trained: 1109220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 139\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.563157894736845\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.666666666666664\n",
      "  player_1: 31.666666666666664\n",
      "  player_2: 50.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.146666666666667\n",
      "  player_1: -1.4533333333333334\n",
      "  player_2: 2.306666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -52.33333333333333\n",
      "  player_1: -57.333333333333336\n",
      "  player_2: -58.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09149718155259912\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3507600283316626\n",
      "  mean_inference_ms: 1.7142189490644781\n",
      "  mean_raw_obs_processing_ms: 0.22126425606213207\n",
      "time_since_restore: 2236.5926542282104\n",
      "time_this_iter_s: 14.887495756149292\n",
      "time_total_s: 2236.5926542282104\n",
      "timers:\n",
      "  learn_throughput: 561.621\n",
      "  learn_time_ms: 14208.864\n",
      "  load_throughput: 1057187.174\n",
      "  load_time_ms: 7.548\n",
      "  sample_throughput: 511.654\n",
      "  sample_time_ms: 15596.47\n",
      "  update_time_ms: 5.576\n",
      "timestamp: 1643538458\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1109220\n",
      "training_iteration: 139\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1125150\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-28-10\n",
      "done: false\n",
      "episode_len_mean: 376.84\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 17\n",
      "episodes_total: 4564\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6163567782441775\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013775545429913716\n",
      "        policy_loss: -0.08919016884018978\n",
      "        total_loss: 40.750396198829016\n",
      "        vf_explained_var: 0.3116472741961479\n",
      "        vf_loss: 40.83028787771861\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5791455240547657\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0150452191490596\n",
      "        policy_loss: -0.07510513755803307\n",
      "        total_loss: 26.47898689786593\n",
      "        vf_explained_var: 0.23046383400758108\n",
      "        vf_loss: 26.543936667044957\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5787793820599715\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014213524624851981\n",
      "        policy_loss: -0.07640651987865567\n",
      "        total_loss: 15.700023698409398\n",
      "        vf_explained_var: 0.12616067389647165\n",
      "        vf_loss: 15.766836041609446\n",
      "  num_agent_steps_sampled: 1125150\n",
      "  num_agent_steps_trained: 1125150\n",
      "  num_steps_sampled: 1125180\n",
      "  num_steps_trained: 1125180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 141\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.939999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.155\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.666666666666664\n",
      "  player_1: 31.666666666666664\n",
      "  player_2: 50.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.1066666666666662\n",
      "  player_1: -2.043333333333334\n",
      "  player_2: 3.936666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -52.33333333333333\n",
      "  player_1: -48.0\n",
      "  player_2: -54.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0914787969371286\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3498099168102799\n",
      "  mean_inference_ms: 1.7113703708892203\n",
      "  mean_raw_obs_processing_ms: 0.22100897455671611\n",
      "time_since_restore: 2268.435427427292\n",
      "time_this_iter_s: 15.93819785118103\n",
      "time_total_s: 2268.435427427292\n",
      "timers:\n",
      "  learn_throughput: 553.969\n",
      "  learn_time_ms: 14405.141\n",
      "  load_throughput: 971196.694\n",
      "  load_time_ms: 8.217\n",
      "  sample_throughput: 510.96\n",
      "  sample_time_ms: 15617.656\n",
      "  update_time_ms: 5.664\n",
      "timestamp: 1643538490\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1125180\n",
      "training_iteration: 141\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1141110\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-28-39\n",
      "done: false\n",
      "episode_len_mean: 362.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 21\n",
      "episodes_total: 4609\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6334704567492008\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015485435874935168\n",
      "        policy_loss: -0.049518321761861446\n",
      "        total_loss: 57.015767610470455\n",
      "        vf_explained_var: 0.004744297762711843\n",
      "        vf_loss: 57.05483348925908\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.622215598175923\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014346927619332444\n",
      "        policy_loss: -0.09335289521142841\n",
      "        total_loss: 49.766002571582796\n",
      "        vf_explained_var: -0.017273816764354705\n",
      "        vf_loss: 49.849671502113345\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.547881766607364\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015182461434445333\n",
      "        policy_loss: -0.07245488530645768\n",
      "        total_loss: 33.73020848194758\n",
      "        vf_explained_var: 0.23666402568419775\n",
      "        vf_loss: 33.79241538405418\n",
      "  num_agent_steps_sampled: 1141110\n",
      "  num_agent_steps_trained: 1141110\n",
      "  num_steps_sampled: 1141140\n",
      "  num_steps_trained: 1141140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 143\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.200000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.16666666666669\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 39.333333333333336\n",
      "  player_2: 50.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.9600000000000002\n",
      "  player_1: -2.8500000000000005\n",
      "  player_2: 6.81\n",
      "policy_reward_min:\n",
      "  player_0: -69.66666666666666\n",
      "  player_1: -47.666666666666664\n",
      "  player_2: -36.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09135998466790757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.34837010459496454\n",
      "  mean_inference_ms: 1.7081421199615336\n",
      "  mean_raw_obs_processing_ms: 0.2204095072725853\n",
      "time_since_restore: 2297.728458404541\n",
      "time_this_iter_s: 14.666571378707886\n",
      "time_total_s: 2297.728458404541\n",
      "timers:\n",
      "  learn_throughput: 565.444\n",
      "  learn_time_ms: 14112.812\n",
      "  load_throughput: 941511.511\n",
      "  load_time_ms: 8.476\n",
      "  sample_throughput: 511.821\n",
      "  sample_time_ms: 15591.403\n",
      "  update_time_ms: 5.661\n",
      "timestamp: 1643538519\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1141140\n",
      "training_iteration: 143\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1157070\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-29-10\n",
      "done: false\n",
      "episode_len_mean: 435.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 4649\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6028904326756795\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012268939868398927\n",
      "        policy_loss: -0.05521903310281535\n",
      "        total_loss: 61.909188551108045\n",
      "        vf_explained_var: 0.009695055286089578\n",
      "        vf_loss: 61.95612601558367\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5943914607415597\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01178965104532684\n",
      "        policy_loss: -0.04840459592950841\n",
      "        total_loss: 42.90727590103944\n",
      "        vf_explained_var: -0.06302182078361511\n",
      "        vf_loss: 42.94772265434265\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6013901188969613\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013068144015500138\n",
      "        policy_loss: -0.09274810708438357\n",
      "        total_loss: 49.527517795562744\n",
      "        vf_explained_var: 0.2801867096622785\n",
      "        vf_loss: 49.61144498944282\n",
      "  num_agent_steps_sampled: 1157070\n",
      "  num_agent_steps_trained: 1157070\n",
      "  num_steps_sampled: 1157100\n",
      "  num_steps_trained: 1157100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 145\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.052631578947366\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 39.333333333333336\n",
      "  player_2: 39.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -2.0866666666666673\n",
      "  player_1: -1.606666666666667\n",
      "  player_2: 6.693333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -69.66666666666666\n",
      "  player_1: -47.666666666666664\n",
      "  player_2: -36.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09139828645638902\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.34756139323603846\n",
      "  mean_inference_ms: 1.7061945706643553\n",
      "  mean_raw_obs_processing_ms: 0.2202550781274406\n",
      "time_since_restore: 2328.492580652237\n",
      "time_this_iter_s: 15.703711032867432\n",
      "time_total_s: 2328.492580652237\n",
      "timers:\n",
      "  learn_throughput: 567.862\n",
      "  learn_time_ms: 14052.708\n",
      "  load_throughput: 871059.436\n",
      "  load_time_ms: 9.161\n",
      "  sample_throughput: 520.012\n",
      "  sample_time_ms: 15345.801\n",
      "  update_time_ms: 5.626\n",
      "timestamp: 1643538550\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1157100\n",
      "training_iteration: 145\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1173030\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-29-43\n",
      "done: false\n",
      "episode_len_mean: 411.82\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 4700\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6774514379103979\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01455144840110582\n",
      "        policy_loss: -0.06366651521840443\n",
      "        total_loss: 72.44316000858943\n",
      "        vf_explained_var: 0.10366285194953283\n",
      "        vf_loss: 72.49700419425965\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6160981924335162\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014181041900491399\n",
      "        policy_loss: -0.06747314448778828\n",
      "        total_loss: 41.91298666596413\n",
      "        vf_explained_var: 0.05668694506088893\n",
      "        vf_loss: 41.97088753183683\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5959366115927697\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014556958664707053\n",
      "        policy_loss: -0.08863582081161439\n",
      "        total_loss: 39.25536634763082\n",
      "        vf_explained_var: 0.12544292618831\n",
      "        vf_loss: 39.33417619546255\n",
      "  num_agent_steps_sampled: 1173030\n",
      "  num_agent_steps_trained: 1173030\n",
      "  num_steps_sampled: 1173060\n",
      "  num_steps_trained: 1173060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 147\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.084999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.20000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.0\n",
      "  player_1: 38.0\n",
      "  player_2: 42.0\n",
      "policy_reward_mean:\n",
      "  player_0: -3.9733333333333336\n",
      "  player_1: -0.47333333333333355\n",
      "  player_2: 7.446666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -69.0\n",
      "  player_1: -47.666666666666664\n",
      "  player_2: -38.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09119188357274283\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3467322559739273\n",
      "  mean_inference_ms: 1.7036612898236279\n",
      "  mean_raw_obs_processing_ms: 0.21983705774723206\n",
      "time_since_restore: 2361.4527702331543\n",
      "time_this_iter_s: 15.886847496032715\n",
      "time_total_s: 2361.4527702331543\n",
      "timers:\n",
      "  learn_throughput: 563.418\n",
      "  learn_time_ms: 14163.547\n",
      "  load_throughput: 727511.437\n",
      "  load_time_ms: 10.969\n",
      "  sample_throughput: 517.863\n",
      "  sample_time_ms: 15409.494\n",
      "  update_time_ms: 5.693\n",
      "timestamp: 1643538583\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1173060\n",
      "training_iteration: 147\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1188990\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-30-15\n",
      "done: false\n",
      "episode_len_mean: 333.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 21\n",
      "episodes_total: 4747\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6190211941301823\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014609730529199018\n",
      "        policy_loss: -0.05288859823718667\n",
      "        total_loss: 68.90541651884715\n",
      "        vf_explained_var: 0.20702247937520346\n",
      "        vf_loss: 68.94844271580378\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5998989529410999\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012110180919150367\n",
      "        policy_loss: -0.06241796744676928\n",
      "        total_loss: 97.45652055740356\n",
      "        vf_explained_var: -0.2988601955771446\n",
      "        vf_loss: 97.51076508045196\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5828319057325522\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012581910850801516\n",
      "        policy_loss: -0.09427582522543768\n",
      "        total_loss: 95.28144932746888\n",
      "        vf_explained_var: 0.05649417400360107\n",
      "        vf_loss: 95.36723249673844\n",
      "  num_agent_steps_sampled: 1188990\n",
      "  num_agent_steps_trained: 1188990\n",
      "  num_steps_sampled: 1189020\n",
      "  num_steps_trained: 1189020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 149\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.975\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.20000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 50.0\n",
      "  player_1: 48.33333333333333\n",
      "  player_2: 55.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -0.6100000000000009\n",
      "  player_1: -1.2300000000000002\n",
      "  player_2: 4.84\n",
      "policy_reward_min:\n",
      "  player_0: -69.0\n",
      "  player_1: -68.66666666666667\n",
      "  player_2: -70.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09113538083419831\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3461287385200514\n",
      "  mean_inference_ms: 1.704214585036273\n",
      "  mean_raw_obs_processing_ms: 0.21952695672051512\n",
      "time_since_restore: 2393.0802347660065\n",
      "time_this_iter_s: 15.751087665557861\n",
      "time_total_s: 2393.0802347660065\n",
      "timers:\n",
      "  learn_throughput: 555.097\n",
      "  learn_time_ms: 14375.854\n",
      "  load_throughput: 706482.902\n",
      "  load_time_ms: 11.295\n",
      "  sample_throughput: 511.797\n",
      "  sample_time_ms: 15592.106\n",
      "  update_time_ms: 5.611\n",
      "timestamp: 1643538615\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1189020\n",
      "training_iteration: 149\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1204951\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-30-46\n",
      "done: false\n",
      "episode_len_mean: 353.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 4792\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6373991486926873\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01375531869236814\n",
      "        policy_loss: -0.08505508301779628\n",
      "        total_loss: 63.27583356698354\n",
      "        vf_explained_var: 0.1470956474542618\n",
      "        vf_loss: 63.35160399754842\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6750271663069725\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017089980364253847\n",
      "        policy_loss: -0.07963020417218407\n",
      "        total_loss: 28.54886774778366\n",
      "        vf_explained_var: -0.02704650829235713\n",
      "        vf_loss: 28.616962296565372\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.597669582615296\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014951139714472863\n",
      "        policy_loss: -0.08837140164027611\n",
      "        total_loss: 33.03369933446248\n",
      "        vf_explained_var: 0.1583526211977005\n",
      "        vf_loss: 33.11197869141897\n",
      "  num_agent_steps_sampled: 1204951\n",
      "  num_agent_steps_trained: 1204951\n",
      "  num_steps_sampled: 1204980\n",
      "  num_steps_trained: 1204980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 151\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.299999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 50.0\n",
      "  player_1: 48.33333333333333\n",
      "  player_2: 55.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.2166666666666668\n",
      "  player_1: -2.583333333333334\n",
      "  player_2: 4.366666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -61.666666666666664\n",
      "  player_1: -68.66666666666667\n",
      "  player_2: -70.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09090975351638876\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3449128311459449\n",
      "  mean_inference_ms: 1.7010764832761143\n",
      "  mean_raw_obs_processing_ms: 0.21912542397617976\n",
      "time_since_restore: 2424.0415513515472\n",
      "time_this_iter_s: 15.20796251296997\n",
      "time_total_s: 2424.0415513515472\n",
      "timers:\n",
      "  learn_throughput: 558.519\n",
      "  learn_time_ms: 14287.781\n",
      "  load_throughput: 742827.535\n",
      "  load_time_ms: 10.743\n",
      "  sample_throughput: 509.543\n",
      "  sample_time_ms: 15661.085\n",
      "  update_time_ms: 5.566\n",
      "timestamp: 1643538646\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1204980\n",
      "training_iteration: 151\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1220910\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-31-15\n",
      "done: false\n",
      "episode_len_mean: 368.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 4837\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6525311660269896\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01374268790407361\n",
      "        policy_loss: -0.035923493259275956\n",
      "        total_loss: 62.7191101861\n",
      "        vf_explained_var: -0.10056846410036087\n",
      "        vf_loss: 62.74575707912445\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6164610306421916\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01569812241710679\n",
      "        policy_loss: -0.07850871281077465\n",
      "        total_loss: 38.976557851632435\n",
      "        vf_explained_var: -0.048129311005274455\n",
      "        vf_loss: 39.04447029193243\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5927348001301289\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012357456395523816\n",
      "        policy_loss: -0.12221831029901901\n",
      "        total_loss: 25.82469105442365\n",
      "        vf_explained_var: 0.1844619177778562\n",
      "        vf_loss: 25.93856800119082\n",
      "  num_agent_steps_sampled: 1220910\n",
      "  num_agent_steps_trained: 1220910\n",
      "  num_steps_sampled: 1220940\n",
      "  num_steps_trained: 1220940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 153\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.833333333333334\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.0\n",
      "  player_1: 45.666666666666664\n",
      "  player_2: 56.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: -1.176666666666667\n",
      "  player_1: -1.3466666666666662\n",
      "  player_2: 5.523333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -55.333333333333336\n",
      "  player_1: -57.33333333333333\n",
      "  player_2: -60.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09087562880820893\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.34450394118055255\n",
      "  mean_inference_ms: 1.7009933279174376\n",
      "  mean_raw_obs_processing_ms: 0.21875400334061473\n",
      "time_since_restore: 2453.2800459861755\n",
      "time_this_iter_s: 14.517949104309082\n",
      "time_total_s: 2453.2800459861755\n",
      "timers:\n",
      "  learn_throughput: 558.069\n",
      "  learn_time_ms: 14299.303\n",
      "  load_throughput: 748110.664\n",
      "  load_time_ms: 10.667\n",
      "  sample_throughput: 512.208\n",
      "  sample_time_ms: 15579.617\n",
      "  update_time_ms: 5.48\n",
      "timestamp: 1643538675\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1220940\n",
      "training_iteration: 153\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1236872\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-31-45\n",
      "done: false\n",
      "episode_len_mean: 341.6\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 4883\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6356659056246281\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0148853329213307\n",
      "        policy_loss: -0.09230659629528722\n",
      "        total_loss: 51.466380054155984\n",
      "        vf_explained_var: 0.03127012997865677\n",
      "        vf_loss: 51.54863926490148\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5644337825477124\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013604618144425255\n",
      "        policy_loss: -0.03748633055947721\n",
      "        total_loss: 49.63773585836093\n",
      "        vf_explained_var: -0.06125916361808777\n",
      "        vf_loss: 49.66603935837745\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6012535706162453\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015099487381485232\n",
      "        policy_loss: -0.05853295991507669\n",
      "        total_loss: 47.26730815688769\n",
      "        vf_explained_var: 0.07477439612150193\n",
      "        vf_loss: 47.31564891934395\n",
      "  num_agent_steps_sampled: 1236872\n",
      "  num_agent_steps_trained: 1236872\n",
      "  num_steps_sampled: 1236900\n",
      "  num_steps_trained: 1236900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 155\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.06842105263158\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.0\n",
      "  player_1: 45.666666666666664\n",
      "  player_2: 56.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: -2.4833333333333325\n",
      "  player_1: -1.4233333333333333\n",
      "  player_2: 6.906666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -55.333333333333336\n",
      "  player_1: -57.33333333333333\n",
      "  player_2: -26.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09072348675426982\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3440276796498292\n",
      "  mean_inference_ms: 1.699143456961051\n",
      "  mean_raw_obs_processing_ms: 0.21891298727271621\n",
      "time_since_restore: 2483.0535230636597\n",
      "time_this_iter_s: 14.892523765563965\n",
      "time_total_s: 2483.0535230636597\n",
      "timers:\n",
      "  learn_throughput: 561.861\n",
      "  learn_time_ms: 14202.788\n",
      "  load_throughput: 842842.651\n",
      "  load_time_ms: 9.468\n",
      "  sample_throughput: 512.736\n",
      "  sample_time_ms: 15563.571\n",
      "  update_time_ms: 5.479\n",
      "timestamp: 1643538705\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1236900\n",
      "training_iteration: 155\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1252830\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-32-16\n",
      "done: false\n",
      "episode_len_mean: 320.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 28\n",
      "episodes_total: 4934\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.603627295345068\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014014685460160858\n",
      "        policy_loss: -0.07874436569477741\n",
      "        total_loss: 72.2301745804151\n",
      "        vf_explained_var: 0.05906861792008082\n",
      "        vf_loss: 72.29945924043655\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6185463610291481\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013025438752508004\n",
      "        policy_loss: -0.06909736617468297\n",
      "        total_loss: 57.51474638183912\n",
      "        vf_explained_var: 0.1863193459312121\n",
      "        vf_loss: 57.575051491657895\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5595772775510947\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01306762657513848\n",
      "        policy_loss: -0.08012854894002279\n",
      "        total_loss: 70.26637997388839\n",
      "        vf_explained_var: 0.33729136566321055\n",
      "        vf_loss: 70.3376877784729\n",
      "  num_agent_steps_sampled: 1252830\n",
      "  num_agent_steps_trained: 1252830\n",
      "  num_steps_sampled: 1252860\n",
      "  num_steps_trained: 1252860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 157\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.468421052631578\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 41.66666666666667\n",
      "  player_2: 37.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -3.1466666666666665\n",
      "  player_1: -0.9666666666666666\n",
      "  player_2: 7.113333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -43.0\n",
      "  player_1: -38.66666666666667\n",
      "  player_2: -48.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09094180598235667\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3438938003562926\n",
      "  mean_inference_ms: 1.6998677313138009\n",
      "  mean_raw_obs_processing_ms: 0.21923704213679898\n",
      "time_since_restore: 2513.865410089493\n",
      "time_this_iter_s: 14.930346250534058\n",
      "time_total_s: 2513.865410089493\n",
      "timers:\n",
      "  learn_throughput: 570.382\n",
      "  learn_time_ms: 13990.617\n",
      "  load_throughput: 1056133.042\n",
      "  load_time_ms: 7.556\n",
      "  sample_throughput: 519.349\n",
      "  sample_time_ms: 15365.401\n",
      "  update_time_ms: 5.448\n",
      "timestamp: 1643538736\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1252860\n",
      "training_iteration: 157\n",
      "trial_id: default\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0130 11:32:18.527514652   23208 backup_poller.cc:134]       Run client channel backup poller: {\"created\":\"@1643538738.527488132\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1643538738.527483974\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 1268790\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-32-48\n",
      "done: false\n",
      "episode_len_mean: 347.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 4980\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6129827000200748\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012555647802309977\n",
      "        policy_loss: -0.07834788932775458\n",
      "        total_loss: 56.91895630200704\n",
      "        vf_explained_var: -0.06355840027332306\n",
      "        vf_loss: 56.98882891178131\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6564957984785239\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014668428881832986\n",
      "        policy_loss: -0.015345861315727233\n",
      "        total_loss: 28.44615055322647\n",
      "        vf_explained_var: 0.2537480938434601\n",
      "        vf_loss: 28.451595332622528\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5703003366291522\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011126182630890375\n",
      "        policy_loss: -0.08739695895773669\n",
      "        total_loss: 44.64607448259989\n",
      "        vf_explained_var: -0.04999742994705836\n",
      "        vf_loss: 44.725961276690164\n",
      "  num_agent_steps_sampled: 1268790\n",
      "  num_agent_steps_trained: 1268790\n",
      "  num_steps_sampled: 1268820\n",
      "  num_steps_trained: 1268820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 159\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.165000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.23000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 40.0\n",
      "  player_1: 41.66666666666667\n",
      "  player_2: 39.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: -2.186666666666667\n",
      "  player_1: 0.17333333333333364\n",
      "  player_2: 5.013333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -74.33333333333333\n",
      "  player_1: -46.0\n",
      "  player_2: -48.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09089917858301826\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3428163603059728\n",
      "  mean_inference_ms: 1.6971429606537078\n",
      "  mean_raw_obs_processing_ms: 0.21863458964912677\n",
      "time_since_restore: 2545.97239613533\n",
      "time_this_iter_s: 16.627732038497925\n",
      "time_total_s: 2545.97239613533\n",
      "timers:\n",
      "  learn_throughput: 568.292\n",
      "  learn_time_ms: 14042.079\n",
      "  load_throughput: 1112421.76\n",
      "  load_time_ms: 7.174\n",
      "  sample_throughput: 523.836\n",
      "  sample_time_ms: 15233.769\n",
      "  update_time_ms: 5.44\n",
      "timestamp: 1643538768\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1268820\n",
      "training_iteration: 159\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1284751\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-33-20\n",
      "done: false\n",
      "episode_len_mean: 326.42\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 5036\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6872963258624076\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014839175180404708\n",
      "        policy_loss: -0.056325725304583706\n",
      "        total_loss: 80.79369758923849\n",
      "        vf_explained_var: 0.26007148712873457\n",
      "        vf_loss: 80.84000679810842\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6007210291922093\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013921748102544977\n",
      "        policy_loss: -0.11251142927755912\n",
      "        total_loss: 37.529667705694834\n",
      "        vf_explained_var: 0.08121205528577169\n",
      "        vf_loss: 37.63278198162715\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5454175753891468\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01432929121846082\n",
      "        policy_loss: -0.07297828525925676\n",
      "        total_loss: 61.62631318092346\n",
      "        vf_explained_var: 0.27210171381632486\n",
      "        vf_loss: 61.68961914698283\n",
      "  num_agent_steps_sampled: 1284751\n",
      "  num_agent_steps_trained: 1284751\n",
      "  num_steps_sampled: 1284780\n",
      "  num_steps_trained: 1284780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 161\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.223809523809523\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.20000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 40.0\n",
      "  player_1: 37.66666666666667\n",
      "  player_2: 39.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 1.7733333333333334\n",
      "  player_1: -1.3166666666666669\n",
      "  player_2: 2.543333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -74.33333333333333\n",
      "  player_1: -57.666666666666664\n",
      "  player_2: -38.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0905487503951619\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.34177243660457735\n",
      "  mean_inference_ms: 1.6948765828487935\n",
      "  mean_raw_obs_processing_ms: 0.21787323015754256\n",
      "time_since_restore: 2578.355893135071\n",
      "time_this_iter_s: 17.25729489326477\n",
      "time_total_s: 2578.355893135071\n",
      "timers:\n",
      "  learn_throughput: 562.582\n",
      "  learn_time_ms: 14184.596\n",
      "  load_throughput: 1123681.732\n",
      "  load_time_ms: 7.102\n",
      "  sample_throughput: 522.954\n",
      "  sample_time_ms: 15259.482\n",
      "  update_time_ms: 5.381\n",
      "timestamp: 1643538800\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1284780\n",
      "training_iteration: 161\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1300710\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-33-51\n",
      "done: false\n",
      "episode_len_mean: 307.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 5078\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5927422226468722\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014458720137976779\n",
      "        policy_loss: -0.10412325067988908\n",
      "        total_loss: 48.57421142657598\n",
      "        vf_explained_var: -0.06263550817966461\n",
      "        vf_loss: 48.66857508897781\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.58072194998463\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012760891116339981\n",
      "        policy_loss: -0.04975778684796144\n",
      "        total_loss: 50.70988311568896\n",
      "        vf_explained_var: 0.018175910909970602\n",
      "        vf_loss: 50.75102718432744\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5580458168685436\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013790478531795998\n",
      "        policy_loss: -0.05706093891523779\n",
      "        total_loss: 40.410107711950936\n",
      "        vf_explained_var: -0.2111807812253634\n",
      "        vf_loss: 40.457860100269315\n",
      "  num_agent_steps_sampled: 1300710\n",
      "  num_agent_steps_trained: 1300710\n",
      "  num_steps_sampled: 1300740\n",
      "  num_steps_trained: 1300740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 163\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.542105263157895\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 42.0\n",
      "  player_1: 33.666666666666664\n",
      "  player_2: 37.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 4.64\n",
      "  player_1: -2.78\n",
      "  player_2: 1.1400000000000008\n",
      "policy_reward_min:\n",
      "  player_0: -46.0\n",
      "  player_1: -57.666666666666664\n",
      "  player_2: -39.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09052710943058981\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3413976921055844\n",
      "  mean_inference_ms: 1.693784280334423\n",
      "  mean_raw_obs_processing_ms: 0.21788196209183283\n",
      "time_since_restore: 2608.6450247764587\n",
      "time_this_iter_s: 15.623697519302368\n",
      "time_total_s: 2608.6450247764587\n",
      "timers:\n",
      "  learn_throughput: 558.479\n",
      "  learn_time_ms: 14288.817\n",
      "  load_throughput: 1066358.667\n",
      "  load_time_ms: 7.483\n",
      "  sample_throughput: 516.051\n",
      "  sample_time_ms: 15463.577\n",
      "  update_time_ms: 5.415\n",
      "timestamp: 1643538831\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1300740\n",
      "training_iteration: 163\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1316670\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-34-20\n",
      "done: false\n",
      "episode_len_mean: 311.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 5135\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6441246571640173\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013474821197790638\n",
      "        policy_loss: -0.00785080900726219\n",
      "        total_loss: 40.02523042678833\n",
      "        vf_explained_var: 0.1236905018488566\n",
      "        vf_loss: 40.02398543516795\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6376102169354757\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016748219852241467\n",
      "        policy_loss: -0.09352287538660069\n",
      "        total_loss: 21.77219963312149\n",
      "        vf_explained_var: 0.01514614870150884\n",
      "        vf_loss: 21.854417443672816\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6037610896428426\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013921756432286733\n",
      "        policy_loss: -0.11733263060450554\n",
      "        total_loss: 26.6924156888326\n",
      "        vf_explained_var: 0.0595205486814181\n",
      "        vf_loss: 26.80035108009974\n",
      "  num_agent_steps_sampled: 1316670\n",
      "  num_agent_steps_trained: 1316670\n",
      "  num_steps_sampled: 1316700\n",
      "  num_steps_trained: 1316700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 165\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.32777777777778\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 42.0\n",
      "  player_1: 33.666666666666664\n",
      "  player_2: 45.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 3.9566666666666674\n",
      "  player_1: -2.6733333333333325\n",
      "  player_2: 1.7166666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -46.333333333333336\n",
      "  player_1: -65.33333333333334\n",
      "  player_2: -39.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09046720488029016\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3402380683554749\n",
      "  mean_inference_ms: 1.6896016436276435\n",
      "  mean_raw_obs_processing_ms: 0.2175893273114479\n",
      "time_since_restore: 2637.918608903885\n",
      "time_this_iter_s: 14.563674449920654\n",
      "time_total_s: 2637.918608903885\n",
      "timers:\n",
      "  learn_throughput: 560.501\n",
      "  learn_time_ms: 14237.26\n",
      "  load_throughput: 1057157.122\n",
      "  load_time_ms: 7.549\n",
      "  sample_throughput: 512.988\n",
      "  sample_time_ms: 15555.914\n",
      "  update_time_ms: 5.407\n",
      "timestamp: 1643538860\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1316700\n",
      "training_iteration: 165\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1332630\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-34-51\n",
      "done: false\n",
      "episode_len_mean: 299.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 5184\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6169731727739175\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015096666843846075\n",
      "        policy_loss: -0.05981494785752148\n",
      "        total_loss: 95.95036607344946\n",
      "        vf_explained_var: 0.2252482916911443\n",
      "        vf_loss: 95.99999048074086\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5858680313328902\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013650876758269608\n",
      "        policy_loss: -0.06090285539006193\n",
      "        total_loss: 53.76312138795853\n",
      "        vf_explained_var: 0.24052431911230088\n",
      "        vf_loss: 53.81481010754903\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5830668764313062\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0135609905236682\n",
      "        policy_loss: -0.06960649096096555\n",
      "        total_loss: 35.7236257092158\n",
      "        vf_explained_var: 0.16302119056383768\n",
      "        vf_loss: 35.7840788714091\n",
      "  num_agent_steps_sampled: 1332630\n",
      "  num_agent_steps_trained: 1332630\n",
      "  num_steps_sampled: 1332660\n",
      "  num_steps_trained: 1332660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 167\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.02222222222222\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.0\n",
      "  player_1: 31.666666666666664\n",
      "  player_2: 45.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -1.0466666666666669\n",
      "  player_1: -2.8366666666666664\n",
      "  player_2: 6.883333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -55.333333333333336\n",
      "  player_1: -65.33333333333334\n",
      "  player_2: -32.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09055072543612418\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.34013684559670027\n",
      "  mean_inference_ms: 1.691831216990156\n",
      "  mean_raw_obs_processing_ms: 0.21760594860602758\n",
      "time_since_restore: 2668.5281944274902\n",
      "time_this_iter_s: 14.75745940208435\n",
      "time_total_s: 2668.5281944274902\n",
      "timers:\n",
      "  learn_throughput: 561.298\n",
      "  learn_time_ms: 14217.046\n",
      "  load_throughput: 1046199.763\n",
      "  load_time_ms: 7.628\n",
      "  sample_throughput: 514.275\n",
      "  sample_time_ms: 15516.984\n",
      "  update_time_ms: 5.409\n",
      "timestamp: 1643538891\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1332660\n",
      "training_iteration: 167\n",
      "trial_id: default\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0130 11:35:08.578887358   24708 backup_poller.cc:134]       Run client channel backup poller: {\"created\":\"@1643538908.578848133\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1643538908.578841791\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 1348590\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-35-23\n",
      "done: false\n",
      "episode_len_mean: 294.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 5237\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5974879901607831\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013940426570018852\n",
      "        policy_loss: -0.079871337255463\n",
      "        total_loss: 65.61883145968119\n",
      "        vf_explained_var: -0.10905184467633565\n",
      "        vf_loss: 65.68929313500722\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5905111984411875\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014372415053646061\n",
      "        policy_loss: -0.0739048929264148\n",
      "        total_loss: 24.501951668659846\n",
      "        vf_explained_var: -0.34945795039335886\n",
      "        vf_loss: 24.566155205965043\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5818925312658151\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014108470724281308\n",
      "        policy_loss: -0.058286125734448434\n",
      "        total_loss: 49.00068770647049\n",
      "        vf_explained_var: 0.11596851001183192\n",
      "        vf_loss: 49.04945060968399\n",
      "  num_agent_steps_sampled: 1348590\n",
      "  num_agent_steps_trained: 1348590\n",
      "  num_steps_sampled: 1348620\n",
      "  num_steps_trained: 1348620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 169\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.264999999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.36000000000001\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.0\n",
      "  player_1: 30.666666666666664\n",
      "  player_2: 48.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: -1.9166666666666663\n",
      "  player_1: -1.6666666666666665\n",
      "  player_2: 6.583333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -55.333333333333336\n",
      "  player_1: -56.0\n",
      "  player_2: -30.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0905771342094657\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.34000229885225997\n",
      "  mean_inference_ms: 1.6921712102688178\n",
      "  mean_raw_obs_processing_ms: 0.2176184945057806\n",
      "time_since_restore: 2700.654330253601\n",
      "time_this_iter_s: 15.653383255004883\n",
      "time_total_s: 2700.654330253601\n",
      "timers:\n",
      "  learn_throughput: 561.358\n",
      "  learn_time_ms: 14215.537\n",
      "  load_throughput: 1049575.596\n",
      "  load_time_ms: 7.603\n",
      "  sample_throughput: 511.785\n",
      "  sample_time_ms: 15592.487\n",
      "  update_time_ms: 6.282\n",
      "timestamp: 1643538923\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1348620\n",
      "training_iteration: 169\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1364550\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-35-54\n",
      "done: false\n",
      "episode_len_mean: 284.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 15\n",
      "episodes_total: 5285\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5653093033532302\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015407332766489219\n",
      "        policy_loss: -0.08186128078959883\n",
      "        total_loss: 40.55595096111298\n",
      "        vf_explained_var: -0.06157793750365575\n",
      "        vf_loss: 40.62741212924322\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5902654487888018\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01463789443783753\n",
      "        policy_loss: -0.07653737867561479\n",
      "        total_loss: 18.91359380165736\n",
      "        vf_explained_var: -0.01638267586628596\n",
      "        vf_loss: 18.98025064865748\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5474713149666787\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013900083573417609\n",
      "        policy_loss: -0.07243224026014407\n",
      "        total_loss: 33.0593842458725\n",
      "        vf_explained_var: 0.10593641916910808\n",
      "        vf_loss: 33.12243400812149\n",
      "  num_agent_steps_sampled: 1364550\n",
      "  num_agent_steps_trained: 1364550\n",
      "  num_steps_sampled: 1364580\n",
      "  num_steps_trained: 1364580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 171\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.927777777777777\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.3\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 28.66666666666667\n",
      "  player_2: 48.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: -1.603333333333333\n",
      "  player_1: -1.183333333333333\n",
      "  player_2: 5.786666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -74.33333333333333\n",
      "  player_1: -46.33333333333333\n",
      "  player_2: -42.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09026075605253288\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.33826322659508823\n",
      "  mean_inference_ms: 1.6849591005532856\n",
      "  mean_raw_obs_processing_ms: 0.21701132059642095\n",
      "time_since_restore: 2731.924163579941\n",
      "time_this_iter_s: 14.832098484039307\n",
      "time_total_s: 2731.924163579941\n",
      "timers:\n",
      "  learn_throughput: 565.669\n",
      "  learn_time_ms: 14107.197\n",
      "  load_throughput: 992634.573\n",
      "  load_time_ms: 8.039\n",
      "  sample_throughput: 510.581\n",
      "  sample_time_ms: 15629.246\n",
      "  update_time_ms: 6.286\n",
      "timestamp: 1643538954\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1364580\n",
      "training_iteration: 171\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1380510\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-36-25\n",
      "done: false\n",
      "episode_len_mean: 333.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 5335\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6062451387445132\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014232589208641002\n",
      "        policy_loss: -0.07164071603988607\n",
      "        total_loss: 103.86750129461288\n",
      "        vf_explained_var: 0.0973968838651975\n",
      "        vf_loss: 103.92953452587128\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.613177984158198\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015639273704825124\n",
      "        policy_loss: -0.07894597010066112\n",
      "        total_loss: 43.75010165850321\n",
      "        vf_explained_var: 0.08789839853843052\n",
      "        vf_loss: 43.81849109808604\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5606060644984245\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01611027224906138\n",
      "        policy_loss: -0.08707652900988858\n",
      "        total_loss: 57.71359397093455\n",
      "        vf_explained_var: 0.20170916577180226\n",
      "        vf_loss: 57.789796011447905\n",
      "  num_agent_steps_sampled: 1380510\n",
      "  num_agent_steps_trained: 1380510\n",
      "  num_steps_sampled: 1380540\n",
      "  num_steps_trained: 1380540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 173\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.950000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.3\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 28.66666666666667\n",
      "  player_2: 48.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: -1.6466666666666665\n",
      "  player_1: -1.9866666666666664\n",
      "  player_2: 6.633333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -74.33333333333333\n",
      "  player_1: -34.0\n",
      "  player_2: -42.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0903019281241215\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.33770606145066506\n",
      "  mean_inference_ms: 1.6834931839435145\n",
      "  mean_raw_obs_processing_ms: 0.21673376310215495\n",
      "time_since_restore: 2763.284622192383\n",
      "time_this_iter_s: 14.723682880401611\n",
      "time_total_s: 2763.284622192383\n",
      "timers:\n",
      "  learn_throughput: 561.219\n",
      "  learn_time_ms: 14219.046\n",
      "  load_throughput: 1002646.498\n",
      "  load_time_ms: 7.959\n",
      "  sample_throughput: 512.199\n",
      "  sample_time_ms: 15579.872\n",
      "  update_time_ms: 6.34\n",
      "timestamp: 1643538985\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1380540\n",
      "training_iteration: 173\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1396470\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-36-55\n",
      "done: false\n",
      "episode_len_mean: 374.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 28\n",
      "episodes_total: 5382\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5865814066926638\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015110261810967434\n",
      "        policy_loss: -0.07191620232226947\n",
      "        total_loss: 55.55223378499349\n",
      "        vf_explained_var: 0.15773039301236472\n",
      "        vf_loss: 55.61395052909851\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6108069895207882\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01487904855892566\n",
      "        policy_loss: -0.060116242313136656\n",
      "        total_loss: 24.03325248479843\n",
      "        vf_explained_var: 0.0781054378549258\n",
      "        vf_loss: 24.083325364987054\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5391170265773932\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013568103163799397\n",
      "        policy_loss: -0.0693519346943746\n",
      "        total_loss: 44.81550483465195\n",
      "        vf_explained_var: -0.10875457088152568\n",
      "        vf_loss: 44.8756978627046\n",
      "  num_agent_steps_sampled: 1396470\n",
      "  num_agent_steps_trained: 1396470\n",
      "  num_steps_sampled: 1396500\n",
      "  num_steps_trained: 1396500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 175\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.855555555555554\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.3\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 43.66666666666667\n",
      "  player_1: 29.0\n",
      "  player_2: 45.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.6099999999999997\n",
      "  player_1: -3.8799999999999994\n",
      "  player_2: 5.27\n",
      "policy_reward_min:\n",
      "  player_0: -62.0\n",
      "  player_1: -58.33333333333333\n",
      "  player_2: -35.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09024137080706897\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3378464387175207\n",
      "  mean_inference_ms: 1.6854198465381967\n",
      "  mean_raw_obs_processing_ms: 0.21666260996157063\n",
      "time_since_restore: 2792.7144668102264\n",
      "time_this_iter_s: 14.867162942886353\n",
      "time_total_s: 2792.7144668102264\n",
      "timers:\n",
      "  learn_throughput: 560.411\n",
      "  learn_time_ms: 14239.56\n",
      "  load_throughput: 1015496.026\n",
      "  load_time_ms: 7.858\n",
      "  sample_throughput: 515.608\n",
      "  sample_time_ms: 15476.876\n",
      "  update_time_ms: 6.438\n",
      "timestamp: 1643539015\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1396500\n",
      "training_iteration: 175\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1412431\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-37-26\n",
      "done: false\n",
      "episode_len_mean: 330.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 5434\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6217299607892831\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012286456954765015\n",
      "        policy_loss: -0.05291350894141942\n",
      "        total_loss: 85.33277509212493\n",
      "        vf_explained_var: -0.04276384323835373\n",
      "        vf_loss: 85.37739466190338\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5912694192429384\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013191260481089557\n",
      "        policy_loss: -0.022478197480862338\n",
      "        total_loss: 30.671714736620586\n",
      "        vf_explained_var: -0.02216550290584564\n",
      "        vf_loss: 30.68528885523478\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5535956992208958\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011725311498776137\n",
      "        policy_loss: -0.12225004621315748\n",
      "        total_loss: 46.909840508302054\n",
      "        vf_explained_var: 0.07688845932483673\n",
      "        vf_loss: 47.02417596499125\n",
      "  num_agent_steps_sampled: 1412431\n",
      "  num_agent_steps_trained: 1412431\n",
      "  num_steps_sampled: 1412460\n",
      "  num_steps_trained: 1412460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 177\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.909999999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.29999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 43.66666666666667\n",
      "  player_1: 39.333333333333336\n",
      "  player_2: 45.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.2\n",
      "  player_1: -4.78\n",
      "  player_2: 5.58\n",
      "policy_reward_min:\n",
      "  player_0: -61.66666666666667\n",
      "  player_1: -71.33333333333334\n",
      "  player_2: -35.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09003144302155926\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3368446974309491\n",
      "  mean_inference_ms: 1.682354577524317\n",
      "  mean_raw_obs_processing_ms: 0.2163195579629916\n",
      "time_since_restore: 2823.4899139404297\n",
      "time_this_iter_s: 16.02698516845703\n",
      "time_total_s: 2823.4899139404297\n",
      "timers:\n",
      "  learn_throughput: 559.693\n",
      "  learn_time_ms: 14257.816\n",
      "  load_throughput: 993341.601\n",
      "  load_time_ms: 8.033\n",
      "  sample_throughput: 518.206\n",
      "  sample_time_ms: 15399.295\n",
      "  update_time_ms: 6.414\n",
      "timestamp: 1643539046\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1412460\n",
      "training_iteration: 177\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1428392\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-37-56\n",
      "done: false\n",
      "episode_len_mean: 298.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 5480\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5577487431466579\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013097917724893762\n",
      "        policy_loss: -0.06258417741085093\n",
      "        total_loss: 55.05203649520874\n",
      "        vf_explained_var: 0.043281769454479216\n",
      "        vf_loss: 55.10577970186869\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6087243557969729\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01468669628222481\n",
      "        policy_loss: -0.06587686127983033\n",
      "        total_loss: 37.22581414182981\n",
      "        vf_explained_var: -0.22898547957340876\n",
      "        vf_loss: 37.281777387460075\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.493123517036438\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01358703177736667\n",
      "        policy_loss: -0.0737010966675977\n",
      "        total_loss: 28.21455104748408\n",
      "        vf_explained_var: 0.2855050115784009\n",
      "        vf_loss: 28.279081016381582\n",
      "  num_agent_steps_sampled: 1428392\n",
      "  num_agent_steps_trained: 1428392\n",
      "  num_steps_sampled: 1428420\n",
      "  num_steps_trained: 1428420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 179\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.163157894736841\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.3\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.666666666666664\n",
      "  player_1: 39.333333333333336\n",
      "  player_2: 46.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -2.8533333333333326\n",
      "  player_1: -3.9733333333333327\n",
      "  player_2: 9.826666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -61.66666666666667\n",
      "  player_1: -71.33333333333334\n",
      "  player_2: -33.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09012342931734757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.33590689726489253\n",
      "  mean_inference_ms: 1.680311693444859\n",
      "  mean_raw_obs_processing_ms: 0.2161940966406655\n",
      "time_since_restore: 2853.686013698578\n",
      "time_this_iter_s: 15.260281562805176\n",
      "time_total_s: 2853.686013698578\n",
      "timers:\n",
      "  learn_throughput: 567.302\n",
      "  learn_time_ms: 14066.588\n",
      "  load_throughput: 937248.391\n",
      "  load_time_ms: 8.514\n",
      "  sample_throughput: 519.068\n",
      "  sample_time_ms: 15373.713\n",
      "  update_time_ms: 5.64\n",
      "timestamp: 1643539076\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1428420\n",
      "training_iteration: 179\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1444350\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-38-29\n",
      "done: false\n",
      "episode_len_mean: 310.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 5531\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6087440524498622\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016459601757998713\n",
      "        policy_loss: -0.08371426718619962\n",
      "        total_loss: 56.208178025086724\n",
      "        vf_explained_var: -0.00947982281446457\n",
      "        vf_loss: 56.28078208605449\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.606952731013298\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01418895222112127\n",
      "        policy_loss: -0.0626132019655779\n",
      "        total_loss: 33.596259429454804\n",
      "        vf_explained_var: 0.10497553000847498\n",
      "        vf_loss: 33.6492951075236\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5439083328346411\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013553611077183614\n",
      "        policy_loss: -0.08563157318159938\n",
      "        total_loss: 35.20580167531967\n",
      "        vf_explained_var: 0.15874037484327952\n",
      "        vf_loss: 35.282284381389616\n",
      "  num_agent_steps_sampled: 1444350\n",
      "  num_agent_steps_trained: 1444350\n",
      "  num_steps_sampled: 1444380\n",
      "  num_steps_trained: 1444380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 181\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.489473684210527\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.3\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.333333333333336\n",
      "  player_1: 30.333333333333336\n",
      "  player_2: 46.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -1.3499999999999996\n",
      "  player_1: -4.210000000000001\n",
      "  player_2: 8.56\n",
      "policy_reward_min:\n",
      "  player_0: -62.333333333333336\n",
      "  player_1: -48.33333333333333\n",
      "  player_2: -43.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09010351489530244\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3355601862639391\n",
      "  mean_inference_ms: 1.679933577772902\n",
      "  mean_raw_obs_processing_ms: 0.21626128465484348\n",
      "time_since_restore: 2886.473544359207\n",
      "time_this_iter_s: 14.814900159835815\n",
      "time_total_s: 2886.473544359207\n",
      "timers:\n",
      "  learn_throughput: 561.419\n",
      "  learn_time_ms: 14213.977\n",
      "  load_throughput: 915398.683\n",
      "  load_time_ms: 8.718\n",
      "  sample_throughput: 515.321\n",
      "  sample_time_ms: 15485.491\n",
      "  update_time_ms: 5.745\n",
      "timestamp: 1643539109\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1444380\n",
      "training_iteration: 181\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1460310\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-39-00\n",
      "done: false\n",
      "episode_len_mean: 311.54\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 5587\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6309940503537654\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014661967641995944\n",
      "        policy_loss: -0.0730388544096301\n",
      "        total_loss: 59.96375179211299\n",
      "        vf_explained_var: 0.13486260384321214\n",
      "        vf_loss: 60.02689404646556\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5949824223915736\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013430249052865596\n",
      "        policy_loss: -0.06548457120545209\n",
      "        total_loss: 55.644160535335544\n",
      "        vf_explained_var: 0.2416178059577942\n",
      "        vf_loss: 55.70057985385259\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5516795106728871\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015984244872646134\n",
      "        policy_loss: -0.08991965535096824\n",
      "        total_loss: 89.3380438176791\n",
      "        vf_explained_var: 0.18157392114400864\n",
      "        vf_loss: 89.41717421213785\n",
      "  num_agent_steps_sampled: 1460310\n",
      "  num_agent_steps_trained: 1460310\n",
      "  num_steps_sampled: 1460340\n",
      "  num_steps_trained: 1460340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 183\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.452631578947367\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.3\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 44.333333333333336\n",
      "  player_1: 37.666666666666664\n",
      "  player_2: 41.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 1.4866666666666666\n",
      "  player_1: -2.5933333333333337\n",
      "  player_2: 4.1066666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -62.333333333333336\n",
      "  player_1: -50.333333333333336\n",
      "  player_2: -59.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08999965344858742\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.33576702805284653\n",
      "  mean_inference_ms: 1.6823524233475096\n",
      "  mean_raw_obs_processing_ms: 0.21619688318673105\n",
      "time_since_restore: 2917.676623106003\n",
      "time_this_iter_s: 15.780995845794678\n",
      "time_total_s: 2917.676623106003\n",
      "timers:\n",
      "  learn_throughput: 562.673\n",
      "  learn_time_ms: 14182.312\n",
      "  load_throughput: 951216.381\n",
      "  load_time_ms: 8.389\n",
      "  sample_throughput: 519.446\n",
      "  sample_time_ms: 15362.518\n",
      "  update_time_ms: 5.72\n",
      "timestamp: 1643539140\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1460340\n",
      "training_iteration: 183\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1476270\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-39-32\n",
      "done: false\n",
      "episode_len_mean: 315.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 5639\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.605173062980175\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015552256053792728\n",
      "        policy_loss: -0.06808723551842073\n",
      "        total_loss: 45.849304059346515\n",
      "        vf_explained_var: 0.32449139455954235\n",
      "        vf_loss: 45.906893559296925\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5894702019294104\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014744663421638127\n",
      "        policy_loss: -0.06274278402328491\n",
      "        total_loss: 22.99936239997546\n",
      "        vf_explained_var: 0.31262893239657086\n",
      "        vf_loss: 23.052152512073516\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5596886477867762\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015932002559699565\n",
      "        policy_loss: -0.10807541049551218\n",
      "        total_loss: 27.15353330373764\n",
      "        vf_explained_var: 0.1616799169778824\n",
      "        vf_loss: 27.250854587554933\n",
      "  num_agent_steps_sampled: 1476270\n",
      "  num_agent_steps_trained: 1476270\n",
      "  num_steps_sampled: 1476300\n",
      "  num_steps_trained: 1476300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 185\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.115789473684213\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.37368421052633\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 44.333333333333336\n",
      "  player_1: 37.666666666666664\n",
      "  player_2: 35.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 1.2299999999999998\n",
      "  player_1: -0.75\n",
      "  player_2: 2.5199999999999996\n",
      "policy_reward_min:\n",
      "  player_0: -56.66666666666667\n",
      "  player_1: -50.333333333333336\n",
      "  player_2: -59.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.09004770580916147\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3352105917314262\n",
      "  mean_inference_ms: 1.6808656188493019\n",
      "  mean_raw_obs_processing_ms: 0.21602349535614263\n",
      "time_since_restore: 2949.2464485168457\n",
      "time_this_iter_s: 15.287645816802979\n",
      "time_total_s: 2949.2464485168457\n",
      "timers:\n",
      "  learn_throughput: 554.423\n",
      "  learn_time_ms: 14393.334\n",
      "  load_throughput: 871057.169\n",
      "  load_time_ms: 9.161\n",
      "  sample_throughput: 510.126\n",
      "  sample_time_ms: 15643.197\n",
      "  update_time_ms: 5.718\n",
      "timestamp: 1643539172\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1476300\n",
      "training_iteration: 185\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1492232\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-40-03\n",
      "done: false\n",
      "episode_len_mean: 296.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 30\n",
      "episodes_total: 5699\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6030973514417807\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013248437661356055\n",
      "        policy_loss: -0.07063616111253698\n",
      "        total_loss: 74.061767745018\n",
      "        vf_explained_var: 0.15598060101270675\n",
      "        vf_loss: 74.12346114079158\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5929439794023832\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013353132471060576\n",
      "        policy_loss: -0.08176083934881415\n",
      "        total_loss: 55.13317593018214\n",
      "        vf_explained_var: 0.11139082829157511\n",
      "        vf_loss: 55.20592333873113\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5543156918386618\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014395653908690065\n",
      "        policy_loss: -0.04628467622213066\n",
      "        total_loss: 40.87196295897166\n",
      "        vf_explained_var: 0.15265183250109354\n",
      "        vf_loss: 40.90853069146474\n",
      "  num_agent_steps_sampled: 1492232\n",
      "  num_agent_steps_trained: 1492232\n",
      "  num_steps_sampled: 1492260\n",
      "  num_steps_trained: 1492260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 187\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.28421052631579\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.40000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.333333333333336\n",
      "  player_1: 41.666666666666664\n",
      "  player_2: 43.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.7900000000000001\n",
      "  player_1: 0.5000000000000003\n",
      "  player_2: 3.29\n",
      "policy_reward_min:\n",
      "  player_0: -64.0\n",
      "  player_1: -45.0\n",
      "  player_2: -61.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08994366373583439\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3341026714367027\n",
      "  mean_inference_ms: 1.6763696363487168\n",
      "  mean_raw_obs_processing_ms: 0.21563827610222616\n",
      "time_since_restore: 2980.0049798488617\n",
      "time_this_iter_s: 15.785263299942017\n",
      "time_total_s: 2980.0049798488617\n",
      "timers:\n",
      "  learn_throughput: 554.462\n",
      "  learn_time_ms: 14392.322\n",
      "  load_throughput: 875740.676\n",
      "  load_time_ms: 9.112\n",
      "  sample_throughput: 508.066\n",
      "  sample_time_ms: 15706.615\n",
      "  update_time_ms: 5.834\n",
      "timestamp: 1643539203\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1492260\n",
      "training_iteration: 187\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1508190\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-40-33\n",
      "done: false\n",
      "episode_len_mean: 265.84\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 5759\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5755388281742732\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014601258145010263\n",
      "        policy_loss: -0.03327240002652009\n",
      "        total_loss: 51.67783643404643\n",
      "        vf_explained_var: 0.2352553207675616\n",
      "        vf_loss: 51.70125306447347\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.608179781784614\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01251556202418025\n",
      "        policy_loss: -0.11305998656898737\n",
      "        total_loss: 46.275115835666654\n",
      "        vf_explained_var: -0.044617775281270346\n",
      "        vf_loss: 46.37972793539365\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5650250094135603\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012590918002327573\n",
      "        policy_loss: -0.06636642993117373\n",
      "        total_loss: 78.04493280808131\n",
      "        vf_explained_var: 0.07592113624016444\n",
      "        vf_loss: 78.10280045668284\n",
      "  num_agent_steps_sampled: 1508190\n",
      "  num_agent_steps_trained: 1508190\n",
      "  num_steps_sampled: 1508220\n",
      "  num_steps_trained: 1508220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 189\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.336842105263157\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.3842105263158\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.33333333333333\n",
      "  player_1: 45.0\n",
      "  player_2: 38.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 0.543333333333333\n",
      "  player_1: -0.696666666666667\n",
      "  player_2: 3.1533333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -49.333333333333336\n",
      "  player_1: -53.666666666666664\n",
      "  player_2: -75.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08966074578357269\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.33305828130517356\n",
      "  mean_inference_ms: 1.67263519329448\n",
      "  mean_raw_obs_processing_ms: 0.21540695084273623\n",
      "time_since_restore: 3010.334682703018\n",
      "time_this_iter_s: 15.266937732696533\n",
      "time_total_s: 3010.334682703018\n",
      "timers:\n",
      "  learn_throughput: 553.906\n",
      "  learn_time_ms: 14406.773\n",
      "  load_throughput: 898861.494\n",
      "  load_time_ms: 8.878\n",
      "  sample_throughput: 508.526\n",
      "  sample_time_ms: 15692.406\n",
      "  update_time_ms: 5.865\n",
      "timestamp: 1643539233\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1508220\n",
      "training_iteration: 189\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1524150\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-41-06\n",
      "done: false\n",
      "episode_len_mean: 267.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 5814\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5728495443363985\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01613362365625998\n",
      "        policy_loss: -0.07367536097454529\n",
      "        total_loss: 97.834799229304\n",
      "        vf_explained_var: 0.4104379250605901\n",
      "        vf_loss: 97.89758476018906\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6079212027788162\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014200635782144673\n",
      "        policy_loss: -0.07087564705560605\n",
      "        total_loss: 54.318983249664306\n",
      "        vf_explained_var: 0.4156854971249898\n",
      "        vf_loss: 54.38027393579483\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5487140975395839\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013644672718234194\n",
      "        policy_loss: -0.07337631202302873\n",
      "        total_loss: 45.193699305852256\n",
      "        vf_explained_var: 0.28332219302654266\n",
      "        vf_loss: 45.257865492900216\n",
      "  num_agent_steps_sampled: 1524150\n",
      "  num_agent_steps_trained: 1524150\n",
      "  num_steps_sampled: 1524180\n",
      "  num_steps_trained: 1524180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 191\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.35263157894737\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.40000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.333333333333336\n",
      "  player_1: 43.33333333333333\n",
      "  player_2: 38.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -1.0233333333333332\n",
      "  player_1: 0.8666666666666668\n",
      "  player_2: 3.156666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -44.666666666666664\n",
      "  player_1: -60.666666666666664\n",
      "  player_2: -75.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08968185022205968\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3331804164846688\n",
      "  mean_inference_ms: 1.6751918816939915\n",
      "  mean_raw_obs_processing_ms: 0.21534315189706937\n",
      "time_since_restore: 3042.8841259479523\n",
      "time_this_iter_s: 15.680188655853271\n",
      "time_total_s: 3042.8841259479523\n",
      "timers:\n",
      "  learn_throughput: 554.522\n",
      "  learn_time_ms: 14390.772\n",
      "  load_throughput: 968686.663\n",
      "  load_time_ms: 8.238\n",
      "  sample_throughput: 511.884\n",
      "  sample_time_ms: 15589.474\n",
      "  update_time_ms: 6.116\n",
      "timestamp: 1643539266\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1524180\n",
      "training_iteration: 191\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1540111\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-41-38\n",
      "done: false\n",
      "episode_len_mean: 297.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 5864\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6329445301493009\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01625552395473221\n",
      "        policy_loss: -0.07826092979560295\n",
      "        total_loss: 47.29459752400716\n",
      "        vf_explained_var: 0.10183337211608887\n",
      "        vf_loss: 47.361885979970296\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5596559133132298\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011940062719962346\n",
      "        policy_loss: -0.06255997989947597\n",
      "        total_loss: 39.16426019986471\n",
      "        vf_explained_var: -0.17525338858366013\n",
      "        vf_loss: 39.21876047849655\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5819236385822296\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014955634388740388\n",
      "        policy_loss: -0.08423437772939603\n",
      "        total_loss: 40.008093903859454\n",
      "        vf_explained_var: 0.14389852245648702\n",
      "        vf_loss: 40.0822333017985\n",
      "  num_agent_steps_sampled: 1540111\n",
      "  num_agent_steps_trained: 1540111\n",
      "  num_steps_sampled: 1540140\n",
      "  num_steps_trained: 1540140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 193\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.820000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.40000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.333333333333336\n",
      "  player_1: 35.0\n",
      "  player_2: 40.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -2.6233333333333344\n",
      "  player_1: 0.8866666666666666\n",
      "  player_2: 4.736666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -50.0\n",
      "  player_1: -60.666666666666664\n",
      "  player_2: -45.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08973720817391583\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.33282641356355214\n",
      "  mean_inference_ms: 1.6748936820437366\n",
      "  mean_raw_obs_processing_ms: 0.21510151498340804\n",
      "time_since_restore: 3074.6732461452484\n",
      "time_this_iter_s: 16.168317556381226\n",
      "time_total_s: 3074.6732461452484\n",
      "timers:\n",
      "  learn_throughput: 551.656\n",
      "  learn_time_ms: 14465.524\n",
      "  load_throughput: 928208.421\n",
      "  load_time_ms: 8.597\n",
      "  sample_throughput: 508.359\n",
      "  sample_time_ms: 15697.557\n",
      "  update_time_ms: 6.131\n",
      "timestamp: 1643539298\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1540140\n",
      "training_iteration: 193\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1556070\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-42-09\n",
      "done: false\n",
      "episode_len_mean: 298.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 5917\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5994916325807571\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013589190432699544\n",
      "        policy_loss: -0.04219644448099037\n",
      "        total_loss: 65.10074888388316\n",
      "        vf_explained_var: 0.3232392222682635\n",
      "        vf_loss: 65.13377266486486\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5562535612285138\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01206361142031407\n",
      "        policy_loss: -0.07613571469128753\n",
      "        total_loss: 46.69180526773135\n",
      "        vf_explained_var: 0.10244857798020045\n",
      "        vf_loss: 46.759798185825346\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5302840048074722\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014238400156318675\n",
      "        policy_loss: -0.08492070398293436\n",
      "        total_loss: 49.299097021023435\n",
      "        vf_explained_var: 0.2494611757993698\n",
      "        vf_loss: 49.374406785964965\n",
      "  num_agent_steps_sampled: 1556070\n",
      "  num_agent_steps_trained: 1556070\n",
      "  num_steps_sampled: 1556100\n",
      "  num_steps_trained: 1556100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 195\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.744444444444444\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.40000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.333333333333336\n",
      "  player_1: 31.666666666666664\n",
      "  player_2: 44.0\n",
      "policy_reward_mean:\n",
      "  player_0: -3.5866666666666656\n",
      "  player_1: -2.536666666666667\n",
      "  player_2: 9.123333333333331\n",
      "policy_reward_min:\n",
      "  player_0: -56.0\n",
      "  player_1: -52.0\n",
      "  player_2: -45.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08975503879984509\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.33252461639483455\n",
      "  mean_inference_ms: 1.6734995606328926\n",
      "  mean_raw_obs_processing_ms: 0.2152081925592196\n",
      "time_since_restore: 3106.2975194454193\n",
      "time_this_iter_s: 15.068246841430664\n",
      "time_total_s: 3106.2975194454193\n",
      "timers:\n",
      "  learn_throughput: 551.45\n",
      "  learn_time_ms: 14470.927\n",
      "  load_throughput: 957524.207\n",
      "  load_time_ms: 8.334\n",
      "  sample_throughput: 506.294\n",
      "  sample_time_ms: 15761.595\n",
      "  update_time_ms: 6.065\n",
      "timestamp: 1643539329\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1556100\n",
      "training_iteration: 195\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1572031\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-42-41\n",
      "done: false\n",
      "episode_len_mean: 296.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 30\n",
      "episodes_total: 5974\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5716599104305108\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01483871381213286\n",
      "        policy_loss: -0.06715550526355704\n",
      "        total_loss: 74.21580765326817\n",
      "        vf_explained_var: 0.10871954113245011\n",
      "        vf_loss: 74.27294682502746\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5412757473190626\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014695404570370177\n",
      "        policy_loss: -0.0776016963335375\n",
      "        total_loss: 39.85431979338328\n",
      "        vf_explained_var: 0.036649570763111115\n",
      "        vf_loss: 39.92200206319491\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5455806683500608\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015521675567940842\n",
      "        policy_loss: -0.07973263609533508\n",
      "        total_loss: 50.000256479581196\n",
      "        vf_explained_var: -0.028409777979056042\n",
      "        vf_loss: 50.0695119992892\n",
      "  num_agent_steps_sampled: 1572031\n",
      "  num_agent_steps_trained: 1572031\n",
      "  num_steps_sampled: 1572060\n",
      "  num_steps_trained: 1572060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 197\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.263157894736842\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.40000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.333333333333336\n",
      "  player_1: 33.666666666666664\n",
      "  player_2: 44.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.7466666666666665\n",
      "  player_1: -4.526666666666666\n",
      "  player_2: 8.273333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -56.0\n",
      "  player_1: -52.0\n",
      "  player_2: -30.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08967388882493671\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.33213192040530665\n",
      "  mean_inference_ms: 1.671848838679937\n",
      "  mean_raw_obs_processing_ms: 0.21528043646311396\n",
      "time_since_restore: 3138.308822154999\n",
      "time_this_iter_s: 14.993049383163452\n",
      "time_total_s: 3138.308822154999\n",
      "timers:\n",
      "  learn_throughput: 546.735\n",
      "  learn_time_ms: 14595.736\n",
      "  load_throughput: 902394.824\n",
      "  load_time_ms: 8.843\n",
      "  sample_throughput: 500.517\n",
      "  sample_time_ms: 15943.505\n",
      "  update_time_ms: 5.896\n",
      "timestamp: 1643539361\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1572060\n",
      "training_iteration: 197\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1587990\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-43-13\n",
      "done: false\n",
      "episode_len_mean: 283.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.0000000000000107\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 30\n",
      "episodes_total: 6033\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5716419600447019\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014427994802002407\n",
      "        policy_loss: -0.07066521851345897\n",
      "        total_loss: 57.29604338089625\n",
      "        vf_explained_var: -0.01904168874025345\n",
      "        vf_loss: 57.35696973721186\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5946497145791848\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015576143086710582\n",
      "        policy_loss: -0.06453323312103748\n",
      "        total_loss: 34.64649911880493\n",
      "        vf_explained_var: 0.12100008656581243\n",
      "        vf_loss: 34.700518539746604\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5858719077706337\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015035428772377676\n",
      "        policy_loss: -0.10147103923683365\n",
      "        total_loss: 47.944641942183175\n",
      "        vf_explained_var: 0.1952617574731509\n",
      "        vf_loss: 48.03596421400706\n",
      "  num_agent_steps_sampled: 1587990\n",
      "  num_agent_steps_trained: 1587990\n",
      "  num_steps_sampled: 1588020\n",
      "  num_steps_trained: 1588020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 199\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.357894736842109\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.40000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 45.0\n",
      "  player_1: 33.666666666666664\n",
      "  player_2: 48.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.0799999999999996\n",
      "  player_1: -3.74\n",
      "  player_2: 5.659999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -54.333333333333336\n",
      "  player_1: -50.0\n",
      "  player_2: -65.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08951580398655334\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3316315157663282\n",
      "  mean_inference_ms: 1.670903933787048\n",
      "  mean_raw_obs_processing_ms: 0.2150529232207777\n",
      "time_since_restore: 3169.6743643283844\n",
      "time_this_iter_s: 15.216579914093018\n",
      "time_total_s: 3169.6743643283844\n",
      "timers:\n",
      "  learn_throughput: 542.778\n",
      "  learn_time_ms: 14702.151\n",
      "  load_throughput: 882829.702\n",
      "  load_time_ms: 9.039\n",
      "  sample_throughput: 499.57\n",
      "  sample_time_ms: 15973.75\n",
      "  update_time_ms: 5.903\n",
      "timestamp: 1643539393\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1588020\n",
      "training_iteration: 199\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1603950\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-43-43\n",
      "done: false\n",
      "episode_len_mean: 268.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 37\n",
      "episodes_total: 6094\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5764213371276855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013218522084354542\n",
      "        policy_loss: -0.07603998419828713\n",
      "        total_loss: 125.92468253135681\n",
      "        vf_explained_var: 0.1344705690940221\n",
      "        vf_loss: 125.99179913838704\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5925896788636843\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01494357769382153\n",
      "        policy_loss: -0.07412057483568787\n",
      "        total_loss: 44.32260358651479\n",
      "        vf_explained_var: 0.1629450731476148\n",
      "        vf_loss: 44.386636970043185\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5733308638135592\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013909520043797178\n",
      "        policy_loss: -0.08930376553287109\n",
      "        total_loss: 62.14156275669734\n",
      "        vf_explained_var: 0.2101567382613818\n",
      "        vf_loss: 62.221477530002595\n",
      "  num_agent_steps_sampled: 1603950\n",
      "  num_agent_steps_trained: 1603950\n",
      "  num_steps_sampled: 1603980\n",
      "  num_steps_trained: 1603980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 201\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.86\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.40000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 45.0\n",
      "  player_1: 33.666666666666664\n",
      "  player_2: 42.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 0.013333333333333143\n",
      "  player_1: -4.796666666666666\n",
      "  player_2: 7.783333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -67.66666666666667\n",
      "  player_1: -33.333333333333336\n",
      "  player_2: -32.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08931229577292857\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3306629769478555\n",
      "  mean_inference_ms: 1.667066358491751\n",
      "  mean_raw_obs_processing_ms: 0.21471540274975662\n",
      "time_since_restore: 3200.3192229270935\n",
      "time_this_iter_s: 15.646365404129028\n",
      "time_total_s: 3200.3192229270935\n",
      "timers:\n",
      "  learn_throughput: 550.047\n",
      "  learn_time_ms: 14507.857\n",
      "  load_throughput: 864563.694\n",
      "  load_time_ms: 9.23\n",
      "  sample_throughput: 505.859\n",
      "  sample_time_ms: 15775.137\n",
      "  update_time_ms: 5.772\n",
      "timestamp: 1643539423\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1603980\n",
      "training_iteration: 201\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1619910\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-44-15\n",
      "done: false\n",
      "episode_len_mean: 252.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 6152\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5982141947249572\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015276063264821478\n",
      "        policy_loss: -0.06740733397814135\n",
      "        total_loss: 43.5099573747317\n",
      "        vf_explained_var: -0.19939389338095984\n",
      "        vf_loss: 43.56705335855484\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5755670450131098\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01508244673745897\n",
      "        policy_loss: -0.08625973315909505\n",
      "        total_loss: 34.01772544264794\n",
      "        vf_explained_var: 0.062495618164539336\n",
      "        vf_loss: 34.09380451281866\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5805900155504544\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013935396726521105\n",
      "        policy_loss: -0.0918774063543727\n",
      "        total_loss: 46.78414620717366\n",
      "        vf_explained_var: -0.1110997254649798\n",
      "        vf_loss: 46.86661730209986\n",
      "  num_agent_steps_sampled: 1619910\n",
      "  num_agent_steps_trained: 1619910\n",
      "  num_steps_sampled: 1619940\n",
      "  num_steps_trained: 1619940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 203\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.46842105263158\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.40000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 40.666666666666664\n",
      "  player_1: 35.666666666666664\n",
      "  player_2: 44.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.08666666666666682\n",
      "  player_1: -3.2966666666666673\n",
      "  player_2: 6.383333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -67.66666666666667\n",
      "  player_1: -37.333333333333336\n",
      "  player_2: -61.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0894458576011523\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.32991602233315165\n",
      "  mean_inference_ms: 1.6639188236310651\n",
      "  mean_raw_obs_processing_ms: 0.2143183748355571\n",
      "time_since_restore: 3232.3421313762665\n",
      "time_this_iter_s: 15.751290082931519\n",
      "time_total_s: 3232.3421313762665\n",
      "timers:\n",
      "  learn_throughput: 549.109\n",
      "  learn_time_ms: 14532.646\n",
      "  load_throughput: 898900.119\n",
      "  load_time_ms: 8.878\n",
      "  sample_throughput: 503.785\n",
      "  sample_time_ms: 15840.104\n",
      "  update_time_ms: 5.707\n",
      "timestamp: 1643539455\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1619940\n",
      "training_iteration: 203\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1635870\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-44-49\n",
      "done: false\n",
      "episode_len_mean: 269.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 33\n",
      "episodes_total: 6214\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5905313522120317\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01567661679113068\n",
      "        policy_loss: -0.05266619744089743\n",
      "        total_loss: 41.582503134409585\n",
      "        vf_explained_var: 0.20014254530270895\n",
      "        vf_loss: 41.624587677319845\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5769622413317362\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01377143743113\n",
      "        policy_loss: -0.10048293401487171\n",
      "        total_loss: 61.08957453171412\n",
      "        vf_explained_var: 0.013504998087882995\n",
      "        vf_loss: 61.180761726697284\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5685518605510393\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014691199803964897\n",
      "        policy_loss: -0.06478268996346742\n",
      "        total_loss: 86.60061330397924\n",
      "        vf_explained_var: 0.27465385307868323\n",
      "        vf_loss: 86.6554791756471\n",
      "  num_agent_steps_sampled: 1635870\n",
      "  num_agent_steps_trained: 1635870\n",
      "  num_steps_sampled: 1635900\n",
      "  num_steps_trained: 1635900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 205\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.331578947368419\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.40000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.0\n",
      "  player_1: 48.333333333333336\n",
      "  player_2: 44.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.2166666666666664\n",
      "  player_1: -0.3933333333333334\n",
      "  player_2: 3.1766666666666663\n",
      "policy_reward_min:\n",
      "  player_0: -49.333333333333336\n",
      "  player_1: -37.333333333333336\n",
      "  player_2: -61.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08932061208819421\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.32944247146588523\n",
      "  mean_inference_ms: 1.6637258878750167\n",
      "  mean_raw_obs_processing_ms: 0.2142150460305715\n",
      "time_since_restore: 3265.5572125911713\n",
      "time_this_iter_s: 15.093450784683228\n",
      "time_total_s: 3265.5572125911713\n",
      "timers:\n",
      "  learn_throughput: 543.165\n",
      "  learn_time_ms: 14691.681\n",
      "  load_throughput: 948372.914\n",
      "  load_time_ms: 8.414\n",
      "  sample_throughput: 500.149\n",
      "  sample_time_ms: 15955.255\n",
      "  update_time_ms: 5.732\n",
      "timestamp: 1643539489\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1635900\n",
      "training_iteration: 205\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1651831\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-45-19\n",
      "done: false\n",
      "episode_len_mean: 253.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 30\n",
      "episodes_total: 6272\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5603823781013488\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0179655052074348\n",
      "        policy_loss: -0.07951819653622806\n",
      "        total_loss: 30.194276209672292\n",
      "        vf_explained_var: 0.15461416333913802\n",
      "        vf_loss: 30.261667643785476\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5818590374787649\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013742313771995213\n",
      "        policy_loss: -0.06373642291873693\n",
      "        total_loss: 39.43481336673101\n",
      "        vf_explained_var: 0.10811023712158203\n",
      "        vf_loss: 39.48927347024282\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.537142431885004\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015020469239221939\n",
      "        policy_loss: -0.06549021366983653\n",
      "        total_loss: 71.95602212746938\n",
      "        vf_explained_var: 0.09969125380118687\n",
      "        vf_loss: 72.01137342532476\n",
      "  num_agent_steps_sampled: 1651831\n",
      "  num_agent_steps_trained: 1651831\n",
      "  num_steps_sampled: 1651860\n",
      "  num_steps_trained: 1651860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 207\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.326315789473684\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.40000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 26.666666666666664\n",
      "  player_1: 54.0\n",
      "  player_2: 41.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.030000000000000106\n",
      "  player_1: 2.0300000000000002\n",
      "  player_2: 0.9999999999999996\n",
      "policy_reward_min:\n",
      "  player_0: -42.666666666666664\n",
      "  player_1: -41.0\n",
      "  player_2: -58.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08930714891707651\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3292748337803693\n",
      "  mean_inference_ms: 1.6655284473676932\n",
      "  mean_raw_obs_processing_ms: 0.21401959434498244\n",
      "time_since_restore: 3295.489540576935\n",
      "time_this_iter_s: 15.087971210479736\n",
      "time_total_s: 3295.489540576935\n",
      "timers:\n",
      "  learn_throughput: 550.843\n",
      "  learn_time_ms: 14486.895\n",
      "  load_throughput: 983427.04\n",
      "  load_time_ms: 8.114\n",
      "  sample_throughput: 507.059\n",
      "  sample_time_ms: 15737.817\n",
      "  update_time_ms: 5.752\n",
      "timestamp: 1643539519\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1651860\n",
      "training_iteration: 207\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1667790\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-45-51\n",
      "done: false\n",
      "episode_len_mean: 264.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 34\n",
      "episodes_total: 6332\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5881427522500356\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016561406741259173\n",
      "        policy_loss: -0.06373066250390062\n",
      "        total_loss: 71.00264057954152\n",
      "        vf_explained_var: 0.2620065520207087\n",
      "        vf_loss: 71.05519217014313\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5470995044211546\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01492813583462123\n",
      "        policy_loss: -0.07264118071490278\n",
      "        total_loss: 43.70407579978307\n",
      "        vf_explained_var: 0.35344675759474437\n",
      "        vf_loss: 43.766640726725264\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5839284605284532\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017170029159794164\n",
      "        policy_loss: -0.08326848382130266\n",
      "        total_loss: 94.84486250559489\n",
      "        vf_explained_var: 0.240098214050134\n",
      "        vf_loss: 94.9165410955747\n",
      "  num_agent_steps_sampled: 1667790\n",
      "  num_agent_steps_trained: 1667790\n",
      "  num_steps_sampled: 1667820\n",
      "  num_steps_trained: 1667820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 209\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.985\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.40000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 41.0\n",
      "  player_1: 54.0\n",
      "  player_2: 41.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.8533333333333335\n",
      "  player_1: -0.6866666666666661\n",
      "  player_2: 1.8333333333333337\n",
      "policy_reward_min:\n",
      "  player_0: -34.333333333333336\n",
      "  player_1: -41.0\n",
      "  player_2: -58.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08940877675176571\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.32963977818249446\n",
      "  mean_inference_ms: 1.6667625039402791\n",
      "  mean_raw_obs_processing_ms: 0.21484205083631644\n",
      "time_since_restore: 3327.5975453853607\n",
      "time_this_iter_s: 15.748655080795288\n",
      "time_total_s: 3327.5975453853607\n",
      "timers:\n",
      "  learn_throughput: 548.055\n",
      "  learn_time_ms: 14560.586\n",
      "  load_throughput: 1011668.483\n",
      "  load_time_ms: 7.888\n",
      "  sample_throughput: 506.033\n",
      "  sample_time_ms: 15769.724\n",
      "  update_time_ms: 5.746\n",
      "timestamp: 1643539551\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1667820\n",
      "training_iteration: 209\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1683750\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-46-23\n",
      "done: false\n",
      "episode_len_mean: 265.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 30\n",
      "episodes_total: 6389\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5790503420432409\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014881176546197708\n",
      "        policy_loss: -0.08278085722277562\n",
      "        total_loss: 70.72982675472895\n",
      "        vf_explained_var: 0.21154435316721598\n",
      "        vf_loss: 70.80256264448165\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.578118214259545\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016064225456842298\n",
      "        policy_loss: -0.07678719136863947\n",
      "        total_loss: 52.1994144097964\n",
      "        vf_explained_var: -0.11124772310256958\n",
      "        vf_loss: 52.265358296235405\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5651479697724183\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01410187572153518\n",
      "        policy_loss: -0.06651441286007563\n",
      "        total_loss: 84.10477176825205\n",
      "        vf_explained_var: 0.11165470123291016\n",
      "        vf_loss: 84.16176691452662\n",
      "  num_agent_steps_sampled: 1683750\n",
      "  num_agent_steps_trained: 1683750\n",
      "  num_steps_sampled: 1683780\n",
      "  num_steps_trained: 1683780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 211\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.34\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.42\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 44.0\n",
      "  player_1: 38.0\n",
      "  player_2: 35.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.983333333333333\n",
      "  player_1: -0.8666666666666663\n",
      "  player_2: -0.11666666666666607\n",
      "policy_reward_min:\n",
      "  player_0: -48.333333333333336\n",
      "  player_1: -41.333333333333336\n",
      "  player_2: -70.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08926880341241322\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.32888786392829966\n",
      "  mean_inference_ms: 1.6642077610754091\n",
      "  mean_raw_obs_processing_ms: 0.21443022897010405\n",
      "time_since_restore: 3359.5694353580475\n",
      "time_this_iter_s: 16.82418704032898\n",
      "time_total_s: 3359.5694353580475\n",
      "timers:\n",
      "  learn_throughput: 543.072\n",
      "  learn_time_ms: 14694.18\n",
      "  load_throughput: 1003635.636\n",
      "  load_time_ms: 7.951\n",
      "  sample_throughput: 503.835\n",
      "  sample_time_ms: 15838.506\n",
      "  update_time_ms: 5.534\n",
      "timestamp: 1643539583\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1683780\n",
      "training_iteration: 211\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1699711\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-46-54\n",
      "done: false\n",
      "episode_len_mean: 270.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 33\n",
      "episodes_total: 6454\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5608221110701561\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014811402499688976\n",
      "        policy_loss: -0.06747844530890386\n",
      "        total_loss: 108.45957803090414\n",
      "        vf_explained_var: -0.08369823495546977\n",
      "        vf_loss: 108.51705885728201\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5765031978487969\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015260183648356171\n",
      "        policy_loss: -0.07891944128088653\n",
      "        total_loss: 38.83248098691305\n",
      "        vf_explained_var: -0.3299706776936849\n",
      "        vf_loss: 38.90109995126724\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5711964456737042\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014922603958010162\n",
      "        policy_loss: -0.08295325613270203\n",
      "        total_loss: 65.59285484949747\n",
      "        vf_explained_var: 0.12336240530014038\n",
      "        vf_loss: 65.66573526700337\n",
      "  num_agent_steps_sampled: 1699711\n",
      "  num_agent_steps_trained: 1699711\n",
      "  num_steps_sampled: 1699740\n",
      "  num_steps_trained: 1699740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 213\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.527777777777779\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.40000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 46.666666666666664\n",
      "  player_1: 38.0\n",
      "  player_2: 42.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.2633333333333336\n",
      "  player_1: -1.3166666666666669\n",
      "  player_2: 1.0533333333333337\n",
      "policy_reward_min:\n",
      "  player_0: -48.333333333333336\n",
      "  player_1: -33.666666666666664\n",
      "  player_2: -70.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08924225835121559\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3288746433996243\n",
      "  mean_inference_ms: 1.6643897372574628\n",
      "  mean_raw_obs_processing_ms: 0.21415402365076372\n",
      "time_since_restore: 3390.5916788578033\n",
      "time_this_iter_s: 14.495720148086548\n",
      "time_total_s: 3390.5916788578033\n",
      "timers:\n",
      "  learn_throughput: 546.787\n",
      "  learn_time_ms: 14594.361\n",
      "  load_throughput: 1018859.271\n",
      "  load_time_ms: 7.832\n",
      "  sample_throughput: 499.466\n",
      "  sample_time_ms: 15977.074\n",
      "  update_time_ms: 5.542\n",
      "timestamp: 1643539614\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1699740\n",
      "training_iteration: 213\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1715670\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-47-25\n",
      "done: false\n",
      "episode_len_mean: 241.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 32\n",
      "episodes_total: 6519\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5657120656967163\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015524460258323719\n",
      "        policy_loss: -0.0829009845169882\n",
      "        total_loss: 81.58648863633474\n",
      "        vf_explained_var: 0.1549389749765396\n",
      "        vf_loss: 81.65891052246094\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49952151263753575\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012467313351100226\n",
      "        policy_loss: -0.09307853311145058\n",
      "        total_loss: 36.42207003037135\n",
      "        vf_explained_var: 0.07699331551790238\n",
      "        vf_loss: 36.50673322995504\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5306735255817572\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01536113226948146\n",
      "        policy_loss: -0.05075297868500153\n",
      "        total_loss: 72.68884377002716\n",
      "        vf_explained_var: 0.0076357218623161315\n",
      "        vf_loss: 72.72922764460246\n",
      "  num_agent_steps_sampled: 1715670\n",
      "  num_agent_steps_trained: 1715670\n",
      "  num_steps_sampled: 1715700\n",
      "  num_steps_trained: 1715700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 215\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.166666666666664\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.5\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 46.666666666666664\n",
      "  player_1: 29.0\n",
      "  player_2: 41.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.10999999999999957\n",
      "  player_1: -2.87\n",
      "  player_2: 5.76\n",
      "policy_reward_min:\n",
      "  player_0: -66.0\n",
      "  player_1: -29.333333333333336\n",
      "  player_2: -54.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0891937500353028\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3281989434276445\n",
      "  mean_inference_ms: 1.663843587264035\n",
      "  mean_raw_obs_processing_ms: 0.21382596314190636\n",
      "time_since_restore: 3421.60614156723\n",
      "time_this_iter_s: 14.663386106491089\n",
      "time_total_s: 3421.60614156723\n",
      "timers:\n",
      "  learn_throughput: 554.914\n",
      "  learn_time_ms: 14380.605\n",
      "  load_throughput: 1012929.88\n",
      "  load_time_ms: 7.878\n",
      "  sample_throughput: 509.118\n",
      "  sample_time_ms: 15674.153\n",
      "  update_time_ms: 5.418\n",
      "timestamp: 1643539645\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1715700\n",
      "training_iteration: 215\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1731630\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-47-59\n",
      "done: false\n",
      "episode_len_mean: 246.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 35\n",
      "episodes_total: 6585\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5667262312273185\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016923447405515997\n",
      "        policy_loss: -0.09428300004452467\n",
      "        total_loss: 73.41936300436656\n",
      "        vf_explained_var: 0.026610528031984965\n",
      "        vf_loss: 73.5022226635615\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5767544702688853\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014886161999196474\n",
      "        policy_loss: -0.04101349146105349\n",
      "        total_loss: 33.23862761894862\n",
      "        vf_explained_var: 0.0031748732924461367\n",
      "        vf_loss: 33.26959280729294\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5523000571628411\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014999802634067408\n",
      "        policy_loss: -0.07694486513733864\n",
      "        total_loss: 50.599736218452456\n",
      "        vf_explained_var: 0.24769192665815354\n",
      "        vf_loss: 50.66655638853709\n",
      "  num_agent_steps_sampled: 1731630\n",
      "  num_agent_steps_trained: 1731630\n",
      "  num_steps_sampled: 1731660\n",
      "  num_steps_trained: 1731660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 217\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.54545454545454\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.5\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.666666666666664\n",
      "  player_1: 39.0\n",
      "  player_2: 36.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -0.12333333333333289\n",
      "  player_1: -2.1533333333333338\n",
      "  player_2: 5.276666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -45.666666666666664\n",
      "  player_1: -30.666666666666664\n",
      "  player_2: -47.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08899640263740671\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3270840724006508\n",
      "  mean_inference_ms: 1.6581365723856638\n",
      "  mean_raw_obs_processing_ms: 0.21365417532979328\n",
      "time_since_restore: 3455.775439977646\n",
      "time_this_iter_s: 17.391170740127563\n",
      "time_total_s: 3455.775439977646\n",
      "timers:\n",
      "  learn_throughput: 539.071\n",
      "  learn_time_ms: 14803.235\n",
      "  load_throughput: 1059872.004\n",
      "  load_time_ms: 7.529\n",
      "  sample_throughput: 504.244\n",
      "  sample_time_ms: 15825.677\n",
      "  update_time_ms: 6.065\n",
      "timestamp: 1643539679\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1731660\n",
      "training_iteration: 217\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1747590\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-48-31\n",
      "done: false\n",
      "episode_len_mean: 247.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 39\n",
      "episodes_total: 6647\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5855171610911687\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01429496675592721\n",
      "        policy_loss: -0.12247745057567955\n",
      "        total_loss: 87.53412740389506\n",
      "        vf_explained_var: 0.16833581954240798\n",
      "        vf_loss: 87.64695593039195\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5895523111522197\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014305773682879703\n",
      "        policy_loss: -0.05331188840481142\n",
      "        total_loss: 30.392204847335815\n",
      "        vf_explained_var: 0.26324654271205267\n",
      "        vf_loss: 30.435860358874002\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.54582775319616\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015004785980389007\n",
      "        policy_loss: -0.05600547295063734\n",
      "        total_loss: 45.630414338906604\n",
      "        vf_explained_var: 0.35905013511578243\n",
      "        vf_loss: 45.676291601657866\n",
      "  num_agent_steps_sampled: 1747590\n",
      "  num_agent_steps_trained: 1747590\n",
      "  num_steps_sampled: 1747620\n",
      "  num_steps_trained: 1747620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 219\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.245000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.5\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 49.333333333333336\n",
      "  player_1: 48.33333333333333\n",
      "  player_2: 42.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.0033333333333334\n",
      "  player_1: -0.6466666666666668\n",
      "  player_2: 2.643333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -53.666666666666664\n",
      "  player_1: -30.666666666666664\n",
      "  player_2: -91.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08895445724871585\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3267480489905805\n",
      "  mean_inference_ms: 1.6566073228570248\n",
      "  mean_raw_obs_processing_ms: 0.21358018969260603\n",
      "time_since_restore: 3487.4606285095215\n",
      "time_this_iter_s: 16.21707773208618\n",
      "time_total_s: 3487.4606285095215\n",
      "timers:\n",
      "  learn_throughput: 540.942\n",
      "  learn_time_ms: 14752.049\n",
      "  load_throughput: 1001653.312\n",
      "  load_time_ms: 7.967\n",
      "  sample_throughput: 499.597\n",
      "  sample_time_ms: 15972.87\n",
      "  update_time_ms: 6.113\n",
      "timestamp: 1643539711\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1747620\n",
      "training_iteration: 219\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1763550\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-49-04\n",
      "done: false\n",
      "episode_len_mean: 251.28\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 30\n",
      "episodes_total: 6709\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5530320917566617\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015567039945202813\n",
      "        policy_loss: -0.06650408977875485\n",
      "        total_loss: 57.91451516787211\n",
      "        vf_explained_var: 0.03793268611033757\n",
      "        vf_loss: 57.970511635144554\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5592652671039104\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014870426080710786\n",
      "        policy_loss: -0.045611812393181024\n",
      "        total_loss: 40.75380745808283\n",
      "        vf_explained_var: 0.02693532943725586\n",
      "        vf_loss: 40.78938173453013\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5097321266929309\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013782950869638929\n",
      "        policy_loss: -0.11634132938459515\n",
      "        total_loss: 38.78614540815354\n",
      "        vf_explained_var: 0.1723806039492289\n",
      "        vf_loss: 38.893183413346605\n",
      "  num_agent_steps_sampled: 1763550\n",
      "  num_agent_steps_trained: 1763550\n",
      "  num_steps_sampled: 1763580\n",
      "  num_steps_trained: 1763580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 221\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.805000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.5\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.666666666666668\n",
      "  player_1: 33.333333333333336\n",
      "  player_2: 37.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: -0.22\n",
      "  player_1: -1.8700000000000003\n",
      "  player_2: 5.090000000000001\n",
      "policy_reward_min:\n",
      "  player_0: -53.666666666666664\n",
      "  player_1: -39.666666666666664\n",
      "  player_2: -37.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0890730025259066\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3266752825693902\n",
      "  mean_inference_ms: 1.6580759941832142\n",
      "  mean_raw_obs_processing_ms: 0.21351406387108388\n",
      "time_since_restore: 3520.140691757202\n",
      "time_this_iter_s: 16.495248556137085\n",
      "time_total_s: 3520.140691757202\n",
      "timers:\n",
      "  learn_throughput: 538.899\n",
      "  learn_time_ms: 14807.97\n",
      "  load_throughput: 975280.266\n",
      "  load_time_ms: 8.182\n",
      "  sample_throughput: 494.468\n",
      "  sample_time_ms: 16138.57\n",
      "  update_time_ms: 6.015\n",
      "timestamp: 1643539744\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1763580\n",
      "training_iteration: 221\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1779510\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-49-34\n",
      "done: false\n",
      "episode_len_mean: 272.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 31\n",
      "episodes_total: 6764\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5745986492931843\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014400666140112055\n",
      "        policy_loss: -0.08661430905262629\n",
      "        total_loss: 48.24221490303675\n",
      "        vf_explained_var: 0.2946818443139394\n",
      "        vf_loss: 48.319108287493385\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5350791358947754\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014286035132121242\n",
      "        policy_loss: -0.06059925298206508\n",
      "        total_loss: 32.10475911537806\n",
      "        vf_explained_var: 0.2141524933775266\n",
      "        vf_loss: 32.15571537812551\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.557132783383131\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015499871508592378\n",
      "        policy_loss: -0.06906340898325046\n",
      "        total_loss: 48.58410424868266\n",
      "        vf_explained_var: 0.19264623949925105\n",
      "        vf_loss: 48.64270524342855\n",
      "  num_agent_steps_sampled: 1779510\n",
      "  num_agent_steps_trained: 1779510\n",
      "  num_steps_sampled: 1779540\n",
      "  num_steps_trained: 1779540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 223\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.794444444444444\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.5\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.333333333333336\n",
      "  player_1: 29.0\n",
      "  player_2: 32.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 1.363333333333333\n",
      "  player_1: -2.4466666666666668\n",
      "  player_2: 4.083333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -35.333333333333336\n",
      "  player_1: -39.666666666666664\n",
      "  player_2: -42.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08886261099712639\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3256944526448411\n",
      "  mean_inference_ms: 1.6545611168174923\n",
      "  mean_raw_obs_processing_ms: 0.2130658985877334\n",
      "time_since_restore: 3550.61071062088\n",
      "time_this_iter_s: 14.426709175109863\n",
      "time_total_s: 3550.61071062088\n",
      "timers:\n",
      "  learn_throughput: 540.938\n",
      "  learn_time_ms: 14752.16\n",
      "  load_throughput: 906459.016\n",
      "  load_time_ms: 8.803\n",
      "  sample_throughput: 497.367\n",
      "  sample_time_ms: 16044.49\n",
      "  update_time_ms: 9.214\n",
      "timestamp: 1643539774\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1779540\n",
      "training_iteration: 223\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1795470\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-50-04\n",
      "done: false\n",
      "episode_len_mean: 267.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 30\n",
      "episodes_total: 6829\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.572783175855875\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01624083242382464\n",
      "        policy_loss: -0.08105877734720707\n",
      "        total_loss: 48.21708052237829\n",
      "        vf_explained_var: 0.33832489073276517\n",
      "        vf_loss: 48.287176477909085\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5591320986052354\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014904814684862988\n",
      "        policy_loss: -0.055700884743904076\n",
      "        total_loss: 32.2796611237526\n",
      "        vf_explained_var: 0.09043253074089685\n",
      "        vf_loss: 32.32530128916105\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5311026656379302\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01460839930867716\n",
      "        policy_loss: -0.0723587851645425\n",
      "        total_loss: 63.25585839907328\n",
      "        vf_explained_var: 0.03946471720933914\n",
      "        vf_loss: 63.31835631688436\n",
      "  num_agent_steps_sampled: 1795470\n",
      "  num_agent_steps_trained: 1795470\n",
      "  num_steps_sampled: 1795500\n",
      "  num_steps_trained: 1795500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 225\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.583333333333334\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.5\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.333333333333336\n",
      "  player_1: 30.66666666666667\n",
      "  player_2: 41.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.21\n",
      "  player_1: -2.3699999999999997\n",
      "  player_2: 4.16\n",
      "policy_reward_min:\n",
      "  player_0: -35.0\n",
      "  player_1: -35.333333333333336\n",
      "  player_2: -50.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08898314496281587\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.32580800539083865\n",
      "  mean_inference_ms: 1.6555098459463544\n",
      "  mean_raw_obs_processing_ms: 0.21313960834657267\n",
      "time_since_restore: 3580.3257212638855\n",
      "time_this_iter_s: 14.658077955245972\n",
      "time_total_s: 3580.3257212638855\n",
      "timers:\n",
      "  learn_throughput: 545.776\n",
      "  learn_time_ms: 14621.396\n",
      "  load_throughput: 912730.181\n",
      "  load_time_ms: 8.743\n",
      "  sample_throughput: 501.397\n",
      "  sample_time_ms: 15915.522\n",
      "  update_time_ms: 9.31\n",
      "timestamp: 1643539804\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1795500\n",
      "training_iteration: 225\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1811430\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-50-35\n",
      "done: false\n",
      "episode_len_mean: 239.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 37\n",
      "episodes_total: 6898\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5549368356664975\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01437985659114929\n",
      "        policy_loss: -0.031641804091632364\n",
      "        total_loss: 89.22307808876037\n",
      "        vf_explained_var: 0.08235881437857946\n",
      "        vf_loss: 89.24501365661621\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5611461422344048\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015241103210234617\n",
      "        policy_loss: -0.07311529570569594\n",
      "        total_loss: 56.80867839574814\n",
      "        vf_explained_var: 0.1580867718656858\n",
      "        vf_loss: 56.871506059964496\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.550580791135629\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015252334301369122\n",
      "        policy_loss: -0.12142443889131149\n",
      "        total_loss: 53.970475671291354\n",
      "        vf_explained_var: 0.3340164123972257\n",
      "        vf_loss: 54.081604549884794\n",
      "  num_agent_steps_sampled: 1811430\n",
      "  num_agent_steps_trained: 1811430\n",
      "  num_steps_sampled: 1811460\n",
      "  num_steps_trained: 1811460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 227\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.194736842105266\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.5\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.666666666666664\n",
      "  player_1: 33.333333333333336\n",
      "  player_2: 38.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.3733333333333329\n",
      "  player_1: -1.6466666666666672\n",
      "  player_2: 3.273333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -45.66666666666667\n",
      "  player_1: -35.333333333333336\n",
      "  player_2: -50.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08883201143151645\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.32552316811727044\n",
      "  mean_inference_ms: 1.6546062290378651\n",
      "  mean_raw_obs_processing_ms: 0.21297844144040565\n",
      "time_since_restore: 3610.9966609477997\n",
      "time_this_iter_s: 15.217320919036865\n",
      "time_total_s: 3610.9966609477997\n",
      "timers:\n",
      "  learn_throughput: 559.229\n",
      "  learn_time_ms: 14269.649\n",
      "  load_throughput: 930158.542\n",
      "  load_time_ms: 8.579\n",
      "  sample_throughput: 505.543\n",
      "  sample_time_ms: 15784.999\n",
      "  update_time_ms: 8.745\n",
      "timestamp: 1643539835\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1811460\n",
      "training_iteration: 227\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1827390\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-51-06\n",
      "done: false\n",
      "episode_len_mean: 225.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 33\n",
      "episodes_total: 6968\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5528293204804261\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015452108290121676\n",
      "        policy_loss: -0.08559252541512251\n",
      "        total_loss: 51.250925070444744\n",
      "        vf_explained_var: 0.255122749209404\n",
      "        vf_loss: 51.32608741283417\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5722592152158419\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015285793080017053\n",
      "        policy_loss: -0.07265062019539376\n",
      "        total_loss: 54.45332140207291\n",
      "        vf_explained_var: 0.1391006201505661\n",
      "        vf_loss: 54.51565438747406\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5092397367954254\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015576367703274438\n",
      "        policy_loss: -0.07582245637274658\n",
      "        total_loss: 33.58810724576314\n",
      "        vf_explained_var: 0.14671226024627684\n",
      "        vf_loss: 33.65341576576233\n",
      "  num_agent_steps_sampled: 1827390\n",
      "  num_agent_steps_trained: 1827390\n",
      "  num_steps_sampled: 1827420\n",
      "  num_steps_trained: 1827420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 229\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.726315789473682\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.5\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.0\n",
      "  player_1: 36.66666666666667\n",
      "  player_2: 33.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.6766666666666672\n",
      "  player_1: -1.9033333333333329\n",
      "  player_2: 3.226666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -58.33333333333333\n",
      "  player_1: -48.0\n",
      "  player_2: -28.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08878031113868669\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3248124930472502\n",
      "  mean_inference_ms: 1.6526476458784176\n",
      "  mean_raw_obs_processing_ms: 0.2127161112653437\n",
      "time_since_restore: 3641.8337066173553\n",
      "time_this_iter_s: 15.753831386566162\n",
      "time_total_s: 3641.8337066173553\n",
      "timers:\n",
      "  learn_throughput: 562.398\n",
      "  learn_time_ms: 14189.245\n",
      "  load_throughput: 967552.566\n",
      "  load_time_ms: 8.248\n",
      "  sample_throughput: 514.191\n",
      "  sample_time_ms: 15519.527\n",
      "  update_time_ms: 8.613\n",
      "timestamp: 1643539866\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1827420\n",
      "training_iteration: 229\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1843351\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-51-36\n",
      "done: false\n",
      "episode_len_mean: 245.42\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 31\n",
      "episodes_total: 7030\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5544738047321638\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016074882324681048\n",
      "        policy_loss: -0.056038151324416204\n",
      "        total_loss: 60.14529545625051\n",
      "        vf_explained_var: 0.09211860477924347\n",
      "        vf_loss: 60.19048304875692\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5685067770381769\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016396580416034017\n",
      "        policy_loss: -0.09355356891639531\n",
      "        total_loss: 42.60233507235845\n",
      "        vf_explained_var: 0.003932354152202606\n",
      "        vf_loss: 42.68482109467188\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5396856564283371\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013674806686255276\n",
      "        policy_loss: -0.09106064495941003\n",
      "        total_loss: 49.58363182465235\n",
      "        vf_explained_var: 0.09822420746088029\n",
      "        vf_loss: 49.665461653868356\n",
      "  num_agent_steps_sampled: 1843351\n",
      "  num_agent_steps_trained: 1843351\n",
      "  num_steps_sampled: 1843380\n",
      "  num_steps_trained: 1843380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 231\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.933333333333334\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.64444444444443\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.0\n",
      "  player_1: 37.333333333333336\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.7900000000000003\n",
      "  player_1: -1.6499999999999997\n",
      "  player_2: 2.8599999999999994\n",
      "policy_reward_min:\n",
      "  player_0: -58.33333333333333\n",
      "  player_1: -48.0\n",
      "  player_2: -40.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08881832421401581\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.32472585954372\n",
      "  mean_inference_ms: 1.652766237634651\n",
      "  mean_raw_obs_processing_ms: 0.21258505266232464\n",
      "time_since_restore: 3671.966324329376\n",
      "time_this_iter_s: 15.15463376045227\n",
      "time_total_s: 3671.966324329376\n",
      "timers:\n",
      "  learn_throughput: 571.972\n",
      "  learn_time_ms: 13951.729\n",
      "  load_throughput: 1018741.43\n",
      "  load_time_ms: 7.833\n",
      "  sample_throughput: 520.059\n",
      "  sample_time_ms: 15344.402\n",
      "  update_time_ms: 8.67\n",
      "timestamp: 1643539896\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1843380\n",
      "training_iteration: 231\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1859310\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-52-05\n",
      "done: false\n",
      "episode_len_mean: 255.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 31\n",
      "episodes_total: 7093\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.539724883288145\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015284898339569584\n",
      "        policy_loss: -0.08455994304890434\n",
      "        total_loss: 73.71615229606628\n",
      "        vf_explained_var: 0.15096198985973994\n",
      "        vf_loss: 73.79039523760478\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5350830661257108\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014218902304382936\n",
      "        policy_loss: -0.06335696760875484\n",
      "        total_loss: 36.94497324625651\n",
      "        vf_explained_var: 0.18806641429662704\n",
      "        vf_loss: 36.99873226245244\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5255565386017164\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014476733560146992\n",
      "        policy_loss: -0.07710357391585906\n",
      "        total_loss: 59.07528090635935\n",
      "        vf_explained_var: 0.17955236822366716\n",
      "        vf_loss: 59.14261303583781\n",
      "  num_agent_steps_sampled: 1859310\n",
      "  num_agent_steps_trained: 1859310\n",
      "  num_steps_sampled: 1859340\n",
      "  num_steps_trained: 1859340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 233\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.33888888888889\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.5611111111111\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.33333333333333\n",
      "  player_1: 37.333333333333336\n",
      "  player_2: 43.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -0.9833333333333336\n",
      "  player_1: -1.6333333333333337\n",
      "  player_2: 5.616666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -47.0\n",
      "  player_1: -34.333333333333336\n",
      "  player_2: -39.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08890454412141968\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3252116328119821\n",
      "  mean_inference_ms: 1.6558082984498594\n",
      "  mean_raw_obs_processing_ms: 0.21291262570801855\n",
      "time_since_restore: 3700.972067117691\n",
      "time_this_iter_s: 14.443536281585693\n",
      "time_total_s: 3700.972067117691\n",
      "timers:\n",
      "  learn_throughput: 577.941\n",
      "  learn_time_ms: 13807.644\n",
      "  load_throughput: 1106138.885\n",
      "  load_time_ms: 7.214\n",
      "  sample_throughput: 529.264\n",
      "  sample_time_ms: 15077.533\n",
      "  update_time_ms: 5.514\n",
      "timestamp: 1643539925\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1859340\n",
      "training_iteration: 233\n",
      "trial_id: default\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0130 11:52:33.593494385    1934 backup_poller.cc:134]       Run client channel backup poller: {\"created\":\"@1643539953.593471952\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1643539953.593469357\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 1875270\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-52-35\n",
      "done: false\n",
      "episode_len_mean: 238.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 7150\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5368898346026738\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015345441545093005\n",
      "        policy_loss: -0.09068584037634234\n",
      "        total_loss: 46.917015519142154\n",
      "        vf_explained_var: 0.01691104958454768\n",
      "        vf_loss: 46.99734324773153\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5476701200505097\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014799093055638271\n",
      "        policy_loss: -0.07956308409261206\n",
      "        total_loss: 26.12393829425176\n",
      "        vf_explained_var: -0.1352107521891594\n",
      "        vf_loss: 26.19351187268893\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.532288322498401\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015055953378784277\n",
      "        policy_loss: -0.08784302814553181\n",
      "        total_loss: 49.62957687616348\n",
      "        vf_explained_var: 0.23475985278685887\n",
      "        vf_loss: 49.7072571182251\n",
      "  num_agent_steps_sampled: 1875270\n",
      "  num_agent_steps_trained: 1875270\n",
      "  num_steps_sampled: 1875300\n",
      "  num_steps_trained: 1875300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 235\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.157894736842108\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.59999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.33333333333333\n",
      "  player_1: 28.666666666666664\n",
      "  player_2: 43.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 0.17666666666666664\n",
      "  player_1: -1.8133333333333335\n",
      "  player_2: 4.636666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -47.0\n",
      "  player_1: -34.333333333333336\n",
      "  player_2: -42.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08882279270339884\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.32423591346497493\n",
      "  mean_inference_ms: 1.6533833938840499\n",
      "  mean_raw_obs_processing_ms: 0.21267225250188393\n",
      "time_since_restore: 3730.4997520446777\n",
      "time_this_iter_s: 15.002390623092651\n",
      "time_total_s: 3730.4997520446777\n",
      "timers:\n",
      "  learn_throughput: 578.847\n",
      "  learn_time_ms: 13786.032\n",
      "  load_throughput: 1091881.84\n",
      "  load_time_ms: 7.308\n",
      "  sample_throughput: 531.38\n",
      "  sample_time_ms: 15017.493\n",
      "  update_time_ms: 6.026\n",
      "timestamp: 1643539955\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1875300\n",
      "training_iteration: 235\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1891232\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-53-04\n",
      "done: false\n",
      "episode_len_mean: 271.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 34\n",
      "episodes_total: 7211\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5402779590090115\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013294676462731634\n",
      "        policy_loss: -0.06472843910877903\n",
      "        total_loss: 53.87381183465322\n",
      "        vf_explained_var: 0.08776131192843119\n",
      "        vf_loss: 53.92956648508708\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5588864454130331\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01408320043945423\n",
      "        policy_loss: -0.04888543830563625\n",
      "        total_loss: 29.753705507119495\n",
      "        vf_explained_var: 0.1908245402574539\n",
      "        vf_loss: 29.79308475255966\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.547204151302576\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014421916470937978\n",
      "        policy_loss: -0.08758246961670617\n",
      "        total_loss: 49.1884876203537\n",
      "        vf_explained_var: 0.24395908027887345\n",
      "        vf_loss: 49.26633514086405\n",
      "  num_agent_steps_sampled: 1891232\n",
      "  num_agent_steps_trained: 1891232\n",
      "  num_steps_sampled: 1891260\n",
      "  num_steps_trained: 1891260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 237\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.111111111111114\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.6222222222222\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 52.666666666666664\n",
      "  player_1: 28.666666666666664\n",
      "  player_2: 30.0\n",
      "policy_reward_mean:\n",
      "  player_0: 5.156666666666667\n",
      "  player_1: -2.563333333333334\n",
      "  player_2: 0.4066666666666668\n",
      "policy_reward_min:\n",
      "  player_0: -35.333333333333336\n",
      "  player_1: -33.33333333333333\n",
      "  player_2: -64.33333333333334\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08879339668843496\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.32347357600746834\n",
      "  mean_inference_ms: 1.6511310483479809\n",
      "  mean_raw_obs_processing_ms: 0.21245761934030752\n",
      "time_since_restore: 3760.12654876709\n",
      "time_this_iter_s: 14.852999448776245\n",
      "time_total_s: 3760.12654876709\n",
      "timers:\n",
      "  learn_throughput: 583.049\n",
      "  learn_time_ms: 13686.672\n",
      "  load_throughput: 1074806.394\n",
      "  load_time_ms: 7.425\n",
      "  sample_throughput: 532.531\n",
      "  sample_time_ms: 14985.032\n",
      "  update_time_ms: 5.884\n",
      "timestamp: 1643539984\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1891260\n",
      "training_iteration: 237\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1907190\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-53-34\n",
      "done: false\n",
      "episode_len_mean: 245.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 33\n",
      "episodes_total: 7275\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.533201435705026\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013900573336001495\n",
      "        policy_loss: -0.06679791103427608\n",
      "        total_loss: 60.4108340994517\n",
      "        vf_explained_var: 0.26497309466203056\n",
      "        vf_loss: 60.468248918851216\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.570910254518191\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013447316177179498\n",
      "        policy_loss: -0.031826627928142746\n",
      "        total_loss: 69.54006502429644\n",
      "        vf_explained_var: 0.1263281394044558\n",
      "        vf_loss: 69.56281467954318\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5439883761604627\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014843472036104496\n",
      "        policy_loss: -0.12063873134010161\n",
      "        total_loss: 40.15203974684079\n",
      "        vf_explained_var: 0.031020613710085552\n",
      "        vf_loss: 40.26265919764837\n",
      "  num_agent_steps_sampled: 1907190\n",
      "  num_agent_steps_trained: 1907190\n",
      "  num_steps_sampled: 1907220\n",
      "  num_steps_trained: 1907220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 239\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.11578947368421\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.59999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 45.33333333333333\n",
      "  player_1: 30.666666666666664\n",
      "  player_2: 31.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 6.213333333333334\n",
      "  player_1: -3.766666666666667\n",
      "  player_2: 0.5533333333333331\n",
      "policy_reward_min:\n",
      "  player_0: -32.333333333333336\n",
      "  player_1: -72.66666666666667\n",
      "  player_2: -35.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08880120032152136\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3239696286861109\n",
      "  mean_inference_ms: 1.6538122261261912\n",
      "  mean_raw_obs_processing_ms: 0.21257649523385708\n",
      "time_since_restore: 3789.7984611988068\n",
      "time_this_iter_s: 15.049835681915283\n",
      "time_total_s: 3789.7984611988068\n",
      "timers:\n",
      "  learn_throughput: 587.797\n",
      "  learn_time_ms: 13576.126\n",
      "  load_throughput: 1119918.155\n",
      "  load_time_ms: 7.126\n",
      "  sample_throughput: 535.265\n",
      "  sample_time_ms: 14908.498\n",
      "  update_time_ms: 5.871\n",
      "timestamp: 1643540014\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1907220\n",
      "training_iteration: 239\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1923150\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-54-07\n",
      "done: false\n",
      "episode_len_mean: 253.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 35\n",
      "episodes_total: 7337\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5236338581144809\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015355143692665175\n",
      "        policy_loss: -0.04573040407771865\n",
      "        total_loss: 54.994823848406476\n",
      "        vf_explained_var: 0.18263092497984568\n",
      "        vf_loss: 55.03018966356913\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.558966359992822\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01605299952271404\n",
      "        policy_loss: -0.08831537500023842\n",
      "        total_loss: 41.96877207835515\n",
      "        vf_explained_var: -0.07054213345050812\n",
      "        vf_loss: 42.04625152985255\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5390831293165683\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015205688187391691\n",
      "        policy_loss: -0.08793055589931707\n",
      "        total_loss: 48.81844560305277\n",
      "        vf_explained_var: 0.31750639379024503\n",
      "        vf_loss: 48.8961120557785\n",
      "  num_agent_steps_sampled: 1923150\n",
      "  num_agent_steps_trained: 1923150\n",
      "  num_steps_sampled: 1923180\n",
      "  num_steps_trained: 1923180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 241\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.194736842105259\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.59999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 45.33333333333333\n",
      "  player_1: 30.666666666666664\n",
      "  player_2: 32.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 1.5966666666666667\n",
      "  player_1: -2.1333333333333333\n",
      "  player_2: 3.536666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -50.0\n",
      "  player_1: -72.66666666666667\n",
      "  player_2: -35.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0886915853297269\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3229505119256361\n",
      "  mean_inference_ms: 1.6487192591084028\n",
      "  mean_raw_obs_processing_ms: 0.21238430756361915\n",
      "time_since_restore: 3822.3017807006836\n",
      "time_this_iter_s: 15.562672138214111\n",
      "time_total_s: 3822.3017807006836\n",
      "timers:\n",
      "  learn_throughput: 577.683\n",
      "  learn_time_ms: 13813.8\n",
      "  load_throughput: 1046932.788\n",
      "  load_time_ms: 7.622\n",
      "  sample_throughput: 531.068\n",
      "  sample_time_ms: 15026.317\n",
      "  update_time_ms: 5.966\n",
      "timestamp: 1643540047\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1923180\n",
      "training_iteration: 241\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1939110\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-54-39\n",
      "done: false\n",
      "episode_len_mean: 247.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 39\n",
      "episodes_total: 7408\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5699495115379493\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014029784586227834\n",
      "        policy_loss: -0.03863860887164871\n",
      "        total_loss: 71.60771843910217\n",
      "        vf_explained_var: 0.05254185974597931\n",
      "        vf_loss: 71.63688766638438\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5652014829715093\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013502813819695803\n",
      "        policy_loss: -0.09577509430547555\n",
      "        total_loss: 87.93817183335622\n",
      "        vf_explained_var: 0.12876583387454352\n",
      "        vf_loss: 88.02483265558878\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.528730133275191\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014979007644793494\n",
      "        policy_loss: -0.07577154433044295\n",
      "        total_loss: 77.98855340798696\n",
      "        vf_explained_var: 0.09665414174397786\n",
      "        vf_loss: 78.05421416123708\n",
      "  num_agent_steps_sampled: 1939110\n",
      "  num_agent_steps_trained: 1939110\n",
      "  num_steps_sampled: 1939140\n",
      "  num_steps_trained: 1939140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 243\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.511111111111111\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.59999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.0\n",
      "  player_1: 41.0\n",
      "  player_2: 57.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 2.093333333333333\n",
      "  player_1: 0.5833333333333335\n",
      "  player_2: 0.3233333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -62.0\n",
      "  player_1: -59.66666666666667\n",
      "  player_2: -57.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08842971204221267\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.32208421771198625\n",
      "  mean_inference_ms: 1.6458528349590444\n",
      "  mean_raw_obs_processing_ms: 0.21196650661463995\n",
      "time_since_restore: 3854.0852971076965\n",
      "time_this_iter_s: 14.81403923034668\n",
      "time_total_s: 3854.0852971076965\n",
      "timers:\n",
      "  learn_throughput: 566.28\n",
      "  learn_time_ms: 14091.957\n",
      "  load_throughput: 1021904.672\n",
      "  load_time_ms: 7.809\n",
      "  sample_throughput: 521.48\n",
      "  sample_time_ms: 15302.587\n",
      "  update_time_ms: 5.988\n",
      "timestamp: 1643540079\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1939140\n",
      "training_iteration: 243\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1955072\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-55-10\n",
      "done: false\n",
      "episode_len_mean: 235.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 31\n",
      "episodes_total: 7474\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5276817036171754\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016420956446036142\n",
      "        policy_loss: -0.12229011481627822\n",
      "        total_loss: 59.274445577462515\n",
      "        vf_explained_var: 0.10578865458567938\n",
      "        vf_loss: 59.38565169413884\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5522983339925607\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016312103413553512\n",
      "        policy_loss: -0.06762391090393066\n",
      "        total_loss: 37.1538788441817\n",
      "        vf_explained_var: 0.15296951830387115\n",
      "        vf_loss: 37.21049199581146\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5395418214797973\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018776967693701182\n",
      "        policy_loss: -0.04638582813243071\n",
      "        total_loss: 30.793719721237817\n",
      "        vf_explained_var: 0.11552971005439758\n",
      "        vf_loss: 30.827431116104126\n",
      "  num_agent_steps_sampled: 1955072\n",
      "  num_agent_steps_trained: 1955072\n",
      "  num_steps_sampled: 1955100\n",
      "  num_steps_trained: 1955100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 245\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.96842105263158\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.60526315789473\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.0\n",
      "  player_1: 27.0\n",
      "  player_2: 57.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 0.7933333333333334\n",
      "  player_1: -2.1466666666666665\n",
      "  player_2: 4.353333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -37.666666666666664\n",
      "  player_1: -59.66666666666667\n",
      "  player_2: -38.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0885075560336043\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.32220789242018116\n",
      "  mean_inference_ms: 1.6469231564768654\n",
      "  mean_raw_obs_processing_ms: 0.212005221070497\n",
      "time_since_restore: 3885.422510623932\n",
      "time_this_iter_s: 15.028057336807251\n",
      "time_total_s: 3885.422510623932\n",
      "timers:\n",
      "  learn_throughput: 559.107\n",
      "  learn_time_ms: 14272.761\n",
      "  load_throughput: 1026206.499\n",
      "  load_time_ms: 7.776\n",
      "  sample_throughput: 514.17\n",
      "  sample_time_ms: 15520.162\n",
      "  update_time_ms: 5.486\n",
      "timestamp: 1643540110\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1955100\n",
      "training_iteration: 245\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1971030\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-55-41\n",
      "done: false\n",
      "episode_len_mean: 231.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 35\n",
      "episodes_total: 7549\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5669413875540098\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016451306984564554\n",
      "        policy_loss: -0.07839291917160153\n",
      "        total_loss: 54.86804294904073\n",
      "        vf_explained_var: 0.3003565882643064\n",
      "        vf_loss: 54.93533140420914\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5547559446096421\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01576853413379164\n",
      "        policy_loss: -0.09179933206799129\n",
      "        total_loss: 45.32224537531535\n",
      "        vf_explained_var: 0.09395405600468318\n",
      "        vf_loss: 45.40340108394623\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5614347841838996\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01557809616718771\n",
      "        policy_loss: -0.07382494973173986\n",
      "        total_loss: 73.35838338295619\n",
      "        vf_explained_var: -0.049368596871693926\n",
      "        vf_loss: 73.4216930826505\n",
      "  num_agent_steps_sampled: 1971030\n",
      "  num_agent_steps_trained: 1971030\n",
      "  num_steps_sampled: 1971060\n",
      "  num_steps_trained: 1971060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 247\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.660000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.625\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.33333333333333\n",
      "  player_1: 27.666666666666664\n",
      "  player_2: 53.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 0.5533333333333335\n",
      "  player_1: -3.376666666666667\n",
      "  player_2: 5.823333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -46.33333333333333\n",
      "  player_1: -58.66666666666667\n",
      "  player_2: -48.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08835650803012307\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.32137676615346017\n",
      "  mean_inference_ms: 1.644544314992006\n",
      "  mean_raw_obs_processing_ms: 0.21169141662926788\n",
      "time_since_restore: 3916.7119104862213\n",
      "time_this_iter_s: 15.46456241607666\n",
      "time_total_s: 3916.7119104862213\n",
      "timers:\n",
      "  learn_throughput: 552.714\n",
      "  learn_time_ms: 14437.857\n",
      "  load_throughput: 972376.078\n",
      "  load_time_ms: 8.207\n",
      "  sample_throughput: 510.643\n",
      "  sample_time_ms: 15627.345\n",
      "  update_time_ms: 5.573\n",
      "timestamp: 1643540141\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1971060\n",
      "training_iteration: 247\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1986992\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-56-15\n",
      "done: false\n",
      "episode_len_mean: 242.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 32\n",
      "episodes_total: 7609\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5313316415250301\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015309844525109119\n",
      "        policy_loss: -0.08487410602470238\n",
      "        total_loss: 76.72958485921224\n",
      "        vf_explained_var: 0.26990518818298975\n",
      "        vf_loss: 76.80412492434183\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5442561079065005\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016375814956615026\n",
      "        policy_loss: -0.0701162777468562\n",
      "        total_loss: 50.55415912707647\n",
      "        vf_explained_var: 0.295118562579155\n",
      "        vf_loss: 50.61322174866994\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5555392354726791\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016578149835113055\n",
      "        policy_loss: -0.07339962934454282\n",
      "        total_loss: 42.848332075277966\n",
      "        vf_explained_var: 0.14095572610696158\n",
      "        vf_loss: 42.91054146369299\n",
      "  num_agent_steps_sampled: 1986992\n",
      "  num_agent_steps_trained: 1986992\n",
      "  num_steps_sampled: 1987020\n",
      "  num_steps_trained: 1987020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 249\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.547619047619047\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.62380952380951\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.666666666666664\n",
      "  player_1: 43.0\n",
      "  player_2: 33.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.313333333333334\n",
      "  player_1: -0.6066666666666666\n",
      "  player_2: 1.2933333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -44.0\n",
      "  player_1: -28.333333333333332\n",
      "  player_2: -48.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08836880652676557\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.32067577045058565\n",
      "  mean_inference_ms: 1.641092116579954\n",
      "  mean_raw_obs_processing_ms: 0.21150096605924618\n",
      "time_since_restore: 3949.9238390922546\n",
      "time_this_iter_s: 16.93661618232727\n",
      "time_total_s: 3949.9238390922546\n",
      "timers:\n",
      "  learn_throughput: 539.421\n",
      "  learn_time_ms: 14793.639\n",
      "  load_throughput: 938096.872\n",
      "  load_time_ms: 8.507\n",
      "  sample_throughput: 503.545\n",
      "  sample_time_ms: 15847.654\n",
      "  update_time_ms: 8.022\n",
      "timestamp: 1643540175\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1987020\n",
      "training_iteration: 249\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2002950\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-56-47\n",
      "done: false\n",
      "episode_len_mean: 225.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 32\n",
      "episodes_total: 7681\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5351129006346067\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016155164540277836\n",
      "        policy_loss: -0.1028831585108613\n",
      "        total_loss: 41.75470079819361\n",
      "        vf_explained_var: 0.1550002854069074\n",
      "        vf_loss: 41.84667908906937\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5589080732564131\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015277824519181327\n",
      "        policy_loss: -0.06169880736619234\n",
      "        total_loss: 25.504761721690496\n",
      "        vf_explained_var: -0.005161663790543874\n",
      "        vf_loss: 25.556148089170456\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5337428592145443\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01527676626892666\n",
      "        policy_loss: -0.06003162488341331\n",
      "        total_loss: 40.60572276274363\n",
      "        vf_explained_var: 0.2640071475505829\n",
      "        vf_loss: 40.6554425406456\n",
      "  num_agent_steps_sampled: 2002950\n",
      "  num_agent_steps_trained: 2002950\n",
      "  num_steps_sampled: 2002980\n",
      "  num_steps_trained: 2002980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 251\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.910000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.59999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.0\n",
      "  player_1: 43.0\n",
      "  player_2: 33.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.476666666666667\n",
      "  player_1: -3.773333333333333\n",
      "  player_2: 4.296666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -41.0\n",
      "  player_1: -53.0\n",
      "  player_2: -32.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08831941636420386\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3210128297967853\n",
      "  mean_inference_ms: 1.6434442257601813\n",
      "  mean_raw_obs_processing_ms: 0.21163172819791798\n",
      "time_since_restore: 3982.603095293045\n",
      "time_this_iter_s: 15.993581056594849\n",
      "time_total_s: 3982.603095293045\n",
      "timers:\n",
      "  learn_throughput: 538.734\n",
      "  learn_time_ms: 14812.516\n",
      "  load_throughput: 986514.008\n",
      "  load_time_ms: 8.089\n",
      "  sample_throughput: 498.185\n",
      "  sample_time_ms: 16018.151\n",
      "  update_time_ms: 7.956\n",
      "timestamp: 1643540207\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2002980\n",
      "training_iteration: 251\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2018911\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-57-18\n",
      "done: false\n",
      "episode_len_mean: 221.28\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 45\n",
      "episodes_total: 7760\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5459334352612495\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015401845370685502\n",
      "        policy_loss: -0.08340692032439014\n",
      "        total_loss: 73.89380722204844\n",
      "        vf_explained_var: 0.22590128789345423\n",
      "        vf_loss: 73.96681782404582\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5887712144851684\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01716444325935337\n",
      "        policy_loss: -0.07644374166304867\n",
      "        total_loss: 106.77670444965362\n",
      "        vf_explained_var: 0.19929088870684306\n",
      "        vf_loss: 106.8415621026357\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5575893091162046\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01578219329268715\n",
      "        policy_loss: -0.08636502475167314\n",
      "        total_loss: 75.44122924486796\n",
      "        vf_explained_var: 0.21103305518627166\n",
      "        vf_loss: 75.5169412390391\n",
      "  num_agent_steps_sampled: 2018911\n",
      "  num_agent_steps_trained: 2018911\n",
      "  num_steps_sampled: 2018940\n",
      "  num_steps_trained: 2018940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 253\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.56111111111111\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.59999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.66666666666667\n",
      "  player_1: 41.333333333333336\n",
      "  player_2: 50.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 1.7866666666666662\n",
      "  player_1: -2.533333333333333\n",
      "  player_2: 3.746666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -36.33333333333333\n",
      "  player_1: -82.33333333333333\n",
      "  player_2: -35.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08825278309829518\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3211128605793083\n",
      "  mean_inference_ms: 1.6447573716041652\n",
      "  mean_raw_obs_processing_ms: 0.21181260246588693\n",
      "time_since_restore: 4012.6815474033356\n",
      "time_this_iter_s: 14.471144914627075\n",
      "time_total_s: 4012.6815474033356\n",
      "timers:\n",
      "  learn_throughput: 544.944\n",
      "  learn_time_ms: 14643.696\n",
      "  load_throughput: 993188.326\n",
      "  load_time_ms: 8.035\n",
      "  sample_throughput: 501.07\n",
      "  sample_time_ms: 15925.934\n",
      "  update_time_ms: 7.918\n",
      "timestamp: 1643540238\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2018940\n",
      "training_iteration: 253\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2034870\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-57-48\n",
      "done: false\n",
      "episode_len_mean: 204.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 35\n",
      "episodes_total: 7830\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5535094932715098\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015823975746174786\n",
      "        policy_loss: -0.08737874152759711\n",
      "        total_loss: 81.52672177791595\n",
      "        vf_explained_var: 0.14952879478534062\n",
      "        vf_loss: 81.60341974576315\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5439677821596464\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016121084450095774\n",
      "        policy_loss: -0.06938753167788188\n",
      "        total_loss: 57.88479910214742\n",
      "        vf_explained_var: 0.13300319840510685\n",
      "        vf_loss: 57.943304931322736\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5475397084653377\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015741713831151857\n",
      "        policy_loss: -0.079967916191866\n",
      "        total_loss: 66.65338292837143\n",
      "        vf_explained_var: 0.22179487625757854\n",
      "        vf_loss: 66.72272498528163\n",
      "  num_agent_steps_sampled: 2034870\n",
      "  num_agent_steps_trained: 2034870\n",
      "  num_steps_sampled: 2034900\n",
      "  num_steps_trained: 2034900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 255\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.257894736842108\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.59999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 48.33333333333333\n",
      "  player_1: 41.333333333333336\n",
      "  player_2: 36.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.6799999999999997\n",
      "  player_1: -3.6399999999999992\n",
      "  player_2: 4.960000000000001\n",
      "policy_reward_min:\n",
      "  player_0: -36.666666666666664\n",
      "  player_1: -58.33333333333333\n",
      "  player_2: -59.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0882639222004084\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3208814885777509\n",
      "  mean_inference_ms: 1.6441914807061462\n",
      "  mean_raw_obs_processing_ms: 0.2115551456397131\n",
      "time_since_restore: 4042.8317074775696\n",
      "time_this_iter_s: 15.200748920440674\n",
      "time_total_s: 4042.8317074775696\n",
      "timers:\n",
      "  learn_throughput: 549.297\n",
      "  learn_time_ms: 14527.658\n",
      "  load_throughput: 993356.342\n",
      "  load_time_ms: 8.033\n",
      "  sample_throughput: 506.455\n",
      "  sample_time_ms: 15756.571\n",
      "  update_time_ms: 8.618\n",
      "timestamp: 1643540268\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2034900\n",
      "training_iteration: 255\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2050831\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-58-17\n",
      "done: false\n",
      "episode_len_mean: 233.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 40\n",
      "episodes_total: 7898\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5332215695579847\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016151401670668218\n",
      "        policy_loss: -0.08025553541878859\n",
      "        total_loss: 61.49968568483988\n",
      "        vf_explained_var: 0.17151911636193592\n",
      "        vf_loss: 61.56903892834981\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5707600936790307\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014877365669505689\n",
      "        policy_loss: -0.0965365117819359\n",
      "        total_loss: 45.4541727177302\n",
      "        vf_explained_var: 0.006069204906622569\n",
      "        vf_loss: 45.54066692352295\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5542627255121867\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014713613460917353\n",
      "        policy_loss: -0.06731939123322567\n",
      "        total_loss: 69.28411746819815\n",
      "        vf_explained_var: 0.15772096186876297\n",
      "        vf_loss: 69.34150509357453\n",
      "  num_agent_steps_sampled: 2050831\n",
      "  num_agent_steps_trained: 2050831\n",
      "  num_steps_sampled: 2050860\n",
      "  num_steps_trained: 2050860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 257\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.711111111111116\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.59999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 48.33333333333333\n",
      "  player_1: 29.666666666666664\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.9633333333333338\n",
      "  player_1: -3.3866666666666676\n",
      "  player_2: 4.423333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -38.0\n",
      "  player_1: -39.666666666666664\n",
      "  player_2: -59.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08816123310331919\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3200888821034076\n",
      "  mean_inference_ms: 1.6408204611660764\n",
      "  mean_raw_obs_processing_ms: 0.21113377443569858\n",
      "time_since_restore: 4072.233944416046\n",
      "time_this_iter_s: 14.838431596755981\n",
      "time_total_s: 4072.233944416046\n",
      "timers:\n",
      "  learn_throughput: 556.402\n",
      "  learn_time_ms: 14342.148\n",
      "  load_throughput: 994631.571\n",
      "  load_time_ms: 8.023\n",
      "  sample_throughput: 510.168\n",
      "  sample_time_ms: 15641.898\n",
      "  update_time_ms: 8.581\n",
      "timestamp: 1643540297\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2050860\n",
      "training_iteration: 257\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2066792\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-58-48\n",
      "done: false\n",
      "episode_len_mean: 223.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 33\n",
      "episodes_total: 7972\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5503622630735239\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014213316506583737\n",
      "        policy_loss: -0.04914687742323925\n",
      "        total_loss: 78.65030230840047\n",
      "        vf_explained_var: 0.10902276217937469\n",
      "        vf_loss: 78.68985603491465\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.56688231681784\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015500086124166803\n",
      "        policy_loss: -0.09338091868131111\n",
      "        total_loss: 39.52457300424576\n",
      "        vf_explained_var: 0.19506249914566676\n",
      "        vf_loss: 39.6074916601181\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.540312141875426\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01543802016806391\n",
      "        policy_loss: -0.0777214860310778\n",
      "        total_loss: 64.50114485581716\n",
      "        vf_explained_var: -0.13861328154802321\n",
      "        vf_loss: 64.56844553629557\n",
      "  num_agent_steps_sampled: 2066792\n",
      "  num_agent_steps_trained: 2066792\n",
      "  num_steps_sampled: 2066820\n",
      "  num_steps_trained: 2066820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 259\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.811111111111112\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.60555555555555\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 41.666666666666664\n",
      "  player_1: 39.0\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.3633333333333333\n",
      "  player_1: -1.5066666666666668\n",
      "  player_2: 2.143333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -49.0\n",
      "  player_1: -36.333333333333336\n",
      "  player_2: -47.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08803474285219685\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3193884374134394\n",
      "  mean_inference_ms: 1.6392392451650928\n",
      "  mean_raw_obs_processing_ms: 0.2110323153822432\n",
      "time_since_restore: 4103.307390213013\n",
      "time_this_iter_s: 14.63158917427063\n",
      "time_total_s: 4103.307390213013\n",
      "timers:\n",
      "  learn_throughput: 564.891\n",
      "  learn_time_ms: 14126.631\n",
      "  load_throughput: 1041055.094\n",
      "  load_time_ms: 7.665\n",
      "  sample_throughput: 511.372\n",
      "  sample_time_ms: 15605.092\n",
      "  update_time_ms: 6.608\n",
      "timestamp: 1643540328\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2066820\n",
      "training_iteration: 259\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2082752\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-59-18\n",
      "done: false\n",
      "episode_len_mean: 202.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 42\n",
      "episodes_total: 8050\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5324565489093462\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014482251688150427\n",
      "        policy_loss: -0.03652687436590592\n",
      "        total_loss: 96.27165536721547\n",
      "        vf_explained_var: 0.2070010773340861\n",
      "        vf_loss: 96.29840659936269\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5353468676904837\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012964330797310263\n",
      "        policy_loss: -0.07033796538909276\n",
      "        total_loss: 83.02562651077906\n",
      "        vf_explained_var: 0.24848742087682088\n",
      "        vf_loss: 83.08721371809642\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5455684299270313\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015398943045517323\n",
      "        policy_loss: -0.10790999800898135\n",
      "        total_loss: 48.88918158372243\n",
      "        vf_explained_var: 0.11774150401353836\n",
      "        vf_loss: 48.986697398026784\n",
      "  num_agent_steps_sampled: 2082752\n",
      "  num_agent_steps_trained: 2082752\n",
      "  num_steps_sampled: 2082780\n",
      "  num_steps_trained: 2082780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 261\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.933333333333334\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.7\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.66666666666667\n",
      "  player_1: 26.333333333333336\n",
      "  player_2: 45.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 1.2466666666666668\n",
      "  player_1: -3.9433333333333325\n",
      "  player_2: 5.696666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -39.0\n",
      "  player_1: -82.33333333333333\n",
      "  player_2: -42.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08811272554788585\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.31970975109992705\n",
      "  mean_inference_ms: 1.6415832207148582\n",
      "  mean_raw_obs_processing_ms: 0.2116506772207608\n",
      "time_since_restore: 4133.35121178627\n",
      "time_this_iter_s: 15.155610799789429\n",
      "time_total_s: 4133.35121178627\n",
      "timers:\n",
      "  learn_throughput: 577.311\n",
      "  learn_time_ms: 13822.705\n",
      "  load_throughput: 1041161.961\n",
      "  load_time_ms: 7.665\n",
      "  sample_throughput: 524.019\n",
      "  sample_time_ms: 15228.453\n",
      "  update_time_ms: 6.567\n",
      "timestamp: 1643540358\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2082780\n",
      "training_iteration: 261\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2098710\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_11-59-48\n",
      "done: false\n",
      "episode_len_mean: 226.54\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 33\n",
      "episodes_total: 8116\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5263615876436234\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01739609492193703\n",
      "        policy_loss: -0.07665782591793686\n",
      "        total_loss: 44.67942554076512\n",
      "        vf_explained_var: 0.3949336692690849\n",
      "        vf_loss: 44.74434086084366\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5416063671807448\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015587974498143921\n",
      "        policy_loss: -0.08570349507654706\n",
      "        total_loss: 44.345397950808206\n",
      "        vf_explained_var: 0.31073353250821434\n",
      "        vf_loss: 44.42057951370875\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5299241074422996\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015703543391718332\n",
      "        policy_loss: -0.07145041808563594\n",
      "        total_loss: 52.78647884925206\n",
      "        vf_explained_var: 0.08907411009073257\n",
      "        vf_loss: 52.8473295545578\n",
      "  num_agent_steps_sampled: 2098710\n",
      "  num_agent_steps_trained: 2098710\n",
      "  num_steps_sampled: 2098740\n",
      "  num_steps_trained: 2098740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 263\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.972222222222221\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.7\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.66666666666667\n",
      "  player_1: 41.0\n",
      "  player_2: 45.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 2.3233333333333333\n",
      "  player_1: -4.626666666666666\n",
      "  player_2: 5.303333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -44.666666666666664\n",
      "  player_1: -82.33333333333333\n",
      "  player_2: -54.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08810152571491724\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.31916264669726807\n",
      "  mean_inference_ms: 1.641018609036826\n",
      "  mean_raw_obs_processing_ms: 0.21145592507679833\n",
      "time_since_restore: 4163.018374681473\n",
      "time_this_iter_s: 14.688804864883423\n",
      "time_total_s: 4163.018374681473\n",
      "timers:\n",
      "  learn_throughput: 579.027\n",
      "  learn_time_ms: 13781.735\n",
      "  load_throughput: 1065384.51\n",
      "  load_time_ms: 7.49\n",
      "  sample_throughput: 530.554\n",
      "  sample_time_ms: 15040.894\n",
      "  update_time_ms: 6.692\n",
      "timestamp: 1643540388\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2098740\n",
      "training_iteration: 263\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2114670\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-00-18\n",
      "done: false\n",
      "episode_len_mean: 210.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 40\n",
      "episodes_total: 8201\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.568775911529859\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018690425489727904\n",
      "        policy_loss: -0.09351338170468808\n",
      "        total_loss: 64.09403599103292\n",
      "        vf_explained_var: 0.10399252752463023\n",
      "        vf_loss: 64.17493356227875\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5799730581541857\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01844139841321521\n",
      "        policy_loss: -0.09336628802663957\n",
      "        total_loss: 50.696646286646526\n",
      "        vf_explained_var: 0.10754537771145503\n",
      "        vf_loss: 50.77756439844767\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5100008253256479\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01656815464136192\n",
      "        policy_loss: -0.08757797642610968\n",
      "        total_loss: 31.602628979682923\n",
      "        vf_explained_var: 0.05158242324988047\n",
      "        vf_loss: 31.679023543993633\n",
      "  num_agent_steps_sampled: 2114670\n",
      "  num_agent_steps_trained: 2114670\n",
      "  num_steps_sampled: 2114700\n",
      "  num_steps_trained: 2114700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 265\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.052631578947365\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.7\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.0\n",
      "  player_1: 32.66666666666667\n",
      "  player_2: 45.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.1866666666666667\n",
      "  player_1: -3.3966666666666665\n",
      "  player_2: 6.583333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -43.666666666666664\n",
      "  player_1: -46.333333333333336\n",
      "  player_2: -46.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08808569755256068\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3193442277071675\n",
      "  mean_inference_ms: 1.6428256334522622\n",
      "  mean_raw_obs_processing_ms: 0.21160008546203676\n",
      "time_since_restore: 4192.902468919754\n",
      "time_this_iter_s: 15.0589120388031\n",
      "time_total_s: 4192.902468919754\n",
      "timers:\n",
      "  learn_throughput: 580.048\n",
      "  learn_time_ms: 13757.477\n",
      "  load_throughput: 1085747.195\n",
      "  load_time_ms: 7.35\n",
      "  sample_throughput: 530.246\n",
      "  sample_time_ms: 15049.62\n",
      "  update_time_ms: 5.976\n",
      "timestamp: 1643540418\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2114700\n",
      "training_iteration: 265\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2130630\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-00-50\n",
      "done: false\n",
      "episode_len_mean: 197.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 38\n",
      "episodes_total: 8283\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5240153869489829\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015575389059144129\n",
      "        policy_loss: -0.040848880056291816\n",
      "        total_loss: 46.47758873780568\n",
      "        vf_explained_var: 0.07080810437599817\n",
      "        vf_loss: 46.50792418162028\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5874089123308659\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01703473483722244\n",
      "        policy_loss: -0.08344852252397686\n",
      "        total_loss: 42.22921126921972\n",
      "        vf_explained_var: 0.0784193750222524\n",
      "        vf_loss: 42.30116131146749\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.530933000644048\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015318874090593036\n",
      "        policy_loss: -0.10845586023293435\n",
      "        total_loss: 47.77401352564494\n",
      "        vf_explained_var: 0.13444874475399654\n",
      "        vf_loss: 47.87212889830271\n",
      "  num_agent_steps_sampled: 2130630\n",
      "  num_agent_steps_trained: 2130630\n",
      "  num_steps_sampled: 2130660\n",
      "  num_steps_trained: 2130660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 267\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.410526315789474\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.7\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.333333333333336\n",
      "  player_1: 31.666666666666664\n",
      "  player_2: 49.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 0.2766666666666667\n",
      "  player_1: -2.163333333333333\n",
      "  player_2: 4.886666666666668\n",
      "policy_reward_min:\n",
      "  player_0: -66.66666666666667\n",
      "  player_1: -29.0\n",
      "  player_2: -23.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0881261407728918\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.31887373728377644\n",
      "  mean_inference_ms: 1.640944770763781\n",
      "  mean_raw_obs_processing_ms: 0.21171739246633162\n",
      "time_since_restore: 4224.351417064667\n",
      "time_this_iter_s: 15.2608163356781\n",
      "time_total_s: 4224.351417064667\n",
      "timers:\n",
      "  learn_throughput: 571.573\n",
      "  learn_time_ms: 13961.468\n",
      "  load_throughput: 1159691.283\n",
      "  load_time_ms: 6.881\n",
      "  sample_throughput: 524.876\n",
      "  sample_time_ms: 15203.585\n",
      "  update_time_ms: 6.081\n",
      "timestamp: 1643540450\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2130660\n",
      "training_iteration: 267\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2146590\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-01-23\n",
      "done: false\n",
      "episode_len_mean: 186.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 38\n",
      "episodes_total: 8366\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5605973621209462\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015044235153952552\n",
      "        policy_loss: -0.053842516609778006\n",
      "        total_loss: 83.77860206365585\n",
      "        vf_explained_var: 0.14685361901919047\n",
      "        vf_loss: 83.82229054768881\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.54817253763477\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014314239062863408\n",
      "        policy_loss: -0.10580001870015016\n",
      "        total_loss: 40.777723124027254\n",
      "        vf_explained_var: 0.008145957092444102\n",
      "        vf_loss: 40.873860886096956\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5147828998665015\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015598970455406137\n",
      "        policy_loss: -0.0695876386637489\n",
      "        total_loss: 43.81217548688253\n",
      "        vf_explained_var: -0.0069910267988840735\n",
      "        vf_loss: 43.87123404026031\n",
      "  num_agent_steps_sampled: 2146590\n",
      "  num_agent_steps_trained: 2146590\n",
      "  num_steps_sampled: 2146620\n",
      "  num_steps_trained: 2146620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 269\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.394736842105264\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.7\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.333333333333336\n",
      "  player_1: 35.333333333333336\n",
      "  player_2: 30.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.78\n",
      "  player_1: -2.7200000000000006\n",
      "  player_2: 2.9399999999999995\n",
      "policy_reward_min:\n",
      "  player_0: -50.666666666666664\n",
      "  player_1: -34.333333333333336\n",
      "  player_2: -42.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08782436106579924\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3173044235575178\n",
      "  mean_inference_ms: 1.6345967674591106\n",
      "  mean_raw_obs_processing_ms: 0.21106588333764656\n",
      "time_since_restore: 4257.148509025574\n",
      "time_this_iter_s: 15.87795352935791\n",
      "time_total_s: 4257.148509025574\n",
      "timers:\n",
      "  learn_throughput: 564.585\n",
      "  learn_time_ms: 14134.264\n",
      "  load_throughput: 1085106.561\n",
      "  load_time_ms: 7.354\n",
      "  sample_throughput: 521.492\n",
      "  sample_time_ms: 15302.244\n",
      "  update_time_ms: 5.748\n",
      "timestamp: 1643540483\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2146620\n",
      "training_iteration: 269\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2162550\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-01-52\n",
      "done: false\n",
      "episode_len_mean: 198.62\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 36\n",
      "episodes_total: 8444\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5522675045331319\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01634619665734647\n",
      "        policy_loss: -0.08196196470099192\n",
      "        total_loss: 67.1404362265269\n",
      "        vf_explained_var: 0.2260500407218933\n",
      "        vf_loss: 67.2113644393285\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5673355899751187\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013462724789975481\n",
      "        policy_loss: -0.05469799661388\n",
      "        total_loss: 42.7196790043513\n",
      "        vf_explained_var: 0.01338932236035665\n",
      "        vf_loss: 42.76528949022293\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.522465246617794\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01571134280349497\n",
      "        policy_loss: -0.08269366258289665\n",
      "        total_loss: 56.78777408917745\n",
      "        vf_explained_var: 0.11003942698240281\n",
      "        vf_loss: 56.859862494468686\n",
      "  num_agent_steps_sampled: 2162550\n",
      "  num_agent_steps_trained: 2162550\n",
      "  num_steps_sampled: 2162580\n",
      "  num_steps_trained: 2162580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 271\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.06842105263158\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.7\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.666666666666664\n",
      "  player_1: 33.333333333333336\n",
      "  player_2: 39.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.123333333333333\n",
      "  player_1: -1.9566666666666666\n",
      "  player_2: 2.8333333333333326\n",
      "policy_reward_min:\n",
      "  player_0: -34.0\n",
      "  player_1: -60.0\n",
      "  player_2: -39.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0877824476206425\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3177608934368379\n",
      "  mean_inference_ms: 1.636354097772512\n",
      "  mean_raw_obs_processing_ms: 0.21135602959435143\n",
      "time_since_restore: 4286.793030500412\n",
      "time_this_iter_s: 14.970643520355225\n",
      "time_total_s: 4286.793030500412\n",
      "timers:\n",
      "  learn_throughput: 564.588\n",
      "  learn_time_ms: 14134.21\n",
      "  load_throughput: 1067821.543\n",
      "  load_time_ms: 7.473\n",
      "  sample_throughput: 519.316\n",
      "  sample_time_ms: 15366.359\n",
      "  update_time_ms: 5.827\n",
      "timestamp: 1643540512\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2162580\n",
      "training_iteration: 271\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2178514\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-02-24\n",
      "done: false\n",
      "episode_len_mean: 191.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 41\n",
      "episodes_total: 8534\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5308412553866705\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015224218461456284\n",
      "        policy_loss: -0.047868770788578936\n",
      "        total_loss: 68.00496210813522\n",
      "        vf_explained_var: -0.07398695796728134\n",
      "        vf_loss: 68.04255521615346\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5842792130510013\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013953714319998057\n",
      "        policy_loss: -0.08921005285345018\n",
      "        total_loss: 78.77166983048122\n",
      "        vf_explained_var: 0.022651889224847156\n",
      "        vf_loss: 78.85146113077799\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5057141974071662\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012365632488891455\n",
      "        policy_loss: -0.0727605143468827\n",
      "        total_loss: 89.87422207832337\n",
      "        vf_explained_var: 0.026724313696225483\n",
      "        vf_loss: 89.93863539536794\n",
      "  num_agent_steps_sampled: 2178514\n",
      "  num_agent_steps_trained: 2178514\n",
      "  num_steps_sampled: 2178540\n",
      "  num_steps_trained: 2178540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 273\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.989473684210527\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.7\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 44.333333333333336\n",
      "  player_2: 39.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.3766666666666665\n",
      "  player_1: -3.6833333333333336\n",
      "  player_2: 3.306666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -38.333333333333336\n",
      "  player_1: -60.0\n",
      "  player_2: -61.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08801995475890408\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3179886821192463\n",
      "  mean_inference_ms: 1.637671751415903\n",
      "  mean_raw_obs_processing_ms: 0.21146668688060122\n",
      "time_since_restore: 4318.283790588379\n",
      "time_this_iter_s: 14.958766222000122\n",
      "time_total_s: 4318.283790588379\n",
      "timers:\n",
      "  learn_throughput: 557.399\n",
      "  learn_time_ms: 14316.503\n",
      "  load_throughput: 1021056.73\n",
      "  load_time_ms: 7.815\n",
      "  sample_throughput: 513.427\n",
      "  sample_time_ms: 15542.609\n",
      "  update_time_ms: 5.722\n",
      "timestamp: 1643540544\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2178540\n",
      "training_iteration: 273\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2194472\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-02-57\n",
      "done: false\n",
      "episode_len_mean: 207.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 38\n",
      "episodes_total: 8610\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5349820072948933\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015518127793190312\n",
      "        policy_loss: -0.06160680608203014\n",
      "        total_loss: 50.28715914408366\n",
      "        vf_explained_var: 0.07904666543006897\n",
      "        vf_loss: 50.33829125563304\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5649085269868374\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017018399413344317\n",
      "        policy_loss: -0.09066116323073704\n",
      "        total_loss: 30.84830650607745\n",
      "        vf_explained_var: 0.01159745713075002\n",
      "        vf_loss: 30.927480132579802\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5428155868252118\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016574868599675482\n",
      "        policy_loss: -0.09789611482061446\n",
      "        total_loss: 36.98884907325109\n",
      "        vf_explained_var: -0.1274937331676483\n",
      "        vf_loss: 37.07555722475052\n",
      "  num_agent_steps_sampled: 2194472\n",
      "  num_agent_steps_trained: 2194472\n",
      "  num_steps_sampled: 2194500\n",
      "  num_steps_trained: 2194500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 275\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.257142857142856\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.70000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 44.333333333333336\n",
      "  player_2: 34.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.31\n",
      "  player_1: -1.9300000000000004\n",
      "  player_2: 3.619999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -40.0\n",
      "  player_1: -54.333333333333336\n",
      "  player_2: -61.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08779708324099846\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.31657807600447574\n",
      "  mean_inference_ms: 1.6323972890336955\n",
      "  mean_raw_obs_processing_ms: 0.21107274986187352\n",
      "time_since_restore: 4351.226588487625\n",
      "time_this_iter_s: 16.546324968338013\n",
      "time_total_s: 4351.226588487625\n",
      "timers:\n",
      "  learn_throughput: 545.678\n",
      "  learn_time_ms: 14624.005\n",
      "  load_throughput: 957105.28\n",
      "  load_time_ms: 8.338\n",
      "  sample_throughput: 507.439\n",
      "  sample_time_ms: 15726.018\n",
      "  update_time_ms: 5.719\n",
      "timestamp: 1643540577\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2194500\n",
      "training_iteration: 275\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2210430\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-03-29\n",
      "done: false\n",
      "episode_len_mean: 194.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 36\n",
      "episodes_total: 8689\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5316404682397843\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01515403509475154\n",
      "        policy_loss: -0.07899920095689594\n",
      "        total_loss: 70.21446608543395\n",
      "        vf_explained_var: 0.22418105433384578\n",
      "        vf_loss: 70.28323628902436\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.6049841137230396\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017323652936261168\n",
      "        policy_loss: -0.07233739075095703\n",
      "        total_loss: 64.22994303544363\n",
      "        vf_explained_var: 0.27682070920864743\n",
      "        vf_loss: 64.29058670361837\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5318383015692234\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014241493302058454\n",
      "        policy_loss: -0.09174497498044124\n",
      "        total_loss: 63.294778876304626\n",
      "        vf_explained_var: 0.07105516135692597\n",
      "        vf_loss: 63.37691078344981\n",
      "  num_agent_steps_sampled: 2210430\n",
      "  num_agent_steps_trained: 2210430\n",
      "  num_steps_sampled: 2210460\n",
      "  num_steps_trained: 2210460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 277\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.084999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.70000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 49.0\n",
      "  player_1: 27.0\n",
      "  player_2: 49.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.5066666666666664\n",
      "  player_1: -6.413333333333334\n",
      "  player_2: 6.906666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -46.33333333333333\n",
      "  player_1: -95.0\n",
      "  player_2: -47.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0878832003725369\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3173095156319774\n",
      "  mean_inference_ms: 1.6357969366152167\n",
      "  mean_raw_obs_processing_ms: 0.21120609237393453\n",
      "time_since_restore: 4383.023763418198\n",
      "time_this_iter_s: 16.246780395507812\n",
      "time_total_s: 4383.023763418198\n",
      "timers:\n",
      "  learn_throughput: 544.316\n",
      "  learn_time_ms: 14660.599\n",
      "  load_throughput: 943377.046\n",
      "  load_time_ms: 8.459\n",
      "  sample_throughput: 504.813\n",
      "  sample_time_ms: 15807.824\n",
      "  update_time_ms: 5.579\n",
      "timestamp: 1643540609\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2210460\n",
      "training_iteration: 277\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2226392\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-03-59\n",
      "done: false\n",
      "episode_len_mean: 205.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 37\n",
      "episodes_total: 8769\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49529904142022135\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014187781130191392\n",
      "        policy_loss: -0.1107482728920877\n",
      "        total_loss: 58.236829398473105\n",
      "        vf_explained_var: 0.08262340654929479\n",
      "        vf_loss: 58.338000733057655\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.576519502600034\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01381435406129962\n",
      "        policy_loss: -0.03913843778582911\n",
      "        total_loss: 99.40281505028406\n",
      "        vf_explained_var: 0.19697397698958716\n",
      "        vf_loss: 99.43262962897619\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5166717126468817\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015089758899939246\n",
      "        policy_loss: -0.07202619166423876\n",
      "        total_loss: 57.48335674126943\n",
      "        vf_explained_var: 0.06207896242539088\n",
      "        vf_loss: 57.545197321573895\n",
      "  num_agent_steps_sampled: 2226392\n",
      "  num_agent_steps_trained: 2226392\n",
      "  num_steps_sampled: 2226420\n",
      "  num_steps_trained: 2226420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 279\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.7\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.74499999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.33333333333333\n",
      "  player_1: 37.333333333333336\n",
      "  player_2: 33.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.163333333333333\n",
      "  player_1: -2.1066666666666674\n",
      "  player_2: 2.943333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -46.666666666666664\n",
      "  player_1: -61.66666666666667\n",
      "  player_2: -37.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08788956820208949\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3173652324779248\n",
      "  mean_inference_ms: 1.636847577205301\n",
      "  mean_raw_obs_processing_ms: 0.2111829682741874\n",
      "time_since_restore: 4413.512588977814\n",
      "time_this_iter_s: 15.818296194076538\n",
      "time_total_s: 4413.512588977814\n",
      "timers:\n",
      "  learn_throughput: 553.015\n",
      "  learn_time_ms: 14429.982\n",
      "  load_throughput: 992004.989\n",
      "  load_time_ms: 8.044\n",
      "  sample_throughput: 509.276\n",
      "  sample_time_ms: 15669.318\n",
      "  update_time_ms: 5.494\n",
      "timestamp: 1643540639\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2226420\n",
      "training_iteration: 279\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2242352\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-04-30\n",
      "done: false\n",
      "episode_len_mean: 223.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 39\n",
      "episodes_total: 8841\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5360043063759804\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015528641578690288\n",
      "        policy_loss: -0.10965867425935964\n",
      "        total_loss: 68.46566520214081\n",
      "        vf_explained_var: 0.17288229356209436\n",
      "        vf_loss: 68.56484186490377\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.563838280091683\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01424390598773949\n",
      "        policy_loss: -0.09650296594481915\n",
      "        total_loss: 68.2641584388415\n",
      "        vf_explained_var: 0.09316048433383306\n",
      "        vf_loss: 68.35104724645615\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5368042962253093\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01567760355351614\n",
      "        policy_loss: -0.029378975598762434\n",
      "        total_loss: 57.48880186398824\n",
      "        vf_explained_var: 0.13149073233207068\n",
      "        vf_loss: 57.50759841759999\n",
      "  num_agent_steps_sampled: 2242352\n",
      "  num_agent_steps_trained: 2242352\n",
      "  num_steps_sampled: 2242380\n",
      "  num_steps_trained: 2242380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 281\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.38888888888889\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.7\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.666666666666664\n",
      "  player_1: 35.66666666666667\n",
      "  player_2: 33.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.79\n",
      "  player_1: -2.9800000000000004\n",
      "  player_2: 2.1900000000000004\n",
      "policy_reward_min:\n",
      "  player_0: -40.0\n",
      "  player_1: -53.0\n",
      "  player_2: -51.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08769926085211903\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3168614304832467\n",
      "  mean_inference_ms: 1.6359525813708973\n",
      "  mean_raw_obs_processing_ms: 0.2110220555743797\n",
      "time_since_restore: 4443.947340011597\n",
      "time_this_iter_s: 14.84987473487854\n",
      "time_total_s: 4443.947340011597\n",
      "timers:\n",
      "  learn_throughput: 550.183\n",
      "  learn_time_ms: 14504.258\n",
      "  load_throughput: 964499.558\n",
      "  load_time_ms: 8.274\n",
      "  sample_throughput: 506.419\n",
      "  sample_time_ms: 15757.694\n",
      "  update_time_ms: 5.396\n",
      "timestamp: 1643540670\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2242380\n",
      "training_iteration: 281\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2258310\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-05-01\n",
      "done: false\n",
      "episode_len_mean: 199.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 39\n",
      "episodes_total: 8923\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5328412462274233\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01558153832184568\n",
      "        policy_loss: -0.11839358041373392\n",
      "        total_loss: 69.46359851916631\n",
      "        vf_explained_var: -0.04025990873575211\n",
      "        vf_loss: 69.57147474288941\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5505171413222949\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01628521969858904\n",
      "        policy_loss: -0.03504162957658991\n",
      "        total_loss: 38.135307963689165\n",
      "        vf_explained_var: 0.04554560959339142\n",
      "        vf_loss: 38.15935710271199\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5139971173306306\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016677190614939264\n",
      "        policy_loss: -0.05768169280917694\n",
      "        total_loss: 61.98913690010706\n",
      "        vf_explained_var: 0.3169062521060308\n",
      "        vf_loss: 62.035561943848926\n",
      "  num_agent_steps_sampled: 2258310\n",
      "  num_agent_steps_trained: 2258310\n",
      "  num_steps_sampled: 2258340\n",
      "  num_steps_trained: 2258340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 283\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.1\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.7\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.333333333333336\n",
      "  player_1: 35.66666666666667\n",
      "  player_2: 39.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.1233333333333335\n",
      "  player_1: -4.596666666666667\n",
      "  player_2: 4.473333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -39.666666666666664\n",
      "  player_1: -62.333333333333336\n",
      "  player_2: -51.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0875885643369246\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3162690378480416\n",
      "  mean_inference_ms: 1.6339082683150499\n",
      "  mean_raw_obs_processing_ms: 0.2108763829138701\n",
      "time_since_restore: 4475.577146530151\n",
      "time_this_iter_s: 15.65877914428711\n",
      "time_total_s: 4475.577146530151\n",
      "timers:\n",
      "  learn_throughput: 549.646\n",
      "  learn_time_ms: 14518.428\n",
      "  load_throughput: 956383.288\n",
      "  load_time_ms: 8.344\n",
      "  sample_throughput: 508.847\n",
      "  sample_time_ms: 15682.528\n",
      "  update_time_ms: 5.395\n",
      "timestamp: 1643540701\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2258340\n",
      "training_iteration: 283\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2274272\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-05-33\n",
      "done: false\n",
      "episode_len_mean: 186.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 41\n",
      "episodes_total: 9007\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5564726253847281\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015716324757778656\n",
      "        policy_loss: -0.06816816400891791\n",
      "        total_loss: 106.0403153626124\n",
      "        vf_explained_var: 0.15259592602650324\n",
      "        vf_loss: 106.09787529150645\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5514926216502984\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015166702779582314\n",
      "        policy_loss: -0.05759407482420405\n",
      "        total_loss: 65.22670806248983\n",
      "        vf_explained_var: 0.16281088640292485\n",
      "        vf_loss: 65.27406470139822\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5331344269712766\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015741801374198113\n",
      "        policy_loss: -0.08720220607239754\n",
      "        total_loss: 67.73488238334656\n",
      "        vf_explained_var: 0.26432269404331843\n",
      "        vf_loss: 67.81145869970322\n",
      "  num_agent_steps_sampled: 2274272\n",
      "  num_agent_steps_trained: 2274272\n",
      "  num_steps_sampled: 2274300\n",
      "  num_steps_trained: 2274300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 285\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.55\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.70000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 50.33333333333333\n",
      "  player_2: 44.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.9533333333333336\n",
      "  player_1: -2.9866666666666664\n",
      "  player_2: 3.033333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -74.66666666666667\n",
      "  player_1: -55.0\n",
      "  player_2: -36.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08769933701144211\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3162786859004103\n",
      "  mean_inference_ms: 1.6342321148272267\n",
      "  mean_raw_obs_processing_ms: 0.21094120026465643\n",
      "time_since_restore: 4507.017423868179\n",
      "time_this_iter_s: 15.987536907196045\n",
      "time_total_s: 4507.017423868179\n",
      "timers:\n",
      "  learn_throughput: 555.541\n",
      "  learn_time_ms: 14364.389\n",
      "  load_throughput: 925030.012\n",
      "  load_time_ms: 8.627\n",
      "  sample_throughput: 509.6\n",
      "  sample_time_ms: 15659.348\n",
      "  update_time_ms: 5.346\n",
      "timestamp: 1643540733\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2274300\n",
      "training_iteration: 285\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2290230\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-06-04\n",
      "done: false\n",
      "episode_len_mean: 185.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 44\n",
      "episodes_total: 9093\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5453905523816744\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017334578589909975\n",
      "        policy_loss: -0.08648233104419584\n",
      "        total_loss: 129.11458810329438\n",
      "        vf_explained_var: 0.07462821016709009\n",
      "        vf_loss: 129.18936951796215\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5323191124200821\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015055542525447739\n",
      "        policy_loss: -0.07839601216527323\n",
      "        total_loss: 52.00626909891764\n",
      "        vf_explained_var: 0.15290340304374694\n",
      "        vf_loss: 52.074502607981366\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5063464752336343\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015485717341648334\n",
      "        policy_loss: -0.07153193694849809\n",
      "        total_loss: 55.123320345878604\n",
      "        vf_explained_var: 0.2816183766722679\n",
      "        vf_loss: 55.18439949989319\n",
      "  num_agent_steps_sampled: 2290230\n",
      "  num_agent_steps_trained: 2290230\n",
      "  num_steps_sampled: 2290260\n",
      "  num_steps_trained: 2290260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 287\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.436842105263155\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.7\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 33.666666666666664\n",
      "  player_2: 39.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: -0.89\n",
      "  player_1: -0.72\n",
      "  player_2: 4.609999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -66.66666666666667\n",
      "  player_1: -45.333333333333336\n",
      "  player_2: -37.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08770068069767949\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.31629243721989275\n",
      "  mean_inference_ms: 1.63451350547358\n",
      "  mean_raw_obs_processing_ms: 0.21091042828478845\n",
      "time_since_restore: 4537.675712823868\n",
      "time_this_iter_s: 15.270944595336914\n",
      "time_total_s: 4537.675712823868\n",
      "timers:\n",
      "  learn_throughput: 560.029\n",
      "  learn_time_ms: 14249.264\n",
      "  load_throughput: 936194.528\n",
      "  load_time_ms: 8.524\n",
      "  sample_throughput: 511.898\n",
      "  sample_time_ms: 15589.054\n",
      "  update_time_ms: 5.333\n",
      "timestamp: 1643540764\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2290260\n",
      "training_iteration: 287\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2306190\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-06-36\n",
      "done: false\n",
      "episode_len_mean: 183.6\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 43\n",
      "episodes_total: 9180\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5291323206325372\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01756072366929061\n",
      "        policy_loss: -0.052698896899819374\n",
      "        total_loss: 51.6068431075414\n",
      "        vf_explained_var: 0.0612948739528656\n",
      "        vf_loss: 51.64768850962321\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5560646942754587\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01603032773679767\n",
      "        policy_loss: -0.07902539475510517\n",
      "        total_loss: 47.22921062350273\n",
      "        vf_explained_var: -0.012030786871910094\n",
      "        vf_loss: 47.297415370543796\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.538954396446546\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016111365512738493\n",
      "        policy_loss: -0.08325418701395393\n",
      "        total_loss: 71.54944782574971\n",
      "        vf_explained_var: 0.1640178061525027\n",
      "        vf_loss: 71.62182681242625\n",
      "  num_agent_steps_sampled: 2306190\n",
      "  num_agent_steps_trained: 2306190\n",
      "  num_steps_sampled: 2306220\n",
      "  num_steps_trained: 2306220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 289\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.829999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.74999999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.0\n",
      "  player_1: 46.0\n",
      "  player_2: 39.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 1.2933333333333334\n",
      "  player_1: -3.3066666666666666\n",
      "  player_2: 5.013333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -66.66666666666667\n",
      "  player_1: -47.333333333333336\n",
      "  player_2: -44.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08763351072623442\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3157709731178229\n",
      "  mean_inference_ms: 1.6331243595342824\n",
      "  mean_raw_obs_processing_ms: 0.21076567103048136\n",
      "time_since_restore: 4569.8897705078125\n",
      "time_this_iter_s: 15.833413362503052\n",
      "time_total_s: 4569.8897705078125\n",
      "timers:\n",
      "  learn_throughput: 553.107\n",
      "  learn_time_ms: 14427.593\n",
      "  load_throughput: 939938.328\n",
      "  load_time_ms: 8.49\n",
      "  sample_throughput: 509.66\n",
      "  sample_time_ms: 15657.489\n",
      "  update_time_ms: 5.36\n",
      "timestamp: 1643540796\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2306220\n",
      "training_iteration: 289\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2322152\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-07-06\n",
      "done: false\n",
      "episode_len_mean: 190.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 45\n",
      "episodes_total: 9265\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5180750707785289\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01721328013404123\n",
      "        policy_loss: -0.06381853068557879\n",
      "        total_loss: 92.53204094409942\n",
      "        vf_explained_var: 0.2837386431296666\n",
      "        vf_loss: 92.58424058119456\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5416579548517862\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016034421332951753\n",
      "        policy_loss: -0.07170542516435185\n",
      "        total_loss: 52.42796325842539\n",
      "        vf_explained_var: 0.27310992976029713\n",
      "        vf_loss: 52.48884538491567\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5358744281033675\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014861638074132013\n",
      "        policy_loss: -0.10108165934992333\n",
      "        total_loss: 51.44590319554011\n",
      "        vf_explained_var: 0.20991566101710002\n",
      "        vf_loss: 51.53695327679316\n",
      "  num_agent_steps_sampled: 2322152\n",
      "  num_agent_steps_trained: 2322152\n",
      "  num_steps_sampled: 2322180\n",
      "  num_steps_trained: 2322180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 291\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.288888888888891\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.84999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.0\n",
      "  player_1: 38.0\n",
      "  player_2: 34.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.4733333333333336\n",
      "  player_1: -3.766666666666667\n",
      "  player_2: 5.293333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -45.0\n",
      "  player_1: -43.666666666666664\n",
      "  player_2: -35.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08760237387850003\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.315110714083444\n",
      "  mean_inference_ms: 1.6298075409185784\n",
      "  mean_raw_obs_processing_ms: 0.2106751461939975\n",
      "time_since_restore: 4600.139856100082\n",
      "time_this_iter_s: 14.42454981803894\n",
      "time_total_s: 4600.139856100082\n",
      "timers:\n",
      "  learn_throughput: 553.513\n",
      "  learn_time_ms: 14416.999\n",
      "  load_throughput: 920281.494\n",
      "  load_time_ms: 8.671\n",
      "  sample_throughput: 509.014\n",
      "  sample_time_ms: 15677.373\n",
      "  update_time_ms: 5.766\n",
      "timestamp: 1643540826\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2322180\n",
      "training_iteration: 291\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2338110\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-07-35\n",
      "done: false\n",
      "episode_len_mean: 192.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 37\n",
      "episodes_total: 9340\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5457144024471442\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016792571551832225\n",
      "        policy_loss: -0.07943677387510736\n",
      "        total_loss: 60.16310008684794\n",
      "        vf_explained_var: 0.09516078730424245\n",
      "        vf_loss: 60.23120243390401\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5328918003539245\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01645635292482543\n",
      "        policy_loss: -0.10730816881172359\n",
      "        total_loss: 32.219654944737755\n",
      "        vf_explained_var: 0.1300571275750796\n",
      "        vf_loss: 32.31585521062215\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5581555899480979\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01706634699135975\n",
      "        policy_loss: -0.05700243163233002\n",
      "        total_loss: 65.63756015141804\n",
      "        vf_explained_var: 0.1606478158632914\n",
      "        vf_loss: 65.68304275989533\n",
      "  num_agent_steps_sampled: 2338110\n",
      "  num_agent_steps_trained: 2338110\n",
      "  num_steps_sampled: 2338140\n",
      "  num_steps_trained: 2338140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 293\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.305555555555555\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.8\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.333333333333336\n",
      "  player_1: 30.666666666666664\n",
      "  player_2: 33.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 1.07\n",
      "  player_1: -3.1200000000000006\n",
      "  player_2: 5.05\n",
      "policy_reward_min:\n",
      "  player_0: -32.33333333333333\n",
      "  player_1: -36.333333333333336\n",
      "  player_2: -31.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0874569028878666\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3142302797666723\n",
      "  mean_inference_ms: 1.627742480116425\n",
      "  mean_raw_obs_processing_ms: 0.21044551783437462\n",
      "time_since_restore: 4629.086793422699\n",
      "time_this_iter_s: 14.56448769569397\n",
      "time_total_s: 4629.086793422699\n",
      "timers:\n",
      "  learn_throughput: 563.789\n",
      "  learn_time_ms: 14154.221\n",
      "  load_throughput: 965075.224\n",
      "  load_time_ms: 8.269\n",
      "  sample_throughput: 515.352\n",
      "  sample_time_ms: 15484.548\n",
      "  update_time_ms: 5.723\n",
      "timestamp: 1643540855\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2338140\n",
      "training_iteration: 293\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2354070\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-08-07\n",
      "done: false\n",
      "episode_len_mean: 193.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 45\n",
      "episodes_total: 9430\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5419007894396782\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017616763379520534\n",
      "        policy_loss: -0.09819745298397417\n",
      "        total_loss: 64.11102069298427\n",
      "        vf_explained_var: 0.13571693579355876\n",
      "        vf_loss: 64.19732661247254\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5344419078528881\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017058092165616472\n",
      "        policy_loss: -0.08695243901262681\n",
      "        total_loss: 51.45903750340144\n",
      "        vf_explained_var: 0.314595284362634\n",
      "        vf_loss: 51.53447566350301\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5156655384103457\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01493757472959885\n",
      "        policy_loss: -0.06826196520278852\n",
      "        total_loss: 98.06698525746664\n",
      "        vf_explained_var: 0.2386953036983808\n",
      "        vf_loss: 98.12516431649526\n",
      "  num_agent_steps_sampled: 2354070\n",
      "  num_agent_steps_trained: 2354070\n",
      "  num_steps_sampled: 2354100\n",
      "  num_steps_trained: 2354100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 295\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.594999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.79999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.666666666666664\n",
      "  player_1: 37.333333333333336\n",
      "  player_2: 38.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.4833333333333336\n",
      "  player_1: -1.9266666666666667\n",
      "  player_2: 3.4433333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -53.333333333333336\n",
      "  player_1: -49.0\n",
      "  player_2: -43.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08762581402627029\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3148911004405527\n",
      "  mean_inference_ms: 1.6285663193451603\n",
      "  mean_raw_obs_processing_ms: 0.21063493511039766\n",
      "time_since_restore: 4660.408732891083\n",
      "time_this_iter_s: 15.739476442337036\n",
      "time_total_s: 4660.408732891083\n",
      "timers:\n",
      "  learn_throughput: 564.069\n",
      "  learn_time_ms: 14147.201\n",
      "  load_throughput: 972099.315\n",
      "  load_time_ms: 8.209\n",
      "  sample_throughput: 518.568\n",
      "  sample_time_ms: 15388.532\n",
      "  update_time_ms: 5.819\n",
      "timestamp: 1643540887\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2354100\n",
      "training_iteration: 295\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2370030\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-08-40\n",
      "done: false\n",
      "episode_len_mean: 188.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 37\n",
      "episodes_total: 9510\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5424477280676365\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01642128378577998\n",
      "        policy_loss: -0.04215685888503989\n",
      "        total_loss: 60.57178162574768\n",
      "        vf_explained_var: 0.08154572387536367\n",
      "        vf_loss: 60.60285420497259\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5601185164848963\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014879092604276707\n",
      "        policy_loss: -0.07039976640914877\n",
      "        total_loss: 92.50533181627591\n",
      "        vf_explained_var: 0.20774631460507711\n",
      "        vf_loss: 92.56568809270858\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5410965812702974\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01586177317104557\n",
      "        policy_loss: -0.10394305252159636\n",
      "        total_loss: 52.89149829705556\n",
      "        vf_explained_var: 0.3110650210579236\n",
      "        vf_loss: 52.98473449865977\n",
      "  num_agent_steps_sampled: 2370030\n",
      "  num_agent_steps_trained: 2370030\n",
      "  num_steps_sampled: 2370060\n",
      "  num_steps_trained: 2370060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 297\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.379999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.79999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.66666666666667\n",
      "  player_1: 31.666666666666664\n",
      "  player_2: 46.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 2.68\n",
      "  player_1: -3.26\n",
      "  player_2: 3.5800000000000005\n",
      "policy_reward_min:\n",
      "  player_0: -33.0\n",
      "  player_1: -80.33333333333333\n",
      "  player_2: -46.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08747957620163918\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.314047655993229\n",
      "  mean_inference_ms: 1.6258760999964812\n",
      "  mean_raw_obs_processing_ms: 0.21051652403434826\n",
      "time_since_restore: 4694.207644939423\n",
      "time_this_iter_s: 16.819626092910767\n",
      "time_total_s: 4694.207644939423\n",
      "timers:\n",
      "  learn_throughput: 551.756\n",
      "  learn_time_ms: 14462.92\n",
      "  load_throughput: 885014.647\n",
      "  load_time_ms: 9.017\n",
      "  sample_throughput: 514.181\n",
      "  sample_time_ms: 15519.827\n",
      "  update_time_ms: 5.863\n",
      "timestamp: 1643540920\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2370060\n",
      "training_iteration: 297\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2385992\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-09-11\n",
      "done: false\n",
      "episode_len_mean: 204.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 43\n",
      "episodes_total: 9593\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5004740635057291\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01474008758795814\n",
      "        policy_loss: -0.04887597465887666\n",
      "        total_loss: 88.54182289759318\n",
      "        vf_explained_var: 0.27779108385245005\n",
      "        vf_loss: 88.58074943065644\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5351222568750381\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015143680923062751\n",
      "        policy_loss: -0.09178756585344672\n",
      "        total_loss: 73.90650033473969\n",
      "        vf_explained_var: 0.29705609659353893\n",
      "        vf_loss: 73.98806584914526\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5291044941047828\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01583085512819745\n",
      "        policy_loss: -0.0709579137293622\n",
      "        total_loss: 43.85845228751501\n",
      "        vf_explained_var: 0.062436143159866335\n",
      "        vf_loss: 43.91872449795405\n",
      "  num_agent_steps_sampled: 2385992\n",
      "  num_agent_steps_trained: 2385992\n",
      "  num_steps_sampled: 2386020\n",
      "  num_steps_trained: 2386020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 299\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.352631578947367\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.8\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.0\n",
      "  player_1: 36.666666666666664\n",
      "  player_2: 34.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.6966666666666669\n",
      "  player_1: -1.5833333333333335\n",
      "  player_2: 3.8866666666666663\n",
      "policy_reward_min:\n",
      "  player_0: -57.0\n",
      "  player_1: -38.666666666666664\n",
      "  player_2: -36.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08736306686899123\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3140121462996094\n",
      "  mean_inference_ms: 1.627718705530839\n",
      "  mean_raw_obs_processing_ms: 0.21036578048056687\n",
      "time_since_restore: 4724.983029842377\n",
      "time_this_iter_s: 15.421302556991577\n",
      "time_total_s: 4724.983029842377\n",
      "timers:\n",
      "  learn_throughput: 557.34\n",
      "  learn_time_ms: 14318.009\n",
      "  load_throughput: 837383.312\n",
      "  load_time_ms: 9.53\n",
      "  sample_throughput: 512.097\n",
      "  sample_time_ms: 15582.984\n",
      "  update_time_ms: 5.833\n",
      "timestamp: 1643540951\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2386020\n",
      "training_iteration: 299\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2401950\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-09-41\n",
      "done: false\n",
      "episode_len_mean: 185.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 39\n",
      "episodes_total: 9674\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5173200279970964\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015963528208420334\n",
      "        policy_loss: -0.14335495467763393\n",
      "        total_loss: 46.33337986469269\n",
      "        vf_explained_var: 0.052686890264352165\n",
      "        vf_loss: 46.46595933834712\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5371222742895285\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014721693550036434\n",
      "        policy_loss: -0.024935437850654125\n",
      "        total_loss: 47.19363787412644\n",
      "        vf_explained_var: 0.17056992510954538\n",
      "        vf_loss: 47.20863609393438\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5275162516534329\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017582161999667487\n",
      "        policy_loss: -0.01569530970106522\n",
      "        total_loss: 37.86293310324351\n",
      "        vf_explained_var: 0.18531653354565303\n",
      "        vf_loss: 37.86676041444143\n",
      "  num_agent_steps_sampled: 2401950\n",
      "  num_agent_steps_trained: 2401950\n",
      "  num_steps_sampled: 2401980\n",
      "  num_steps_trained: 2401980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 301\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.85263157894737\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.8\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.0\n",
      "  player_1: 40.0\n",
      "  player_2: 46.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 1.0466666666666669\n",
      "  player_1: -4.093333333333333\n",
      "  player_2: 6.046666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -77.66666666666667\n",
      "  player_1: -45.0\n",
      "  player_2: -54.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08728592188791584\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3135090582189741\n",
      "  mean_inference_ms: 1.6241385485665234\n",
      "  mean_raw_obs_processing_ms: 0.21028580276134579\n",
      "time_since_restore: 4755.077109575272\n",
      "time_this_iter_s: 14.923877954483032\n",
      "time_total_s: 4755.077109575272\n",
      "timers:\n",
      "  learn_throughput: 557.915\n",
      "  learn_time_ms: 14303.243\n",
      "  load_throughput: 892947.932\n",
      "  load_time_ms: 8.937\n",
      "  sample_throughput: 515.502\n",
      "  sample_time_ms: 15480.049\n",
      "  update_time_ms: 5.581\n",
      "timestamp: 1643540981\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2401980\n",
      "training_iteration: 301\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2417910\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-10-12\n",
      "done: false\n",
      "episode_len_mean: 196.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 36\n",
      "episodes_total: 9753\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4896721903979778\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013896344533450625\n",
      "        policy_loss: -0.03589449699347218\n",
      "        total_loss: 45.759482480684916\n",
      "        vf_explained_var: 0.2649544691046079\n",
      "        vf_loss: 45.785996918678286\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5413629311323166\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016246321933588964\n",
      "        policy_loss: -0.09618592200179894\n",
      "        total_loss: 39.754398612976075\n",
      "        vf_explained_var: 0.16350700587034225\n",
      "        vf_loss: 39.83961835304896\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5199302926162879\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01741199959209856\n",
      "        policy_loss: -0.09037773087620735\n",
      "        total_loss: 37.299124745527905\n",
      "        vf_explained_var: 0.19487731476624806\n",
      "        vf_loss: 37.377749267419176\n",
      "  num_agent_steps_sampled: 2417910\n",
      "  num_agent_steps_trained: 2417910\n",
      "  num_steps_sampled: 2417940\n",
      "  num_steps_trained: 2417940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 303\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.342105263157896\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.8\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 40.0\n",
      "  player_1: 34.333333333333336\n",
      "  player_2: 36.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.6566666666666673\n",
      "  player_1: -1.683333333333333\n",
      "  player_2: 4.026666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -37.666666666666664\n",
      "  player_1: -42.0\n",
      "  player_2: -50.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08736118664661398\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.31358977265113736\n",
      "  mean_inference_ms: 1.6253721442899343\n",
      "  mean_raw_obs_processing_ms: 0.21021716364006882\n",
      "time_since_restore: 4785.936465024948\n",
      "time_this_iter_s: 15.552755117416382\n",
      "time_total_s: 4785.936465024948\n",
      "timers:\n",
      "  learn_throughput: 550.62\n",
      "  learn_time_ms: 14492.742\n",
      "  load_throughput: 894195.596\n",
      "  load_time_ms: 8.924\n",
      "  sample_throughput: 510.89\n",
      "  sample_time_ms: 15619.797\n",
      "  update_time_ms: 5.619\n",
      "timestamp: 1643541012\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2417940\n",
      "training_iteration: 303\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2433870\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-10-44\n",
      "done: false\n",
      "episode_len_mean: 196.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 32\n",
      "episodes_total: 9827\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.502544011771679\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014324340431818617\n",
      "        policy_loss: -0.05108067489539583\n",
      "        total_loss: 39.74713177124659\n",
      "        vf_explained_var: 0.12826330403486888\n",
      "        vf_loss: 39.78854379971822\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5154181376596292\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015261867751396494\n",
      "        policy_loss: -0.09118236122963329\n",
      "        total_loss: 41.719057072003686\n",
      "        vf_explained_var: -0.08691439191500346\n",
      "        vf_loss: 41.79993772347768\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5155155045787493\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016373113632420673\n",
      "        policy_loss: -0.0863738853080819\n",
      "        total_loss: 30.671691032250724\n",
      "        vf_explained_var: 0.02947001039981842\n",
      "        vf_loss: 30.747013064225516\n",
      "  num_agent_steps_sampled: 2433870\n",
      "  num_agent_steps_trained: 2433870\n",
      "  num_steps_sampled: 2433900\n",
      "  num_steps_trained: 2433900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 305\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.768421052631581\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.8\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.333333333333336\n",
      "  player_1: 38.666666666666664\n",
      "  player_2: 31.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -0.4500000000000001\n",
      "  player_1: -0.12000000000000043\n",
      "  player_2: 3.569999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -38.0\n",
      "  player_1: -43.33333333333333\n",
      "  player_2: -43.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08722730048564704\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3133861601227206\n",
      "  mean_inference_ms: 1.624041493947743\n",
      "  mean_raw_obs_processing_ms: 0.2102554250900217\n",
      "time_since_restore: 4817.630117893219\n",
      "time_this_iter_s: 15.195747137069702\n",
      "time_total_s: 4817.630117893219\n",
      "timers:\n",
      "  learn_throughput: 549.135\n",
      "  learn_time_ms: 14531.958\n",
      "  load_throughput: 920147.406\n",
      "  load_time_ms: 8.673\n",
      "  sample_throughput: 504.813\n",
      "  sample_time_ms: 15807.822\n",
      "  update_time_ms: 5.436\n",
      "timestamp: 1643541044\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2433900\n",
      "training_iteration: 305\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2449832\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-11-14\n",
      "done: false\n",
      "episode_len_mean: 209.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 39\n",
      "episodes_total: 9906\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5142685559391975\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01485749453471423\n",
      "        policy_loss: -0.08586763210284214\n",
      "        total_loss: 76.09717439293861\n",
      "        vf_explained_var: 0.03526929905017217\n",
      "        vf_loss: 76.17301356315613\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.536962203681469\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01587979236748538\n",
      "        policy_loss: -0.05525534560903907\n",
      "        total_loss: 47.744606877962745\n",
      "        vf_explained_var: 0.37205009390910465\n",
      "        vf_loss: 47.789143323898315\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5200850803156694\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013960412896502324\n",
      "        policy_loss: -0.06980664816995462\n",
      "        total_loss: 91.65072732289632\n",
      "        vf_explained_var: 0.3035136098663012\n",
      "        vf_loss: 91.71111060937245\n",
      "  num_agent_steps_sampled: 2449832\n",
      "  num_agent_steps_trained: 2449832\n",
      "  num_steps_sampled: 2449860\n",
      "  num_steps_trained: 2449860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 307\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.616666666666667\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.8\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.666666666666668\n",
      "  player_1: 41.666666666666664\n",
      "  player_2: 39.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 0.04666666666666625\n",
      "  player_1: 0.5066666666666665\n",
      "  player_2: 2.4466666666666668\n",
      "policy_reward_min:\n",
      "  player_0: -51.333333333333336\n",
      "  player_1: -28.333333333333336\n",
      "  player_2: -66.33333333333334\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08726880747369382\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3136160322195526\n",
      "  mean_inference_ms: 1.625438514167653\n",
      "  mean_raw_obs_processing_ms: 0.21028340960983796\n",
      "time_since_restore: 4847.214205741882\n",
      "time_this_iter_s: 14.746821641921997\n",
      "time_total_s: 4847.214205741882\n",
      "timers:\n",
      "  learn_throughput: 565.499\n",
      "  learn_time_ms: 14111.43\n",
      "  load_throughput: 1009846.245\n",
      "  load_time_ms: 7.902\n",
      "  sample_throughput: 513.442\n",
      "  sample_time_ms: 15542.15\n",
      "  update_time_ms: 5.417\n",
      "timestamp: 1643541074\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2449860\n",
      "training_iteration: 307\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2465790\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-11-46\n",
      "done: false\n",
      "episode_len_mean: 214.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.999999999999998\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 40\n",
      "episodes_total: 9986\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48567474707961084\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016206928635208822\n",
      "        policy_loss: -0.08511611759041747\n",
      "        total_loss: 45.42777013619741\n",
      "        vf_explained_var: 0.1178409876426061\n",
      "        vf_loss: 45.501946609814965\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5481935651103655\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015970981465961057\n",
      "        policy_loss: -0.07419753496845563\n",
      "        total_loss: 44.256052271525064\n",
      "        vf_explained_var: -0.10105571140845616\n",
      "        vf_loss: 44.31946953455607\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.527296687066555\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017579353729237066\n",
      "        policy_loss: -0.07339084043477972\n",
      "        total_loss: 52.66410476366679\n",
      "        vf_explained_var: 0.3137704774737358\n",
      "        vf_loss: 52.72562947273254\n",
      "  num_agent_steps_sampled: 2465790\n",
      "  num_agent_steps_trained: 2465790\n",
      "  num_steps_sampled: 2465820\n",
      "  num_steps_trained: 2465820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 309\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.03684210526316\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.8\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 27.666666666666664\n",
      "  player_1: 41.666666666666664\n",
      "  player_2: 41.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -1.1800000000000008\n",
      "  player_1: 0.8399999999999993\n",
      "  player_2: 3.3399999999999994\n",
      "policy_reward_min:\n",
      "  player_0: -35.66666666666667\n",
      "  player_1: -40.666666666666664\n",
      "  player_2: -66.33333333333334\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08720736727676519\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3129275972518499\n",
      "  mean_inference_ms: 1.6238676045661977\n",
      "  mean_raw_obs_processing_ms: 0.2100452144259349\n",
      "time_since_restore: 4879.493550539017\n",
      "time_this_iter_s: 15.201147317886353\n",
      "time_total_s: 4879.493550539017\n",
      "timers:\n",
      "  learn_throughput: 559.574\n",
      "  learn_time_ms: 14260.857\n",
      "  load_throughput: 1043216.118\n",
      "  load_time_ms: 7.649\n",
      "  sample_throughput: 514.869\n",
      "  sample_time_ms: 15499.083\n",
      "  update_time_ms: 5.473\n",
      "timestamp: 1643541106\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2465820\n",
      "training_iteration: 309\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2481751\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-12-17\n",
      "done: false\n",
      "episode_len_mean: 181.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 40\n",
      "episodes_total: 10075\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49542905643582347\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01629018059753889\n",
      "        policy_loss: -0.05924900690093637\n",
      "        total_loss: 45.89379274050395\n",
      "        vf_explained_var: 0.16413021445274353\n",
      "        vf_loss: 45.942045702934266\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5376093520224094\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01316438652083794\n",
      "        policy_loss: -0.06925593146588653\n",
      "        total_loss: 57.815559341112774\n",
      "        vf_explained_var: -0.04416215588649114\n",
      "        vf_loss: 57.875928990046184\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5184019578496615\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015675611843141722\n",
      "        policy_loss: -0.09005906488746404\n",
      "        total_loss: 39.345260705153144\n",
      "        vf_explained_var: 0.16023657063643137\n",
      "        vf_loss: 39.42473888715108\n",
      "  num_agent_steps_sampled: 2481751\n",
      "  num_agent_steps_trained: 2481751\n",
      "  num_steps_sampled: 2481780\n",
      "  num_steps_trained: 2481780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 311\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.12631578947368\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.8\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.33333333333333\n",
      "  player_1: 36.666666666666664\n",
      "  player_2: 41.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -1.7066666666666677\n",
      "  player_1: -2.8966666666666665\n",
      "  player_2: 7.6033333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -39.0\n",
      "  player_1: -54.66666666666667\n",
      "  player_2: -36.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08729276595004908\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3128708165771141\n",
      "  mean_inference_ms: 1.6231711930881512\n",
      "  mean_raw_obs_processing_ms: 0.2099900242049103\n",
      "time_since_restore: 4910.457550764084\n",
      "time_this_iter_s: 15.518011808395386\n",
      "time_total_s: 4910.457550764084\n",
      "timers:\n",
      "  learn_throughput: 556.16\n",
      "  learn_time_ms: 14348.378\n",
      "  load_throughput: 1030202.126\n",
      "  load_time_ms: 7.746\n",
      "  sample_throughput: 514.809\n",
      "  sample_time_ms: 15500.884\n",
      "  update_time_ms: 5.424\n",
      "timestamp: 1643541137\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2481780\n",
      "training_iteration: 311\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2497712\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-12-49\n",
      "done: false\n",
      "episode_len_mean: 184.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 41\n",
      "episodes_total: 10162\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5109869073331356\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01507124104224914\n",
      "        policy_loss: -0.10810953675148388\n",
      "        total_loss: 57.92326600074768\n",
      "        vf_explained_var: 0.2129130225380262\n",
      "        vf_loss: 58.0212024195989\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5427437953154246\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015496491602413395\n",
      "        policy_loss: -0.04454718376199404\n",
      "        total_loss: 39.86909379800161\n",
      "        vf_explained_var: 0.22084389527638754\n",
      "        vf_loss: 39.903180828094484\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5223331463336944\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014282512193263226\n",
      "        policy_loss: -0.03774582131455342\n",
      "        total_loss: 51.197515266736346\n",
      "        vf_explained_var: 0.24724070648352306\n",
      "        vf_loss: 51.225620245933534\n",
      "  num_agent_steps_sampled: 2497712\n",
      "  num_agent_steps_trained: 2497712\n",
      "  num_steps_sampled: 2497740\n",
      "  num_steps_trained: 2497740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 313\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.142105263157896\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.8\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 42.33333333333333\n",
      "  player_1: 26.333333333333336\n",
      "  player_2: 44.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.8900000000000002\n",
      "  player_1: -2.95\n",
      "  player_2: 6.84\n",
      "policy_reward_min:\n",
      "  player_0: -41.0\n",
      "  player_1: -54.0\n",
      "  player_2: -60.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08714669731321976\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.312953874311787\n",
      "  mean_inference_ms: 1.6245603093283236\n",
      "  mean_raw_obs_processing_ms: 0.2101662389869733\n",
      "time_since_restore: 4942.677201509476\n",
      "time_this_iter_s: 15.659271240234375\n",
      "time_total_s: 4942.677201509476\n",
      "timers:\n",
      "  learn_throughput: 550.979\n",
      "  learn_time_ms: 14483.301\n",
      "  load_throughput: 1021449.352\n",
      "  load_time_ms: 7.812\n",
      "  sample_throughput: 508.685\n",
      "  sample_time_ms: 15687.51\n",
      "  update_time_ms: 5.48\n",
      "timestamp: 1643541169\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2497740\n",
      "training_iteration: 313\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2513670\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-13-22\n",
      "done: false\n",
      "episode_len_mean: 192.62\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 37\n",
      "episodes_total: 10246\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.52421254341801\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01726710724901447\n",
      "        policy_loss: -0.09925055569037795\n",
      "        total_loss: 65.79698078552882\n",
      "        vf_explained_var: -0.04591372142235438\n",
      "        vf_loss: 65.88457586129506\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.533446432997783\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016547211963307405\n",
      "        policy_loss: -0.0652656370587647\n",
      "        total_loss: 29.52917445341746\n",
      "        vf_explained_var: 0.30752174466848375\n",
      "        vf_loss: 29.5832705950737\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5162159195542335\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01631514217233189\n",
      "        policy_loss: -0.07274613187337915\n",
      "        total_loss: 51.98509520848592\n",
      "        vf_explained_var: 0.2089252879222234\n",
      "        vf_loss: 52.046828678448996\n",
      "  num_agent_steps_sampled: 2513670\n",
      "  num_agent_steps_trained: 2513670\n",
      "  num_steps_sampled: 2513700\n",
      "  num_steps_trained: 2513700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 315\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.50952380952381\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.79999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.666666666666664\n",
      "  player_1: 41.0\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: -2.236666666666667\n",
      "  player_1: -0.5466666666666662\n",
      "  player_2: 5.783333333333331\n",
      "policy_reward_min:\n",
      "  player_0: -39.66666666666667\n",
      "  player_1: -35.0\n",
      "  player_2: -60.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08718067386852262\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3122804830294557\n",
      "  mean_inference_ms: 1.6217492401124902\n",
      "  mean_raw_obs_processing_ms: 0.21003189694430663\n",
      "time_since_restore: 4975.048648118973\n",
      "time_this_iter_s: 16.782626628875732\n",
      "time_total_s: 4975.048648118973\n",
      "timers:\n",
      "  learn_throughput: 548.514\n",
      "  learn_time_ms: 14548.388\n",
      "  load_throughput: 1068458.977\n",
      "  load_time_ms: 7.469\n",
      "  sample_throughput: 511.216\n",
      "  sample_time_ms: 15609.831\n",
      "  update_time_ms: 5.586\n",
      "timestamp: 1643541202\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2513700\n",
      "training_iteration: 315\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2529636\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-13-53\n",
      "done: false\n",
      "episode_len_mean: 197.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 40\n",
      "episodes_total: 10331\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5140863299866517\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016776625200367525\n",
      "        policy_loss: -0.10819717846966038\n",
      "        total_loss: 53.84693786144257\n",
      "        vf_explained_var: 0.05021472096443176\n",
      "        vf_loss: 53.94381091117859\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5396467902759711\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014945196534306718\n",
      "        policy_loss: -0.06369610649223129\n",
      "        total_loss: 50.02125727256139\n",
      "        vf_explained_var: 0.19252832641204198\n",
      "        vf_loss: 50.074865516821546\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5200559549530347\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015826421732781454\n",
      "        policy_loss: -0.059563314647724235\n",
      "        total_loss: 47.95497721910477\n",
      "        vf_explained_var: 0.28777874569098155\n",
      "        vf_loss: 48.00385776519775\n",
      "  num_agent_steps_sampled: 2529636\n",
      "  num_agent_steps_trained: 2529636\n",
      "  num_steps_sampled: 2529660\n",
      "  num_steps_trained: 2529660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 317\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.390000000000004\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.79999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 42.0\n",
      "  player_1: 35.0\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.9766666666666675\n",
      "  player_1: -3.3033333333333332\n",
      "  player_2: 4.326666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -41.666666666666664\n",
      "  player_1: -54.33333333333333\n",
      "  player_2: -46.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0871908402356506\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.31252375691669776\n",
      "  mean_inference_ms: 1.6229807226371877\n",
      "  mean_raw_obs_processing_ms: 0.20997080727847706\n",
      "time_since_restore: 5006.378149986267\n",
      "time_this_iter_s: 16.12995719909668\n",
      "time_total_s: 5006.378149986267\n",
      "timers:\n",
      "  learn_throughput: 542.072\n",
      "  learn_time_ms: 14721.3\n",
      "  load_throughput: 1021097.224\n",
      "  load_time_ms: 7.815\n",
      "  sample_throughput: 504.928\n",
      "  sample_time_ms: 15804.229\n",
      "  update_time_ms: 5.585\n",
      "timestamp: 1643541233\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2529660\n",
      "training_iteration: 317\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2545590\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-14-24\n",
      "done: false\n",
      "episode_len_mean: 186.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 41\n",
      "episodes_total: 10416\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5164033788442611\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01602303955502464\n",
      "        policy_loss: -0.036507922674839696\n",
      "        total_loss: 51.743873564402264\n",
      "        vf_explained_var: 0.29503528038660687\n",
      "        vf_loss: 51.76956616083781\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5441080349187056\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01575384543794011\n",
      "        policy_loss: -0.08576263787845771\n",
      "        total_loss: 61.589104976654056\n",
      "        vf_explained_var: 0.11305414269367854\n",
      "        vf_loss: 61.66423350969951\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.52181641827027\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015193398610038761\n",
      "        policy_loss: -0.08826956760759155\n",
      "        total_loss: 48.0390037115415\n",
      "        vf_explained_var: 0.22693388412396112\n",
      "        vf_loss: 48.1170173851649\n",
      "  num_agent_steps_sampled: 2545590\n",
      "  num_agent_steps_trained: 2545590\n",
      "  num_steps_sampled: 2545620\n",
      "  num_steps_trained: 2545620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 319\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.783333333333335\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.8\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.0\n",
      "  player_1: 33.666666666666664\n",
      "  player_2: 35.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -1.15\n",
      "  player_1: -3.0000000000000004\n",
      "  player_2: 7.149999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -36.666666666666664\n",
      "  player_1: -39.0\n",
      "  player_2: -34.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08707084489226888\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.31212691081144145\n",
      "  mean_inference_ms: 1.622362088302612\n",
      "  mean_raw_obs_processing_ms: 0.2098261372620576\n",
      "time_since_restore: 5037.086544513702\n",
      "time_this_iter_s: 14.593806505203247\n",
      "time_total_s: 5037.086544513702\n",
      "timers:\n",
      "  learn_throughput: 547.836\n",
      "  learn_time_ms: 14566.4\n",
      "  load_throughput: 1047017.938\n",
      "  load_time_ms: 7.622\n",
      "  sample_throughput: 503.615\n",
      "  sample_time_ms: 15845.431\n",
      "  update_time_ms: 5.678\n",
      "timestamp: 1643541264\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2545620\n",
      "training_iteration: 319\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2561551\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-14-54\n",
      "done: false\n",
      "episode_len_mean: 190.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 45\n",
      "episodes_total: 10504\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5062974485258261\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016994092415158472\n",
      "        policy_loss: -0.0528906693837295\n",
      "        total_loss: 81.2596111424764\n",
      "        vf_explained_var: 0.1394198716680209\n",
      "        vf_loss: 81.3010309012731\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5499488411347071\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017434849035322865\n",
      "        policy_loss: -0.09341663516126573\n",
      "        total_loss: 39.02002462784449\n",
      "        vf_explained_var: 0.19992184052864712\n",
      "        vf_loss: 39.1016726676623\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5112700190146764\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016558525540734385\n",
      "        policy_loss: -0.10902437952657541\n",
      "        total_loss: 61.230477215449014\n",
      "        vf_explained_var: 0.14765846500794091\n",
      "        vf_loss: 61.32832435369492\n",
      "  num_agent_steps_sampled: 2561551\n",
      "  num_agent_steps_trained: 2561551\n",
      "  num_steps_sampled: 2561580\n",
      "  num_steps_trained: 2561580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 321\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.738888888888889\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.8\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.666666666666664\n",
      "  player_1: 33.666666666666664\n",
      "  player_2: 34.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.4733333333333327\n",
      "  player_1: -2.786666666666667\n",
      "  player_2: 2.313333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -37.666666666666664\n",
      "  player_1: -31.666666666666664\n",
      "  player_2: -34.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0871465567118523\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.31181108597310664\n",
      "  mean_inference_ms: 1.6210265302334326\n",
      "  mean_raw_obs_processing_ms: 0.20974789216163572\n",
      "time_since_restore: 5067.484257698059\n",
      "time_this_iter_s: 14.810235023498535\n",
      "time_total_s: 5067.484257698059\n",
      "timers:\n",
      "  learn_throughput: 549.931\n",
      "  learn_time_ms: 14510.901\n",
      "  load_throughput: 1023351.288\n",
      "  load_time_ms: 7.798\n",
      "  sample_throughput: 505.043\n",
      "  sample_time_ms: 15800.635\n",
      "  update_time_ms: 5.661\n",
      "timestamp: 1643541294\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2561580\n",
      "training_iteration: 321\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2577512\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-15-27\n",
      "done: false\n",
      "episode_len_mean: 178.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 38\n",
      "episodes_total: 10583\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4939579209685326\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016020478457448917\n",
      "        policy_loss: -0.07581857987834761\n",
      "        total_loss: 67.63608958244323\n",
      "        vf_explained_var: 0.12706850270430248\n",
      "        vf_loss: 67.7010940917333\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5250893332560858\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01370373425823857\n",
      "        policy_loss: -0.08557082639308647\n",
      "        total_loss: 76.66737401803334\n",
      "        vf_explained_var: 0.06967787553866704\n",
      "        vf_loss: 76.74369446277619\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5082569011549155\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015840007764135747\n",
      "        policy_loss: -0.07013590663050612\n",
      "        total_loss: 48.344377434253694\n",
      "        vf_explained_var: 0.02109454572200775\n",
      "        vf_loss: 48.40382177035014\n",
      "  num_agent_steps_sampled: 2577512\n",
      "  num_agent_steps_trained: 2577512\n",
      "  num_steps_sampled: 2577540\n",
      "  num_steps_trained: 2577540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 323\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.465\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.79999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 49.0\n",
      "  player_1: 69.33333333333333\n",
      "  player_2: 43.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 0.7366666666666668\n",
      "  player_1: -0.603333333333333\n",
      "  player_2: 2.8666666666666676\n",
      "policy_reward_min:\n",
      "  player_0: -77.66666666666667\n",
      "  player_1: -52.0\n",
      "  player_2: -75.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08692127970735687\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.31148054072571535\n",
      "  mean_inference_ms: 1.619246329924245\n",
      "  mean_raw_obs_processing_ms: 0.20973348366655686\n",
      "time_since_restore: 5100.073048353195\n",
      "time_this_iter_s: 16.417587280273438\n",
      "time_total_s: 5100.073048353195\n",
      "timers:\n",
      "  learn_throughput: 548.576\n",
      "  learn_time_ms: 14546.757\n",
      "  load_throughput: 1009523.385\n",
      "  load_time_ms: 7.905\n",
      "  sample_throughput: 508.469\n",
      "  sample_time_ms: 15694.157\n",
      "  update_time_ms: 5.578\n",
      "timestamp: 1643541327\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2577540\n",
      "training_iteration: 323\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2593473\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-15-58\n",
      "done: false\n",
      "episode_len_mean: 190.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 44\n",
      "episodes_total: 10673\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5123160961270332\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016386812571864483\n",
      "        policy_loss: -0.06881588275078684\n",
      "        total_loss: 70.06021033128103\n",
      "        vf_explained_var: 0.14062818129857382\n",
      "        vf_loss: 70.11796512762706\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.544001330335935\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015043191386369965\n",
      "        policy_loss: -0.07128767588486275\n",
      "        total_loss: 36.672697137196856\n",
      "        vf_explained_var: -0.05915546069542567\n",
      "        vf_loss: 36.73383080879847\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5088539961477121\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016124532686524543\n",
      "        policy_loss: -0.06602397845437129\n",
      "        total_loss: 58.43914753675461\n",
      "        vf_explained_var: 0.21627149869998297\n",
      "        vf_loss: 58.494287327130635\n",
      "  num_agent_steps_sampled: 2593473\n",
      "  num_agent_steps_trained: 2593473\n",
      "  num_steps_sampled: 2593500\n",
      "  num_steps_trained: 2593500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 325\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.200000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.8\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 49.0\n",
      "  player_1: 32.0\n",
      "  player_2: 28.333333333333332\n",
      "policy_reward_mean:\n",
      "  player_0: 1.3666666666666671\n",
      "  player_1: -0.663333333333333\n",
      "  player_2: 2.2966666666666673\n",
      "policy_reward_min:\n",
      "  player_0: -33.0\n",
      "  player_1: -52.0\n",
      "  player_2: -46.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08698354085786118\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.31148482783131376\n",
      "  mean_inference_ms: 1.6204289653924548\n",
      "  mean_raw_obs_processing_ms: 0.20954416188840055\n",
      "time_since_restore: 5131.103779792786\n",
      "time_this_iter_s: 14.784409284591675\n",
      "time_total_s: 5131.103779792786\n",
      "timers:\n",
      "  learn_throughput: 553.678\n",
      "  learn_time_ms: 14412.705\n",
      "  load_throughput: 1011757.168\n",
      "  load_time_ms: 7.887\n",
      "  sample_throughput: 503.858\n",
      "  sample_time_ms: 15837.795\n",
      "  update_time_ms: 5.632\n",
      "timestamp: 1643541358\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2593500\n",
      "training_iteration: 325\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2609430\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-16-29\n",
      "done: false\n",
      "episode_len_mean: 172.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 45\n",
      "episodes_total: 10765\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5224356155097485\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016305067200499554\n",
      "        policy_loss: -0.09440945862481992\n",
      "        total_loss: 56.505253421465554\n",
      "        vf_explained_var: 0.10352481245994567\n",
      "        vf_loss: 56.588657019933066\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5643233037988344\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016597538983625478\n",
      "        policy_loss: -0.0773149128475537\n",
      "        total_loss: 72.65751832326254\n",
      "        vf_explained_var: 0.23394872695207597\n",
      "        vf_loss: 72.72363007704416\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5334108736614386\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015878487444133497\n",
      "        policy_loss: -0.08221202809053163\n",
      "        total_loss: 88.7679882701238\n",
      "        vf_explained_var: 0.24122803012530009\n",
      "        vf_loss: 88.83948242505392\n",
      "  num_agent_steps_sampled: 2609430\n",
      "  num_agent_steps_trained: 2609430\n",
      "  num_steps_sampled: 2609460\n",
      "  num_steps_trained: 2609460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 327\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.14736842105263\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.8\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.33333333333333\n",
      "  player_1: 53.0\n",
      "  player_2: 39.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 1.5133333333333336\n",
      "  player_1: -4.706666666666666\n",
      "  player_2: 6.1933333333333325\n",
      "policy_reward_min:\n",
      "  player_0: -36.666666666666664\n",
      "  player_1: -68.66666666666667\n",
      "  player_2: -52.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08704154768589367\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3109254537788883\n",
      "  mean_inference_ms: 1.6184779348810918\n",
      "  mean_raw_obs_processing_ms: 0.20944984690307789\n",
      "time_since_restore: 5162.027184724808\n",
      "time_this_iter_s: 15.324379444122314\n",
      "time_total_s: 5162.027184724808\n",
      "timers:\n",
      "  learn_throughput: 555.116\n",
      "  learn_time_ms: 14375.373\n",
      "  load_throughput: 1037392.827\n",
      "  load_time_ms: 7.692\n",
      "  sample_throughput: 509.071\n",
      "  sample_time_ms: 15675.598\n",
      "  update_time_ms: 5.736\n",
      "timestamp: 1643541389\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2609460\n",
      "training_iteration: 327\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2625390\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-17-00\n",
      "done: false\n",
      "episode_len_mean: 175.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 45\n",
      "episodes_total: 10856\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5041953755418459\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015266296164178735\n",
      "        policy_loss: -0.051753202828889094\n",
      "        total_loss: 70.59083393891652\n",
      "        vf_explained_var: 0.3850501412153244\n",
      "        vf_loss: 70.63228261470795\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.555284907023112\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015973977875401033\n",
      "        policy_loss: -0.11331827579687039\n",
      "        total_loss: 55.62285325368246\n",
      "        vf_explained_var: -0.17912532558043798\n",
      "        vf_loss: 55.72538928349813\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5015329385300478\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015060241163737373\n",
      "        policy_loss: -0.06564555646541217\n",
      "        total_loss: 73.37632434686024\n",
      "        vf_explained_var: 0.2325770460565885\n",
      "        vf_loss: 73.43180406490961\n",
      "  num_agent_steps_sampled: 2625390\n",
      "  num_agent_steps_trained: 2625390\n",
      "  num_steps_sampled: 2625420\n",
      "  num_steps_trained: 2625420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 329\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.894736842105264\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.8\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.0\n",
      "  player_1: 27.333333333333336\n",
      "  player_2: 39.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.5299999999999994\n",
      "  player_1: -4.620000000000001\n",
      "  player_2: 5.09\n",
      "policy_reward_min:\n",
      "  player_0: -45.666666666666664\n",
      "  player_1: -70.0\n",
      "  player_2: -32.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08708605445977165\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3110250193285914\n",
      "  mean_inference_ms: 1.6182182796699183\n",
      "  mean_raw_obs_processing_ms: 0.20960136372763047\n",
      "time_since_restore: 5193.271834611893\n",
      "time_this_iter_s: 15.489574432373047\n",
      "time_total_s: 5193.271834611893\n",
      "timers:\n",
      "  learn_throughput: 553.187\n",
      "  learn_time_ms: 14425.51\n",
      "  load_throughput: 1047476.675\n",
      "  load_time_ms: 7.618\n",
      "  sample_throughput: 512.759\n",
      "  sample_time_ms: 15562.868\n",
      "  update_time_ms: 5.542\n",
      "timestamp: 1643541420\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2625420\n",
      "training_iteration: 329\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2641350\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-17-32\n",
      "done: false\n",
      "episode_len_mean: 168.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 51\n",
      "episodes_total: 10951\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4711861217021942\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014707890665387519\n",
      "        policy_loss: -0.0649772189107413\n",
      "        total_loss: 64.04322107474009\n",
      "        vf_explained_var: 0.22554868151744206\n",
      "        vf_loss: 64.09827063878377\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5460123080511888\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015799623019263055\n",
      "        policy_loss: -0.1174703935533762\n",
      "        total_loss: 49.64538659095764\n",
      "        vf_explained_var: 0.16729069739580155\n",
      "        vf_loss: 49.7521924606959\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5229952472945054\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017814729338029262\n",
      "        policy_loss: -0.07088608201903601\n",
      "        total_loss: 65.78096880594889\n",
      "        vf_explained_var: 0.15433634748061498\n",
      "        vf_loss: 65.83982982317606\n",
      "  num_agent_steps_sampled: 2641350\n",
      "  num_agent_steps_trained: 2641350\n",
      "  num_steps_sampled: 2641380\n",
      "  num_steps_trained: 2641380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 331\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.27894736842105\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.0\n",
      "  player_1: 48.33333333333333\n",
      "  player_2: 32.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -0.5466666666666667\n",
      "  player_1: -1.3866666666666665\n",
      "  player_2: 4.933333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -52.0\n",
      "  player_1: -29.666666666666668\n",
      "  player_2: -59.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08697684344794404\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.31019048102347946\n",
      "  mean_inference_ms: 1.6159863638383336\n",
      "  mean_raw_obs_processing_ms: 0.20922097909665896\n",
      "time_since_restore: 5224.722641468048\n",
      "time_this_iter_s: 15.195057392120361\n",
      "time_total_s: 5224.722641468048\n",
      "timers:\n",
      "  learn_throughput: 549.32\n",
      "  learn_time_ms: 14527.061\n",
      "  load_throughput: 1078536.473\n",
      "  load_time_ms: 7.399\n",
      "  sample_throughput: 507.774\n",
      "  sample_time_ms: 15715.649\n",
      "  update_time_ms: 5.481\n",
      "timestamp: 1643541452\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2641380\n",
      "training_iteration: 331\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2657312\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-18-04\n",
      "done: false\n",
      "episode_len_mean: 167.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 50\n",
      "episodes_total: 11047\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47490790183345477\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016121436640033078\n",
      "        policy_loss: -0.07943284068256616\n",
      "        total_loss: 75.91692827860514\n",
      "        vf_explained_var: 0.3759533511598905\n",
      "        vf_loss: 75.98547880808512\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5593962558110555\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014822307495273985\n",
      "        policy_loss: -0.07369062350131571\n",
      "        total_loss: 77.67861070315043\n",
      "        vf_explained_var: 0.1351720987757047\n",
      "        vf_loss: 77.7422959915797\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4811208997666836\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015335169525108843\n",
      "        policy_loss: -0.07116112124485274\n",
      "        total_loss: 75.34782310326894\n",
      "        vf_explained_var: 0.20997912337382635\n",
      "        vf_loss: 75.4086329682668\n",
      "  num_agent_steps_sampled: 2657312\n",
      "  num_agent_steps_trained: 2657312\n",
      "  num_steps_sampled: 2657340\n",
      "  num_steps_trained: 2657340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 333\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.461111111111112\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.8\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.33333333333333\n",
      "  player_1: 41.333333333333336\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.2700000000000002\n",
      "  player_1: -1.3900000000000003\n",
      "  player_2: 3.1200000000000006\n",
      "policy_reward_min:\n",
      "  player_0: -54.666666666666664\n",
      "  player_1: -56.0\n",
      "  player_2: -47.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08694525238270684\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3104644207838617\n",
      "  mean_inference_ms: 1.6167372424397772\n",
      "  mean_raw_obs_processing_ms: 0.209247167078783\n",
      "time_since_restore: 5256.51981139183\n",
      "time_this_iter_s: 14.883601903915405\n",
      "time_total_s: 5256.51981139183\n",
      "timers:\n",
      "  learn_throughput: 552.277\n",
      "  learn_time_ms: 14449.271\n",
      "  load_throughput: 980250.225\n",
      "  load_time_ms: 8.141\n",
      "  sample_throughput: 504.24\n",
      "  sample_time_ms: 15825.797\n",
      "  update_time_ms: 5.479\n",
      "timestamp: 1643541484\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2657340\n",
      "training_iteration: 333\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2673272\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-18-35\n",
      "done: false\n",
      "episode_len_mean: 169.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 42\n",
      "episodes_total: 11134\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47383614083131154\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015958697782771005\n",
      "        policy_loss: -0.03009930595755577\n",
      "        total_loss: 79.98749622980753\n",
      "        vf_explained_var: 0.19700324207544326\n",
      "        vf_loss: 80.00682348569234\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5525774931410948\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01528891940545994\n",
      "        policy_loss: -0.053614752081533275\n",
      "        total_loss: 82.10132702668508\n",
      "        vf_explained_var: 0.14523958216110866\n",
      "        vf_loss: 82.14462219397227\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.503567663927873\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015407322333512639\n",
      "        policy_loss: -0.1379373609321192\n",
      "        total_loss: 38.52742464900017\n",
      "        vf_explained_var: 0.27686121940612796\n",
      "        vf_loss: 38.654962097803754\n",
      "  num_agent_steps_sampled: 2673272\n",
      "  num_agent_steps_trained: 2673272\n",
      "  num_steps_sampled: 2673300\n",
      "  num_steps_trained: 2673300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 335\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.854999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 51.0\n",
      "  player_1: 37.0\n",
      "  player_2: 29.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.7766666666666668\n",
      "  player_1: -5.613333333333333\n",
      "  player_2: 6.836666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -36.333333333333336\n",
      "  player_1: -77.0\n",
      "  player_2: -26.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08688676697295644\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3104261288278498\n",
      "  mean_inference_ms: 1.6169391020495103\n",
      "  mean_raw_obs_processing_ms: 0.20936653172306893\n",
      "time_since_restore: 5287.8027539253235\n",
      "time_this_iter_s: 16.332043647766113\n",
      "time_total_s: 5287.8027539253235\n",
      "timers:\n",
      "  learn_throughput: 551.4\n",
      "  learn_time_ms: 14472.251\n",
      "  load_throughput: 992699.342\n",
      "  load_time_ms: 8.039\n",
      "  sample_throughput: 513.538\n",
      "  sample_time_ms: 15539.269\n",
      "  update_time_ms: 5.481\n",
      "timestamp: 1643541515\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2673300\n",
      "training_iteration: 335\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2689230\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-19-05\n",
      "done: false\n",
      "episode_len_mean: 190.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 45\n",
      "episodes_total: 11219\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5011126316090425\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015527133194546915\n",
      "        policy_loss: -0.04485884582623839\n",
      "        total_loss: 59.02245734055837\n",
      "        vf_explained_var: 0.11076826572418214\n",
      "        vf_loss: 59.0568354956309\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5584772707521916\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014565289342262986\n",
      "        policy_loss: -0.055596158945312106\n",
      "        total_loss: 49.079764712651574\n",
      "        vf_explained_var: 0.23719679633776347\n",
      "        vf_loss: 49.12552916526794\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.516833801617225\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015866768736221767\n",
      "        policy_loss: -0.09893635921490689\n",
      "        total_loss: 63.11606423060099\n",
      "        vf_explained_var: 0.102662692964077\n",
      "        vf_loss: 63.20429050127665\n",
      "  num_agent_steps_sampled: 2689230\n",
      "  num_agent_steps_trained: 2689230\n",
      "  num_steps_sampled: 2689260\n",
      "  num_steps_trained: 2689260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 337\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.442105263157895\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.8\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 51.0\n",
      "  player_1: 36.333333333333336\n",
      "  player_2: 29.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.176666666666667\n",
      "  player_1: -4.223333333333333\n",
      "  player_2: 5.046666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -26.333333333333336\n",
      "  player_1: -77.0\n",
      "  player_2: -30.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08680596427400054\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3099355811952055\n",
      "  mean_inference_ms: 1.6147261035008782\n",
      "  mean_raw_obs_processing_ms: 0.20935376527202176\n",
      "time_since_restore: 5318.041931629181\n",
      "time_this_iter_s: 15.286651134490967\n",
      "time_total_s: 5318.041931629181\n",
      "timers:\n",
      "  learn_throughput: 554.899\n",
      "  learn_time_ms: 14380.989\n",
      "  load_throughput: 970492.688\n",
      "  load_time_ms: 8.223\n",
      "  sample_throughput: 510.538\n",
      "  sample_time_ms: 15630.559\n",
      "  update_time_ms: 5.4\n",
      "timestamp: 1643541545\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2689260\n",
      "training_iteration: 337\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2705190\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-19-37\n",
      "done: false\n",
      "episode_len_mean: 190.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 41\n",
      "episodes_total: 11303\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4890475969513257\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01681246247942738\n",
      "        policy_loss: -0.08124252974366149\n",
      "        total_loss: 51.58805873155594\n",
      "        vf_explained_var: 0.10902864158153534\n",
      "        vf_loss: 51.65795303662618\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5615869228541851\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015657925463445396\n",
      "        policy_loss: -0.0713849262924244\n",
      "        total_loss: 37.00830271244049\n",
      "        vf_explained_var: 0.021083962122599283\n",
      "        vf_loss: 37.06911854426066\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5039911079903444\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014739324997784327\n",
      "        policy_loss: -0.06496477120555938\n",
      "        total_loss: 45.329239087104796\n",
      "        vf_explained_var: 0.22162851214408874\n",
      "        vf_loss: 45.38425492048263\n",
      "  num_agent_steps_sampled: 2705190\n",
      "  num_agent_steps_trained: 2705190\n",
      "  num_steps_sampled: 2705220\n",
      "  num_steps_trained: 2705220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 339\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.680000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.815\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 40.333333333333336\n",
      "  player_1: 36.333333333333336\n",
      "  player_2: 32.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.9699999999999998\n",
      "  player_1: 0.12000000000000036\n",
      "  player_2: 3.850000000000002\n",
      "policy_reward_min:\n",
      "  player_0: -31.333333333333336\n",
      "  player_1: -33.666666666666664\n",
      "  player_2: -31.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08684946966906194\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.31016830211585633\n",
      "  mean_inference_ms: 1.616433092628004\n",
      "  mean_raw_obs_processing_ms: 0.20940969681500785\n",
      "time_since_restore: 5349.487502336502\n",
      "time_this_iter_s: 16.10091781616211\n",
      "time_total_s: 5349.487502336502\n",
      "timers:\n",
      "  learn_throughput: 553.927\n",
      "  learn_time_ms: 14406.232\n",
      "  load_throughput: 963977.326\n",
      "  load_time_ms: 8.278\n",
      "  sample_throughput: 512.117\n",
      "  sample_time_ms: 15582.37\n",
      "  update_time_ms: 5.446\n",
      "timestamp: 1643541577\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2705220\n",
      "training_iteration: 339\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2721150\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-20-07\n",
      "done: false\n",
      "episode_len_mean: 182.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 42\n",
      "episodes_total: 11392\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4870564612746239\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01586488023053486\n",
      "        policy_loss: -0.09083065608671556\n",
      "        total_loss: 45.62335131963094\n",
      "        vf_explained_var: 0.20309897979100544\n",
      "        vf_loss: 45.70347328027089\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5480436365306377\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01625452026255516\n",
      "        policy_loss: -0.061853502305845416\n",
      "        total_loss: 29.845853908061983\n",
      "        vf_explained_var: 0.2803844635685285\n",
      "        vf_loss: 29.89673573652903\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48169571354985236\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016625125793625937\n",
      "        policy_loss: -0.07175084825605155\n",
      "        total_loss: 73.95763362089792\n",
      "        vf_explained_var: 0.27550989111264546\n",
      "        vf_loss: 74.0181625922521\n",
      "  num_agent_steps_sampled: 2721150\n",
      "  num_agent_steps_trained: 2721150\n",
      "  num_steps_sampled: 2721180\n",
      "  num_steps_trained: 2721180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 341\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.772222222222224\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.81666666666666\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.0\n",
      "  player_1: 26.333333333333336\n",
      "  player_2: 33.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -0.36000000000000026\n",
      "  player_1: -1.81\n",
      "  player_2: 5.17\n",
      "policy_reward_min:\n",
      "  player_0: -45.333333333333336\n",
      "  player_1: -39.0\n",
      "  player_2: -38.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08685132532986614\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3096379055907541\n",
      "  mean_inference_ms: 1.6147390926781737\n",
      "  mean_raw_obs_processing_ms: 0.2092409527694823\n",
      "time_since_restore: 5379.338212966919\n",
      "time_this_iter_s: 14.822789907455444\n",
      "time_total_s: 5379.338212966919\n",
      "timers:\n",
      "  learn_throughput: 559.992\n",
      "  learn_time_ms: 14250.215\n",
      "  load_throughput: 965041.834\n",
      "  load_time_ms: 8.269\n",
      "  sample_throughput: 513.945\n",
      "  sample_time_ms: 15526.946\n",
      "  update_time_ms: 5.454\n",
      "timestamp: 1643541607\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2721180\n",
      "training_iteration: 341\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2737110\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-20-40\n",
      "done: false\n",
      "episode_len_mean: 182.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 43\n",
      "episodes_total: 11482\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4971009010573228\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017257942578929337\n",
      "        policy_loss: -0.09374885094972948\n",
      "        total_loss: 47.58143674055735\n",
      "        vf_explained_var: 0.019280326465765635\n",
      "        vf_loss: 47.66353684266408\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.541884868790706\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013427450280198816\n",
      "        policy_loss: -0.10078614852080743\n",
      "        total_loss: 43.02484489917755\n",
      "        vf_explained_var: 0.09367832034826279\n",
      "        vf_loss: 43.11656753698985\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4889543709158897\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01620608863619964\n",
      "        policy_loss: -0.037920031708975635\n",
      "        total_loss: 45.456254852612815\n",
      "        vf_explained_var: 0.35982784976561866\n",
      "        vf_loss: 45.483235564231876\n",
      "  num_agent_steps_sampled: 2737110\n",
      "  num_agent_steps_trained: 2737110\n",
      "  num_steps_sampled: 2737140\n",
      "  num_steps_trained: 2737140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 343\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.527777777777779\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.85555555555555\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.666666666666664\n",
      "  player_1: 34.0\n",
      "  player_2: 41.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.2233333333333336\n",
      "  player_1: -3.516666666666667\n",
      "  player_2: 4.293333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -28.0\n",
      "  player_1: -47.66666666666667\n",
      "  player_2: -38.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0869622815389549\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.31018720194617877\n",
      "  mean_inference_ms: 1.6173668835115342\n",
      "  mean_raw_obs_processing_ms: 0.2094024240372236\n",
      "time_since_restore: 5412.348343849182\n",
      "time_this_iter_s: 14.752021074295044\n",
      "time_total_s: 5412.348343849182\n",
      "timers:\n",
      "  learn_throughput: 555.217\n",
      "  learn_time_ms: 14372.764\n",
      "  load_throughput: 1000886.517\n",
      "  load_time_ms: 7.973\n",
      "  sample_throughput: 510.65\n",
      "  sample_time_ms: 15627.131\n",
      "  update_time_ms: 5.621\n",
      "timestamp: 1643541640\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2737140\n",
      "training_iteration: 343\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2753070\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-21-13\n",
      "done: false\n",
      "episode_len_mean: 193.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 41\n",
      "episodes_total: 11560\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48722431863347687\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015603123106078177\n",
      "        policy_loss: -0.06568814994146427\n",
      "        total_loss: 44.03115326404571\n",
      "        vf_explained_var: 0.2356885729233424\n",
      "        vf_loss: 44.08630925655365\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5498693476120631\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015904943541623123\n",
      "        policy_loss: -0.09137469177755217\n",
      "        total_loss: 39.548222506046294\n",
      "        vf_explained_var: 0.3844244210918744\n",
      "        vf_loss: 39.628861399491626\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4993468516568343\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014749829610728397\n",
      "        policy_loss: -0.09000069978026053\n",
      "        total_loss: 47.26904293616613\n",
      "        vf_explained_var: 0.1553364071249962\n",
      "        vf_loss: 47.34908740838369\n",
      "  num_agent_steps_sampled: 2753070\n",
      "  num_agent_steps_trained: 2753070\n",
      "  num_steps_sampled: 2753100\n",
      "  num_steps_trained: 2753100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 345\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.521052631578948\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.82105263157897\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.0\n",
      "  player_1: 33.333333333333336\n",
      "  player_2: 33.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.1966666666666668\n",
      "  player_1: -1.3833333333333335\n",
      "  player_2: 3.1866666666666674\n",
      "policy_reward_min:\n",
      "  player_0: -28.0\n",
      "  player_1: -42.0\n",
      "  player_2: -49.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08671535096861036\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30959687150148196\n",
      "  mean_inference_ms: 1.616548732908555\n",
      "  mean_raw_obs_processing_ms: 0.20916627067075802\n",
      "time_since_restore: 5444.990771532059\n",
      "time_this_iter_s: 15.696457386016846\n",
      "time_total_s: 5444.990771532059\n",
      "timers:\n",
      "  learn_throughput: 549.857\n",
      "  learn_time_ms: 14512.86\n",
      "  load_throughput: 982393.688\n",
      "  load_time_ms: 8.123\n",
      "  sample_throughput: 504.664\n",
      "  sample_time_ms: 15812.501\n",
      "  update_time_ms: 5.606\n",
      "timestamp: 1643541673\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2753100\n",
      "training_iteration: 345\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2769030\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-21-46\n",
      "done: false\n",
      "episode_len_mean: 184.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 46\n",
      "episodes_total: 11649\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4986700921257337\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015058299295415054\n",
      "        policy_loss: -0.03782188214982549\n",
      "        total_loss: 49.29406244277954\n",
      "        vf_explained_var: 0.2822789463400841\n",
      "        vf_loss: 49.32172000726064\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5425019889573256\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014774752492656565\n",
      "        policy_loss: -0.0855434084435304\n",
      "        total_loss: 72.94637099107106\n",
      "        vf_explained_var: 0.10409527917702993\n",
      "        vf_loss: 73.02194152673086\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48688724100589753\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013524027452470666\n",
      "        policy_loss: -0.07978834428824484\n",
      "        total_loss: 71.28278916915258\n",
      "        vf_explained_var: 0.3304610911011696\n",
      "        vf_loss: 71.35344882965087\n",
      "  num_agent_steps_sampled: 2769030\n",
      "  num_agent_steps_trained: 2769030\n",
      "  num_steps_sampled: 2769060\n",
      "  num_steps_trained: 2769060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 347\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.33181818181818\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.89090909090912\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.333333333333336\n",
      "  player_1: 50.0\n",
      "  player_2: 39.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.4833333333333332\n",
      "  player_1: -2.816666666666667\n",
      "  player_2: 4.333333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -43.0\n",
      "  player_1: -43.0\n",
      "  player_2: -40.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08689837131485931\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30988218188845496\n",
      "  mean_inference_ms: 1.616553694140803\n",
      "  mean_raw_obs_processing_ms: 0.2093872705131865\n",
      "time_since_restore: 5478.519817829132\n",
      "time_this_iter_s: 17.31334638595581\n",
      "time_total_s: 5478.519817829132\n",
      "timers:\n",
      "  learn_throughput: 537.01\n",
      "  learn_time_ms: 14860.071\n",
      "  load_throughput: 1030198.955\n",
      "  load_time_ms: 7.746\n",
      "  sample_throughput: 502.625\n",
      "  sample_time_ms: 15876.637\n",
      "  update_time_ms: 8.048\n",
      "timestamp: 1643541706\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2769060\n",
      "training_iteration: 347\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2784990\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-22-16\n",
      "done: false\n",
      "episode_len_mean: 191.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 43\n",
      "episodes_total: 11734\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5039920940498511\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016928966499156862\n",
      "        policy_loss: -0.11420873944958052\n",
      "        total_loss: 75.65360687335333\n",
      "        vf_explained_var: 0.255267815887928\n",
      "        vf_loss: 75.75638846238454\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5331875577569007\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015577418632669265\n",
      "        policy_loss: -0.04752084815874696\n",
      "        total_loss: 29.99718281110128\n",
      "        vf_explained_var: 0.2086267505089442\n",
      "        vf_loss: 30.03418874581655\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4812781464556853\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01562657365693061\n",
      "        policy_loss: -0.04998757820265989\n",
      "        total_loss: 85.62284307161967\n",
      "        vf_explained_var: -0.0379331370194753\n",
      "        vf_loss: 85.66228347937266\n",
      "  num_agent_steps_sampled: 2784990\n",
      "  num_agent_steps_trained: 2784990\n",
      "  num_steps_sampled: 2785020\n",
      "  num_steps_trained: 2785020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 349\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.442105263157897\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.66666666666667\n",
      "  player_1: 50.0\n",
      "  player_2: 47.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.3866666666666672\n",
      "  player_1: -3.3633333333333333\n",
      "  player_2: 4.976666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -36.0\n",
      "  player_1: -43.0\n",
      "  player_2: -40.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08663144468435593\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3087903222065853\n",
      "  mean_inference_ms: 1.6119150318148718\n",
      "  mean_raw_obs_processing_ms: 0.20910350430639177\n",
      "time_since_restore: 5508.598117828369\n",
      "time_this_iter_s: 15.355978965759277\n",
      "time_total_s: 5508.598117828369\n",
      "timers:\n",
      "  learn_throughput: 542.105\n",
      "  learn_time_ms: 14720.388\n",
      "  load_throughput: 1033038.352\n",
      "  load_time_ms: 7.725\n",
      "  sample_throughput: 498.028\n",
      "  sample_time_ms: 16023.183\n",
      "  update_time_ms: 8.068\n",
      "timestamp: 1643541736\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2785020\n",
      "training_iteration: 349\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2800950\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-22-47\n",
      "done: false\n",
      "episode_len_mean: 166.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 44\n",
      "episodes_total: 11825\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4799717922012011\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017176335716521864\n",
      "        policy_loss: -0.08910416305065155\n",
      "        total_loss: 62.91148352940877\n",
      "        vf_explained_var: 0.2796128483613332\n",
      "        vf_loss: 62.988993374506634\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5561432090401649\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015877598459866533\n",
      "        policy_loss: -0.045460231529238324\n",
      "        total_loss: 42.676173782348634\n",
      "        vf_explained_var: 0.18073692550261816\n",
      "        vf_loss: 42.710916635990145\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5007767643034459\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016140947033315266\n",
      "        policy_loss: -0.05548078253865242\n",
      "        total_loss: 50.26486483414968\n",
      "        vf_explained_var: 0.1879378009835879\n",
      "        vf_loss: 50.309450370470685\n",
      "  num_agent_steps_sampled: 2800950\n",
      "  num_agent_steps_trained: 2800950\n",
      "  num_steps_sampled: 2800980\n",
      "  num_steps_trained: 2800980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 351\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.969999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.89000000000001\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.333333333333336\n",
      "  player_1: 43.666666666666664\n",
      "  player_2: 47.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.5233333333333331\n",
      "  player_1: -2.563333333333333\n",
      "  player_2: 6.086666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -41.666666666666664\n",
      "  player_1: -35.666666666666664\n",
      "  player_2: -52.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0866636190280134\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30835176047858054\n",
      "  mean_inference_ms: 1.6091239034798934\n",
      "  mean_raw_obs_processing_ms: 0.2090237168152839\n",
      "time_since_restore: 5539.065280199051\n",
      "time_this_iter_s: 15.579375743865967\n",
      "time_total_s: 5539.065280199051\n",
      "timers:\n",
      "  learn_throughput: 539.925\n",
      "  learn_time_ms: 14779.832\n",
      "  load_throughput: 1030240.178\n",
      "  load_time_ms: 7.746\n",
      "  sample_throughput: 500.889\n",
      "  sample_time_ms: 15931.672\n",
      "  update_time_ms: 8.314\n",
      "timestamp: 1643541767\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2800980\n",
      "training_iteration: 351\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2816910\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-23-17\n",
      "done: false\n",
      "episode_len_mean: 177.42\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 49\n",
      "episodes_total: 11919\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.492499245206515\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016182755593872004\n",
      "        policy_loss: -0.08647940408904105\n",
      "        total_loss: 64.82983712514242\n",
      "        vf_explained_var: 0.12863319943348567\n",
      "        vf_loss: 64.90539324442545\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5350409998993079\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015417790218751861\n",
      "        policy_loss: -0.09251530839130283\n",
      "        total_loss: 64.93464691241583\n",
      "        vf_explained_var: 0.21035600264867146\n",
      "        vf_loss: 65.0167556810379\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4920256627599398\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014734143960932517\n",
      "        policy_loss: -0.04381310613515477\n",
      "        total_loss: 101.07971510410309\n",
      "        vf_explained_var: 0.20061627507209778\n",
      "        vf_loss: 101.11358289082845\n",
      "  num_agent_steps_sampled: 2816910\n",
      "  num_agent_steps_trained: 2816910\n",
      "  num_steps_sampled: 2816940\n",
      "  num_steps_trained: 2816940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 353\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.178947368421055\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.333333333333336\n",
      "  player_1: 43.0\n",
      "  player_2: 30.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 0.15666666666666693\n",
      "  player_1: -0.9933333333333333\n",
      "  player_2: 3.836666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -39.0\n",
      "  player_1: -34.66666666666667\n",
      "  player_2: -65.66666666666666\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08671592919003673\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30852840542604065\n",
      "  mean_inference_ms: 1.6119093591561977\n",
      "  mean_raw_obs_processing_ms: 0.20897706867333277\n",
      "time_since_restore: 5569.051693677902\n",
      "time_this_iter_s: 15.156312704086304\n",
      "time_total_s: 5569.051693677902\n",
      "timers:\n",
      "  learn_throughput: 551.265\n",
      "  learn_time_ms: 14475.802\n",
      "  load_throughput: 1127554.252\n",
      "  load_time_ms: 7.077\n",
      "  sample_throughput: 509.387\n",
      "  sample_time_ms: 15665.877\n",
      "  update_time_ms: 8.161\n",
      "timestamp: 1643541797\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2816940\n",
      "training_iteration: 353\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2832872\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-23-48\n",
      "done: false\n",
      "episode_len_mean: 175.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 44\n",
      "episodes_total: 12008\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4853659169375896\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016602630420114603\n",
      "        policy_loss: -0.10962118574107686\n",
      "        total_loss: 73.49258098443349\n",
      "        vf_explained_var: 0.19665444711844127\n",
      "        vf_loss: 73.59099532047908\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5542549366752306\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01804356371968557\n",
      "        policy_loss: -0.06757640984219809\n",
      "        total_loss: 68.50826247612635\n",
      "        vf_explained_var: 0.14293009171883264\n",
      "        vf_loss: 68.56365955591201\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5050471843282381\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017162403727907075\n",
      "        policy_loss: -0.05385451466776431\n",
      "        total_loss: 61.24003150145213\n",
      "        vf_explained_var: 0.13041465510924657\n",
      "        vf_loss: 61.282301201820374\n",
      "  num_agent_steps_sampled: 2832872\n",
      "  num_agent_steps_trained: 2832872\n",
      "  num_steps_sampled: 2832900\n",
      "  num_steps_trained: 2832900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 355\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.889999999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 40.333333333333336\n",
      "  player_1: 34.333333333333336\n",
      "  player_2: 50.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.1766666666666667\n",
      "  player_1: -3.233333333333333\n",
      "  player_2: 4.056666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -40.333333333333336\n",
      "  player_1: -78.0\n",
      "  player_2: -65.66666666666666\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08666470388138478\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.307894561623323\n",
      "  mean_inference_ms: 1.609396711892269\n",
      "  mean_raw_obs_processing_ms: 0.20888635725142232\n",
      "time_since_restore: 5599.654979705811\n",
      "time_this_iter_s: 15.981325387954712\n",
      "time_total_s: 5599.654979705811\n",
      "timers:\n",
      "  learn_throughput: 559.202\n",
      "  learn_time_ms: 14270.337\n",
      "  load_throughput: 1139671.143\n",
      "  load_time_ms: 7.002\n",
      "  sample_throughput: 515.692\n",
      "  sample_time_ms: 15474.345\n",
      "  update_time_ms: 8.102\n",
      "timestamp: 1643541828\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2832900\n",
      "training_iteration: 355\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2848830\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-24-18\n",
      "done: false\n",
      "episode_len_mean: 164.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 50\n",
      "episodes_total: 12106\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5035842770338058\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016525212230756287\n",
      "        policy_loss: -0.08407744465706249\n",
      "        total_loss: 51.69558827400208\n",
      "        vf_explained_var: -0.027764902710914613\n",
      "        vf_loss: 51.7685111618042\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5822511116663615\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017170634604992376\n",
      "        policy_loss: -0.074609809803466\n",
      "        total_loss: 71.78030087788899\n",
      "        vf_explained_var: 0.07130841841300328\n",
      "        vf_loss: 71.84332034269968\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5174454781909784\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014979490208577508\n",
      "        policy_loss: -0.08121887067332864\n",
      "        total_loss: 76.75762258211772\n",
      "        vf_explained_var: 0.04780830423037211\n",
      "        vf_loss: 76.82872979799906\n",
      "  num_agent_steps_sampled: 2848830\n",
      "  num_agent_steps_trained: 2848830\n",
      "  num_steps_sampled: 2848860\n",
      "  num_steps_trained: 2848860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 357\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.363157894736844\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.333333333333336\n",
      "  player_1: 35.666666666666664\n",
      "  player_2: 39.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 4.326666666666667\n",
      "  player_1: -2.273333333333333\n",
      "  player_2: 0.9466666666666668\n",
      "policy_reward_min:\n",
      "  player_0: -30.666666666666664\n",
      "  player_1: -39.333333333333336\n",
      "  player_2: -42.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08663197824393787\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30790747502987625\n",
      "  mean_inference_ms: 1.6087459757248126\n",
      "  mean_raw_obs_processing_ms: 0.208825042346056\n",
      "time_since_restore: 5629.762493848801\n",
      "time_this_iter_s: 15.621227979660034\n",
      "time_total_s: 5629.762493848801\n",
      "timers:\n",
      "  learn_throughput: 572.824\n",
      "  learn_time_ms: 13930.988\n",
      "  load_throughput: 1150380.335\n",
      "  load_time_ms: 6.937\n",
      "  sample_throughput: 520.417\n",
      "  sample_time_ms: 15333.847\n",
      "  update_time_ms: 5.687\n",
      "timestamp: 1643541858\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2848860\n",
      "training_iteration: 357\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2864790\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-24-49\n",
      "done: false\n",
      "episode_len_mean: 172.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 44\n",
      "episodes_total: 12197\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5009821395576001\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016782341260569544\n",
      "        policy_loss: -0.10171420836200316\n",
      "        total_loss: 51.46176548957825\n",
      "        vf_explained_var: 0.17532957663138707\n",
      "        vf_loss: 51.55215145587921\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5485790230830511\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014304863420912473\n",
      "        policy_loss: -0.0801935945233951\n",
      "        total_loss: 71.06828579266866\n",
      "        vf_explained_var: 0.06075171997149786\n",
      "        vf_loss: 71.13882361094157\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5121374847988288\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015885306717658143\n",
      "        policy_loss: -0.03595932538465907\n",
      "        total_loss: 64.55282906214396\n",
      "        vf_explained_var: 0.17693709899981816\n",
      "        vf_loss: 64.57806563059489\n",
      "  num_agent_steps_sampled: 2864790\n",
      "  num_agent_steps_trained: 2864790\n",
      "  num_steps_sampled: 2864820\n",
      "  num_steps_trained: 2864820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 359\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.969999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.66666666666667\n",
      "  player_1: 34.0\n",
      "  player_2: 35.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 2.4633333333333334\n",
      "  player_1: -2.266666666666666\n",
      "  player_2: 2.8033333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -47.33333333333333\n",
      "  player_1: -53.66666666666667\n",
      "  player_2: -46.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0867942273660664\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3077322191001538\n",
      "  mean_inference_ms: 1.609808376598619\n",
      "  mean_raw_obs_processing_ms: 0.2087053645445211\n",
      "time_since_restore: 5660.9080722332\n",
      "time_this_iter_s: 15.830989360809326\n",
      "time_total_s: 5660.9080722332\n",
      "timers:\n",
      "  learn_throughput: 568.475\n",
      "  learn_time_ms: 14037.565\n",
      "  load_throughput: 1079984.187\n",
      "  load_time_ms: 7.389\n",
      "  sample_throughput: 524.397\n",
      "  sample_time_ms: 15217.493\n",
      "  update_time_ms: 5.718\n",
      "timestamp: 1643541889\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2864820\n",
      "training_iteration: 359\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2880750\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-25-22\n",
      "done: false\n",
      "episode_len_mean: 165.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 42\n",
      "episodes_total: 12291\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5029586609701315\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017308314394628987\n",
      "        policy_loss: -0.09253910311187306\n",
      "        total_loss: 57.741555142402646\n",
      "        vf_explained_var: 0.2031089659531911\n",
      "        vf_loss: 57.822410966555275\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5276258108516534\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014435934175623211\n",
      "        policy_loss: -0.05155898191655676\n",
      "        total_loss: 32.896742666562396\n",
      "        vf_explained_var: 0.20353595306475958\n",
      "        vf_loss: 32.938557341893514\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49558499510089554\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01635985268094752\n",
      "        policy_loss: -0.06573520384728909\n",
      "        total_loss: 44.24955091794332\n",
      "        vf_explained_var: 0.13285060932238896\n",
      "        vf_loss: 44.30424321095149\n",
      "  num_agent_steps_sampled: 2880750\n",
      "  num_agent_steps_trained: 2880750\n",
      "  num_steps_sampled: 2880780\n",
      "  num_steps_trained: 2880780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 361\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.231578947368423\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.66666666666667\n",
      "  player_1: 30.333333333333336\n",
      "  player_2: 37.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.16\n",
      "  player_1: -5.1\n",
      "  player_2: 5.939999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -43.666666666666664\n",
      "  player_1: -44.33333333333333\n",
      "  player_2: -34.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08664703590578914\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30773674446597327\n",
      "  mean_inference_ms: 1.6090447377753503\n",
      "  mean_raw_obs_processing_ms: 0.20888875366569654\n",
      "time_since_restore: 5693.436088562012\n",
      "time_this_iter_s: 15.464001178741455\n",
      "time_total_s: 5693.436088562012\n",
      "timers:\n",
      "  learn_throughput: 560.279\n",
      "  learn_time_ms: 14242.903\n",
      "  load_throughput: 1041395.202\n",
      "  load_time_ms: 7.663\n",
      "  sample_throughput: 515.288\n",
      "  sample_time_ms: 15486.487\n",
      "  update_time_ms: 5.522\n",
      "timestamp: 1643541922\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2880780\n",
      "training_iteration: 361\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2896710\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-25-52\n",
      "done: false\n",
      "episode_len_mean: 174.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 42\n",
      "episodes_total: 12379\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48087348332007723\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016552336827969708\n",
      "        policy_loss: -0.08405510030376415\n",
      "        total_loss: 50.473648711045584\n",
      "        vf_explained_var: 0.2842471252878507\n",
      "        vf_loss: 50.54653082291285\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5203291675945123\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01566422147323768\n",
      "        policy_loss: -0.08384855142484109\n",
      "        total_loss: 42.70725795269013\n",
      "        vf_explained_var: 0.35443896730740865\n",
      "        vf_loss: 42.78053323904673\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4896043055256208\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013634903086276608\n",
      "        policy_loss: -0.06261016879153128\n",
      "        total_loss: 101.1632791185379\n",
      "        vf_explained_var: 0.3126743183533351\n",
      "        vf_loss: 101.21668561458587\n",
      "  num_agent_steps_sampled: 2896710\n",
      "  num_agent_steps_trained: 2896710\n",
      "  num_steps_sampled: 2896740\n",
      "  num_steps_trained: 2896740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 363\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.989473684210527\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.0\n",
      "  player_1: 28.0\n",
      "  player_2: 39.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 4.086666666666667\n",
      "  player_1: -3.9033333333333338\n",
      "  player_2: 2.816666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -51.0\n",
      "  player_1: -34.0\n",
      "  player_2: -57.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08668405166788734\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30728487458502374\n",
      "  mean_inference_ms: 1.6079682987625916\n",
      "  mean_raw_obs_processing_ms: 0.2085239843288879\n",
      "time_since_restore: 5724.249621629715\n",
      "time_this_iter_s: 15.091307163238525\n",
      "time_total_s: 5724.249621629715\n",
      "timers:\n",
      "  learn_throughput: 556.97\n",
      "  learn_time_ms: 14327.526\n",
      "  load_throughput: 1037183.874\n",
      "  load_time_ms: 7.694\n",
      "  sample_throughput: 512.942\n",
      "  sample_time_ms: 15557.321\n",
      "  update_time_ms: 5.595\n",
      "timestamp: 1643541952\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2896740\n",
      "training_iteration: 363\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2912670\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-26-22\n",
      "done: false\n",
      "episode_len_mean: 163.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 49\n",
      "episodes_total: 12482\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4856566760937373\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013598400111240305\n",
      "        policy_loss: -0.0950365090649575\n",
      "        total_loss: 78.95303560892741\n",
      "        vf_explained_var: 0.23640658577283225\n",
      "        vf_loss: 79.03889357884725\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5394720274706681\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014515627237050618\n",
      "        policy_loss: -0.044098850525915625\n",
      "        total_loss: 56.99938158830007\n",
      "        vf_explained_var: 0.20683081865310668\n",
      "        vf_loss: 57.033682648340864\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5141876603662968\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01459748098931584\n",
      "        policy_loss: -0.06100534621315698\n",
      "        total_loss: 68.91414943536122\n",
      "        vf_explained_var: 0.2353940502802531\n",
      "        vf_loss: 68.96530190785725\n",
      "  num_agent_steps_sampled: 2912670\n",
      "  num_agent_steps_trained: 2912670\n",
      "  num_steps_sampled: 2912700\n",
      "  num_steps_trained: 2912700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 365\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.311764705882354\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.9\n",
      "  vram_util_percent0: 0.3741861979166667\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 43.333333333333336\n",
      "  player_1: 39.0\n",
      "  player_2: 32.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 4.303333333333333\n",
      "  player_1: -1.3066666666666666\n",
      "  player_2: 0.003333333333333712\n",
      "policy_reward_min:\n",
      "  player_0: -54.333333333333336\n",
      "  player_1: -34.333333333333336\n",
      "  player_2: -55.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08653959996158972\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30750101567837373\n",
      "  mean_inference_ms: 1.6090203899976825\n",
      "  mean_raw_obs_processing_ms: 0.20866728588041297\n",
      "time_since_restore: 5753.962690591812\n",
      "time_this_iter_s: 14.408567905426025\n",
      "time_total_s: 5753.962690591812\n",
      "timers:\n",
      "  learn_throughput: 560.434\n",
      "  learn_time_ms: 14238.973\n",
      "  load_throughput: 1043622.715\n",
      "  load_time_ms: 7.646\n",
      "  sample_throughput: 510.861\n",
      "  sample_time_ms: 15620.675\n",
      "  update_time_ms: 5.582\n",
      "timestamp: 1643541982\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2912700\n",
      "training_iteration: 365\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2928631\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-26-53\n",
      "done: false\n",
      "episode_len_mean: 153.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 42\n",
      "episodes_total: 12584\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4828239704668522\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016097147411943902\n",
      "        policy_loss: -0.08040050496036807\n",
      "        total_loss: 41.42771311124166\n",
      "        vf_explained_var: 0.35043474634488425\n",
      "        vf_loss: 41.49724804242452\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5190848502516746\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01548610662221165\n",
      "        policy_loss: -0.10943403683292369\n",
      "        total_loss: 41.96383134285609\n",
      "        vf_explained_var: 0.06627297977606456\n",
      "        vf_loss: 42.06281218449275\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5049401007095973\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016122238934497848\n",
      "        policy_loss: -0.05223571084439754\n",
      "        total_loss: 60.045782070954644\n",
      "        vf_explained_var: 0.21228783120711645\n",
      "        vf_loss: 60.0871352426211\n",
      "  num_agent_steps_sampled: 2928631\n",
      "  num_agent_steps_trained: 2928631\n",
      "  num_steps_sampled: 2928660\n",
      "  num_steps_trained: 2928660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 367\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.994736842105265\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 42.0\n",
      "  player_1: 36.666666666666664\n",
      "  player_2: 40.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 0.41333333333333344\n",
      "  player_1: -2.516666666666666\n",
      "  player_2: 5.1033333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -48.333333333333336\n",
      "  player_1: -41.666666666666664\n",
      "  player_2: -63.33333333333334\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08661263483276706\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3082681061746274\n",
      "  mean_inference_ms: 1.6113308186459179\n",
      "  mean_raw_obs_processing_ms: 0.20890275153217652\n",
      "time_since_restore: 5784.526462554932\n",
      "time_this_iter_s: 15.246899843215942\n",
      "time_total_s: 5784.526462554932\n",
      "timers:\n",
      "  learn_throughput: 558.7\n",
      "  learn_time_ms: 14283.16\n",
      "  load_throughput: 1026061.787\n",
      "  load_time_ms: 7.777\n",
      "  sample_throughput: 513.349\n",
      "  sample_time_ms: 15544.968\n",
      "  update_time_ms: 5.652\n",
      "timestamp: 1643542013\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2928660\n",
      "training_iteration: 367\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2944590\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-27-25\n",
      "done: false\n",
      "episode_len_mean: 176.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 49\n",
      "episodes_total: 12677\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4777131004631519\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015691502069801307\n",
      "        policy_loss: -0.0508221364642183\n",
      "        total_loss: 57.946205999056495\n",
      "        vf_explained_var: 0.23325367470582326\n",
      "        vf_loss: 57.98643637975057\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5359963005284468\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01653255217733507\n",
      "        policy_loss: -0.10557824299981197\n",
      "        total_loss: 41.17818963209788\n",
      "        vf_explained_var: 0.18562121679385504\n",
      "        vf_loss: 41.27260848840078\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5094371389349301\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016731977507072317\n",
      "        policy_loss: -0.07343404480566582\n",
      "        total_loss: 84.06078142881394\n",
      "        vf_explained_var: 0.2157197641332944\n",
      "        vf_loss: 84.122921778361\n",
      "  num_agent_steps_sampled: 2944590\n",
      "  num_agent_steps_trained: 2944590\n",
      "  num_steps_sampled: 2944620\n",
      "  num_steps_trained: 2944620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 369\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.747619047619047\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.0\n",
      "  player_1: 25.666666666666664\n",
      "  player_2: 37.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 4.22\n",
      "  player_1: -4.27\n",
      "  player_2: 3.05\n",
      "policy_reward_min:\n",
      "  player_0: -38.666666666666664\n",
      "  player_1: -64.0\n",
      "  player_2: -40.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08654783965942245\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3071245112437919\n",
      "  mean_inference_ms: 1.6067480272714323\n",
      "  mean_raw_obs_processing_ms: 0.20863085225628467\n",
      "time_since_restore: 5816.759827613831\n",
      "time_this_iter_s: 16.93171000480652\n",
      "time_total_s: 5816.759827613831\n",
      "timers:\n",
      "  learn_throughput: 554.502\n",
      "  learn_time_ms: 14391.28\n",
      "  load_throughput: 1031608.751\n",
      "  load_time_ms: 7.735\n",
      "  sample_throughput: 514.652\n",
      "  sample_time_ms: 15505.608\n",
      "  update_time_ms: 5.579\n",
      "timestamp: 1643542045\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2944620\n",
      "training_iteration: 369\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2960550\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-27-58\n",
      "done: false\n",
      "episode_len_mean: 155.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 53\n",
      "episodes_total: 12781\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4763982792695363\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016246925420517946\n",
      "        policy_loss: -0.0892041660224398\n",
      "        total_loss: 63.64647738933563\n",
      "        vf_explained_var: 0.34627271513144176\n",
      "        vf_loss: 63.72471459388733\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.530057552208503\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015460515152162013\n",
      "        policy_loss: -0.06746432673806946\n",
      "        total_loss: 56.74887341976166\n",
      "        vf_explained_var: 0.023577359914779664\n",
      "        vf_loss: 56.805901829401655\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5131048286457857\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014948969376823319\n",
      "        policy_loss: -0.0765022414860626\n",
      "        total_loss: 84.60269359747569\n",
      "        vf_explained_var: 0.22373803913593293\n",
      "        vf_loss: 84.66910519440968\n",
      "  num_agent_steps_sampled: 2960550\n",
      "  num_agent_steps_trained: 2960550\n",
      "  num_steps_sampled: 2960580\n",
      "  num_steps_trained: 2960580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 371\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.352631578947365\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 40.0\n",
      "  player_2: 30.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.536666666666667\n",
      "  player_1: -1.3433333333333326\n",
      "  player_2: 0.8066666666666673\n",
      "policy_reward_min:\n",
      "  player_0: -26.333333333333332\n",
      "  player_1: -53.0\n",
      "  player_2: -51.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0864890767632133\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30718302265325925\n",
      "  mean_inference_ms: 1.6074251383258176\n",
      "  mean_raw_obs_processing_ms: 0.2087706433340712\n",
      "time_since_restore: 5849.09422659874\n",
      "time_this_iter_s: 15.31209397315979\n",
      "time_total_s: 5849.09422659874\n",
      "timers:\n",
      "  learn_throughput: 555.234\n",
      "  learn_time_ms: 14372.317\n",
      "  load_throughput: 1012115.123\n",
      "  load_time_ms: 7.884\n",
      "  sample_throughput: 511.337\n",
      "  sample_time_ms: 15606.133\n",
      "  update_time_ms: 5.548\n",
      "timestamp: 1643542078\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2960580\n",
      "training_iteration: 371\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2976511\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-28-28\n",
      "done: false\n",
      "episode_len_mean: 153.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 44\n",
      "episodes_total: 12879\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4740029793481032\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01550851484348944\n",
      "        policy_loss: -0.0940696087355415\n",
      "        total_loss: 44.61253971417745\n",
      "        vf_explained_var: 0.12435539354880651\n",
      "        vf_loss: 44.69614090760549\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5241560229162375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015272515795268192\n",
      "        policy_loss: -0.11237527249380946\n",
      "        total_loss: 42.03850822766622\n",
      "        vf_explained_var: 0.15252725253502528\n",
      "        vf_loss: 42.140574595133465\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4808908056716124\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015393979252800137\n",
      "        policy_loss: -0.04168972280031691\n",
      "        total_loss: 47.2942831047376\n",
      "        vf_explained_var: -0.010287970006465912\n",
      "        vf_loss: 47.32558183670044\n",
      "  num_agent_steps_sampled: 2976511\n",
      "  num_agent_steps_trained: 2976511\n",
      "  num_steps_sampled: 2976540\n",
      "  num_steps_trained: 2976540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 373\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.873684210526317\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.333333333333336\n",
      "  player_1: 32.333333333333336\n",
      "  player_2: 36.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.5133333333333336\n",
      "  player_1: -2.056666666666667\n",
      "  player_2: 1.5433333333333337\n",
      "policy_reward_min:\n",
      "  player_0: -32.666666666666664\n",
      "  player_1: -34.333333333333336\n",
      "  player_2: -48.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08644538268797333\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30663443718041433\n",
      "  mean_inference_ms: 1.6049673518404253\n",
      "  mean_raw_obs_processing_ms: 0.20865375308085113\n",
      "time_since_restore: 5879.813232898712\n",
      "time_this_iter_s: 15.157306432723999\n",
      "time_total_s: 5879.813232898712\n",
      "timers:\n",
      "  learn_throughput: 555.699\n",
      "  learn_time_ms: 14360.289\n",
      "  load_throughput: 936980.769\n",
      "  load_time_ms: 8.517\n",
      "  sample_throughput: 512.214\n",
      "  sample_time_ms: 15579.425\n",
      "  update_time_ms: 5.503\n",
      "timestamp: 1643542108\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2976540\n",
      "training_iteration: 373\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 2992470\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-28-58\n",
      "done: false\n",
      "episode_len_mean: 169.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 47\n",
      "episodes_total: 12976\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48615495815873144\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016146037228707354\n",
      "        policy_loss: -0.07073749060121676\n",
      "        total_loss: 62.512081850369775\n",
      "        vf_explained_var: 0.41517247051000594\n",
      "        vf_loss: 62.5719207962354\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5293776430686314\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014168686094029681\n",
      "        policy_loss: -0.12113020059963067\n",
      "        total_loss: 42.12745029767354\n",
      "        vf_explained_var: 0.2604869854450226\n",
      "        vf_loss: 42.23901671409607\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47969283948342006\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013929517813399931\n",
      "        policy_loss: -0.04422341542706514\n",
      "        total_loss: 111.73032515207926\n",
      "        vf_explained_var: 0.31727250854174294\n",
      "        vf_loss: 111.76514618555704\n",
      "  num_agent_steps_sampled: 2992470\n",
      "  num_agent_steps_trained: 2992470\n",
      "  num_steps_sampled: 2992500\n",
      "  num_steps_trained: 2992500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 375\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.410526315789474\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 41.66666666666667\n",
      "  player_2: 32.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 0.99\n",
      "  player_1: -1.53\n",
      "  player_2: 3.5399999999999996\n",
      "policy_reward_min:\n",
      "  player_0: -39.66666666666667\n",
      "  player_1: -39.333333333333336\n",
      "  player_2: -70.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0864474028792584\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30675268149774715\n",
      "  mean_inference_ms: 1.6067127170616977\n",
      "  mean_raw_obs_processing_ms: 0.20851934408616946\n",
      "time_since_restore: 5909.676244735718\n",
      "time_this_iter_s: 15.081648826599121\n",
      "time_total_s: 5909.676244735718\n",
      "timers:\n",
      "  learn_throughput: 555.207\n",
      "  learn_time_ms: 14373.03\n",
      "  load_throughput: 930774.166\n",
      "  load_time_ms: 8.574\n",
      "  sample_throughput: 513.72\n",
      "  sample_time_ms: 15533.753\n",
      "  update_time_ms: 5.576\n",
      "timestamp: 1643542138\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2992500\n",
      "training_iteration: 375\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3008432\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-29-29\n",
      "done: false\n",
      "episode_len_mean: 159.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 48\n",
      "episodes_total: 13075\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4735355902214845\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01564139950996226\n",
      "        policy_loss: -0.07651860916987062\n",
      "        total_loss: 43.902242356936135\n",
      "        vf_explained_var: 0.06808017730712891\n",
      "        vf_loss: 43.968202946980796\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.525833780169487\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015400402131844734\n",
      "        policy_loss: -0.07248311094318827\n",
      "        total_loss: 47.099852206309635\n",
      "        vf_explained_var: -0.047156968613465626\n",
      "        vf_loss: 47.16193998416265\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49299541115760803\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015421559413116483\n",
      "        policy_loss: -0.07632635402182739\n",
      "        total_loss: 73.68819538593293\n",
      "        vf_explained_var: 0.128048437833786\n",
      "        vf_loss: 73.75411231517792\n",
      "  num_agent_steps_sampled: 3008432\n",
      "  num_agent_steps_trained: 3008432\n",
      "  num_steps_sampled: 3008460\n",
      "  num_steps_trained: 3008460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 377\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.463157894736844\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.333333333333336\n",
      "  player_1: 30.33333333333333\n",
      "  player_2: 42.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.9899999999999998\n",
      "  player_1: -4.8\n",
      "  player_2: 6.809999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -36.666666666666664\n",
      "  player_1: -45.0\n",
      "  player_2: -38.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08649883533459729\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3061068085206191\n",
      "  mean_inference_ms: 1.6031060605877838\n",
      "  mean_raw_obs_processing_ms: 0.2084887054901995\n",
      "time_since_restore: 5940.686458110809\n",
      "time_this_iter_s: 15.467065572738647\n",
      "time_total_s: 5940.686458110809\n",
      "timers:\n",
      "  learn_throughput: 553.38\n",
      "  learn_time_ms: 14420.472\n",
      "  load_throughput: 940157.465\n",
      "  load_time_ms: 8.488\n",
      "  sample_throughput: 510.996\n",
      "  sample_time_ms: 15616.57\n",
      "  update_time_ms: 5.519\n",
      "timestamp: 1643542169\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3008460\n",
      "training_iteration: 377\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3024390\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-30-02\n",
      "done: false\n",
      "episode_len_mean: 162.28\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 45\n",
      "episodes_total: 13168\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4625175876170397\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016491346138478018\n",
      "        policy_loss: -0.07000273023732007\n",
      "        total_loss: 51.478068748315174\n",
      "        vf_explained_var: 0.24814479500055314\n",
      "        vf_loss: 51.53693977514903\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5293521212538084\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014557905555687967\n",
      "        policy_loss: -0.10157309747766703\n",
      "        total_loss: 42.975284570852914\n",
      "        vf_explained_var: 0.20365864177544912\n",
      "        vf_loss: 43.06703088283539\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4781874476869901\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014258898888074327\n",
      "        policy_loss: -0.07310572439183792\n",
      "        total_loss: 73.48281980196634\n",
      "        vf_explained_var: 0.2772138806184133\n",
      "        vf_loss: 73.54630036830902\n",
      "  num_agent_steps_sampled: 3024390\n",
      "  num_agent_steps_trained: 3024390\n",
      "  num_steps_sampled: 3024420\n",
      "  num_steps_trained: 3024420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 379\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.519047619047617\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.0\n",
      "  player_1: 34.33333333333333\n",
      "  player_2: 40.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.38\n",
      "  player_1: -2.929999999999999\n",
      "  player_2: 3.5500000000000007\n",
      "policy_reward_min:\n",
      "  player_0: -36.0\n",
      "  player_1: -59.666666666666664\n",
      "  player_2: -68.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08636564045305292\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3057906478788031\n",
      "  mean_inference_ms: 1.6027869233606418\n",
      "  mean_raw_obs_processing_ms: 0.20839718472288077\n",
      "time_since_restore: 5973.562166690826\n",
      "time_this_iter_s: 17.22813320159912\n",
      "time_total_s: 5973.562166690826\n",
      "timers:\n",
      "  learn_throughput: 550.826\n",
      "  learn_time_ms: 14487.334\n",
      "  load_throughput: 981011.593\n",
      "  load_time_ms: 8.134\n",
      "  sample_throughput: 509.097\n",
      "  sample_time_ms: 15674.811\n",
      "  update_time_ms: 5.702\n",
      "timestamp: 1643542202\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3024420\n",
      "training_iteration: 379\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3040351\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-30-35\n",
      "done: false\n",
      "episode_len_mean: 175.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 50\n",
      "episodes_total: 13264\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47816645974914235\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01645577312503387\n",
      "        policy_loss: -0.07396687950318058\n",
      "        total_loss: 79.84091282526651\n",
      "        vf_explained_var: 0.30278784374396006\n",
      "        vf_loss: 79.90377185821534\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5279027519623438\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01426481792823722\n",
      "        policy_loss: -0.1207046723086387\n",
      "        total_loss: 64.8162148698171\n",
      "        vf_explained_var: 0.17556827237208683\n",
      "        vf_loss: 64.92729076464971\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.493062880585591\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015236291397659443\n",
      "        policy_loss: -0.05017147763942679\n",
      "        total_loss: 99.97137545426686\n",
      "        vf_explained_var: 0.05659127374490102\n",
      "        vf_loss: 100.01126229604085\n",
      "  num_agent_steps_sampled: 3040351\n",
      "  num_agent_steps_trained: 3040351\n",
      "  num_steps_sampled: 3040380\n",
      "  num_steps_trained: 3040380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 381\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.777777777777779\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 57.33333333333333\n",
      "  player_1: 51.0\n",
      "  player_2: 36.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.3000000000000003\n",
      "  player_1: -0.2699999999999997\n",
      "  player_2: 0.9699999999999998\n",
      "policy_reward_min:\n",
      "  player_0: -60.0\n",
      "  player_1: -37.0\n",
      "  player_2: -59.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08638446346607895\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3061527048205195\n",
      "  mean_inference_ms: 1.6045244564345402\n",
      "  mean_raw_obs_processing_ms: 0.20861247153164708\n",
      "time_since_restore: 6006.578036308289\n",
      "time_this_iter_s: 14.673987865447998\n",
      "time_total_s: 6006.578036308289\n",
      "timers:\n",
      "  learn_throughput: 548.195\n",
      "  learn_time_ms: 14556.868\n",
      "  load_throughput: 1021337.143\n",
      "  load_time_ms: 7.813\n",
      "  sample_throughput: 503.895\n",
      "  sample_time_ms: 15836.636\n",
      "  update_time_ms: 5.765\n",
      "timestamp: 1643542235\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3040380\n",
      "training_iteration: 381\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3056310\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-31-08\n",
      "done: false\n",
      "episode_len_mean: 176.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 45\n",
      "episodes_total: 13355\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46887087414662043\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01665950000453449\n",
      "        policy_loss: -0.09881398061911265\n",
      "        total_loss: 49.353997915585836\n",
      "        vf_explained_var: 0.08248917460441589\n",
      "        vf_loss: 49.44156667391459\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5500416098535061\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015140512276988337\n",
      "        policy_loss: -0.03430976121220738\n",
      "        total_loss: 41.213375143210094\n",
      "        vf_explained_var: 0.1647192077835401\n",
      "        vf_loss: 41.237465036710105\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4793475389480591\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01633053491396216\n",
      "        policy_loss: -0.08321288894318665\n",
      "        total_loss: 76.34960589408874\n",
      "        vf_explained_var: 0.27514331112305324\n",
      "        vf_loss: 76.42179573376974\n",
      "  num_agent_steps_sampled: 3056310\n",
      "  num_agent_steps_trained: 3056310\n",
      "  num_steps_sampled: 3056340\n",
      "  num_steps_trained: 3056340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 383\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.29047619047619\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 40.0\n",
      "  player_1: 27.0\n",
      "  player_2: 29.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 5.420000000000001\n",
      "  player_1: -1.72\n",
      "  player_2: -0.6999999999999997\n",
      "policy_reward_min:\n",
      "  player_0: -31.666666666666664\n",
      "  player_1: -34.333333333333336\n",
      "  player_2: -55.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08652522744415173\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3064557098851734\n",
      "  mean_inference_ms: 1.6075548360849374\n",
      "  mean_raw_obs_processing_ms: 0.20859582668864354\n",
      "time_since_restore: 6039.144142150879\n",
      "time_this_iter_s: 17.12187910079956\n",
      "time_total_s: 6039.144142150879\n",
      "timers:\n",
      "  learn_throughput: 541.382\n",
      "  learn_time_ms: 14740.061\n",
      "  load_throughput: 1018651.516\n",
      "  load_time_ms: 7.834\n",
      "  sample_throughput: 506.282\n",
      "  sample_time_ms: 15761.98\n",
      "  update_time_ms: 5.855\n",
      "timestamp: 1643542268\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3056340\n",
      "training_iteration: 383\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3072272\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-31-39\n",
      "done: false\n",
      "episode_len_mean: 160.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 51\n",
      "episodes_total: 13455\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4707831545670827\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017014713407226243\n",
      "        policy_loss: -0.06997681144935389\n",
      "        total_loss: 40.949508771101634\n",
      "        vf_explained_var: 0.34224522988001504\n",
      "        vf_loss: 41.008000401655835\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.510975289295117\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015497866673429334\n",
      "        policy_loss: -0.11928617426504691\n",
      "        total_loss: 40.11265138705571\n",
      "        vf_explained_var: 0.22786824276049933\n",
      "        vf_loss: 40.22147661765416\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.495792994449536\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01563805443301438\n",
      "        policy_loss: -0.05165187292421858\n",
      "        total_loss: 64.0860267829895\n",
      "        vf_explained_var: 0.2288980836669604\n",
      "        vf_loss: 64.12712306817373\n",
      "  num_agent_steps_sampled: 3072272\n",
      "  num_agent_steps_trained: 3072272\n",
      "  num_steps_sampled: 3072300\n",
      "  num_steps_trained: 3072300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 385\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.828571428571431\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 27.0\n",
      "  player_2: 36.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 5.456666666666667\n",
      "  player_1: -5.783333333333332\n",
      "  player_2: 3.326666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -38.0\n",
      "  player_1: -36.666666666666664\n",
      "  player_2: -36.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0864054139539111\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3058820473332876\n",
      "  mean_inference_ms: 1.6039922343629178\n",
      "  mean_raw_obs_processing_ms: 0.20849192407866243\n",
      "time_since_restore: 6070.574909925461\n",
      "time_this_iter_s: 16.656165838241577\n",
      "time_total_s: 6070.574909925461\n",
      "timers:\n",
      "  learn_throughput: 536.04\n",
      "  learn_time_ms: 14886.962\n",
      "  load_throughput: 1022932.192\n",
      "  load_time_ms: 7.801\n",
      "  sample_throughput: 499.807\n",
      "  sample_time_ms: 15966.178\n",
      "  update_time_ms: 5.853\n",
      "timestamp: 1643542299\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3072300\n",
      "training_iteration: 385\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3088232\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-32-10\n",
      "done: false\n",
      "episode_len_mean: 157.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 48\n",
      "episodes_total: 13551\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4555844440559546\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016862097972444777\n",
      "        policy_loss: -0.06924429103732109\n",
      "        total_loss: 47.00247006336848\n",
      "        vf_explained_var: 0.24237972577412922\n",
      "        vf_loss: 47.06033232132594\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5195800266166528\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015944354775068253\n",
      "        policy_loss: -0.07823030803973476\n",
      "        total_loss: 59.39977285861969\n",
      "        vf_explained_var: 0.20425869246323902\n",
      "        vf_loss: 59.46724085330963\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4806181454161803\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015240730882440327\n",
      "        policy_loss: -0.06756730616558343\n",
      "        total_loss: 79.58408196767171\n",
      "        vf_explained_var: 0.21284666150808335\n",
      "        vf_loss: 79.64136170864106\n",
      "  num_agent_steps_sampled: 3088232\n",
      "  num_agent_steps_trained: 3088232\n",
      "  num_steps_sampled: 3088260\n",
      "  num_steps_trained: 3088260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 387\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.82777777777778\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.0\n",
      "  player_1: 31.0\n",
      "  player_2: 36.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 3.4233333333333333\n",
      "  player_1: -3.236666666666667\n",
      "  player_2: 2.8133333333333326\n",
      "policy_reward_min:\n",
      "  player_0: -43.333333333333336\n",
      "  player_1: -46.666666666666664\n",
      "  player_2: -53.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08632312646889571\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30546909349654827\n",
      "  mean_inference_ms: 1.6038814897947125\n",
      "  mean_raw_obs_processing_ms: 0.20822124732415204\n",
      "time_since_restore: 6100.736400127411\n",
      "time_this_iter_s: 14.877684116363525\n",
      "time_total_s: 6100.736400127411\n",
      "timers:\n",
      "  learn_throughput: 539.096\n",
      "  learn_time_ms: 14802.566\n",
      "  load_throughput: 1031389.408\n",
      "  load_time_ms: 7.737\n",
      "  sample_throughput: 495.913\n",
      "  sample_time_ms: 16091.517\n",
      "  update_time_ms: 5.861\n",
      "timestamp: 1643542330\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3088260\n",
      "training_iteration: 387\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3104193\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-32-42\n",
      "done: false\n",
      "episode_len_mean: 166.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 43\n",
      "episodes_total: 13638\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4786019748946031\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015626181679638952\n",
      "        policy_loss: -0.09222916603088378\n",
      "        total_loss: 43.02555962244669\n",
      "        vf_explained_var: 0.24740854809681576\n",
      "        vf_loss: 43.10724115928014\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5086983274420103\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016722031907545065\n",
      "        policy_loss: -0.11022099149102967\n",
      "        total_loss: 38.848623124758404\n",
      "        vf_explained_var: 0.13005929738283156\n",
      "        vf_loss: 38.9475568596522\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4782565793395042\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015373682324376811\n",
      "        policy_loss: -0.029014378148131074\n",
      "        total_loss: 50.82017655769984\n",
      "        vf_explained_var: 0.17732549140850704\n",
      "        vf_loss: 50.83881379524867\n",
      "  num_agent_steps_sampled: 3104193\n",
      "  num_agent_steps_trained: 3104193\n",
      "  num_steps_sampled: 3104220\n",
      "  num_steps_trained: 3104220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 389\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.527272727272726\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000003\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 40.33333333333333\n",
      "  player_1: 37.0\n",
      "  player_2: 29.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.5\n",
      "  player_1: -5.03\n",
      "  player_2: 4.53\n",
      "policy_reward_min:\n",
      "  player_0: -41.0\n",
      "  player_1: -46.666666666666664\n",
      "  player_2: -52.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08634014413250067\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30492604287257447\n",
      "  mean_inference_ms: 1.6018073130114951\n",
      "  mean_raw_obs_processing_ms: 0.20815182050186748\n",
      "time_since_restore: 6132.877209663391\n",
      "time_this_iter_s: 17.35818123817444\n",
      "time_total_s: 6132.877209663391\n",
      "timers:\n",
      "  learn_throughput: 541.98\n",
      "  learn_time_ms: 14723.777\n",
      "  load_throughput: 1047345.566\n",
      "  load_time_ms: 7.619\n",
      "  sample_throughput: 500.407\n",
      "  sample_time_ms: 15947.021\n",
      "  update_time_ms: 5.638\n",
      "timestamp: 1643542362\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3104220\n",
      "training_iteration: 389\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3120150\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-33-13\n",
      "done: false\n",
      "episode_len_mean: 186.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 45\n",
      "episodes_total: 13730\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4323796315987905\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015993088455497854\n",
      "        policy_loss: -0.08858346195270618\n",
      "        total_loss: 45.44975650946299\n",
      "        vf_explained_var: 0.20754258473714193\n",
      "        vf_loss: 45.5275447567304\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5084692855676015\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014770844236234855\n",
      "        policy_loss: -0.08791001575843742\n",
      "        total_loss: 47.234562055269876\n",
      "        vf_explained_var: 0.1881400760014852\n",
      "        vf_loss: 47.312501700719196\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48607241744796437\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015443106771078116\n",
      "        policy_loss: -0.04133092262316495\n",
      "        total_loss: 65.7108544309934\n",
      "        vf_explained_var: 0.38689030597607293\n",
      "        vf_loss: 65.74176127036412\n",
      "  num_agent_steps_sampled: 3120150\n",
      "  num_agent_steps_trained: 3120150\n",
      "  num_steps_sampled: 3120180\n",
      "  num_steps_trained: 3120180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 391\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.945000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.96000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 47.333333333333336\n",
      "  player_1: 30.0\n",
      "  player_2: 32.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.0399999999999998\n",
      "  player_1: -4.3500000000000005\n",
      "  player_2: 6.31\n",
      "policy_reward_min:\n",
      "  player_0: -35.666666666666664\n",
      "  player_1: -59.666666666666664\n",
      "  player_2: -38.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0863161100540588\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3052940348633556\n",
      "  mean_inference_ms: 1.6042293021899676\n",
      "  mean_raw_obs_processing_ms: 0.20820046426430147\n",
      "time_since_restore: 6164.229834794998\n",
      "time_this_iter_s: 16.309839010238647\n",
      "time_total_s: 6164.229834794998\n",
      "timers:\n",
      "  learn_throughput: 548.195\n",
      "  learn_time_ms: 14556.863\n",
      "  load_throughput: 1012537.653\n",
      "  load_time_ms: 7.881\n",
      "  sample_throughput: 510.345\n",
      "  sample_time_ms: 15636.48\n",
      "  update_time_ms: 5.61\n",
      "timestamp: 1643542393\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3120180\n",
      "training_iteration: 391\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3136110\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-33-46\n",
      "done: false\n",
      "episode_len_mean: 165.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 55\n",
      "episodes_total: 13830\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4839684633910656\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01783515100471675\n",
      "        policy_loss: -0.07481733413723608\n",
      "        total_loss: 53.61064558506012\n",
      "        vf_explained_var: 0.2784332847595215\n",
      "        vf_loss: 53.67342426776886\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5435313032567501\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01648200017498065\n",
      "        policy_loss: -0.042361961783220374\n",
      "        total_loss: 38.18769604365031\n",
      "        vf_explained_var: 0.2686366495490074\n",
      "        vf_loss: 38.218932546774546\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5012214926878611\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015586276750509569\n",
      "        policy_loss: -0.12467229940462858\n",
      "        total_loss: 58.49492455323537\n",
      "        vf_explained_var: 0.34755732069412865\n",
      "        vf_loss: 58.60907621383667\n",
      "  num_agent_steps_sampled: 3136110\n",
      "  num_agent_steps_trained: 3136110\n",
      "  num_steps_sampled: 3136140\n",
      "  num_steps_trained: 3136140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 393\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.368421052631579\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.92105263157897\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 40.33333333333333\n",
      "  player_1: 33.0\n",
      "  player_2: 32.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 1.4966666666666666\n",
      "  player_1: -1.473333333333334\n",
      "  player_2: 2.976666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -38.333333333333336\n",
      "  player_1: -56.66666666666667\n",
      "  player_2: -45.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08621897016652248\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3051864790036634\n",
      "  mean_inference_ms: 1.60286118552523\n",
      "  mean_raw_obs_processing_ms: 0.2082738945865255\n",
      "time_since_restore: 6196.72606086731\n",
      "time_this_iter_s: 15.71383023262024\n",
      "time_total_s: 6196.72606086731\n",
      "timers:\n",
      "  learn_throughput: 548.387\n",
      "  learn_time_ms: 14551.777\n",
      "  load_throughput: 1090398.522\n",
      "  load_time_ms: 7.318\n",
      "  sample_throughput: 500.876\n",
      "  sample_time_ms: 15932.071\n",
      "  update_time_ms: 5.474\n",
      "timestamp: 1643542426\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3136140\n",
      "training_iteration: 393\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3152071\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-34-18\n",
      "done: false\n",
      "episode_len_mean: 165.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 49\n",
      "episodes_total: 13929\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48296846712629\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017935691647714216\n",
      "        policy_loss: -0.10593797610451777\n",
      "        total_loss: 52.43089439551036\n",
      "        vf_explained_var: 0.19996224542458851\n",
      "        vf_loss: 52.52472569942474\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5154284033179283\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015389903996663179\n",
      "        policy_loss: -0.06382689420133829\n",
      "        total_loss: 40.69175913095474\n",
      "        vf_explained_var: 0.07098503698905309\n",
      "        vf_loss: 40.7451979192098\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.500891816119353\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01729072918311431\n",
      "        policy_loss: -0.0725275192471842\n",
      "        total_loss: 52.43429662227631\n",
      "        vf_explained_var: 0.14746484031279883\n",
      "        vf_loss: 52.49515290737152\n",
      "  num_agent_steps_sampled: 3152071\n",
      "  num_agent_steps_trained: 3152071\n",
      "  num_steps_sampled: 3152100\n",
      "  num_steps_trained: 3152100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 395\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.683333333333334\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 48.333333333333336\n",
      "  player_1: 37.0\n",
      "  player_2: 40.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.2466666666666675\n",
      "  player_1: -1.9633333333333332\n",
      "  player_2: 2.716666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -47.66666666666667\n",
      "  player_1: -39.0\n",
      "  player_2: -49.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08633135702962132\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3053245185933188\n",
      "  mean_inference_ms: 1.6022349511704272\n",
      "  mean_raw_obs_processing_ms: 0.2083050110323499\n",
      "time_since_restore: 6228.898663759232\n",
      "time_this_iter_s: 14.51643991470337\n",
      "time_total_s: 6228.898663759232\n",
      "timers:\n",
      "  learn_throughput: 545.235\n",
      "  learn_time_ms: 14635.88\n",
      "  load_throughput: 1010337.023\n",
      "  load_time_ms: 7.898\n",
      "  sample_throughput: 496.63\n",
      "  sample_time_ms: 16068.296\n",
      "  update_time_ms: 5.414\n",
      "timestamp: 1643542458\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3152100\n",
      "training_iteration: 395\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3168030\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-34-49\n",
      "done: false\n",
      "episode_len_mean: 150.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 45\n",
      "episodes_total: 14029\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4712591317296028\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014911455038770024\n",
      "        policy_loss: -0.07106141363581021\n",
      "        total_loss: 42.51803444623947\n",
      "        vf_explained_var: 0.10140212933222453\n",
      "        vf_loss: 42.579030605157214\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5279661864538987\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016305884492930054\n",
      "        policy_loss: -0.08806722586974501\n",
      "        total_loss: 50.15550491809845\n",
      "        vf_explained_var: 0.08857273409763972\n",
      "        vf_loss: 50.23256587346395\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.530566627283891\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015445036277612493\n",
      "        policy_loss: -0.08368839700395862\n",
      "        total_loss: 73.11739382425944\n",
      "        vf_explained_var: 0.1736418652534485\n",
      "        vf_loss: 73.19065690040588\n",
      "  num_agent_steps_sampled: 3168030\n",
      "  num_agent_steps_trained: 3168030\n",
      "  num_steps_sampled: 3168060\n",
      "  num_steps_trained: 3168060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 397\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.744444444444447\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 37.666666666666664\n",
      "  player_2: 30.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.7966666666666665\n",
      "  player_1: -1.6233333333333337\n",
      "  player_2: 3.8266666666666675\n",
      "policy_reward_min:\n",
      "  player_0: -27.0\n",
      "  player_1: -27.0\n",
      "  player_2: -49.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08641639013210409\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3055928830409907\n",
      "  mean_inference_ms: 1.6036236019322274\n",
      "  mean_raw_obs_processing_ms: 0.2084041533783635\n",
      "time_since_restore: 6260.183035135269\n",
      "time_this_iter_s: 14.68560242652893\n",
      "time_total_s: 6260.183035135269\n",
      "timers:\n",
      "  learn_throughput: 541.062\n",
      "  learn_time_ms: 14748.759\n",
      "  load_throughput: 943462.14\n",
      "  load_time_ms: 8.458\n",
      "  sample_throughput: 498.762\n",
      "  sample_time_ms: 15999.624\n",
      "  update_time_ms: 5.442\n",
      "timestamp: 1643542489\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3168060\n",
      "training_iteration: 397\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3183990\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-35-22\n",
      "done: false\n",
      "episode_len_mean: 154.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 53\n",
      "episodes_total: 14137\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47655441259344417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016518360815898633\n",
      "        policy_loss: -0.06109171965159476\n",
      "        total_loss: 80.40803687810897\n",
      "        vf_explained_var: 0.07663272649049759\n",
      "        vf_loss: 80.4579788295428\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5407720375061035\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016430121379898993\n",
      "        policy_loss: -0.058052530344575644\n",
      "        total_loss: 81.27336774984995\n",
      "        vf_explained_var: 0.27838335156440736\n",
      "        vf_loss: 81.32032922426859\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5020442669590314\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016821679128409716\n",
      "        policy_loss: -0.1097444347757846\n",
      "        total_loss: 77.70509556611378\n",
      "        vf_explained_var: 0.15821683784325918\n",
      "        vf_loss: 77.80348498185475\n",
      "  num_agent_steps_sampled: 3183990\n",
      "  num_agent_steps_trained: 3183990\n",
      "  num_steps_sampled: 3184020\n",
      "  num_steps_trained: 3184020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 399\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.390909090909092\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000003\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 36.666666666666664\n",
      "  player_2: 34.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.3066666666666666\n",
      "  player_1: -2.8333333333333326\n",
      "  player_2: 3.5266666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -44.333333333333336\n",
      "  player_1: -49.0\n",
      "  player_2: -32.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08626988951765605\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3051397979546157\n",
      "  mean_inference_ms: 1.603620088832871\n",
      "  mean_raw_obs_processing_ms: 0.208275185891876\n",
      "time_since_restore: 6292.565253973007\n",
      "time_this_iter_s: 17.454786777496338\n",
      "time_total_s: 6292.565253973007\n",
      "timers:\n",
      "  learn_throughput: 540.084\n",
      "  learn_time_ms: 14775.469\n",
      "  load_throughput: 933874.972\n",
      "  load_time_ms: 8.545\n",
      "  sample_throughput: 498.902\n",
      "  sample_time_ms: 15995.124\n",
      "  update_time_ms: 5.62\n",
      "timestamp: 1643542522\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3184020\n",
      "training_iteration: 399\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3199952\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-35-54\n",
      "done: false\n",
      "episode_len_mean: 155.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 52\n",
      "episodes_total: 14244\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4883071101705233\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015670451673276298\n",
      "        policy_loss: -0.05273285450103382\n",
      "        total_loss: 100.97782741864522\n",
      "        vf_explained_var: 0.11920837779839834\n",
      "        vf_loss: 101.01998296181361\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5593374108274778\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01681252343849053\n",
      "        policy_loss: -0.08149926903347174\n",
      "        total_loss: 70.23660717805227\n",
      "        vf_explained_var: 0.08208693385124206\n",
      "        vf_loss: 70.3067577457428\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48561596269408863\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014172700568857787\n",
      "        policy_loss: -0.08504379643748204\n",
      "        total_loss: 61.76258565743764\n",
      "        vf_explained_var: 0.20436739772558213\n",
      "        vf_loss: 61.8380628935496\n",
      "  num_agent_steps_sampled: 3199952\n",
      "  num_agent_steps_trained: 3199952\n",
      "  num_steps_sampled: 3199980\n",
      "  num_steps_trained: 3199980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 401\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.744444444444444\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 46.0\n",
      "  player_2: 35.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -1.2633333333333328\n",
      "  player_1: -0.1333333333333331\n",
      "  player_2: 4.3966666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -64.33333333333334\n",
      "  player_1: -25.0\n",
      "  player_2: -41.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08611929605111847\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30452134846433426\n",
      "  mean_inference_ms: 1.6005591016832676\n",
      "  mean_raw_obs_processing_ms: 0.20809250018104486\n",
      "time_since_restore: 6324.88719367981\n",
      "time_this_iter_s: 14.708402395248413\n",
      "time_total_s: 6324.88719367981\n",
      "timers:\n",
      "  learn_throughput: 536.571\n",
      "  learn_time_ms: 14872.212\n",
      "  load_throughput: 939882.9\n",
      "  load_time_ms: 8.49\n",
      "  sample_throughput: 490.81\n",
      "  sample_time_ms: 16258.844\n",
      "  update_time_ms: 6.067\n",
      "timestamp: 1643542554\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3199980\n",
      "training_iteration: 401\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3215910\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-36-27\n",
      "done: false\n",
      "episode_len_mean: 156.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 50\n",
      "episodes_total: 14343\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4675430073340734\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016808912935631307\n",
      "        policy_loss: -0.06733609937752287\n",
      "        total_loss: 71.38156626224517\n",
      "        vf_explained_var: 0.18007670710484186\n",
      "        vf_loss: 71.43755646864574\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5266959120333194\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016450511316773865\n",
      "        policy_loss: -0.10470891864970326\n",
      "        total_loss: 35.83395165125529\n",
      "        vf_explained_var: 0.2907362573345502\n",
      "        vf_loss: 35.92755648930868\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4850073788066705\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016563287648884095\n",
      "        policy_loss: -0.06241210311961671\n",
      "        total_loss: 44.178441781997684\n",
      "        vf_explained_var: 0.2350260845820109\n",
      "        vf_loss: 44.22967395623525\n",
      "  num_agent_steps_sampled: 3215910\n",
      "  num_agent_steps_trained: 3215910\n",
      "  num_steps_sampled: 3215940\n",
      "  num_steps_trained: 3215940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 403\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.785714285714283\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.333333333333336\n",
      "  player_1: 29.333333333333336\n",
      "  player_2: 32.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.6833333333333331\n",
      "  player_1: -2.5466666666666664\n",
      "  player_2: 3.8633333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -52.666666666666664\n",
      "  player_1: -32.0\n",
      "  player_2: -42.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08620572226823078\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3045564740433981\n",
      "  mean_inference_ms: 1.6010163896083955\n",
      "  mean_raw_obs_processing_ms: 0.2081848658939673\n",
      "time_since_restore: 6357.393162727356\n",
      "time_this_iter_s: 16.73240065574646\n",
      "time_total_s: 6357.393162727356\n",
      "timers:\n",
      "  learn_throughput: 536.773\n",
      "  learn_time_ms: 14866.63\n",
      "  load_throughput: 945642.569\n",
      "  load_time_ms: 8.439\n",
      "  sample_throughput: 498.517\n",
      "  sample_time_ms: 16007.482\n",
      "  update_time_ms: 6.088\n",
      "timestamp: 1643542587\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3215940\n",
      "training_iteration: 403\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3231870\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-37-00\n",
      "done: false\n",
      "episode_len_mean: 154.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 52\n",
      "episodes_total: 14443\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46355368475119274\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01672005933336493\n",
      "        policy_loss: -0.06270497829342882\n",
      "        total_loss: 54.267720322608945\n",
      "        vf_explained_var: 0.2576578865448634\n",
      "        vf_loss: 54.31913911660512\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5206304547687371\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015259826482706558\n",
      "        policy_loss: -0.06099956248576442\n",
      "        total_loss: 70.589122235775\n",
      "        vf_explained_var: -0.022444279591242473\n",
      "        vf_loss: 70.63982138554255\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4775917998949687\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016204279119049828\n",
      "        policy_loss: -0.10941481287280719\n",
      "        total_loss: 78.78617182413737\n",
      "        vf_explained_var: 0.2503263515233993\n",
      "        vf_loss: 78.88464893817901\n",
      "  num_agent_steps_sampled: 3231870\n",
      "  num_agent_steps_trained: 3231870\n",
      "  num_steps_sampled: 3231900\n",
      "  num_steps_trained: 3231900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 405\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.526315789473687\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.91578947368423\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 30.33333333333333\n",
      "  player_2: 35.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.7333333333333325\n",
      "  player_1: -0.6366666666666665\n",
      "  player_2: -0.09666666666666689\n",
      "policy_reward_min:\n",
      "  player_0: -41.666666666666664\n",
      "  player_1: -46.666666666666664\n",
      "  player_2: -46.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08627393353259909\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3050687101251666\n",
      "  mean_inference_ms: 1.6039758020331425\n",
      "  mean_raw_obs_processing_ms: 0.20818989009174124\n",
      "time_since_restore: 6390.566911458969\n",
      "time_this_iter_s: 15.677299976348877\n",
      "time_total_s: 6390.566911458969\n",
      "timers:\n",
      "  learn_throughput: 533.098\n",
      "  learn_time_ms: 14969.116\n",
      "  load_throughput: 910223.212\n",
      "  load_time_ms: 8.767\n",
      "  sample_throughput: 496.112\n",
      "  sample_time_ms: 16085.078\n",
      "  update_time_ms: 6.265\n",
      "timestamp: 1643542620\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3231900\n",
      "training_iteration: 405\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3247830\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-37-33\n",
      "done: false\n",
      "episode_len_mean: 145.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 55\n",
      "episodes_total: 14553\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47363913426796594\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016341479878513684\n",
      "        policy_loss: -0.07040862335513036\n",
      "        total_loss: 79.5432002512614\n",
      "        vf_explained_var: 0.2715588931242625\n",
      "        vf_loss: 79.60257830460866\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5292684491972128\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016191632809235065\n",
      "        policy_loss: -0.08681266771629453\n",
      "        total_loss: 40.95000259995461\n",
      "        vf_explained_var: 0.3384125570456187\n",
      "        vf_loss: 41.02588590304057\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5087214247385661\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016359268428641372\n",
      "        policy_loss: -0.08373395008345445\n",
      "        total_loss: 85.7471187432607\n",
      "        vf_explained_var: 0.2670931859811147\n",
      "        vf_loss: 85.81981042385101\n",
      "  num_agent_steps_sampled: 3247830\n",
      "  num_agent_steps_trained: 3247830\n",
      "  num_steps_sampled: 3247860\n",
      "  num_steps_trained: 3247860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 407\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.274999999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.666666666666664\n",
      "  player_1: 38.0\n",
      "  player_2: 38.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 0.4366666666666666\n",
      "  player_1: 0.436666666666667\n",
      "  player_2: 2.126666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -42.666666666666664\n",
      "  player_1: -35.333333333333336\n",
      "  player_2: -43.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08608403589036745\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30436431129362107\n",
      "  mean_inference_ms: 1.6009240808144596\n",
      "  mean_raw_obs_processing_ms: 0.20802940778582424\n",
      "time_since_restore: 6423.556545257568\n",
      "time_this_iter_s: 16.306451320648193\n",
      "time_total_s: 6423.556545257568\n",
      "timers:\n",
      "  learn_throughput: 527.163\n",
      "  learn_time_ms: 15137.645\n",
      "  load_throughput: 922846.797\n",
      "  load_time_ms: 8.647\n",
      "  sample_throughput: 492.398\n",
      "  sample_time_ms: 16206.392\n",
      "  update_time_ms: 6.198\n",
      "timestamp: 1643542653\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3247860\n",
      "training_iteration: 407\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3263790\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-38-05\n",
      "done: false\n",
      "episode_len_mean: 155.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 48\n",
      "episodes_total: 14651\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4603202290832996\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015985496136876425\n",
      "        policy_loss: -0.07975321294739843\n",
      "        total_loss: 62.12799823443095\n",
      "        vf_explained_var: 0.15547268678744633\n",
      "        vf_loss: 62.1969611342748\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5277772068480651\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016898744157424518\n",
      "        policy_loss: -0.08016890554766481\n",
      "        total_loss: 40.55040596961975\n",
      "        vf_explained_var: -0.18116534173488616\n",
      "        vf_loss: 40.619168101946514\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4863225011527538\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0161639263838642\n",
      "        policy_loss: -0.06871700311700503\n",
      "        total_loss: 45.269173181851706\n",
      "        vf_explained_var: 0.19314485331376394\n",
      "        vf_loss: 45.32697950363159\n",
      "  num_agent_steps_sampled: 3263790\n",
      "  num_agent_steps_trained: 3263790\n",
      "  num_steps_sampled: 3263820\n",
      "  num_steps_trained: 3263820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 409\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.677777777777777\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 36.0\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.42\n",
      "  player_1: -2.5199999999999996\n",
      "  player_2: 3.0999999999999996\n",
      "policy_reward_min:\n",
      "  player_0: -36.333333333333336\n",
      "  player_1: -44.333333333333336\n",
      "  player_2: -36.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08630050378682323\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3040682035961887\n",
      "  mean_inference_ms: 1.5986114074167284\n",
      "  mean_raw_obs_processing_ms: 0.20817043785091535\n",
      "time_since_restore: 6455.335225582123\n",
      "time_this_iter_s: 14.591432809829712\n",
      "time_total_s: 6455.335225582123\n",
      "timers:\n",
      "  learn_throughput: 529.238\n",
      "  learn_time_ms: 15078.285\n",
      "  load_throughput: 875843.798\n",
      "  load_time_ms: 9.111\n",
      "  sample_throughput: 480.909\n",
      "  sample_time_ms: 16593.588\n",
      "  update_time_ms: 6.019\n",
      "timestamp: 1643542685\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3263820\n",
      "training_iteration: 409\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3279750\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-38-38\n",
      "done: false\n",
      "episode_len_mean: 160.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 51\n",
      "episodes_total: 14754\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44499424397945403\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015101666425956258\n",
      "        policy_loss: -0.04414694608344386\n",
      "        total_loss: 63.61297750155131\n",
      "        vf_explained_var: 0.19007341146469117\n",
      "        vf_loss: 63.64693063259125\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5266379476090273\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01542504294622025\n",
      "        policy_loss: -0.07604199268234273\n",
      "        total_loss: 56.68778124809265\n",
      "        vf_explained_var: 0.029654337565104168\n",
      "        vf_loss: 56.753411492506665\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48985127344727514\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01613196079873963\n",
      "        policy_loss: -0.08462712155965467\n",
      "        total_loss: 112.3460465335846\n",
      "        vf_explained_var: 0.3400405935446421\n",
      "        vf_loss: 112.41978456497192\n",
      "  num_agent_steps_sampled: 3279750\n",
      "  num_agent_steps_trained: 3279750\n",
      "  num_steps_sampled: 3279780\n",
      "  num_steps_trained: 3279780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 411\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.065000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.0\n",
      "  player_1: 48.0\n",
      "  player_2: 34.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 0.0899999999999999\n",
      "  player_1: -0.31999999999999995\n",
      "  player_2: 3.23\n",
      "policy_reward_min:\n",
      "  player_0: -34.0\n",
      "  player_1: -32.0\n",
      "  player_2: -51.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08616635115622817\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3041835895137319\n",
      "  mean_inference_ms: 1.5999624495720763\n",
      "  mean_raw_obs_processing_ms: 0.20812826487760322\n",
      "time_since_restore: 6487.905249834061\n",
      "time_this_iter_s: 16.067190170288086\n",
      "time_total_s: 6487.905249834061\n",
      "timers:\n",
      "  learn_throughput: 528.407\n",
      "  learn_time_ms: 15102.006\n",
      "  load_throughput: 877812.348\n",
      "  load_time_ms: 9.091\n",
      "  sample_throughput: 492.829\n",
      "  sample_time_ms: 16192.231\n",
      "  update_time_ms: 5.66\n",
      "timestamp: 1643542718\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3279780\n",
      "training_iteration: 411\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3295711\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-39-08\n",
      "done: false\n",
      "episode_len_mean: 144.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 57\n",
      "episodes_total: 14863\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4742094965279102\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01687578926422797\n",
      "        policy_loss: -0.07114942936226726\n",
      "        total_loss: 64.93550570805867\n",
      "        vf_explained_var: 0.3828755483031273\n",
      "        vf_loss: 64.9952639802297\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5312031222383181\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01738214427778985\n",
      "        policy_loss: -0.09230446432096263\n",
      "        total_loss: 70.40353455543519\n",
      "        vf_explained_var: 0.07014199256896973\n",
      "        vf_loss: 70.48410618146261\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5016480255623659\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015801575247945343\n",
      "        policy_loss: -0.07876244875136763\n",
      "        total_loss: 64.4656777938207\n",
      "        vf_explained_var: 0.3651903133591016\n",
      "        vf_loss: 64.53377415180206\n",
      "  num_agent_steps_sampled: 3295711\n",
      "  num_agent_steps_trained: 3295711\n",
      "  num_steps_sampled: 3295740\n",
      "  num_steps_trained: 3295740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 413\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.089473684210526\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.9105263157895\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.0\n",
      "  player_1: 33.333333333333336\n",
      "  player_2: 50.0\n",
      "policy_reward_mean:\n",
      "  player_0: 5.246666666666667\n",
      "  player_1: -4.713333333333333\n",
      "  player_2: 2.466666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -33.666666666666664\n",
      "  player_1: -86.0\n",
      "  player_2: -37.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08600564647448718\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3033688599167025\n",
      "  mean_inference_ms: 1.5982033211355726\n",
      "  mean_raw_obs_processing_ms: 0.2077923203647221\n",
      "time_since_restore: 6518.799969434738\n",
      "time_this_iter_s: 16.078059434890747\n",
      "time_total_s: 6518.799969434738\n",
      "timers:\n",
      "  learn_throughput: 534.452\n",
      "  learn_time_ms: 14931.175\n",
      "  load_throughput: 878891.095\n",
      "  load_time_ms: 9.08\n",
      "  sample_throughput: 491.345\n",
      "  sample_time_ms: 16241.131\n",
      "  update_time_ms: 5.659\n",
      "timestamp: 1643542748\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3295740\n",
      "training_iteration: 413\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3311670\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-39-41\n",
      "done: false\n",
      "episode_len_mean: 149.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 52\n",
      "episodes_total: 14971\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48415181731184326\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016131194745469958\n",
      "        policy_loss: -0.047864156984724106\n",
      "        total_loss: 79.84651057720184\n",
      "        vf_explained_var: 0.2398268089691798\n",
      "        vf_loss: 79.8834864584605\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5278130333622296\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015192744302836635\n",
      "        policy_loss: -0.08081958390461902\n",
      "        total_loss: 66.83623926321665\n",
      "        vf_explained_var: 0.209049054980278\n",
      "        vf_loss: 66.90680377006531\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4793357028067112\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016515130543969577\n",
      "        policy_loss: -0.10519066577777267\n",
      "        total_loss: 45.382061795393625\n",
      "        vf_explained_var: 0.36626406570275627\n",
      "        vf_loss: 45.4761049858729\n",
      "  num_agent_steps_sampled: 3311670\n",
      "  num_agent_steps_trained: 3311670\n",
      "  num_steps_sampled: 3311700\n",
      "  num_steps_trained: 3311700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 415\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.955000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90500000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 48.66666666666667\n",
      "  player_2: 31.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -0.3666666666666663\n",
      "  player_1: -1.1566666666666667\n",
      "  player_2: 4.5233333333333325\n",
      "policy_reward_min:\n",
      "  player_0: -40.33333333333333\n",
      "  player_1: -45.333333333333336\n",
      "  player_2: -34.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08609376176198218\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3039574835558557\n",
      "  mean_inference_ms: 1.5996990170744507\n",
      "  mean_raw_obs_processing_ms: 0.20807085510072196\n",
      "time_since_restore: 6551.189354896545\n",
      "time_this_iter_s: 16.397496700286865\n",
      "time_total_s: 6551.189354896545\n",
      "timers:\n",
      "  learn_throughput: 537.271\n",
      "  learn_time_ms: 14852.837\n",
      "  load_throughput: 976971.232\n",
      "  load_time_ms: 8.168\n",
      "  sample_throughput: 498.243\n",
      "  sample_time_ms: 16016.27\n",
      "  update_time_ms: 5.684\n",
      "timestamp: 1643542781\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3311700\n",
      "training_iteration: 415\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3327630\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-40-12\n",
      "done: false\n",
      "episode_len_mean: 158.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 44\n",
      "episodes_total: 15065\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4458655605216821\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015500691365943264\n",
      "        policy_loss: -0.09874743113915126\n",
      "        total_loss: 57.88390286763509\n",
      "        vf_explained_var: 0.3276238734523455\n",
      "        vf_loss: 57.97218715349833\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49851816624403\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016180044429159372\n",
      "        policy_loss: -0.08493241763984163\n",
      "        total_loss: 49.501322293281554\n",
      "        vf_explained_var: 0.2452801525592804\n",
      "        vf_loss: 49.575333188374834\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.510262236793836\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016313259381601407\n",
      "        policy_loss: -0.06727174394764006\n",
      "        total_loss: 106.11529704093932\n",
      "        vf_explained_var: 0.13693920642137528\n",
      "        vf_loss: 106.17155722618104\n",
      "  num_agent_steps_sampled: 3327630\n",
      "  num_agent_steps_trained: 3327630\n",
      "  num_steps_sampled: 3327660\n",
      "  num_steps_trained: 3327660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 417\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.126315789473683\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.90000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.66666666666667\n",
      "  player_1: 33.333333333333336\n",
      "  player_2: 31.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.3833333333333337\n",
      "  player_1: -1.8766666666666665\n",
      "  player_2: 1.493333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -30.666666666666664\n",
      "  player_1: -47.0\n",
      "  player_2: -45.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08611093222637776\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3038829320975072\n",
      "  mean_inference_ms: 1.6012779721679373\n",
      "  mean_raw_obs_processing_ms: 0.20802560451132604\n",
      "time_since_restore: 6582.7123057842255\n",
      "time_this_iter_s: 15.442644596099854\n",
      "time_total_s: 6582.7123057842255\n",
      "timers:\n",
      "  learn_throughput: 542.76\n",
      "  learn_time_ms: 14702.627\n",
      "  load_throughput: 1001287.74\n",
      "  load_time_ms: 7.97\n",
      "  sample_throughput: 497.887\n",
      "  sample_time_ms: 16027.737\n",
      "  update_time_ms: 5.641\n",
      "timestamp: 1643542812\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3327660\n",
      "training_iteration: 417\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3343590\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-40-44\n",
      "done: false\n",
      "episode_len_mean: 159.54\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 54\n",
      "episodes_total: 15169\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4622438142200311\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015657481251281524\n",
      "        policy_loss: -0.07208693124353886\n",
      "        total_loss: 95.59497561772665\n",
      "        vf_explained_var: 0.28685548494259516\n",
      "        vf_loss: 95.65649429321289\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5177078860004743\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015238708853367674\n",
      "        policy_loss: -0.08135512445432444\n",
      "        total_loss: 80.33304662704468\n",
      "        vf_explained_var: 0.3091932479540507\n",
      "        vf_loss: 80.40411562760671\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.504985848814249\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016818715504184355\n",
      "        policy_loss: -0.07186147520318627\n",
      "        total_loss: 91.77910767714182\n",
      "        vf_explained_var: 0.19954505761464436\n",
      "        vf_loss: 91.83961629390717\n",
      "  num_agent_steps_sampled: 3343590\n",
      "  num_agent_steps_trained: 3343590\n",
      "  num_steps_sampled: 3343620\n",
      "  num_steps_trained: 3343620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 419\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.585714285714285\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.97142857142858\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 42.33333333333333\n",
      "  player_1: 40.0\n",
      "  player_2: 40.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 0.6099999999999998\n",
      "  player_1: -0.7299999999999998\n",
      "  player_2: 3.12\n",
      "policy_reward_min:\n",
      "  player_0: -47.666666666666664\n",
      "  player_1: -50.66666666666667\n",
      "  player_2: -36.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0860970295420029\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30418864818357294\n",
      "  mean_inference_ms: 1.6017731225524579\n",
      "  mean_raw_obs_processing_ms: 0.2081472941764784\n",
      "time_since_restore: 6613.870868682861\n",
      "time_this_iter_s: 16.28387475013733\n",
      "time_total_s: 6613.870868682861\n",
      "timers:\n",
      "  learn_throughput: 545.104\n",
      "  learn_time_ms: 14639.42\n",
      "  load_throughput: 1072894.264\n",
      "  load_time_ms: 7.438\n",
      "  sample_throughput: 507.859\n",
      "  sample_time_ms: 15713.016\n",
      "  update_time_ms: 5.685\n",
      "timestamp: 1643542844\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3343620\n",
      "training_iteration: 419\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3359550\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-41-16\n",
      "done: false\n",
      "episode_len_mean: 158.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 49\n",
      "episodes_total: 15270\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45889992932478585\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016030332205148927\n",
      "        policy_loss: -0.06376273536433776\n",
      "        total_loss: 43.48540544350942\n",
      "        vf_explained_var: 0.2726290906469027\n",
      "        vf_loss: 43.538347765604655\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5195540337761243\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015284351406696903\n",
      "        policy_loss: -0.08197984949685633\n",
      "        total_loss: 74.37284416834514\n",
      "        vf_explained_var: 0.004176732500394185\n",
      "        vf_loss: 74.44450749079387\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4999397157629331\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016315810838952415\n",
      "        policy_loss: -0.06571252604325613\n",
      "        total_loss: 57.15125168164571\n",
      "        vf_explained_var: 0.3162774196267128\n",
      "        vf_loss: 57.205950951576234\n",
      "  num_agent_steps_sampled: 3359550\n",
      "  num_agent_steps_trained: 3359550\n",
      "  num_steps_sampled: 3359580\n",
      "  num_steps_trained: 3359580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 421\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.526315789473687\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.9263157894737\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 33.333333333333336\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.5599999999999996\n",
      "  player_1: -3.57\n",
      "  player_2: 3.0099999999999993\n",
      "policy_reward_min:\n",
      "  player_0: -28.333333333333336\n",
      "  player_1: -61.0\n",
      "  player_2: -42.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08612580396185204\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3034648410753644\n",
      "  mean_inference_ms: 1.597824578352743\n",
      "  mean_raw_obs_processing_ms: 0.20805483122538265\n",
      "time_since_restore: 6645.8272931575775\n",
      "time_this_iter_s: 15.149747371673584\n",
      "time_total_s: 6645.8272931575775\n",
      "timers:\n",
      "  learn_throughput: 547.355\n",
      "  learn_time_ms: 14579.204\n",
      "  load_throughput: 1131308.231\n",
      "  load_time_ms: 7.054\n",
      "  sample_throughput: 501.42\n",
      "  sample_time_ms: 15914.809\n",
      "  update_time_ms: 5.767\n",
      "timestamp: 1643542876\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3359580\n",
      "training_iteration: 421\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3375510\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-41-47\n",
      "done: false\n",
      "episode_len_mean: 145.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 48\n",
      "episodes_total: 15381\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48524033134182293\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01493466280891899\n",
      "        policy_loss: -0.04613825128103296\n",
      "        total_loss: 65.17189555962881\n",
      "        vf_explained_var: 0.29177910576264066\n",
      "        vf_loss: 65.20795289993286\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5047629847625892\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015606916516964968\n",
      "        policy_loss: -0.10073810252050559\n",
      "        total_loss: 44.829733604590096\n",
      "        vf_explained_var: 0.09842689702908199\n",
      "        vf_loss: 44.91993683179219\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5160547214746475\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01626502535849672\n",
      "        policy_loss: -0.08419858028491338\n",
      "        total_loss: 58.69756852149963\n",
      "        vf_explained_var: 0.07353727589050929\n",
      "        vf_loss: 58.77078807512919\n",
      "  num_agent_steps_sampled: 3375510\n",
      "  num_agent_steps_trained: 3375510\n",
      "  num_steps_sampled: 3375540\n",
      "  num_steps_trained: 3375540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 423\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.089999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.93\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.666666666666668\n",
      "  player_1: 26.666666666666664\n",
      "  player_2: 35.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.4099999999999999\n",
      "  player_1: -0.82\n",
      "  player_2: 3.41\n",
      "policy_reward_min:\n",
      "  player_0: -36.0\n",
      "  player_1: -48.0\n",
      "  player_2: -33.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0860590352788697\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3032618985332439\n",
      "  mean_inference_ms: 1.5981874728010197\n",
      "  mean_raw_obs_processing_ms: 0.20789123522221636\n",
      "time_since_restore: 6676.805343866348\n",
      "time_this_iter_s: 15.715057849884033\n",
      "time_total_s: 6676.805343866348\n",
      "timers:\n",
      "  learn_throughput: 546.564\n",
      "  learn_time_ms: 14600.298\n",
      "  load_throughput: 867121.574\n",
      "  load_time_ms: 9.203\n",
      "  sample_throughput: 503.512\n",
      "  sample_time_ms: 15848.669\n",
      "  update_time_ms: 5.835\n",
      "timestamp: 1643542907\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3375540\n",
      "training_iteration: 423\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3391474\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-42-19\n",
      "done: false\n",
      "episode_len_mean: 145.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 55\n",
      "episodes_total: 15494\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47623755877216656\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018285894556474887\n",
      "        policy_loss: -0.061362537561605375\n",
      "        total_loss: 55.11685483614604\n",
      "        vf_explained_var: 0.4012274949749311\n",
      "        vf_loss: 55.16587441762288\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5174675208330154\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0176628990454689\n",
      "        policy_loss: -0.06666763435738782\n",
      "        total_loss: 39.84521241426468\n",
      "        vf_explained_var: 0.16067278464635212\n",
      "        vf_loss: 39.89995742241542\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5069686576227347\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016417377521574964\n",
      "        policy_loss: -0.11747133474331349\n",
      "        total_loss: 45.58433479785919\n",
      "        vf_explained_var: 0.22603413144747417\n",
      "        vf_loss: 45.6907244126002\n",
      "  num_agent_steps_sampled: 3391474\n",
      "  num_agent_steps_trained: 3391474\n",
      "  num_steps_sampled: 3391500\n",
      "  num_steps_trained: 3391500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 425\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.352631578947367\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 24.0\n",
      "  player_1: 29.333333333333336\n",
      "  player_2: 30.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -1.3866666666666665\n",
      "  player_1: -3.276666666666667\n",
      "  player_2: 7.663333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -37.666666666666664\n",
      "  player_1: -29.333333333333336\n",
      "  player_2: -31.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08600615885954255\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3032468350192888\n",
      "  mean_inference_ms: 1.5985996456846445\n",
      "  mean_raw_obs_processing_ms: 0.2079664737314829\n",
      "time_since_restore: 6708.8134977817535\n",
      "time_this_iter_s: 15.2919020652771\n",
      "time_total_s: 6708.8134977817535\n",
      "timers:\n",
      "  learn_throughput: 548.105\n",
      "  learn_time_ms: 14559.257\n",
      "  load_throughput: 821220.006\n",
      "  load_time_ms: 9.717\n",
      "  sample_throughput: 501.754\n",
      "  sample_time_ms: 15904.223\n",
      "  update_time_ms: 5.672\n",
      "timestamp: 1643542939\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3391500\n",
      "training_iteration: 425\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3407430\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-42-53\n",
      "done: false\n",
      "episode_len_mean: 154.62\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 52\n",
      "episodes_total: 15595\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4621870418389638\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01692410996643408\n",
      "        policy_loss: -0.087573375091888\n",
      "        total_loss: 46.1210662094752\n",
      "        vf_explained_var: 0.29158218880494435\n",
      "        vf_loss: 46.19721569697062\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49316789967318375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016505202762350943\n",
      "        policy_loss: -0.08099866503849626\n",
      "        total_loss: 53.292126626173655\n",
      "        vf_explained_var: 0.22605287671089172\n",
      "        vf_loss: 53.36198418458303\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48121813535690305\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01573002289320385\n",
      "        policy_loss: -0.0560205556328098\n",
      "        total_loss: 52.29918184598287\n",
      "        vf_explained_var: 0.24287947684526442\n",
      "        vf_loss: 52.34458465894063\n",
      "  num_agent_steps_sampled: 3407430\n",
      "  num_agent_steps_trained: 3407430\n",
      "  num_steps_sampled: 3407460\n",
      "  num_steps_trained: 3407460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 427\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.125\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.333333333333336\n",
      "  player_1: 39.333333333333336\n",
      "  player_2: 35.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.4899999999999998\n",
      "  player_1: -2.19\n",
      "  player_2: 2.7000000000000006\n",
      "policy_reward_min:\n",
      "  player_0: -30.0\n",
      "  player_1: -34.666666666666664\n",
      "  player_2: -44.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08624675087560257\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30366188000406724\n",
      "  mean_inference_ms: 1.60045801915825\n",
      "  mean_raw_obs_processing_ms: 0.20802641745887687\n",
      "time_since_restore: 6742.615862607956\n",
      "time_this_iter_s: 16.90564489364624\n",
      "time_total_s: 6742.615862607956\n",
      "timers:\n",
      "  learn_throughput: 541.116\n",
      "  learn_time_ms: 14747.303\n",
      "  load_throughput: 748493.777\n",
      "  load_time_ms: 10.661\n",
      "  sample_throughput: 502.085\n",
      "  sample_time_ms: 15893.71\n",
      "  update_time_ms: 5.821\n",
      "timestamp: 1643542973\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3407460\n",
      "training_iteration: 427\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3423392\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-43-27\n",
      "done: false\n",
      "episode_len_mean: 151.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 52\n",
      "episodes_total: 15701\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4694658803443114\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017044692958653514\n",
      "        policy_loss: -0.09099312180653214\n",
      "        total_loss: 55.41311748663584\n",
      "        vf_explained_var: 0.06957260568936666\n",
      "        vf_loss: 55.49260526974996\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5422770419716835\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01689628758325417\n",
      "        policy_loss: -0.053630336908002695\n",
      "        total_loss: 64.23491631666819\n",
      "        vf_explained_var: 0.10963716328144074\n",
      "        vf_loss: 64.27714182933171\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49068353508909546\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014735749402031312\n",
      "        policy_loss: -0.07600494747050107\n",
      "        total_loss: 71.32243394056955\n",
      "        vf_explained_var: 0.08480590770641963\n",
      "        vf_loss: 71.38849203904469\n",
      "  num_agent_steps_sampled: 3423392\n",
      "  num_agent_steps_trained: 3423392\n",
      "  num_steps_sampled: 3423420\n",
      "  num_steps_trained: 3423420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 429\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.809999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 45.333333333333336\n",
      "  player_1: 39.33333333333333\n",
      "  player_2: 33.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.8399999999999997\n",
      "  player_1: -0.8400000000000004\n",
      "  player_2: 2.9999999999999996\n",
      "policy_reward_min:\n",
      "  player_0: -38.0\n",
      "  player_1: -40.333333333333336\n",
      "  player_2: -53.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08614305085313397\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30330384515660247\n",
      "  mean_inference_ms: 1.5998914407432363\n",
      "  mean_raw_obs_processing_ms: 0.20819936164162706\n",
      "time_since_restore: 6777.336292266846\n",
      "time_this_iter_s: 16.372275352478027\n",
      "time_total_s: 6777.336292266846\n",
      "timers:\n",
      "  learn_throughput: 529.609\n",
      "  learn_time_ms: 15067.732\n",
      "  load_throughput: 708236.44\n",
      "  load_time_ms: 11.267\n",
      "  sample_throughput: 487.185\n",
      "  sample_time_ms: 16379.829\n",
      "  update_time_ms: 5.79\n",
      "timestamp: 1643543007\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3423420\n",
      "training_iteration: 429\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3439350\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-44-03\n",
      "done: false\n",
      "episode_len_mean: 144.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 52\n",
      "episodes_total: 15812\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4614997315406799\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017108323223719708\n",
      "        policy_loss: -0.10114643476903439\n",
      "        total_loss: 52.54034168402354\n",
      "        vf_explained_var: 0.2621557890375455\n",
      "        vf_loss: 52.62993998686473\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5367473547160625\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015214392201017593\n",
      "        policy_loss: -0.027111244324284297\n",
      "        total_loss: 39.82975980122884\n",
      "        vf_explained_var: 0.10564527481794357\n",
      "        vf_loss: 39.84660126288732\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5070957646270593\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016111854834434402\n",
      "        policy_loss: -0.08910546349051097\n",
      "        total_loss: 89.02398731072743\n",
      "        vf_explained_var: 0.15799397180477778\n",
      "        vf_loss: 89.10221653779348\n",
      "  num_agent_steps_sampled: 3439350\n",
      "  num_agent_steps_trained: 3439350\n",
      "  num_steps_sampled: 3439380\n",
      "  num_steps_trained: 3439380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 431\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.48095238095238\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.9952380952381\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.333333333333336\n",
      "  player_1: 48.33333333333333\n",
      "  player_2: 38.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 4.113333333333334\n",
      "  player_1: -2.116666666666667\n",
      "  player_2: 1.0033333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -30.333333333333332\n",
      "  player_1: -28.0\n",
      "  player_2: -60.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0860806662829568\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30345226685778504\n",
      "  mean_inference_ms: 1.6009635603184489\n",
      "  mean_raw_obs_processing_ms: 0.20812779894682504\n",
      "time_since_restore: 6813.202876806259\n",
      "time_this_iter_s: 17.195486545562744\n",
      "time_total_s: 6813.202876806259\n",
      "timers:\n",
      "  learn_throughput: 517.206\n",
      "  learn_time_ms: 15429.066\n",
      "  load_throughput: 670994.465\n",
      "  load_time_ms: 11.893\n",
      "  sample_throughput: 481.572\n",
      "  sample_time_ms: 16570.714\n",
      "  update_time_ms: 5.744\n",
      "timestamp: 1643543043\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3439380\n",
      "training_iteration: 431\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3455310\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-44-39\n",
      "done: false\n",
      "episode_len_mean: 146.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 53\n",
      "episodes_total: 15920\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4727416403094927\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015996151968902268\n",
      "        policy_loss: -0.09889988804546496\n",
      "        total_loss: 66.28999667962393\n",
      "        vf_explained_var: 0.3365701261162758\n",
      "        vf_loss: 66.37809909025827\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5211223851144314\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016486864299660623\n",
      "        policy_loss: -0.07952319172366212\n",
      "        total_loss: 33.822527913252515\n",
      "        vf_explained_var: 0.15081233620643616\n",
      "        vf_loss: 33.89092258214951\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4825737042725086\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016046732281149237\n",
      "        policy_loss: -0.03635085160223146\n",
      "        total_loss: 35.909135661125184\n",
      "        vf_explained_var: 0.32293258438507716\n",
      "        vf_loss: 35.93465499401093\n",
      "  num_agent_steps_sampled: 3455310\n",
      "  num_agent_steps_trained: 3455310\n",
      "  num_steps_sampled: 3455340\n",
      "  num_steps_trained: 3455340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 433\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.99130434782609\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.3741861979166667\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.0\n",
      "  player_1: 34.0\n",
      "  player_2: 32.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 0.27333333333333315\n",
      "  player_1: -1.7366666666666666\n",
      "  player_2: 4.463333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -43.666666666666664\n",
      "  player_1: -29.333333333333336\n",
      "  player_2: -45.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08619912048432202\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30310861021509433\n",
      "  mean_inference_ms: 1.5986926154140924\n",
      "  mean_raw_obs_processing_ms: 0.2082114986516962\n",
      "time_since_restore: 6848.957392692566\n",
      "time_this_iter_s: 18.570999145507812\n",
      "time_total_s: 6848.957392692566\n",
      "timers:\n",
      "  learn_throughput: 502.797\n",
      "  learn_time_ms: 15871.224\n",
      "  load_throughput: 634316.712\n",
      "  load_time_ms: 12.58\n",
      "  sample_throughput: 470.12\n",
      "  sample_time_ms: 16974.389\n",
      "  update_time_ms: 5.848\n",
      "timestamp: 1643543079\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3455340\n",
      "training_iteration: 433\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3471270\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-45-16\n",
      "done: false\n",
      "episode_len_mean: 141.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 55\n",
      "episodes_total: 16032\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47471388851602875\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01701607952289919\n",
      "        policy_loss: -0.08611514680398007\n",
      "        total_loss: 62.66041550159454\n",
      "        vf_explained_var: 0.3155055400729179\n",
      "        vf_loss: 62.73504469712575\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5668064345419407\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014642521882146866\n",
      "        policy_loss: -0.07540147720448052\n",
      "        total_loss: 90.49941894133886\n",
      "        vf_explained_var: 0.05316452950239182\n",
      "        vf_loss: 90.56493679126103\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5119080372651418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016493817008386032\n",
      "        policy_loss: -0.08149996989406645\n",
      "        total_loss: 99.03406163692475\n",
      "        vf_explained_var: 0.2630160026748975\n",
      "        vf_loss: 99.10442831993103\n",
      "  num_agent_steps_sampled: 3471270\n",
      "  num_agent_steps_trained: 3471270\n",
      "  num_steps_sampled: 3471300\n",
      "  num_steps_trained: 3471300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 435\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.26190476190476\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.66666666666667\n",
      "  player_1: 32.333333333333336\n",
      "  player_2: 40.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 2.12\n",
      "  player_1: -3.3299999999999996\n",
      "  player_2: 4.210000000000001\n",
      "policy_reward_min:\n",
      "  player_0: -40.666666666666664\n",
      "  player_1: -72.33333333333333\n",
      "  player_2: -42.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08607115562008691\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.303565643057481\n",
      "  mean_inference_ms: 1.6029116334062348\n",
      "  mean_raw_obs_processing_ms: 0.20843087758563564\n",
      "time_since_restore: 6885.776610374451\n",
      "time_this_iter_s: 18.036087036132812\n",
      "time_total_s: 6885.776610374451\n",
      "timers:\n",
      "  learn_throughput: 489.313\n",
      "  learn_time_ms: 16308.595\n",
      "  load_throughput: 609200.025\n",
      "  load_time_ms: 13.099\n",
      "  sample_throughput: 456.652\n",
      "  sample_time_ms: 17475.018\n",
      "  update_time_ms: 5.963\n",
      "timestamp: 1643543116\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3471300\n",
      "training_iteration: 435\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3487231\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-45-51\n",
      "done: false\n",
      "episode_len_mean: 151.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 49\n",
      "episodes_total: 16134\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48066446353991826\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016815295894567348\n",
      "        policy_loss: -0.08887972025821607\n",
      "        total_loss: 54.55530329068502\n",
      "        vf_explained_var: 0.1940322404106458\n",
      "        vf_loss: 54.632832508087155\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5390002410113811\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016576416030566178\n",
      "        policy_loss: -0.059081475517402095\n",
      "        total_loss: 54.81819074471792\n",
      "        vf_explained_var: 0.12736784805854162\n",
      "        vf_loss: 54.86608317772547\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5139781473577023\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01621505768223604\n",
      "        policy_loss: -0.09609671798224251\n",
      "        total_loss: 63.374479366938274\n",
      "        vf_explained_var: 0.15694788932800294\n",
      "        vf_loss: 63.459630772272746\n",
      "  num_agent_steps_sampled: 3487231\n",
      "  num_agent_steps_trained: 3487231\n",
      "  num_steps_sampled: 3487260\n",
      "  num_steps_trained: 3487260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 437\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.490476190476194\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 46.0\n",
      "  player_2: 34.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.94\n",
      "  player_1: -2.27\n",
      "  player_2: 3.33\n",
      "policy_reward_min:\n",
      "  player_0: -40.666666666666664\n",
      "  player_1: -40.333333333333336\n",
      "  player_2: -66.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08612066343304609\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30345453371733294\n",
      "  mean_inference_ms: 1.6024150940814204\n",
      "  mean_raw_obs_processing_ms: 0.20836586715038583\n",
      "time_since_restore: 6921.002737045288\n",
      "time_this_iter_s: 17.373226642608643\n",
      "time_total_s: 6921.002737045288\n",
      "timers:\n",
      "  learn_throughput: 485.258\n",
      "  learn_time_ms: 16444.875\n",
      "  load_throughput: 615445.007\n",
      "  load_time_ms: 12.966\n",
      "  sample_throughput: 447.591\n",
      "  sample_time_ms: 17828.757\n",
      "  update_time_ms: 5.895\n",
      "timestamp: 1643543151\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3487260\n",
      "training_iteration: 437\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3503191\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-46-26\n",
      "done: false\n",
      "episode_len_mean: 148.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 52\n",
      "episodes_total: 16240\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4476913194358349\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015428583151195123\n",
      "        policy_loss: -0.05874309158883989\n",
      "        total_loss: 61.507917652130125\n",
      "        vf_explained_var: 0.43022843182086945\n",
      "        vf_loss: 61.556246399879456\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.51164553369085\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0152826651127783\n",
      "        policy_loss: -0.04735941465944052\n",
      "        total_loss: 69.37406316916147\n",
      "        vf_explained_var: 0.2833069298664729\n",
      "        vf_loss: 69.4111066834132\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4727318630119165\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014615034591958344\n",
      "        policy_loss: -0.10415134059886137\n",
      "        total_loss: 63.525395398139956\n",
      "        vf_explained_var: 0.13055274953444798\n",
      "        vf_loss: 63.61968182881673\n",
      "  num_agent_steps_sampled: 3503191\n",
      "  num_agent_steps_trained: 3503191\n",
      "  num_steps_sampled: 3503220\n",
      "  num_steps_trained: 3503220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 439\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.380952380952381\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.02857142857142\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.33333333333333\n",
      "  player_1: 45.0\n",
      "  player_2: 31.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.5733333333333335\n",
      "  player_1: -1.6166666666666665\n",
      "  player_2: 3.043333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -47.666666666666664\n",
      "  player_1: -40.66666666666667\n",
      "  player_2: -38.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08632991695404003\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3034886238475439\n",
      "  mean_inference_ms: 1.6041068955845907\n",
      "  mean_raw_obs_processing_ms: 0.20872780135580002\n",
      "time_since_restore: 6955.11279797554\n",
      "time_this_iter_s: 17.565004348754883\n",
      "time_total_s: 6955.11279797554\n",
      "timers:\n",
      "  learn_throughput: 487.521\n",
      "  learn_time_ms: 16368.529\n",
      "  load_throughput: 598046.08\n",
      "  load_time_ms: 13.343\n",
      "  sample_throughput: 450.847\n",
      "  sample_time_ms: 17700.035\n",
      "  update_time_ms: 6.061\n",
      "timestamp: 1643543186\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3503220\n",
      "training_iteration: 439\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3519150\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-47-00\n",
      "done: false\n",
      "episode_len_mean: 146.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 49\n",
      "episodes_total: 16342\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46192370479305583\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015900553245973868\n",
      "        policy_loss: -0.06814002756572639\n",
      "        total_loss: 67.48658784548442\n",
      "        vf_explained_var: 0.19003595014413197\n",
      "        vf_loss: 67.54399482568105\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5486091094712416\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01713289037019346\n",
      "        policy_loss: -0.0496483423995475\n",
      "        total_loss: 58.55573488553365\n",
      "        vf_explained_var: 0.2832808843255043\n",
      "        vf_loss: 58.593818519115445\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4762733815610409\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0157411101822269\n",
      "        policy_loss: -0.0979010792193003\n",
      "        total_loss: 98.06111859957377\n",
      "        vf_explained_var: 0.36279115637143455\n",
      "        vf_loss: 98.14839468955994\n",
      "  num_agent_steps_sampled: 3519150\n",
      "  num_agent_steps_trained: 3519150\n",
      "  num_steps_sampled: 3519180\n",
      "  num_steps_trained: 3519180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 441\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.389999999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 41.66666666666667\n",
      "  player_1: 38.0\n",
      "  player_2: 35.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 3.989999999999999\n",
      "  player_1: -1.08\n",
      "  player_2: 0.08999999999999993\n",
      "policy_reward_min:\n",
      "  player_0: -40.666666666666664\n",
      "  player_1: -41.333333333333336\n",
      "  player_2: -60.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08629306705937005\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30302125890443815\n",
      "  mean_inference_ms: 1.600724672650135\n",
      "  mean_raw_obs_processing_ms: 0.20877481479515958\n",
      "time_since_restore: 6989.535610437393\n",
      "time_this_iter_s: 16.25663137435913\n",
      "time_total_s: 6989.535610437393\n",
      "timers:\n",
      "  learn_throughput: 492.501\n",
      "  learn_time_ms: 16203.004\n",
      "  load_throughput: 602385.854\n",
      "  load_time_ms: 13.247\n",
      "  sample_throughput: 449.05\n",
      "  sample_time_ms: 17770.859\n",
      "  update_time_ms: 6.043\n",
      "timestamp: 1643543220\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3519180\n",
      "training_iteration: 441\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3535111\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-47-35\n",
      "done: false\n",
      "episode_len_mean: 149.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 57\n",
      "episodes_total: 16459\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48861875116825104\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0175762702927121\n",
      "        policy_loss: -0.08691407673060894\n",
      "        total_loss: 61.55206511497497\n",
      "        vf_explained_var: 0.30380188177029294\n",
      "        vf_loss: 61.62711523373922\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5114094691971938\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015290973312142644\n",
      "        policy_loss: -0.1020271615916863\n",
      "        total_loss: 45.48611673037211\n",
      "        vf_explained_var: 0.1699853530526161\n",
      "        vf_loss: 45.57782231489817\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4810782741010189\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01631324445028658\n",
      "        policy_loss: -0.08141776223046084\n",
      "        total_loss: 74.3805789756775\n",
      "        vf_explained_var: 0.2725247376163801\n",
      "        vf_loss: 74.45098511377971\n",
      "  num_agent_steps_sampled: 3535111\n",
      "  num_agent_steps_trained: 3535111\n",
      "  num_steps_sampled: 3535140\n",
      "  num_steps_trained: 3535140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 443\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.805000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 42.666666666666664\n",
      "  player_1: 29.666666666666664\n",
      "  player_2: 29.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.4133333333333336\n",
      "  player_1: -2.8266666666666667\n",
      "  player_2: 3.413333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -37.333333333333336\n",
      "  player_1: -28.333333333333336\n",
      "  player_2: -69.33333333333334\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08633839748209143\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3034818336598566\n",
      "  mean_inference_ms: 1.606178487955213\n",
      "  mean_raw_obs_processing_ms: 0.20887281752332484\n",
      "time_since_restore: 7024.033266067505\n",
      "time_this_iter_s: 16.387173891067505\n",
      "time_total_s: 7024.033266067505\n",
      "timers:\n",
      "  learn_throughput: 496.636\n",
      "  learn_time_ms: 16068.093\n",
      "  load_throughput: 624236.194\n",
      "  load_time_ms: 12.784\n",
      "  sample_throughput: 449.284\n",
      "  sample_time_ms: 17761.595\n",
      "  update_time_ms: 6.016\n",
      "timestamp: 1643543255\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3535140\n",
      "training_iteration: 443\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3551070\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-48-10\n",
      "done: false\n",
      "episode_len_mean: 158.42\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 50\n",
      "episodes_total: 16559\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46384006828069685\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016004130256330504\n",
      "        policy_loss: -0.09675739687557022\n",
      "        total_loss: 66.61738851388296\n",
      "        vf_explained_var: 0.23826816896597544\n",
      "        vf_loss: 66.7033429257075\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.507142130235831\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016230256081804555\n",
      "        policy_loss: -0.07667329632677138\n",
      "        total_loss: 49.85129845937093\n",
      "        vf_explained_var: 0.15602910409371057\n",
      "        vf_loss: 49.91701631387075\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4698222671449184\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016435544193403376\n",
      "        policy_loss: -0.057788161967570584\n",
      "        total_loss: 48.95674667040507\n",
      "        vf_explained_var: 0.10972335577011108\n",
      "        vf_loss: 49.00344096978505\n",
      "  num_agent_steps_sampled: 3551070\n",
      "  num_agent_steps_trained: 3551070\n",
      "  num_steps_sampled: 3551100\n",
      "  num_steps_trained: 3551100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 445\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.227272727272727\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.666666666666664\n",
      "  player_1: 26.666666666666664\n",
      "  player_2: 28.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 0.9399999999999998\n",
      "  player_1: -2.08\n",
      "  player_2: 4.14\n",
      "policy_reward_min:\n",
      "  player_0: -26.333333333333336\n",
      "  player_1: -25.666666666666664\n",
      "  player_2: -42.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08660127930654692\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3039802582159633\n",
      "  mean_inference_ms: 1.607629553607633\n",
      "  mean_raw_obs_processing_ms: 0.20923577810743071\n",
      "time_since_restore: 7059.456268310547\n",
      "time_this_iter_s: 17.897823095321655\n",
      "time_total_s: 7059.456268310547\n",
      "timers:\n",
      "  learn_throughput: 501.082\n",
      "  learn_time_ms: 15925.544\n",
      "  load_throughput: 646461.534\n",
      "  load_time_ms: 12.344\n",
      "  sample_throughput: 458.377\n",
      "  sample_time_ms: 17409.253\n",
      "  update_time_ms: 5.973\n",
      "timestamp: 1643543290\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3551100\n",
      "training_iteration: 445\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3567030\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-48-46\n",
      "done: false\n",
      "episode_len_mean: 149.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 41\n",
      "episodes_total: 16648\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4421631932258606\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01772069322043573\n",
      "        policy_loss: -0.07929073703164856\n",
      "        total_loss: 36.65933411637942\n",
      "        vf_explained_var: 0.176608078678449\n",
      "        vf_loss: 36.72666345834732\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4783157933751742\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014012746647925572\n",
      "        policy_loss: -0.032229810087010265\n",
      "        total_loss: 34.647493968804675\n",
      "        vf_explained_var: 0.024524853825569154\n",
      "        vf_loss: 34.670265171130495\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46913043821851413\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015819491676311942\n",
      "        policy_loss: -0.09404714292536179\n",
      "        total_loss: 54.904009663263956\n",
      "        vf_explained_var: 0.16819100926319758\n",
      "        vf_loss: 54.987378673553465\n",
      "  num_agent_steps_sampled: 3567030\n",
      "  num_agent_steps_trained: 3567030\n",
      "  num_steps_sampled: 3567060\n",
      "  num_steps_trained: 3567060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 447\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.977272727272727\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.333333333333336\n",
      "  player_1: 29.666666666666664\n",
      "  player_2: 30.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.743333333333333\n",
      "  player_1: -2.1466666666666665\n",
      "  player_2: 1.4033333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -40.333333333333336\n",
      "  player_1: -28.666666666666664\n",
      "  player_2: -57.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08679055278259294\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30397979595508745\n",
      "  mean_inference_ms: 1.6079812780694125\n",
      "  mean_raw_obs_processing_ms: 0.20934517716690024\n",
      "time_since_restore: 7095.77720952034\n",
      "time_this_iter_s: 17.863794088363647\n",
      "time_total_s: 7095.77720952034\n",
      "timers:\n",
      "  learn_throughput: 497.761\n",
      "  learn_time_ms: 16031.78\n",
      "  load_throughput: 641082.754\n",
      "  load_time_ms: 12.448\n",
      "  sample_throughput: 457.194\n",
      "  sample_time_ms: 17454.281\n",
      "  update_time_ms: 6.081\n",
      "timestamp: 1643543326\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3567060\n",
      "training_iteration: 447\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3582990\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-49-22\n",
      "done: false\n",
      "episode_len_mean: 177.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 50\n",
      "episodes_total: 16749\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4643105461200078\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016073618451781423\n",
      "        policy_loss: -0.11430416947541137\n",
      "        total_loss: 42.00836057980855\n",
      "        vf_explained_var: 0.5003253616889318\n",
      "        vf_loss: 42.111815085411074\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5175895825525125\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01644036669737943\n",
      "        policy_loss: -0.05085656698793173\n",
      "        total_loss: 42.468982869784035\n",
      "        vf_explained_var: 0.23269597659508387\n",
      "        vf_loss: 42.508742162386575\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47546313643455507\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01452964518171863\n",
      "        policy_loss: -0.07115264487763245\n",
      "        total_loss: 71.91776059468587\n",
      "        vf_explained_var: 0.42973693122466405\n",
      "        vf_loss: 71.97910588582357\n",
      "  num_agent_steps_sampled: 3582990\n",
      "  num_agent_steps_trained: 3582990\n",
      "  num_steps_sampled: 3583020\n",
      "  num_steps_trained: 3583020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 449\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.82272727272727\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.01363636363637\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 39.333333333333336\n",
      "  player_2: 31.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 4.170000000000001\n",
      "  player_1: -1.6299999999999997\n",
      "  player_2: 0.4600000000000003\n",
      "policy_reward_min:\n",
      "  player_0: -64.33333333333334\n",
      "  player_1: -55.0\n",
      "  player_2: -51.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08670658065709291\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.303708220162001\n",
      "  mean_inference_ms: 1.6069569644471156\n",
      "  mean_raw_obs_processing_ms: 0.2093448657362175\n",
      "time_since_restore: 7131.3163340091705\n",
      "time_this_iter_s: 17.866560697555542\n",
      "time_total_s: 7131.3163340091705\n",
      "timers:\n",
      "  learn_throughput: 493.351\n",
      "  learn_time_ms: 16175.112\n",
      "  load_throughput: 642776.818\n",
      "  load_time_ms: 12.415\n",
      "  sample_throughput: 452.966\n",
      "  sample_time_ms: 17617.2\n",
      "  update_time_ms: 6.084\n",
      "timestamp: 1643543362\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3583020\n",
      "training_iteration: 449\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3598950\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-50-00\n",
      "done: false\n",
      "episode_len_mean: 159.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 50\n",
      "episodes_total: 16852\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4489784407615662\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015074122817241383\n",
      "        policy_loss: -0.06248738060705364\n",
      "        total_loss: 37.80791831731796\n",
      "        vf_explained_var: 0.08799216747283936\n",
      "        vf_loss: 37.86023080428441\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.514752138008674\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014014143659048083\n",
      "        policy_loss: -0.07063842727337033\n",
      "        total_loss: 49.995493136247\n",
      "        vf_explained_var: 0.21000304520130159\n",
      "        vf_loss: 50.056671908696494\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49567326784133914\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01638624057445213\n",
      "        policy_loss: -0.08337660394764194\n",
      "        total_loss: 61.740927040576935\n",
      "        vf_explained_var: 0.14033853511015573\n",
      "        vf_loss: 61.813242817719775\n",
      "  num_agent_steps_sampled: 3598950\n",
      "  num_agent_steps_trained: 3598950\n",
      "  num_steps_sampled: 3598980\n",
      "  num_steps_trained: 3598980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 451\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.691304347826087\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.3741861979166667\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.33333333333333\n",
      "  player_1: 26.66666666666667\n",
      "  player_2: 35.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 2.6699999999999995\n",
      "  player_1: -2.6899999999999995\n",
      "  player_2: 3.02\n",
      "policy_reward_min:\n",
      "  player_0: -38.666666666666664\n",
      "  player_1: -63.66666666666667\n",
      "  player_2: -41.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08650500229912933\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30409760859554885\n",
      "  mean_inference_ms: 1.6111196349498271\n",
      "  mean_raw_obs_processing_ms: 0.20923589644825455\n",
      "time_since_restore: 7168.761915445328\n",
      "time_this_iter_s: 19.049909830093384\n",
      "time_total_s: 7168.761915445328\n",
      "timers:\n",
      "  learn_throughput: 484.183\n",
      "  learn_time_ms: 16481.36\n",
      "  load_throughput: 621557.227\n",
      "  load_time_ms: 12.839\n",
      "  sample_throughput: 451.656\n",
      "  sample_time_ms: 17668.299\n",
      "  update_time_ms: 6.21\n",
      "timestamp: 1643543400\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3598980\n",
      "training_iteration: 451\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3614910\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-50-35\n",
      "done: false\n",
      "episode_len_mean: 136.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 55\n",
      "episodes_total: 16966\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4660532526175181\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014665321007714548\n",
      "        policy_loss: -0.09356137019271651\n",
      "        total_loss: 75.93129454135895\n",
      "        vf_explained_var: 0.23810841341813405\n",
      "        vf_loss: 76.01495679219563\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5313463325798512\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01594499241665896\n",
      "        policy_loss: -0.059878386113171776\n",
      "        total_loss: 65.49554176807403\n",
      "        vf_explained_var: 0.15364230463902157\n",
      "        vf_loss: 65.54465715726217\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48867671713232996\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015569017687158274\n",
      "        policy_loss: -0.07560772364338239\n",
      "        total_loss: 93.12673172950744\n",
      "        vf_explained_var: 0.29951937387386957\n",
      "        vf_loss: 93.19183013280232\n",
      "  num_agent_steps_sampled: 3614910\n",
      "  num_agent_steps_trained: 3614910\n",
      "  num_steps_sampled: 3614940\n",
      "  num_steps_trained: 3614940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 453\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.181818181818185\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.666666666666664\n",
      "  player_1: 36.66666666666667\n",
      "  player_2: 40.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 0.7833333333333337\n",
      "  player_1: -0.26666666666666655\n",
      "  player_2: 2.483333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -44.333333333333336\n",
      "  player_1: -53.33333333333333\n",
      "  player_2: -47.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08662041273089408\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3036208065900919\n",
      "  mean_inference_ms: 1.6103260236650698\n",
      "  mean_raw_obs_processing_ms: 0.20939082481886678\n",
      "time_since_restore: 7204.605645656586\n",
      "time_this_iter_s: 18.020981311798096\n",
      "time_total_s: 7204.605645656586\n",
      "timers:\n",
      "  learn_throughput: 480.252\n",
      "  learn_time_ms: 16616.276\n",
      "  load_throughput: 756899.407\n",
      "  load_time_ms: 10.543\n",
      "  sample_throughput: 445.323\n",
      "  sample_time_ms: 17919.591\n",
      "  update_time_ms: 6.151\n",
      "timestamp: 1643543435\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3614940\n",
      "training_iteration: 453\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3630870\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-51-12\n",
      "done: false\n",
      "episode_len_mean: 145.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 53\n",
      "episodes_total: 17074\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.455126620978117\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016800281692453987\n",
      "        policy_loss: -0.06479307538829744\n",
      "        total_loss: 57.34678907632828\n",
      "        vf_explained_var: 0.4010786179701487\n",
      "        vf_loss: 57.40024189472199\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5249507498244445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016028771142873665\n",
      "        policy_loss: -0.06348454712890089\n",
      "        total_loss: 64.34417129993439\n",
      "        vf_explained_var: 0.05148414154847463\n",
      "        vf_loss: 64.39683653434118\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48892793253064154\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016090216025477274\n",
      "        policy_loss: -0.10167659286720057\n",
      "        total_loss: 74.77414653619131\n",
      "        vf_explained_var: 0.15328026394049327\n",
      "        vf_loss: 74.86496170679729\n",
      "  num_agent_steps_sampled: 3630870\n",
      "  num_agent_steps_trained: 3630870\n",
      "  num_steps_sampled: 3630900\n",
      "  num_steps_trained: 3630900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 455\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.840909090909092\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.98181818181818\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.0\n",
      "  player_1: 41.666666666666664\n",
      "  player_2: 30.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 2.0833333333333335\n",
      "  player_1: -2.6966666666666663\n",
      "  player_2: 3.613333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -39.666666666666664\n",
      "  player_1: -46.666666666666664\n",
      "  player_2: -61.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08671246744061228\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30323563863432107\n",
      "  mean_inference_ms: 1.606439770540823\n",
      "  mean_raw_obs_processing_ms: 0.2095003376228879\n",
      "time_since_restore: 7241.175612211227\n",
      "time_this_iter_s: 18.103318691253662\n",
      "time_total_s: 7241.175612211227\n",
      "timers:\n",
      "  learn_throughput: 477.059\n",
      "  learn_time_ms: 16727.501\n",
      "  load_throughput: 753059.232\n",
      "  load_time_ms: 10.597\n",
      "  sample_throughput: 438.956\n",
      "  sample_time_ms: 18179.501\n",
      "  update_time_ms: 6.128\n",
      "timestamp: 1643543472\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3630900\n",
      "training_iteration: 455\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3646831\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-51-47\n",
      "done: false\n",
      "episode_len_mean: 146.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 56\n",
      "episodes_total: 17194\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45498136242230736\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016307660901870236\n",
      "        policy_loss: -0.08704173284272353\n",
      "        total_loss: 48.86829084714254\n",
      "        vf_explained_var: 0.3031272988518079\n",
      "        vf_loss: 48.94432500521342\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5561155233283838\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015727441572927794\n",
      "        policy_loss: -0.07019594446445505\n",
      "        total_loss: 62.57999792416891\n",
      "        vf_explained_var: 0.23122501452763874\n",
      "        vf_loss: 62.63957814057668\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49525872031847634\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015302943386860762\n",
      "        policy_loss: -0.07085709596984088\n",
      "        total_loss: 102.32639235814412\n",
      "        vf_explained_var: 0.24985143065452575\n",
      "        vf_loss: 102.38692069848379\n",
      "  num_agent_steps_sampled: 3646831\n",
      "  num_agent_steps_trained: 3646831\n",
      "  num_steps_sampled: 3646860\n",
      "  num_steps_trained: 3646860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 457\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.161904761904761\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.94761904761907\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.33333333333333\n",
      "  player_1: 42.333333333333336\n",
      "  player_2: 34.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.9733333333333336\n",
      "  player_1: -2.3166666666666664\n",
      "  player_2: 2.3433333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -33.0\n",
      "  player_1: -53.0\n",
      "  player_2: -64.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08657599071173641\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3037756734402166\n",
      "  mean_inference_ms: 1.6132466353758756\n",
      "  mean_raw_obs_processing_ms: 0.2096144108241994\n",
      "time_since_restore: 7276.315370798111\n",
      "time_this_iter_s: 17.229772329330444\n",
      "time_total_s: 7276.315370798111\n",
      "timers:\n",
      "  learn_throughput: 480.48\n",
      "  learn_time_ms: 16608.405\n",
      "  load_throughput: 794566.222\n",
      "  load_time_ms: 10.043\n",
      "  sample_throughput: 439.809\n",
      "  sample_time_ms: 18144.239\n",
      "  update_time_ms: 6.016\n",
      "timestamp: 1643543507\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3646860\n",
      "training_iteration: 457\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3662790\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-52-22\n",
      "done: false\n",
      "episode_len_mean: 142.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 52\n",
      "episodes_total: 17301\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44843924924731254\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016112304838873874\n",
      "        policy_loss: -0.096420877383401\n",
      "        total_loss: 42.50083925485611\n",
      "        vf_explained_var: 0.2245585929354032\n",
      "        vf_loss: 42.586384512583415\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5176638878385226\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015241763321744959\n",
      "        policy_loss: -0.0747450358668963\n",
      "        total_loss: 52.571472291946414\n",
      "        vf_explained_var: 0.1311489231387774\n",
      "        vf_loss: 52.63592902501424\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4758844475448132\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014665124199974192\n",
      "        policy_loss: -0.05131748026392112\n",
      "        total_loss: 79.13782998243968\n",
      "        vf_explained_var: 0.1996471337477366\n",
      "        vf_loss: 79.17924822409948\n",
      "  num_agent_steps_sampled: 3662790\n",
      "  num_agent_steps_trained: 3662790\n",
      "  num_steps_sampled: 3662820\n",
      "  num_steps_trained: 3662820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 459\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.70526315789474\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 64.99473684210527\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 26.66666666666667\n",
      "  player_1: 37.0\n",
      "  player_2: 32.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.86\n",
      "  player_1: -2.92\n",
      "  player_2: 4.06\n",
      "policy_reward_min:\n",
      "  player_0: -33.0\n",
      "  player_1: -36.333333333333336\n",
      "  player_2: -50.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08664286651600087\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30384696192247485\n",
      "  mean_inference_ms: 1.6125989945180745\n",
      "  mean_raw_obs_processing_ms: 0.20962841959197534\n",
      "time_since_restore: 7311.178586006165\n",
      "time_this_iter_s: 15.993082523345947\n",
      "time_total_s: 7311.178586006165\n",
      "timers:\n",
      "  learn_throughput: 482.268\n",
      "  learn_time_ms: 16546.805\n",
      "  load_throughput: 871447.248\n",
      "  load_time_ms: 9.157\n",
      "  sample_throughput: 438.55\n",
      "  sample_time_ms: 18196.308\n",
      "  update_time_ms: 6.027\n",
      "timestamp: 1643543542\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3662820\n",
      "training_iteration: 459\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3678750\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-52-57\n",
      "done: false\n",
      "episode_len_mean: 144.6\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 54\n",
      "episodes_total: 17411\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46513208145896595\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017227682136262957\n",
      "        policy_loss: -0.1129335676288853\n",
      "        total_loss: 46.58961268266042\n",
      "        vf_explained_var: 0.34011064141988756\n",
      "        vf_loss: 46.69091766277949\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5263961972296238\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016032745853462226\n",
      "        policy_loss: -0.06615742084415009\n",
      "        total_loss: 40.025804756482444\n",
      "        vf_explained_var: -0.061798737446467085\n",
      "        vf_loss: 40.08114014148712\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5047514857848485\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015896556908002044\n",
      "        policy_loss: -0.04421349615169068\n",
      "        total_loss: 65.83169778982798\n",
      "        vf_explained_var: 0.15855332801739375\n",
      "        vf_loss: 65.86518140474955\n",
      "  num_agent_steps_sampled: 3678750\n",
      "  num_agent_steps_trained: 3678750\n",
      "  num_steps_sampled: 3678780\n",
      "  num_steps_trained: 3678780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 461\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.51818181818182\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 25.666666666666664\n",
      "  player_1: 44.666666666666664\n",
      "  player_2: 34.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 0.40333333333333354\n",
      "  player_1: 0.8333333333333335\n",
      "  player_2: 1.763333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -37.333333333333336\n",
      "  player_1: -29.666666666666664\n",
      "  player_2: -46.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08659077255128574\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.303860863201751\n",
      "  mean_inference_ms: 1.6132419206874977\n",
      "  mean_raw_obs_processing_ms: 0.20984011980179987\n",
      "time_since_restore: 7346.099569320679\n",
      "time_this_iter_s: 18.307826042175293\n",
      "time_total_s: 7346.099569320679\n",
      "timers:\n",
      "  learn_throughput: 489.846\n",
      "  learn_time_ms: 16290.839\n",
      "  load_throughput: 965448.245\n",
      "  load_time_ms: 8.266\n",
      "  sample_throughput: 447.422\n",
      "  sample_time_ms: 17835.492\n",
      "  update_time_ms: 5.851\n",
      "timestamp: 1643543577\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3678780\n",
      "training_iteration: 461\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3694712\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-53-31\n",
      "done: false\n",
      "episode_len_mean: 136.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 62\n",
      "episodes_total: 17522\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46578587343295413\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016773896380424654\n",
      "        policy_loss: -0.07177735340160628\n",
      "        total_loss: 88.99950088183085\n",
      "        vf_explained_var: 0.31784006297588346\n",
      "        vf_loss: 89.05995574156444\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5575956404209137\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015643390734990135\n",
      "        policy_loss: -0.1040951759616534\n",
      "        total_loss: 63.30672881603241\n",
      "        vf_explained_var: 0.20999021132787068\n",
      "        vf_loss: 63.400264727274575\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4740330386161804\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016241341420859878\n",
      "        policy_loss: -0.058137276219204066\n",
      "        total_loss: 80.19144967397054\n",
      "        vf_explained_var: 0.2093214358886083\n",
      "        vf_loss: 80.23862405459086\n",
      "  num_agent_steps_sampled: 3694712\n",
      "  num_agent_steps_trained: 3694712\n",
      "  num_steps_sampled: 3694740\n",
      "  num_steps_trained: 3694740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 463\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.569999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.015\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.0\n",
      "  player_1: 33.66666666666667\n",
      "  player_2: 34.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.063333333333333\n",
      "  player_1: -0.9566666666666666\n",
      "  player_2: 0.8933333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -43.666666666666664\n",
      "  player_1: -34.0\n",
      "  player_2: -40.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0868101839314263\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30389165255012196\n",
      "  mean_inference_ms: 1.6144452440860875\n",
      "  mean_raw_obs_processing_ms: 0.2100568412198986\n",
      "time_since_restore: 7379.791065454483\n",
      "time_this_iter_s: 16.71319031715393\n",
      "time_total_s: 7379.791065454483\n",
      "timers:\n",
      "  learn_throughput: 496.427\n",
      "  learn_time_ms: 16074.88\n",
      "  load_throughput: 969410.512\n",
      "  load_time_ms: 8.232\n",
      "  sample_throughput: 451.421\n",
      "  sample_time_ms: 17677.522\n",
      "  update_time_ms: 5.871\n",
      "timestamp: 1643543611\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3694740\n",
      "training_iteration: 463\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3710670\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-54-09\n",
      "done: false\n",
      "episode_len_mean: 161.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 54\n",
      "episodes_total: 17620\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.43958960528175034\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015902677178110405\n",
      "        policy_loss: -0.08035682378336788\n",
      "        total_loss: 66.22002624114354\n",
      "        vf_explained_var: 0.19828528424104055\n",
      "        vf_loss: 66.28964843988419\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5079055376599232\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014389466214236962\n",
      "        policy_loss: -0.05215210103740295\n",
      "        total_loss: 58.134996016025546\n",
      "        vf_explained_var: 0.0997826987504959\n",
      "        vf_loss: 58.17743498086929\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4739140788714091\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017169413234244455\n",
      "        policy_loss: -0.09181926131248475\n",
      "        total_loss: 52.666958014170326\n",
      "        vf_explained_var: 0.22478514273961386\n",
      "        vf_loss: 52.747187647819516\n",
      "  num_agent_steps_sampled: 3710670\n",
      "  num_agent_steps_trained: 3710670\n",
      "  num_steps_sampled: 3710700\n",
      "  num_steps_trained: 3710700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 465\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.415999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.012\n",
      "  vram_util_percent0: 0.37418619791666663\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.33333333333333\n",
      "  player_1: 40.0\n",
      "  player_2: 34.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.5433333333333334\n",
      "  player_1: -2.826666666666666\n",
      "  player_2: 4.283333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -36.0\n",
      "  player_1: -34.666666666666664\n",
      "  player_2: -41.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08696577418396473\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3043214779064319\n",
      "  mean_inference_ms: 1.614360519099479\n",
      "  mean_raw_obs_processing_ms: 0.21011459519753015\n",
      "time_since_restore: 7417.596455574036\n",
      "time_this_iter_s: 20.681642770767212\n",
      "time_total_s: 7417.596455574036\n",
      "timers:\n",
      "  learn_throughput: 492.54\n",
      "  learn_time_ms: 16201.727\n",
      "  load_throughput: 775743.652\n",
      "  load_time_ms: 10.287\n",
      "  sample_throughput: 458.402\n",
      "  sample_time_ms: 17408.296\n",
      "  update_time_ms: 6.105\n",
      "timestamp: 1643543649\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3710700\n",
      "training_iteration: 465\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3726630\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-54-43\n",
      "done: false\n",
      "episode_len_mean: 136.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 61\n",
      "episodes_total: 17740\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46060185189048447\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01712696630039981\n",
      "        policy_loss: -0.07649877126018206\n",
      "        total_loss: 70.84247620979944\n",
      "        vf_explained_var: 0.3933244051535924\n",
      "        vf_loss: 70.90741425395012\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5362427772084872\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013167871729959302\n",
      "        policy_loss: -0.08551600164423387\n",
      "        total_loss: 89.54700186550618\n",
      "        vf_explained_var: 0.26211319883664447\n",
      "        vf_loss: 89.62362913827101\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4887249440451463\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014818068343992082\n",
      "        policy_loss: -0.07381165897163253\n",
      "        total_loss: 107.45369148095449\n",
      "        vf_explained_var: 0.2823560105760892\n",
      "        vf_loss: 107.5175010116895\n",
      "  num_agent_steps_sampled: 3726630\n",
      "  num_agent_steps_trained: 3726630\n",
      "  num_steps_sampled: 3726660\n",
      "  num_steps_trained: 3726660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 467\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.242857142857142\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.66666666666667\n",
      "  player_1: 39.0\n",
      "  player_2: 58.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 1.91\n",
      "  player_1: -1.2399999999999998\n",
      "  player_2: 2.33\n",
      "policy_reward_min:\n",
      "  player_0: -33.66666666666667\n",
      "  player_1: -72.66666666666667\n",
      "  player_2: -50.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08691241721363578\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30434944246270373\n",
      "  mean_inference_ms: 1.6180901653709565\n",
      "  mean_raw_obs_processing_ms: 0.21035579439334007\n",
      "time_since_restore: 7451.562666893005\n",
      "time_this_iter_s: 17.803943872451782\n",
      "time_total_s: 7451.562666893005\n",
      "timers:\n",
      "  learn_throughput: 495.908\n",
      "  learn_time_ms: 16091.695\n",
      "  load_throughput: 742832.481\n",
      "  load_time_ms: 10.743\n",
      "  sample_throughput: 456.159\n",
      "  sample_time_ms: 17493.917\n",
      "  update_time_ms: 6.192\n",
      "timestamp: 1643543683\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3726660\n",
      "training_iteration: 467\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3742590\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-55-17\n",
      "done: false\n",
      "episode_len_mean: 136.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 55\n",
      "episodes_total: 17854\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4856965703765551\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01710651551892037\n",
      "        policy_loss: -0.08983124747872352\n",
      "        total_loss: 69.31699494361878\n",
      "        vf_explained_var: 0.27299737006425856\n",
      "        vf_loss: 69.39527901967367\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5387682994703452\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015516947552679692\n",
      "        policy_loss: -0.0417268457574149\n",
      "        total_loss: 51.288024357159934\n",
      "        vf_explained_var: 0.1109590537349383\n",
      "        vf_loss: 51.319277313550316\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4856170493364334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017058963466640004\n",
      "        policy_loss: -0.07200773523499568\n",
      "        total_loss: 46.05582965373993\n",
      "        vf_explained_var: 0.12410561035076777\n",
      "        vf_loss: 46.116322447458906\n",
      "  num_agent_steps_sampled: 3742590\n",
      "  num_agent_steps_trained: 3742590\n",
      "  num_steps_sampled: 3742620\n",
      "  num_steps_trained: 3742620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 469\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.620000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.666666666666664\n",
      "  player_1: 44.666666666666664\n",
      "  player_2: 40.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -1.6\n",
      "  player_1: -3.0099999999999993\n",
      "  player_2: 7.61\n",
      "policy_reward_min:\n",
      "  player_0: -46.333333333333336\n",
      "  player_1: -42.333333333333336\n",
      "  player_2: -37.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08682898023522888\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30404317226281524\n",
      "  mean_inference_ms: 1.6176102707173663\n",
      "  mean_raw_obs_processing_ms: 0.2103232741720108\n",
      "time_since_restore: 7485.20639705658\n",
      "time_this_iter_s: 16.725781679153442\n",
      "time_total_s: 7485.20639705658\n",
      "timers:\n",
      "  learn_throughput: 499.766\n",
      "  learn_time_ms: 15967.462\n",
      "  load_throughput: 722846.633\n",
      "  load_time_ms: 11.04\n",
      "  sample_throughput: 459.603\n",
      "  sample_time_ms: 17362.816\n",
      "  update_time_ms: 6.446\n",
      "timestamp: 1643543717\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3742620\n",
      "training_iteration: 469\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3758550\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-55-51\n",
      "done: false\n",
      "episode_len_mean: 161.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 53\n",
      "episodes_total: 17956\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4724940453469753\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01627113628294107\n",
      "        policy_loss: -0.06778473113353053\n",
      "        total_loss: 69.11408653577169\n",
      "        vf_explained_var: 0.11671869913736979\n",
      "        vf_loss: 69.1708880519867\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5284345191220442\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015341912869457132\n",
      "        policy_loss: -0.0725640413692842\n",
      "        total_loss: 50.79247327009837\n",
      "        vf_explained_var: 0.30216393649578094\n",
      "        vf_loss: 50.854681505362194\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4783839278916518\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016478441988660202\n",
      "        policy_loss: -0.10102128095303972\n",
      "        total_loss: 67.37211323738099\n",
      "        vf_explained_var: 0.22494340320428213\n",
      "        vf_loss: 67.46201145966847\n",
      "  num_agent_steps_sampled: 3758550\n",
      "  num_agent_steps_trained: 3758550\n",
      "  num_steps_sampled: 3758580\n",
      "  num_steps_trained: 3758580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 471\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.261904761904765\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.02857142857142\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.333333333333336\n",
      "  player_1: 41.66666666666667\n",
      "  player_2: 39.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -1.6000000000000003\n",
      "  player_1: -1.1499999999999997\n",
      "  player_2: 5.75\n",
      "policy_reward_min:\n",
      "  player_0: -50.33333333333333\n",
      "  player_1: -35.666666666666664\n",
      "  player_2: -43.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08714588889689477\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30467205869489467\n",
      "  mean_inference_ms: 1.6182067862076206\n",
      "  mean_raw_obs_processing_ms: 0.21044751963419667\n",
      "time_since_restore: 7519.512911081314\n",
      "time_this_iter_s: 18.15599012374878\n",
      "time_total_s: 7519.512911081314\n",
      "timers:\n",
      "  learn_throughput: 501.526\n",
      "  learn_time_ms: 15911.447\n",
      "  load_throughput: 677710.157\n",
      "  load_time_ms: 11.775\n",
      "  sample_throughput: 458.741\n",
      "  sample_time_ms: 17395.425\n",
      "  update_time_ms: 6.541\n",
      "timestamp: 1643543751\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3758580\n",
      "training_iteration: 471\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3774512\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-56-26\n",
      "done: false\n",
      "episode_len_mean: 136.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 61\n",
      "episodes_total: 18071\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4794428112109502\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016006249661434897\n",
      "        policy_loss: -0.07155478052794934\n",
      "        total_loss: 84.67009358247121\n",
      "        vf_explained_var: 0.22065365314483643\n",
      "        vf_loss: 84.73084425767263\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5203667760888735\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014766641059523333\n",
      "        policy_loss: -0.0761439999130865\n",
      "        total_loss: 64.07977310657502\n",
      "        vf_explained_var: 0.23737680127223332\n",
      "        vf_loss: 64.14594950993856\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49134747236967086\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01622932061857161\n",
      "        policy_loss: -0.06534642471621434\n",
      "        total_loss: 97.22191081841787\n",
      "        vf_explained_var: 0.1900769360860189\n",
      "        vf_loss: 97.27630202770233\n",
      "  num_agent_steps_sampled: 3774512\n",
      "  num_agent_steps_trained: 3774512\n",
      "  num_steps_sampled: 3774540\n",
      "  num_steps_trained: 3774540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 473\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.285000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.333333333333336\n",
      "  player_1: 51.33333333333333\n",
      "  player_2: 35.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -0.42999999999999994\n",
      "  player_1: -1.1100000000000003\n",
      "  player_2: 4.539999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -50.0\n",
      "  player_1: -25.666666666666664\n",
      "  player_2: -55.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08696233920133963\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3043026419717454\n",
      "  mean_inference_ms: 1.6202135307472911\n",
      "  mean_raw_obs_processing_ms: 0.21066677387078914\n",
      "time_since_restore: 7554.009353399277\n",
      "time_this_iter_s: 16.415600538253784\n",
      "time_total_s: 7554.009353399277\n",
      "timers:\n",
      "  learn_throughput: 499.03\n",
      "  learn_time_ms: 15991.037\n",
      "  load_throughput: 649450.701\n",
      "  load_time_ms: 12.287\n",
      "  sample_throughput: 456.114\n",
      "  sample_time_ms: 17495.627\n",
      "  update_time_ms: 6.574\n",
      "timestamp: 1643543786\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3774540\n",
      "training_iteration: 473\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3790473\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-57-01\n",
      "done: false\n",
      "episode_len_mean: 155.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 57\n",
      "episodes_total: 18177\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4623543485502402\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01609040440814624\n",
      "        policy_loss: -0.055043758843094114\n",
      "        total_loss: 73.85212567090989\n",
      "        vf_explained_var: 0.23592212527990342\n",
      "        vf_loss: 73.8963080271085\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5146776696046194\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01540310742106764\n",
      "        policy_loss: -0.09643266112233202\n",
      "        total_loss: 37.940617071787514\n",
      "        vf_explained_var: 0.18259815007448196\n",
      "        vf_loss: 38.02665257612864\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48549840673804284\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016026195863181558\n",
      "        policy_loss: -0.078713339913326\n",
      "        total_loss: 88.86033935705821\n",
      "        vf_explained_var: 0.35204559753338494\n",
      "        vf_loss: 88.92823486963908\n",
      "  num_agent_steps_sampled: 3790473\n",
      "  num_agent_steps_trained: 3790473\n",
      "  num_steps_sampled: 3790500\n",
      "  num_steps_trained: 3790500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 475\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.109090909090906\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.01818181818182\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.333333333333336\n",
      "  player_1: 34.666666666666664\n",
      "  player_2: 32.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.1066666666666669\n",
      "  player_1: -2.766666666666666\n",
      "  player_2: 5.873333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -46.333333333333336\n",
      "  player_1: -34.0\n",
      "  player_2: -47.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08714906245157512\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.304092882975649\n",
      "  mean_inference_ms: 1.6171254915938507\n",
      "  mean_raw_obs_processing_ms: 0.2107415861871293\n",
      "time_since_restore: 7589.607941389084\n",
      "time_this_iter_s: 18.27664303779602\n",
      "time_total_s: 7589.607941389084\n",
      "timers:\n",
      "  learn_throughput: 506.031\n",
      "  learn_time_ms: 15769.795\n",
      "  load_throughput: 569406.121\n",
      "  load_time_ms: 14.015\n",
      "  sample_throughput: 456.394\n",
      "  sample_time_ms: 17484.901\n",
      "  update_time_ms: 6.364\n",
      "timestamp: 1643543821\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3790500\n",
      "training_iteration: 475\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3806431\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-57-37\n",
      "done: false\n",
      "episode_len_mean: 153.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 55\n",
      "episodes_total: 18281\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4962548178931077\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015980063715993916\n",
      "        policy_loss: -0.05292910723015666\n",
      "        total_loss: 52.85945580005646\n",
      "        vf_explained_var: 0.23521297057469687\n",
      "        vf_loss: 52.90159837881724\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5284340400497118\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01610013257416237\n",
      "        policy_loss: -0.08345786730758846\n",
      "        total_loss: 46.12573544343313\n",
      "        vf_explained_var: 0.14431047509113948\n",
      "        vf_loss: 46.19832582632701\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4876116376121839\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016182764946744706\n",
      "        policy_loss: -0.08558584640423457\n",
      "        total_loss: 64.07793315728506\n",
      "        vf_explained_var: 0.15982759376366934\n",
      "        vf_loss: 64.1525957218806\n",
      "  num_agent_steps_sampled: 3806431\n",
      "  num_agent_steps_trained: 3806431\n",
      "  num_steps_sampled: 3806460\n",
      "  num_steps_trained: 3806460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 477\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.52727272727273\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.0\n",
      "  player_1: 31.0\n",
      "  player_2: 28.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 0.2466666666666659\n",
      "  player_1: -1.7533333333333334\n",
      "  player_2: 4.506666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -44.333333333333336\n",
      "  player_1: -24.0\n",
      "  player_2: -36.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08706487055737412\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3048985806297784\n",
      "  mean_inference_ms: 1.6209626016754997\n",
      "  mean_raw_obs_processing_ms: 0.21099096730877356\n",
      "time_since_restore: 7625.657397031784\n",
      "time_this_iter_s: 17.864340782165527\n",
      "time_total_s: 7625.657397031784\n",
      "timers:\n",
      "  learn_throughput: 499.534\n",
      "  learn_time_ms: 15974.885\n",
      "  load_throughput: 588329.676\n",
      "  load_time_ms: 13.564\n",
      "  sample_throughput: 457.417\n",
      "  sample_time_ms: 17445.795\n",
      "  update_time_ms: 6.338\n",
      "timestamp: 1643543857\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3806460\n",
      "training_iteration: 477\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3822392\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-58-12\n",
      "done: false\n",
      "episode_len_mean: 141.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 59\n",
      "episodes_total: 18396\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4835814237097899\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01610975269048396\n",
      "        policy_loss: -0.09619468773404757\n",
      "        total_loss: 53.86325383822123\n",
      "        vf_explained_var: 0.2800912178556124\n",
      "        vf_loss: 53.948574274381\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5347579943140348\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015908918515723847\n",
      "        policy_loss: -0.06544333014637232\n",
      "        total_loss: 48.199838267962136\n",
      "        vf_explained_var: 0.4552272394299507\n",
      "        vf_loss: 48.25454312960307\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4785031685233116\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01608574901057125\n",
      "        policy_loss: -0.0776306585315615\n",
      "        total_loss: 60.03287697792053\n",
      "        vf_explained_var: 0.31414107044537865\n",
      "        vf_loss: 60.09964977582296\n",
      "  num_agent_steps_sampled: 3822392\n",
      "  num_agent_steps_trained: 3822392\n",
      "  num_steps_sampled: 3822420\n",
      "  num_steps_trained: 3822420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 479\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.814285714285713\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.333333333333336\n",
      "  player_1: 36.33333333333333\n",
      "  player_2: 56.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.5499999999999996\n",
      "  player_1: -2.94\n",
      "  player_2: 6.49\n",
      "policy_reward_min:\n",
      "  player_0: -33.0\n",
      "  player_1: -32.0\n",
      "  player_2: -40.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0871576669744216\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3043336919281569\n",
      "  mean_inference_ms: 1.6206416284235154\n",
      "  mean_raw_obs_processing_ms: 0.2109617141490552\n",
      "time_since_restore: 7660.633026599884\n",
      "time_this_iter_s: 17.274559020996094\n",
      "time_total_s: 7660.633026599884\n",
      "timers:\n",
      "  learn_throughput: 495.369\n",
      "  learn_time_ms: 16109.189\n",
      "  load_throughput: 596384.793\n",
      "  load_time_ms: 13.381\n",
      "  sample_throughput: 455.298\n",
      "  sample_time_ms: 17526.97\n",
      "  update_time_ms: 6.214\n",
      "timestamp: 1643543892\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3822420\n",
      "training_iteration: 479\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3838350\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-58-50\n",
      "done: false\n",
      "episode_len_mean: 149.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 55\n",
      "episodes_total: 18506\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4721053775648276\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01723708508328855\n",
      "        policy_loss: -0.06863691388318936\n",
      "        total_loss: 68.614710466067\n",
      "        vf_explained_var: 0.4433099808295568\n",
      "        vf_loss: 68.67171244621277\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5197455741465091\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015677565140794818\n",
      "        policy_loss: -0.0677237472621103\n",
      "        total_loss: 47.52912006696065\n",
      "        vf_explained_var: 0.3033416854341825\n",
      "        vf_loss: 47.586261276404066\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4737802456319332\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016132930327779224\n",
      "        policy_loss: -0.09052530830415587\n",
      "        total_loss: 60.73538215478261\n",
      "        vf_explained_var: 0.4444016178448995\n",
      "        vf_loss: 60.81501824220022\n",
      "  num_agent_steps_sampled: 3838350\n",
      "  num_agent_steps_trained: 3838350\n",
      "  num_steps_sampled: 3838380\n",
      "  num_steps_trained: 3838380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 481\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 12.920833333333334\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0125\n",
      "  vram_util_percent0: 0.3741861979166667\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 38.666666666666664\n",
      "  player_2: 36.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 0.23333333333333361\n",
      "  player_1: -1.4066666666666665\n",
      "  player_2: 4.173333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -32.666666666666664\n",
      "  player_1: -31.666666666666664\n",
      "  player_2: -55.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08698487023023063\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3041206713739944\n",
      "  mean_inference_ms: 1.6206912756162526\n",
      "  mean_raw_obs_processing_ms: 0.21096491039334836\n",
      "time_since_restore: 7697.870095014572\n",
      "time_this_iter_s: 19.56644630432129\n",
      "time_total_s: 7697.870095014572\n",
      "timers:\n",
      "  learn_throughput: 486.65\n",
      "  learn_time_ms: 16397.824\n",
      "  load_throughput: 628597.35\n",
      "  load_time_ms: 12.695\n",
      "  sample_throughput: 450.151\n",
      "  sample_time_ms: 17727.392\n",
      "  update_time_ms: 6.318\n",
      "timestamp: 1643543930\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3838380\n",
      "training_iteration: 481\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3854310\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_12-59-27\n",
      "done: false\n",
      "episode_len_mean: 139.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 59\n",
      "episodes_total: 18621\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46203914140661556\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015496256477268275\n",
      "        policy_loss: -0.08623388298476736\n",
      "        total_loss: 71.83946190039318\n",
      "        vf_explained_var: 0.16077605068683623\n",
      "        vf_loss: 71.91523621400198\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5459764498472214\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016452038838917438\n",
      "        policy_loss: -0.07552359749407818\n",
      "        total_loss: 56.37425483862559\n",
      "        vf_explained_var: 0.20282145112752914\n",
      "        vf_loss: 56.43867339769999\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4952400078376134\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017336136709066637\n",
      "        policy_loss: -0.07502824924886227\n",
      "        total_loss: 66.99309020837148\n",
      "        vf_explained_var: 0.34045492817958195\n",
      "        vf_loss: 67.05641664981842\n",
      "  num_agent_steps_sampled: 3854310\n",
      "  num_agent_steps_trained: 3854310\n",
      "  num_steps_sampled: 3854340\n",
      "  num_steps_trained: 3854340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 483\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.354545454545455\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 26.666666666666668\n",
      "  player_1: 28.333333333333336\n",
      "  player_2: 31.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 0.4800000000000005\n",
      "  player_1: -0.9199999999999997\n",
      "  player_2: 3.4399999999999995\n",
      "policy_reward_min:\n",
      "  player_0: -46.66666666666667\n",
      "  player_1: -41.0\n",
      "  player_2: -38.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08706639393566083\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30438440587544446\n",
      "  mean_inference_ms: 1.6230489699340942\n",
      "  mean_raw_obs_processing_ms: 0.2109991829685884\n",
      "time_since_restore: 7735.347191095352\n",
      "time_this_iter_s: 18.085444927215576\n",
      "time_total_s: 7735.347191095352\n",
      "timers:\n",
      "  learn_throughput: 477.965\n",
      "  learn_time_ms: 16695.787\n",
      "  load_throughput: 601317.702\n",
      "  load_time_ms: 13.271\n",
      "  sample_throughput: 443.47\n",
      "  sample_time_ms: 17994.46\n",
      "  update_time_ms: 6.441\n",
      "timestamp: 1643543967\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3854340\n",
      "training_iteration: 483\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3870270\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-00-01\n",
      "done: false\n",
      "episode_len_mean: 140.42\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 52\n",
      "episodes_total: 18730\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4650362228353818\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016522794201590234\n",
      "        policy_loss: -0.07617112095157305\n",
      "        total_loss: 80.60236941496531\n",
      "        vf_explained_var: 0.20903693854808808\n",
      "        vf_loss: 80.6673877286911\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5538473707934221\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015612287043477408\n",
      "        policy_loss: -0.10052599182662865\n",
      "        total_loss: 38.559050005277\n",
      "        vf_explained_var: 0.067063600619634\n",
      "        vf_loss: 38.649037675062814\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48587535475691157\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017722857437456697\n",
      "        policy_loss: -0.07423368704194824\n",
      "        total_loss: 72.20298114935557\n",
      "        vf_explained_var: 0.13528818458318712\n",
      "        vf_loss: 72.2652518971761\n",
      "  num_agent_steps_sampled: 3870270\n",
      "  num_agent_steps_trained: 3870270\n",
      "  num_steps_sampled: 3870300\n",
      "  num_steps_trained: 3870300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 485\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.909523809523808\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.02857142857142\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 47.66666666666667\n",
      "  player_1: 23.333333333333336\n",
      "  player_2: 32.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.676666666666667\n",
      "  player_1: -3.083333333333333\n",
      "  player_2: 5.406666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -44.0\n",
      "  player_1: -70.33333333333333\n",
      "  player_2: -46.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08717881344992015\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3044183475720538\n",
      "  mean_inference_ms: 1.624508278628461\n",
      "  mean_raw_obs_processing_ms: 0.2111839564258911\n",
      "time_since_restore: 7769.562795877457\n",
      "time_this_iter_s: 17.461916208267212\n",
      "time_total_s: 7769.562795877457\n",
      "timers:\n",
      "  learn_throughput: 481.966\n",
      "  learn_time_ms: 16557.181\n",
      "  load_throughput: 851854.246\n",
      "  load_time_ms: 9.368\n",
      "  sample_throughput: 440.709\n",
      "  sample_time_ms: 18107.182\n",
      "  update_time_ms: 6.517\n",
      "timestamp: 1643544001\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3870300\n",
      "training_iteration: 485\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3886233\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-00-35\n",
      "done: false\n",
      "episode_len_mean: 145.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 52\n",
      "episodes_total: 18844\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4584196231265863\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016075139532445253\n",
      "        policy_loss: -0.07984094874622921\n",
      "        total_loss: 54.82525661468506\n",
      "        vf_explained_var: 0.2922763058543205\n",
      "        vf_loss: 54.894246745109555\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5346443471809228\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01669206381427671\n",
      "        policy_loss: -0.039035189499457675\n",
      "        total_loss: 42.01070804039637\n",
      "        vf_explained_var: 0.1941755094130834\n",
      "        vf_loss: 42.03847624858221\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4689665688077609\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015790973932076232\n",
      "        policy_loss: -0.10980165262396137\n",
      "        total_loss: 56.74482338428497\n",
      "        vf_explained_var: 0.2018676314751307\n",
      "        vf_loss: 56.84396587292353\n",
      "  num_agent_steps_sampled: 3886233\n",
      "  num_agent_steps_trained: 3886233\n",
      "  num_steps_sampled: 3886260\n",
      "  num_steps_trained: 3886260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 487\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.719999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.02000000000001\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 40.333333333333336\n",
      "  player_1: 30.0\n",
      "  player_2: 40.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 3.380000000000001\n",
      "  player_1: -2.9899999999999993\n",
      "  player_2: 2.6100000000000008\n",
      "policy_reward_min:\n",
      "  player_0: -29.666666666666664\n",
      "  player_1: -48.333333333333336\n",
      "  player_2: -34.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08718494278502206\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30454417639070436\n",
      "  mean_inference_ms: 1.6256331400379758\n",
      "  mean_raw_obs_processing_ms: 0.21136297960678674\n",
      "time_since_restore: 7803.186031579971\n",
      "time_this_iter_s: 16.344073057174683\n",
      "time_total_s: 7803.186031579971\n",
      "timers:\n",
      "  learn_throughput: 489.207\n",
      "  learn_time_ms: 16312.098\n",
      "  load_throughput: 803145.966\n",
      "  load_time_ms: 9.936\n",
      "  sample_throughput: 444.96\n",
      "  sample_time_ms: 17934.178\n",
      "  update_time_ms: 6.599\n",
      "timestamp: 1643544035\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3886260\n",
      "training_iteration: 487\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3902192\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-01-10\n",
      "done: false\n",
      "episode_len_mean: 153.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 46\n",
      "episodes_total: 18941\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47604498341679574\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018204748336450128\n",
      "        policy_loss: -0.08802728882059455\n",
      "        total_loss: 53.0397437731425\n",
      "        vf_explained_var: 0.327894127368927\n",
      "        vf_loss: 53.115482867558796\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5121380900343259\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015002871070646507\n",
      "        policy_loss: -0.12712569814175367\n",
      "        total_loss: 42.74242900212606\n",
      "        vf_explained_var: 0.08946044574181239\n",
      "        vf_loss: 42.85942796309789\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4567079033950965\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016391140359951352\n",
      "        policy_loss: -0.031235922739530603\n",
      "        total_loss: 51.505607504049934\n",
      "        vf_explained_var: 0.04958154737949371\n",
      "        vf_loss: 51.52577925046285\n",
      "  num_agent_steps_sampled: 3902192\n",
      "  num_agent_steps_trained: 3902192\n",
      "  num_steps_sampled: 3902220\n",
      "  num_steps_trained: 3902220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 489\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.909090909090908\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.333333333333336\n",
      "  player_1: 46.0\n",
      "  player_2: 35.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.880000000000001\n",
      "  player_1: -3.26\n",
      "  player_2: 3.38\n",
      "policy_reward_min:\n",
      "  player_0: -28.0\n",
      "  player_1: -53.0\n",
      "  player_2: -60.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08737682111405767\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30396143326035907\n",
      "  mean_inference_ms: 1.6211264061559887\n",
      "  mean_raw_obs_processing_ms: 0.21130143575133822\n",
      "time_since_restore: 7838.404625415802\n",
      "time_this_iter_s: 17.73252034187317\n",
      "time_total_s: 7838.404625415802\n",
      "timers:\n",
      "  learn_throughput: 488.524\n",
      "  learn_time_ms: 16334.918\n",
      "  load_throughput: 629308.842\n",
      "  load_time_ms: 12.681\n",
      "  sample_throughput: 449.37\n",
      "  sample_time_ms: 17758.201\n",
      "  update_time_ms: 6.411\n",
      "timestamp: 1643544070\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3902220\n",
      "training_iteration: 489\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3918152\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-01-44\n",
      "done: false\n",
      "episode_len_mean: 143.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 52\n",
      "episodes_total: 19050\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45225958377122877\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01617780961560489\n",
      "        policy_loss: -0.08067740588293722\n",
      "        total_loss: 51.92962009787559\n",
      "        vf_explained_var: 0.3591336539387703\n",
      "        vf_loss: 51.999377513329186\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5347446816166243\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01619318888491762\n",
      "        policy_loss: -0.09556603299453854\n",
      "        total_loss: 47.990675411224366\n",
      "        vf_explained_var: 0.1889321459333102\n",
      "        vf_loss: 48.07531095027924\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48450662379463516\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015915762881871465\n",
      "        policy_loss: -0.05982684979525705\n",
      "        total_loss: 81.29964329799016\n",
      "        vf_explained_var: 0.19482339759667713\n",
      "        vf_loss: 81.34872711737951\n",
      "  num_agent_steps_sampled: 3918152\n",
      "  num_agent_steps_trained: 3918152\n",
      "  num_steps_sampled: 3918180\n",
      "  num_steps_trained: 3918180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 491\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.423809523809526\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.666666666666664\n",
      "  player_1: 37.0\n",
      "  player_2: 34.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.2899999999999998\n",
      "  player_1: -0.1400000000000003\n",
      "  player_2: 1.8499999999999994\n",
      "policy_reward_min:\n",
      "  player_0: -41.0\n",
      "  player_1: -39.333333333333336\n",
      "  player_2: -52.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08733983167244062\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30385398078136316\n",
      "  mean_inference_ms: 1.6226106772726294\n",
      "  mean_raw_obs_processing_ms: 0.21139192682837135\n",
      "time_since_restore: 7872.155872344971\n",
      "time_this_iter_s: 16.92013645172119\n",
      "time_total_s: 7872.155872344971\n",
      "timers:\n",
      "  learn_throughput: 499.146\n",
      "  learn_time_ms: 15987.32\n",
      "  load_throughput: 608104.278\n",
      "  load_time_ms: 13.123\n",
      "  sample_throughput: 450.354\n",
      "  sample_time_ms: 17719.407\n",
      "  update_time_ms: 6.271\n",
      "timestamp: 1643544104\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3918180\n",
      "training_iteration: 491\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3934110\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-02-21\n",
      "done: false\n",
      "episode_len_mean: 157.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 57\n",
      "episodes_total: 19159\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48518152529994646\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01652671754881036\n",
      "        policy_loss: -0.04791066149870555\n",
      "        total_loss: 60.77633987665176\n",
      "        vf_explained_var: 0.15623021950324376\n",
      "        vf_loss: 60.81309491872788\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5369836247960726\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015780688969545053\n",
      "        policy_loss: -0.11801702450029553\n",
      "        total_loss: 36.78548304716746\n",
      "        vf_explained_var: 0.20713574469089507\n",
      "        vf_loss: 36.89284812927246\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4717321150501569\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01541990699795728\n",
      "        policy_loss: -0.07055553557661673\n",
      "        total_loss: 79.73028600056966\n",
      "        vf_explained_var: 0.31766287406285604\n",
      "        vf_loss: 79.79043259620667\n",
      "  num_agent_steps_sampled: 3934110\n",
      "  num_agent_steps_trained: 3934110\n",
      "  num_steps_sampled: 3934140\n",
      "  num_steps_trained: 3934140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 493\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 12.952173913043483\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.3741861979166667\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 35.333333333333336\n",
      "  player_2: 32.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.37\n",
      "  player_1: -3.04\n",
      "  player_2: 5.67\n",
      "policy_reward_min:\n",
      "  player_0: -36.666666666666664\n",
      "  player_1: -29.666666666666664\n",
      "  player_2: -52.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08733575985758936\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30411317352057404\n",
      "  mean_inference_ms: 1.6241502651449928\n",
      "  mean_raw_obs_processing_ms: 0.21145030204189488\n",
      "time_since_restore: 7909.173614740372\n",
      "time_this_iter_s: 19.170984506607056\n",
      "time_total_s: 7909.173614740372\n",
      "timers:\n",
      "  learn_throughput: 500.407\n",
      "  learn_time_ms: 15947.011\n",
      "  load_throughput: 638453.581\n",
      "  load_time_ms: 12.499\n",
      "  sample_throughput: 461.291\n",
      "  sample_time_ms: 17299.276\n",
      "  update_time_ms: 6.061\n",
      "timestamp: 1643544141\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3934140\n",
      "training_iteration: 493\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3950071\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-02-57\n",
      "done: false\n",
      "episode_len_mean: 133.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 58\n",
      "episodes_total: 19278\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49480133861303327\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016332299703562494\n",
      "        policy_loss: -0.08728790235395233\n",
      "        total_loss: 73.76902630964915\n",
      "        vf_explained_var: 0.14433270265658696\n",
      "        vf_loss: 73.84528967857361\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5250290163358052\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016495145334962824\n",
      "        policy_loss: -0.09635536205333968\n",
      "        total_loss: 43.66252163887024\n",
      "        vf_explained_var: 0.196645431915919\n",
      "        vf_loss: 43.74774283091227\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4845704614619414\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016462222993658883\n",
      "        policy_loss: -0.06348449186421931\n",
      "        total_loss: 63.81793644110362\n",
      "        vf_explained_var: 0.11151128947734833\n",
      "        vf_loss: 63.87030887921651\n",
      "  num_agent_steps_sampled: 3950071\n",
      "  num_agent_steps_trained: 3950071\n",
      "  num_steps_sampled: 3950100\n",
      "  num_steps_trained: 3950100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 495\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.790000000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.333333333333336\n",
      "  player_1: 37.333333333333336\n",
      "  player_2: 36.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -0.44666666666666643\n",
      "  player_1: -0.38666666666666655\n",
      "  player_2: 3.833333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -59.333333333333336\n",
      "  player_1: -28.333333333333336\n",
      "  player_2: -52.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0874270495116066\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3049203751563803\n",
      "  mean_inference_ms: 1.629344883603236\n",
      "  mean_raw_obs_processing_ms: 0.21160309624762155\n",
      "time_since_restore: 7944.9879829883575\n",
      "time_this_iter_s: 17.00755524635315\n",
      "time_total_s: 7944.9879829883575\n",
      "timers:\n",
      "  learn_throughput: 495.383\n",
      "  learn_time_ms: 16108.74\n",
      "  load_throughput: 625277.576\n",
      "  load_time_ms: 12.762\n",
      "  sample_throughput: 453.121\n",
      "  sample_time_ms: 17611.205\n",
      "  update_time_ms: 6.016\n",
      "timestamp: 1643544177\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3950100\n",
      "training_iteration: 495\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3966032\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-03-32\n",
      "done: false\n",
      "episode_len_mean: 147.6\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 57\n",
      "episodes_total: 19385\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4526446005702019\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01646488139191149\n",
      "        policy_loss: -0.08193695479383072\n",
      "        total_loss: 59.90201325575511\n",
      "        vf_explained_var: 0.2549868662158648\n",
      "        vf_loss: 59.97283648173014\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5416227106750011\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015254993720548858\n",
      "        policy_loss: -0.012617810213317473\n",
      "        total_loss: 34.476426858107246\n",
      "        vf_explained_var: 0.1414673000574112\n",
      "        vf_loss: 34.47874760389328\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5018923224508762\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01492727397746421\n",
      "        policy_loss: -0.11761608824754755\n",
      "        total_loss: 73.0311722834905\n",
      "        vf_explained_var: 0.038964043458302816\n",
      "        vf_loss: 73.13871285597483\n",
      "  num_agent_steps_sampled: 3966032\n",
      "  num_agent_steps_trained: 3966032\n",
      "  num_steps_sampled: 3966060\n",
      "  num_steps_trained: 3966060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 497\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.33333333333333\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.15238095238095\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.333333333333336\n",
      "  player_1: 37.333333333333336\n",
      "  player_2: 33.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.5833333333333335\n",
      "  player_1: -2.106666666666667\n",
      "  player_2: 3.523333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -24.333333333333332\n",
      "  player_1: -32.0\n",
      "  player_2: -47.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08731989071871574\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30417414627793965\n",
      "  mean_inference_ms: 1.6254957062429594\n",
      "  mean_raw_obs_processing_ms: 0.21164739128832735\n",
      "time_since_restore: 7980.13813495636\n",
      "time_this_iter_s: 17.026696920394897\n",
      "time_total_s: 7980.13813495636\n",
      "timers:\n",
      "  learn_throughput: 490.741\n",
      "  learn_time_ms: 16261.117\n",
      "  load_throughput: 629602.417\n",
      "  load_time_ms: 12.675\n",
      "  sample_throughput: 452.046\n",
      "  sample_time_ms: 17653.074\n",
      "  update_time_ms: 5.986\n",
      "timestamp: 1643544212\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3966060\n",
      "training_iteration: 497\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3981990\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-04-06\n",
      "done: false\n",
      "episode_len_mean: 134.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 60\n",
      "episodes_total: 19504\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4920806812743346\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016977050862871768\n",
      "        policy_loss: -0.06513912380983432\n",
      "        total_loss: 72.02030927975973\n",
      "        vf_explained_var: 0.21931001176436743\n",
      "        vf_loss: 72.07398881912232\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.535526594221592\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014376303817320111\n",
      "        policy_loss: -0.09846385997875283\n",
      "        total_loss: 58.223721472422284\n",
      "        vf_explained_var: 0.21740194221337636\n",
      "        vf_loss: 58.31248106161753\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4912570317586263\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016577334283260824\n",
      "        policy_loss: -0.058311784271306046\n",
      "        total_loss: 73.2149950726827\n",
      "        vf_explained_var: 0.17808587570985157\n",
      "        vf_loss: 73.26211714903513\n",
      "  num_agent_steps_sampled: 3981990\n",
      "  num_agent_steps_trained: 3981990\n",
      "  num_steps_sampled: 3982020\n",
      "  num_steps_trained: 3982020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 499\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.042105263157895\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 43.666666666666664\n",
      "  player_1: 41.66666666666667\n",
      "  player_2: 30.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.436666666666667\n",
      "  player_1: -2.6733333333333333\n",
      "  player_2: 3.2366666666666672\n",
      "policy_reward_min:\n",
      "  player_0: -38.666666666666664\n",
      "  player_1: -48.33333333333333\n",
      "  player_2: -55.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08734380425752625\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30500035869174186\n",
      "  mean_inference_ms: 1.6309018419653443\n",
      "  mean_raw_obs_processing_ms: 0.21178108773864565\n",
      "time_since_restore: 8013.749300003052\n",
      "time_this_iter_s: 16.059996128082275\n",
      "time_total_s: 8013.749300003052\n",
      "timers:\n",
      "  learn_throughput: 495.607\n",
      "  learn_time_ms: 16101.478\n",
      "  load_throughput: 634043.947\n",
      "  load_time_ms: 12.586\n",
      "  sample_throughput: 450.156\n",
      "  sample_time_ms: 17727.206\n",
      "  update_time_ms: 6.094\n",
      "timestamp: 1643544246\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3982020\n",
      "training_iteration: 499\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 3997950\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-04-41\n",
      "done: false\n",
      "episode_len_mean: 126.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 63\n",
      "episodes_total: 19630\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48218280434608457\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01627753663901975\n",
      "        policy_loss: -0.06878703802824021\n",
      "        total_loss: 77.32336652119955\n",
      "        vf_explained_var: 0.44517215261856713\n",
      "        vf_loss: 77.3811661084493\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5337126791477204\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016272590787152885\n",
      "        policy_loss: -0.041567613364507754\n",
      "        total_loss: 45.257348821957905\n",
      "        vf_explained_var: 0.3326496891180674\n",
      "        vf_loss: 45.28793239911397\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4747070983548959\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016195059915881795\n",
      "        policy_loss: -0.10391452265282472\n",
      "        total_loss: 82.57302908579508\n",
      "        vf_explained_var: 0.3575487972299258\n",
      "        vf_loss: 82.66601162274678\n",
      "  num_agent_steps_sampled: 3997950\n",
      "  num_agent_steps_trained: 3997950\n",
      "  num_steps_sampled: 3997980\n",
      "  num_steps_trained: 3997980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 501\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.909523809523805\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 47.33333333333333\n",
      "  player_1: 46.0\n",
      "  player_2: 29.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 6.339999999999999\n",
      "  player_1: -2.0\n",
      "  player_2: -1.3400000000000003\n",
      "policy_reward_min:\n",
      "  player_0: -34.0\n",
      "  player_1: -28.0\n",
      "  player_2: -58.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08739479076008486\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30461413756078987\n",
      "  mean_inference_ms: 1.630335374595642\n",
      "  mean_raw_obs_processing_ms: 0.2117468036356636\n",
      "time_since_restore: 8048.880990743637\n",
      "time_this_iter_s: 16.877137899398804\n",
      "time_total_s: 8048.880990743637\n",
      "timers:\n",
      "  learn_throughput: 491.511\n",
      "  learn_time_ms: 16235.634\n",
      "  load_throughput: 624352.637\n",
      "  load_time_ms: 12.781\n",
      "  sample_throughput: 450.764\n",
      "  sample_time_ms: 17703.287\n",
      "  update_time_ms: 8.425\n",
      "timestamp: 1643544281\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3997980\n",
      "training_iteration: 501\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4013910\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-05-16\n",
      "done: false\n",
      "episode_len_mean: 139.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 55\n",
      "episodes_total: 19741\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47279221326112747\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015524993465331439\n",
      "        policy_loss: -0.0950913528756549\n",
      "        total_loss: 66.81194139480591\n",
      "        vf_explained_var: 0.2943244860569636\n",
      "        vf_loss: 66.89655343373616\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5256259909768899\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016427490502425524\n",
      "        policy_loss: -0.0812201188900508\n",
      "        total_loss: 43.63884788910548\n",
      "        vf_explained_var: 0.2424925989905993\n",
      "        vf_loss: 43.70897925615311\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47832219769557316\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016463557693938733\n",
      "        policy_loss: -0.08195399108653267\n",
      "        total_loss: 81.97364764849345\n",
      "        vf_explained_var: 0.04390217045942942\n",
      "        vf_loss: 82.04448856035869\n",
      "  num_agent_steps_sampled: 4013910\n",
      "  num_agent_steps_trained: 4013910\n",
      "  num_steps_sampled: 4013940\n",
      "  num_steps_trained: 4013940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 503\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.635000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.16\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 52.333333333333336\n",
      "  player_1: 33.666666666666664\n",
      "  player_2: 30.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.86\n",
      "  player_1: -3.26\n",
      "  player_2: 3.3999999999999995\n",
      "policy_reward_min:\n",
      "  player_0: -29.666666666666664\n",
      "  player_1: -38.666666666666664\n",
      "  player_2: -45.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08760923876403222\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3047664498943378\n",
      "  mean_inference_ms: 1.6290207226977644\n",
      "  mean_raw_obs_processing_ms: 0.21209854355515045\n",
      "time_since_restore: 8083.757600307465\n",
      "time_this_iter_s: 16.44848942756653\n",
      "time_total_s: 8083.757600307465\n",
      "timers:\n",
      "  learn_throughput: 498.02\n",
      "  learn_time_ms: 16023.459\n",
      "  load_throughput: 622428.751\n",
      "  load_time_ms: 12.821\n",
      "  sample_throughput: 449.313\n",
      "  sample_time_ms: 17760.459\n",
      "  update_time_ms: 8.489\n",
      "timestamp: 1643544316\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4013940\n",
      "training_iteration: 503\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4029870\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-05-52\n",
      "done: false\n",
      "episode_len_mean: 146.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 56\n",
      "episodes_total: 19855\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4742649846275647\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016114397221018067\n",
      "        policy_loss: -0.09346452986511092\n",
      "        total_loss: 74.60853279272715\n",
      "        vf_explained_var: 0.25271449436744053\n",
      "        vf_loss: 74.69112008889516\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.532114864786466\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01536672385187785\n",
      "        policy_loss: -0.07055940156181653\n",
      "        total_loss: 76.69442089796067\n",
      "        vf_explained_var: 0.17680538733800252\n",
      "        vf_loss: 76.75460800886154\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4703257799645265\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01736034482105623\n",
      "        policy_loss: -0.08487222677717607\n",
      "        total_loss: 128.16706012090046\n",
      "        vf_explained_var: 0.19839934269587198\n",
      "        vf_loss: 128.24021428108216\n",
      "  num_agent_steps_sampled: 4029870\n",
      "  num_agent_steps_trained: 4029870\n",
      "  num_steps_sampled: 4029900\n",
      "  num_steps_trained: 4029900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 505\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.747826086956522\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.3741861979166667\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 27.0\n",
      "  player_1: 40.33333333333333\n",
      "  player_2: 40.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -0.3633333333333335\n",
      "  player_1: 1.4266666666666663\n",
      "  player_2: 1.9366666666666663\n",
      "policy_reward_min:\n",
      "  player_0: -36.0\n",
      "  player_1: -34.666666666666664\n",
      "  player_2: -47.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08745881325097987\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30437204122630585\n",
      "  mean_inference_ms: 1.6287796366735663\n",
      "  mean_raw_obs_processing_ms: 0.21199789699492386\n",
      "time_since_restore: 8119.8737535476685\n",
      "time_this_iter_s: 18.98966360092163\n",
      "time_total_s: 8119.8737535476685\n",
      "timers:\n",
      "  learn_throughput: 496.946\n",
      "  learn_time_ms: 16058.097\n",
      "  load_throughput: 636456.475\n",
      "  load_time_ms: 12.538\n",
      "  sample_throughput: 460.687\n",
      "  sample_time_ms: 17321.967\n",
      "  update_time_ms: 8.509\n",
      "timestamp: 1643544352\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4029900\n",
      "training_iteration: 505\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4045831\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-06-31\n",
      "done: false\n",
      "episode_len_mean: 141.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 56\n",
      "episodes_total: 19966\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4757990429798762\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017275215125703958\n",
      "        policy_loss: -0.07100977510524292\n",
      "        total_loss: 70.47763499259949\n",
      "        vf_explained_var: 0.4445221429069837\n",
      "        vf_loss: 70.53698417345683\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5196414652963479\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01659619430314327\n",
      "        policy_loss: -0.09092495217298469\n",
      "        total_loss: 39.18693078358968\n",
      "        vf_explained_var: 0.21474219252665838\n",
      "        vf_loss: 39.26665322820345\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4729705210030079\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017161885822392833\n",
      "        policy_loss: -0.07558159338931243\n",
      "        total_loss: 60.26749449412028\n",
      "        vf_explained_var: 0.21264455546935399\n",
      "        vf_loss: 60.33149149099986\n",
      "  num_agent_steps_sampled: 4045831\n",
      "  num_agent_steps_trained: 4045831\n",
      "  num_steps_sampled: 4045860\n",
      "  num_steps_trained: 4045860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 507\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.104347826086958\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.3741861979166667\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 41.0\n",
      "  player_1: 29.0\n",
      "  player_2: 38.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 0.31666666666666693\n",
      "  player_1: -2.5733333333333337\n",
      "  player_2: 5.256666666666668\n",
      "policy_reward_min:\n",
      "  player_0: -53.0\n",
      "  player_1: -35.666666666666664\n",
      "  player_2: -42.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08756117901522732\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3050526255204274\n",
      "  mean_inference_ms: 1.6326046380275019\n",
      "  mean_raw_obs_processing_ms: 0.21214067515168084\n",
      "time_since_restore: 8158.155820846558\n",
      "time_this_iter_s: 19.13890027999878\n",
      "time_total_s: 8158.155820846558\n",
      "timers:\n",
      "  learn_throughput: 487.814\n",
      "  learn_time_ms: 16358.709\n",
      "  load_throughput: 640894.939\n",
      "  load_time_ms: 12.451\n",
      "  sample_throughput: 452.479\n",
      "  sample_time_ms: 17636.195\n",
      "  update_time_ms: 8.637\n",
      "timestamp: 1643544391\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4045860\n",
      "training_iteration: 507\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4061792\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-07-10\n",
      "done: false\n",
      "episode_len_mean: 150.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 54\n",
      "episodes_total: 20073\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45270083809892336\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01596028045234334\n",
      "        policy_loss: -0.08311890968432029\n",
      "        total_loss: 43.688023223876954\n",
      "        vf_explained_var: 0.14391320178906122\n",
      "        vf_loss: 43.76036891301473\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5340032435953617\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01579464073108852\n",
      "        policy_loss: -0.05051633598903815\n",
      "        total_loss: 57.313406483332315\n",
      "        vf_explained_var: 0.006954788664976756\n",
      "        vf_loss: 57.35326162815094\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45802478611469266\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016830812412099477\n",
      "        policy_loss: -0.07795954580729206\n",
      "        total_loss: 46.70509581804276\n",
      "        vf_explained_var: 0.14384478042523066\n",
      "        vf_loss: 46.771694464683534\n",
      "  num_agent_steps_sampled: 4061792\n",
      "  num_agent_steps_trained: 4061792\n",
      "  num_steps_sampled: 4061820\n",
      "  num_steps_trained: 4061820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 509\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.247826086956522\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.3741861979166667\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 27.666666666666664\n",
      "  player_1: 33.666666666666664\n",
      "  player_2: 26.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -0.1533333333333331\n",
      "  player_1: -0.153333333333333\n",
      "  player_2: 3.3066666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -39.666666666666664\n",
      "  player_1: -40.333333333333336\n",
      "  player_2: -42.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08762391455117224\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3049457660623587\n",
      "  mean_inference_ms: 1.632501967290124\n",
      "  mean_raw_obs_processing_ms: 0.21237365577186743\n",
      "time_since_restore: 8197.1865670681\n",
      "time_this_iter_s: 19.28146743774414\n",
      "time_total_s: 8197.1865670681\n",
      "timers:\n",
      "  learn_throughput: 473.234\n",
      "  learn_time_ms: 16862.688\n",
      "  load_throughput: 785700.944\n",
      "  load_time_ms: 10.157\n",
      "  sample_throughput: 441.751\n",
      "  sample_time_ms: 18064.457\n",
      "  update_time_ms: 8.634\n",
      "timestamp: 1643544430\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4061820\n",
      "training_iteration: 509\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4077750\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-07-47\n",
      "done: false\n",
      "episode_len_mean: 147.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 52\n",
      "episodes_total: 20178\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4808551014959812\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016823016915397677\n",
      "        policy_loss: -0.03661546535789967\n",
      "        total_loss: 35.20915986378988\n",
      "        vf_explained_var: 0.21166195700565973\n",
      "        vf_loss: 35.234419779777525\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5367737096548081\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015338374294906695\n",
      "        policy_loss: -0.12785417372981708\n",
      "        total_loss: 41.93262717247009\n",
      "        vf_explained_var: 0.1848964207371076\n",
      "        vf_loss: 42.050128111044565\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4641841844717662\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015590827634165407\n",
      "        policy_loss: -0.05601731069851667\n",
      "        total_loss: 74.54267699877421\n",
      "        vf_explained_var: 0.26445348739624025\n",
      "        vf_loss: 74.58817010243733\n",
      "  num_agent_steps_sampled: 4077750\n",
      "  num_agent_steps_trained: 4077750\n",
      "  num_steps_sampled: 4077780\n",
      "  num_steps_trained: 4077780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 511\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.48181818181818\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.666666666666664\n",
      "  player_1: 47.0\n",
      "  player_2: 42.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.3999999999999998\n",
      "  player_1: -1.1400000000000003\n",
      "  player_2: 3.7399999999999993\n",
      "policy_reward_min:\n",
      "  player_0: -86.0\n",
      "  player_1: -45.333333333333336\n",
      "  player_2: -64.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08766258190543269\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30491562654691134\n",
      "  mean_inference_ms: 1.6328599546753555\n",
      "  mean_raw_obs_processing_ms: 0.21244801942243297\n",
      "time_since_restore: 8234.406404018402\n",
      "time_this_iter_s: 17.928109884262085\n",
      "time_total_s: 8234.406404018402\n",
      "timers:\n",
      "  learn_throughput: 467.342\n",
      "  learn_time_ms: 17075.276\n",
      "  load_throughput: 672137.107\n",
      "  load_time_ms: 11.873\n",
      "  sample_throughput: 431.827\n",
      "  sample_time_ms: 18479.61\n",
      "  update_time_ms: 6.271\n",
      "timestamp: 1643544467\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4077780\n",
      "training_iteration: 511\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4093710\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-08-22\n",
      "done: false\n",
      "episode_len_mean: 160.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 54\n",
      "episodes_total: 20279\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.451791039754947\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015780356446904686\n",
      "        policy_loss: -0.09974349956028164\n",
      "        total_loss: 47.41206039309502\n",
      "        vf_explained_var: 0.29776843498150507\n",
      "        vf_loss: 47.5011521478494\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5206077052156131\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01597369710705986\n",
      "        policy_loss: -0.08634448449437816\n",
      "        total_loss: 38.182068117459615\n",
      "        vf_explained_var: 0.3189295917749405\n",
      "        vf_loss: 38.25763024727503\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4634299231072267\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014838471358816226\n",
      "        policy_loss: -0.06679197820524374\n",
      "        total_loss: 85.26456247409185\n",
      "        vf_explained_var: 0.4061539791027705\n",
      "        vf_loss: 85.32133900006612\n",
      "  num_agent_steps_sampled: 4093710\n",
      "  num_agent_steps_trained: 4093710\n",
      "  num_steps_sampled: 4093740\n",
      "  num_steps_trained: 4093740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 513\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.695454545454544\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 41.333333333333336\n",
      "  player_1: 30.666666666666664\n",
      "  player_2: 32.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.6599999999999993\n",
      "  player_1: -0.93\n",
      "  player_2: 1.2699999999999998\n",
      "policy_reward_min:\n",
      "  player_0: -35.0\n",
      "  player_1: -51.66666666666667\n",
      "  player_2: -66.33333333333334\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08786441483931236\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3055606934202647\n",
      "  mean_inference_ms: 1.6351309612679659\n",
      "  mean_raw_obs_processing_ms: 0.21282407704718304\n",
      "time_since_restore: 8269.713827610016\n",
      "time_this_iter_s: 18.23443055152893\n",
      "time_total_s: 8269.713827610016\n",
      "timers:\n",
      "  learn_throughput: 466.204\n",
      "  learn_time_ms: 17116.957\n",
      "  load_throughput: 649581.785\n",
      "  load_time_ms: 12.285\n",
      "  sample_throughput: 432.627\n",
      "  sample_time_ms: 18445.447\n",
      "  update_time_ms: 6.399\n",
      "timestamp: 1643544502\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4093740\n",
      "training_iteration: 513\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4109670\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-08-56\n",
      "done: false\n",
      "episode_len_mean: 158.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 47\n",
      "episodes_total: 20377\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4823701311647892\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017813608382153536\n",
      "        policy_loss: -0.08746426358198126\n",
      "        total_loss: 30.17492743730545\n",
      "        vf_explained_var: 0.3589148448904355\n",
      "        vf_loss: 30.25036741574605\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5063908476630846\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01660712296037559\n",
      "        policy_loss: -0.05678528344646717\n",
      "        total_loss: 31.60708571354548\n",
      "        vf_explained_var: 0.2310618088642756\n",
      "        vf_loss: 31.65266134262085\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4670482701063156\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017319072441978278\n",
      "        policy_loss: -0.07650815008829037\n",
      "        total_loss: 41.9621959177653\n",
      "        vf_explained_var: 0.2741197175780932\n",
      "        vf_loss: 42.02701376438141\n",
      "  num_agent_steps_sampled: 4109670\n",
      "  num_agent_steps_trained: 4109670\n",
      "  num_steps_sampled: 4109700\n",
      "  num_steps_trained: 4109700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 515\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.130000000000004\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 37.33333333333333\n",
      "  player_2: 31.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 1.893333333333333\n",
      "  player_1: -4.3566666666666665\n",
      "  player_2: 5.463333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -53.66666666666667\n",
      "  player_1: -28.666666666666668\n",
      "  player_2: -36.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08778065747979547\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3047819069088816\n",
      "  mean_inference_ms: 1.6324583025902704\n",
      "  mean_raw_obs_processing_ms: 0.21260296936945614\n",
      "time_since_restore: 8303.409049510956\n",
      "time_this_iter_s: 16.108885288238525\n",
      "time_total_s: 8303.409049510956\n",
      "timers:\n",
      "  learn_throughput: 473.041\n",
      "  learn_time_ms: 16869.556\n",
      "  load_throughput: 620867.752\n",
      "  load_time_ms: 12.853\n",
      "  sample_throughput: 427.433\n",
      "  sample_time_ms: 18669.608\n",
      "  update_time_ms: 6.429\n",
      "timestamp: 1643544536\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4109700\n",
      "training_iteration: 515\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4125631\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-09-30\n",
      "done: false\n",
      "episode_len_mean: 133.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 59\n",
      "episodes_total: 20495\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4803616750240326\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015659645131490738\n",
      "        policy_loss: -0.09986847657710314\n",
      "        total_loss: 54.39928675015767\n",
      "        vf_explained_var: 0.4550371819734573\n",
      "        vf_loss: 54.48858498414358\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5463778191804886\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01597508217549186\n",
      "        policy_loss: -0.009318841150961816\n",
      "        total_loss: 60.43323673884074\n",
      "        vf_explained_var: 0.3935157011946042\n",
      "        vf_loss: 60.431772530873616\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47709095299243925\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01610773536247052\n",
      "        policy_loss: -0.116442802362144\n",
      "        total_loss: 69.44451977332433\n",
      "        vf_explained_var: 0.40961380789677304\n",
      "        vf_loss: 69.55008989969889\n",
      "  num_agent_steps_sampled: 4125631\n",
      "  num_agent_steps_trained: 4125631\n",
      "  num_steps_sampled: 4125660\n",
      "  num_steps_trained: 4125660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 517\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.065000000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.0\n",
      "  player_1: 32.66666666666667\n",
      "  player_2: 41.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.233333333333334\n",
      "  player_1: -4.916666666666667\n",
      "  player_2: 4.683333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -31.666666666666664\n",
      "  player_1: -49.0\n",
      "  player_2: -50.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08773434706889902\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3052079086070002\n",
      "  mean_inference_ms: 1.6370241796329077\n",
      "  mean_raw_obs_processing_ms: 0.21269285279220781\n",
      "time_since_restore: 8337.505033493042\n",
      "time_this_iter_s: 16.18114995956421\n",
      "time_total_s: 8337.505033493042\n",
      "timers:\n",
      "  learn_throughput: 483.959\n",
      "  learn_time_ms: 16488.989\n",
      "  load_throughput: 624727.881\n",
      "  load_time_ms: 12.774\n",
      "  sample_throughput: 437.954\n",
      "  sample_time_ms: 18221.096\n",
      "  update_time_ms: 6.28\n",
      "timestamp: 1643544570\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4125660\n",
      "training_iteration: 517\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4141591\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-10-01\n",
      "done: false\n",
      "episode_len_mean: 148.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 54\n",
      "episodes_total: 20604\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47953177293141686\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01552248484083994\n",
      "        policy_loss: -0.08813354263082146\n",
      "        total_loss: 72.2621598482132\n",
      "        vf_explained_var: 0.08289086987574895\n",
      "        vf_loss: 72.33981578191121\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5247343650956948\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014755084174578693\n",
      "        policy_loss: -0.060455427666505177\n",
      "        total_loss: 55.52274578173955\n",
      "        vf_explained_var: 0.1363919746875763\n",
      "        vf_loss: 55.57324161052704\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.455188263207674\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01750012269847256\n",
      "        policy_loss: -0.05763907901011407\n",
      "        total_loss: 49.82985037724177\n",
      "        vf_explained_var: 0.19410129606723786\n",
      "        vf_loss: 49.87567673126856\n",
      "  num_agent_steps_sampled: 4141591\n",
      "  num_agent_steps_trained: 4141591\n",
      "  num_steps_sampled: 4141620\n",
      "  num_steps_trained: 4141620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 519\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.805555555555552\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.02222222222223\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 43.66666666666667\n",
      "  player_1: 42.66666666666667\n",
      "  player_2: 32.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -0.4866666666666667\n",
      "  player_1: -3.016666666666667\n",
      "  player_2: 6.503333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -60.33333333333333\n",
      "  player_1: -45.333333333333336\n",
      "  player_2: -57.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08772087612522317\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30447471969985224\n",
      "  mean_inference_ms: 1.632785313346929\n",
      "  mean_raw_obs_processing_ms: 0.21260253439592777\n",
      "time_since_restore: 8368.161861896515\n",
      "time_this_iter_s: 15.100650310516357\n",
      "time_total_s: 8368.161861896515\n",
      "timers:\n",
      "  learn_throughput: 507.165\n",
      "  learn_time_ms: 15734.532\n",
      "  load_throughput: 628000.561\n",
      "  load_time_ms: 12.707\n",
      "  sample_throughput: 455.735\n",
      "  sample_time_ms: 17510.185\n",
      "  update_time_ms: 6.171\n",
      "timestamp: 1643544601\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4141620\n",
      "training_iteration: 519\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4157550\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-10-34\n",
      "done: false\n",
      "episode_len_mean: 137.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 59\n",
      "episodes_total: 20718\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4610717777411143\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014609263452439337\n",
      "        policy_loss: -0.11798570273754497\n",
      "        total_loss: 57.29359132925669\n",
      "        vf_explained_var: 0.41547316054503125\n",
      "        vf_loss: 57.40171574672063\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5221418230235577\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016250283140461005\n",
      "        policy_loss: -0.06514349380663285\n",
      "        total_loss: 47.88433539787928\n",
      "        vf_explained_var: 0.074345343708992\n",
      "        vf_loss: 47.93851001580556\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4512319480876128\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013843158706293556\n",
      "        policy_loss: -0.051481535167743765\n",
      "        total_loss: 68.58493722597758\n",
      "        vf_explained_var: 0.2160178878903389\n",
      "        vf_loss: 68.62707453330358\n",
      "  num_agent_steps_sampled: 4157550\n",
      "  num_agent_steps_trained: 4157550\n",
      "  num_steps_sampled: 4157580\n",
      "  num_steps_trained: 4157580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 521\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.273684210526316\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 44.333333333333336\n",
      "  player_1: 27.666666666666664\n",
      "  player_2: 35.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.9866666666666666\n",
      "  player_1: -4.563333333333334\n",
      "  player_2: 6.576666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -50.0\n",
      "  player_1: -35.666666666666664\n",
      "  player_2: -53.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08774261551597606\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3047568325708824\n",
      "  mean_inference_ms: 1.6344567448520462\n",
      "  mean_raw_obs_processing_ms: 0.21274815977488956\n",
      "time_since_restore: 8400.69611787796\n",
      "time_this_iter_s: 16.104816198349\n",
      "time_total_s: 8400.69611787796\n",
      "timers:\n",
      "  learn_throughput: 521.153\n",
      "  learn_time_ms: 15312.191\n",
      "  load_throughput: 752165.692\n",
      "  load_time_ms: 10.609\n",
      "  sample_throughput: 474.506\n",
      "  sample_time_ms: 16817.484\n",
      "  update_time_ms: 6.035\n",
      "timestamp: 1643544634\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4157580\n",
      "training_iteration: 521\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4173510\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-11-04\n",
      "done: false\n",
      "episode_len_mean: 150.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 57\n",
      "episodes_total: 20825\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45673483242591223\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01489246172473751\n",
      "        policy_loss: -0.08418815008054177\n",
      "        total_loss: 96.17908046881358\n",
      "        vf_explained_var: 0.21109694570302964\n",
      "        vf_loss: 96.25321603139241\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5455538059771061\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01379611210619089\n",
      "        policy_loss: -0.00639878034281234\n",
      "        total_loss: 48.89398579438527\n",
      "        vf_explained_var: 0.06921963592370352\n",
      "        vf_loss: 48.89107242663702\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4693090759217739\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01378774116001144\n",
      "        policy_loss: -0.11285557365665833\n",
      "        total_loss: 78.42136311372121\n",
      "        vf_explained_var: 0.05118105004231135\n",
      "        vf_loss: 78.5249118121465\n",
      "  num_agent_steps_sampled: 4173510\n",
      "  num_agent_steps_trained: 4173510\n",
      "  num_steps_sampled: 4173540\n",
      "  num_steps_trained: 4173540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 523\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.666666666666666\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.02222222222223\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 43.666666666666664\n",
      "  player_1: 54.333333333333336\n",
      "  player_2: 39.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.2100000000000002\n",
      "  player_1: -0.22000000000000028\n",
      "  player_2: 3.4300000000000006\n",
      "policy_reward_min:\n",
      "  player_0: -52.0\n",
      "  player_1: -44.333333333333336\n",
      "  player_2: -64.66666666666666\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08763855076195562\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3044668520626483\n",
      "  mean_inference_ms: 1.6328325226677722\n",
      "  mean_raw_obs_processing_ms: 0.21240986203120765\n",
      "time_since_restore: 8431.150674581528\n",
      "time_this_iter_s: 14.83978819847107\n",
      "time_total_s: 8431.150674581528\n",
      "timers:\n",
      "  learn_throughput: 536.718\n",
      "  learn_time_ms: 14868.138\n",
      "  load_throughput: 826045.705\n",
      "  load_time_ms: 9.66\n",
      "  sample_throughput: 483.837\n",
      "  sample_time_ms: 16493.173\n",
      "  update_time_ms: 5.934\n",
      "timestamp: 1643544664\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4173540\n",
      "training_iteration: 523\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4189470\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-11-36\n",
      "done: false\n",
      "episode_len_mean: 141.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 55\n",
      "episodes_total: 20939\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45622132703661916\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01673137553640155\n",
      "        policy_loss: -0.048617493684093155\n",
      "        total_loss: 52.891110367774964\n",
      "        vf_explained_var: 0.4012465276320775\n",
      "        vf_loss: 52.928434052467345\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5356976283093293\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016483956511195476\n",
      "        policy_loss: -0.09337299432605504\n",
      "        total_loss: 48.616519593397776\n",
      "        vf_explained_var: 0.44658894926309584\n",
      "        vf_loss: 48.69876605669658\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4607120891412099\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016661740361790293\n",
      "        policy_loss: -0.08644072133426865\n",
      "        total_loss: 36.91460529168447\n",
      "        vf_explained_var: 0.24084630688031514\n",
      "        vf_loss: 36.98979935487112\n",
      "  num_agent_steps_sampled: 4189470\n",
      "  num_agent_steps_trained: 4189470\n",
      "  num_steps_sampled: 4189500\n",
      "  num_steps_trained: 4189500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 525\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.690000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.03999999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.666666666666664\n",
      "  player_1: 30.0\n",
      "  player_2: 36.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.3966666666666668\n",
      "  player_1: -3.696666666666667\n",
      "  player_2: 7.093333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -37.333333333333336\n",
      "  player_1: -47.333333333333336\n",
      "  player_2: -41.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08760955045719747\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3044026637862366\n",
      "  mean_inference_ms: 1.6334675887373613\n",
      "  mean_raw_obs_processing_ms: 0.2125514241720796\n",
      "time_since_restore: 8462.636695861816\n",
      "time_this_iter_s: 15.698365926742554\n",
      "time_total_s: 8462.636695861816\n",
      "timers:\n",
      "  learn_throughput: 543.123\n",
      "  learn_time_ms: 14692.815\n",
      "  load_throughput: 927254.405\n",
      "  load_time_ms: 8.606\n",
      "  sample_throughput: 499.53\n",
      "  sample_time_ms: 15975.007\n",
      "  update_time_ms: 5.897\n",
      "timestamp: 1643544696\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4189500\n",
      "training_iteration: 525\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4205430\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-12-07\n",
      "done: false\n",
      "episode_len_mean: 125.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 58\n",
      "episodes_total: 21057\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49048042232791583\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016204444160192868\n",
      "        policy_loss: -0.09102205829073985\n",
      "        total_loss: 62.72499844392141\n",
      "        vf_explained_var: 0.4744421008229256\n",
      "        vf_loss: 62.80508260567983\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5622195201118787\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016372416544775965\n",
      "        policy_loss: -0.08106712459586561\n",
      "        total_loss: 52.46121672312419\n",
      "        vf_explained_var: 0.05679399351278941\n",
      "        vf_loss: 52.53123245318731\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47852630252639455\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017111392992495287\n",
      "        policy_loss: -0.06793041550864776\n",
      "        total_loss: 77.13520984490712\n",
      "        vf_explained_var: 0.4256545347968737\n",
      "        vf_loss: 77.19158989270528\n",
      "  num_agent_steps_sampled: 4205430\n",
      "  num_agent_steps_trained: 4205430\n",
      "  num_steps_sampled: 4205460\n",
      "  num_steps_trained: 4205460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 527\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.669999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.03999999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 41.0\n",
      "  player_1: 42.0\n",
      "  player_2: 30.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.753333333333333\n",
      "  player_1: -2.056666666666666\n",
      "  player_2: 2.3033333333333337\n",
      "policy_reward_min:\n",
      "  player_0: -36.333333333333336\n",
      "  player_1: -30.666666666666664\n",
      "  player_2: -57.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08769774839533762\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30471247740630936\n",
      "  mean_inference_ms: 1.6350175453256717\n",
      "  mean_raw_obs_processing_ms: 0.2126533192143628\n",
      "time_since_restore: 8494.076839208603\n",
      "time_this_iter_s: 16.367295026779175\n",
      "time_total_s: 8494.076839208603\n",
      "timers:\n",
      "  learn_throughput: 552.216\n",
      "  learn_time_ms: 14450.866\n",
      "  load_throughput: 996120.518\n",
      "  load_time_ms: 8.011\n",
      "  sample_throughput: 509.292\n",
      "  sample_time_ms: 15668.806\n",
      "  update_time_ms: 5.839\n",
      "timestamp: 1643544727\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4205460\n",
      "training_iteration: 527\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4221390\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-12-38\n",
      "done: false\n",
      "episode_len_mean: 153.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 52\n",
      "episodes_total: 21168\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4681729431450367\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016359003533167802\n",
      "        policy_loss: -0.08380310072253148\n",
      "        total_loss: 49.0903262869517\n",
      "        vf_explained_var: 0.2543060250083605\n",
      "        vf_loss: 49.163087142308555\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5286203987399737\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016006883248272364\n",
      "        policy_loss: -0.06502027486761411\n",
      "        total_loss: 31.647245004971822\n",
      "        vf_explained_var: 0.12608739207188288\n",
      "        vf_loss: 31.701460711956024\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4619453965127468\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018097288332864992\n",
      "        policy_loss: -0.08043931022596856\n",
      "        total_loss: 41.47680988709132\n",
      "        vf_explained_var: 0.22375188986460368\n",
      "        vf_loss: 41.54503334919612\n",
      "  num_agent_steps_sampled: 4221390\n",
      "  num_agent_steps_trained: 4221390\n",
      "  num_steps_sampled: 4221420\n",
      "  num_steps_trained: 4221420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 529\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.063157894736841\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.02105263157895\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 48.666666666666664\n",
      "  player_1: 26.0\n",
      "  player_2: 29.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.82\n",
      "  player_1: -3.8399999999999994\n",
      "  player_2: 4.0200000000000005\n",
      "policy_reward_min:\n",
      "  player_0: -28.333333333333336\n",
      "  player_1: -30.333333333333336\n",
      "  player_2: -54.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08764947426971252\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30451376840061295\n",
      "  mean_inference_ms: 1.6332701673715742\n",
      "  mean_raw_obs_processing_ms: 0.21261196474623223\n",
      "time_since_restore: 8525.361543893814\n",
      "time_this_iter_s: 15.258373022079468\n",
      "time_total_s: 8525.361543893814\n",
      "timers:\n",
      "  learn_throughput: 549.851\n",
      "  learn_time_ms: 14513.029\n",
      "  load_throughput: 1024648.28\n",
      "  load_time_ms: 7.788\n",
      "  sample_throughput: 507.166\n",
      "  sample_time_ms: 15734.507\n",
      "  update_time_ms: 5.865\n",
      "timestamp: 1643544758\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4221420\n",
      "training_iteration: 529\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4237352\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-13-09\n",
      "done: false\n",
      "episode_len_mean: 142.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 51\n",
      "episodes_total: 21275\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45680381019910177\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01617283289701845\n",
      "        policy_loss: -0.10133137660411497\n",
      "        total_loss: 52.26647916475932\n",
      "        vf_explained_var: 0.1129255473613739\n",
      "        vf_loss: 52.356893997987115\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5431549139817555\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017811477533728255\n",
      "        policy_loss: -0.09868000828272973\n",
      "        total_loss: 38.78286967992783\n",
      "        vf_explained_var: 0.1330233966310819\n",
      "        vf_loss: 38.869526831309\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4587100409468015\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014955307735521046\n",
      "        policy_loss: -0.03686507678901156\n",
      "        total_loss: 50.079734023412065\n",
      "        vf_explained_var: 0.2637570482492447\n",
      "        vf_loss: 50.10650403022766\n",
      "  num_agent_steps_sampled: 4237352\n",
      "  num_agent_steps_trained: 4237352\n",
      "  num_steps_sampled: 4237380\n",
      "  num_steps_trained: 4237380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 531\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.799999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.02222222222223\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.333333333333336\n",
      "  player_1: 33.0\n",
      "  player_2: 29.0\n",
      "policy_reward_mean:\n",
      "  player_0: 5.046666666666666\n",
      "  player_1: -2.7033333333333336\n",
      "  player_2: 0.6566666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -29.0\n",
      "  player_1: -31.0\n",
      "  player_2: -54.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08762339437080455\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3045532486136596\n",
      "  mean_inference_ms: 1.6327784630273061\n",
      "  mean_raw_obs_processing_ms: 0.2125825298340249\n",
      "time_since_restore: 8555.772686004639\n",
      "time_this_iter_s: 15.092540502548218\n",
      "time_total_s: 8555.772686004639\n",
      "timers:\n",
      "  learn_throughput: 557.919\n",
      "  learn_time_ms: 14303.157\n",
      "  load_throughput: 1023320.001\n",
      "  load_time_ms: 7.798\n",
      "  sample_throughput: 510.385\n",
      "  sample_time_ms: 15635.242\n",
      "  update_time_ms: 6.173\n",
      "timestamp: 1643544789\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4237380\n",
      "training_iteration: 531\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4253310\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-13-40\n",
      "done: false\n",
      "episode_len_mean: 148.12\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 57\n",
      "episodes_total: 21386\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.485017491231362\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016641611895550832\n",
      "        policy_loss: -0.0612407494088014\n",
      "        total_loss: 50.92353871663411\n",
      "        vf_explained_var: 0.20493922700484593\n",
      "        vf_loss: 50.97354620615641\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5298983285824458\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01589533623650065\n",
      "        policy_loss: -0.07724324563518166\n",
      "        total_loss: 52.484883993466696\n",
      "        vf_explained_var: 0.05746855566898982\n",
      "        vf_loss: 52.55139792442322\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45367172996203103\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017139710195090932\n",
      "        policy_loss: -0.08971374152228236\n",
      "        total_loss: 77.62972395261129\n",
      "        vf_explained_var: 0.3898298927148183\n",
      "        vf_loss: 77.70786814371745\n",
      "  num_agent_steps_sampled: 4253310\n",
      "  num_agent_steps_trained: 4253310\n",
      "  num_steps_sampled: 4253340\n",
      "  num_steps_trained: 4253340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 533\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.214999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.24000000000001\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.333333333333336\n",
      "  player_1: 36.33333333333333\n",
      "  player_2: 32.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.0999999999999996\n",
      "  player_1: -2.4899999999999998\n",
      "  player_2: 3.39\n",
      "policy_reward_min:\n",
      "  player_0: -30.0\n",
      "  player_1: -42.666666666666664\n",
      "  player_2: -48.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08767859650730941\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30463605303995533\n",
      "  mean_inference_ms: 1.6339994118489418\n",
      "  mean_raw_obs_processing_ms: 0.21263003388403415\n",
      "time_since_restore: 8586.941838264465\n",
      "time_this_iter_s: 16.00730872154236\n",
      "time_total_s: 8586.941838264465\n",
      "timers:\n",
      "  learn_throughput: 555.11\n",
      "  learn_time_ms: 14375.526\n",
      "  load_throughput: 1010947.349\n",
      "  load_time_ms: 7.894\n",
      "  sample_throughput: 515.269\n",
      "  sample_time_ms: 15487.065\n",
      "  update_time_ms: 6.069\n",
      "timestamp: 1643544820\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4253340\n",
      "training_iteration: 533\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4269271\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-14-12\n",
      "done: false\n",
      "episode_len_mean: 141.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 55\n",
      "episodes_total: 21504\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4678398142258326\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01629763830387674\n",
      "        policy_loss: -0.06903466043217729\n",
      "        total_loss: 52.548212005297344\n",
      "        vf_explained_var: 0.4650791634122531\n",
      "        vf_loss: 52.606246042251584\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5202207140127818\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014785911144711766\n",
      "        policy_loss: -0.0878860518305252\n",
      "        total_loss: 51.87880391597748\n",
      "        vf_explained_var: 0.15859698782364529\n",
      "        vf_loss: 51.95670957565308\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.474593431254228\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01700177292822976\n",
      "        policy_loss: -0.07085311057666938\n",
      "        total_loss: 56.735890952746075\n",
      "        vf_explained_var: 0.3038480586806933\n",
      "        vf_loss: 56.795267839431766\n",
      "  num_agent_steps_sampled: 4269271\n",
      "  num_agent_steps_trained: 4269271\n",
      "  num_steps_sampled: 4269300\n",
      "  num_steps_trained: 4269300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 535\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.955000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.03999999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.0\n",
      "  player_1: 32.333333333333336\n",
      "  player_2: 37.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 4.666666666666667\n",
      "  player_1: -5.603333333333333\n",
      "  player_2: 3.9366666666666674\n",
      "policy_reward_min:\n",
      "  player_0: -34.333333333333336\n",
      "  player_1: -49.333333333333336\n",
      "  player_2: -42.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08760063465948523\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30403750727114504\n",
      "  mean_inference_ms: 1.632971970718786\n",
      "  mean_raw_obs_processing_ms: 0.2123996478818185\n",
      "time_since_restore: 8618.524526834488\n",
      "time_this_iter_s: 16.38242244720459\n",
      "time_total_s: 8618.524526834488\n",
      "timers:\n",
      "  learn_throughput: 554.715\n",
      "  learn_time_ms: 14385.769\n",
      "  load_throughput: 962137.362\n",
      "  load_time_ms: 8.294\n",
      "  sample_throughput: 513.249\n",
      "  sample_time_ms: 15547.996\n",
      "  update_time_ms: 5.929\n",
      "timestamp: 1643544852\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4269300\n",
      "training_iteration: 535\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4285230\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-14-44\n",
      "done: false\n",
      "episode_len_mean: 125.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 58\n",
      "episodes_total: 21626\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47961712633570036\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01629642302856458\n",
      "        policy_loss: -0.10313619332853705\n",
      "        total_loss: 67.56125280539194\n",
      "        vf_explained_var: 0.20791735231876374\n",
      "        vf_loss: 67.65338872273763\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.532383336921533\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016304528748355362\n",
      "        policy_loss: -0.0818442344168822\n",
      "        total_loss: 43.38816298643748\n",
      "        vf_explained_var: 0.20562965055306753\n",
      "        vf_loss: 43.4590017302831\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4726435079177221\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01572320420641783\n",
      "        policy_loss: -0.046643133172765375\n",
      "        total_loss: 70.41441169897715\n",
      "        vf_explained_var: 0.164622829357783\n",
      "        vf_loss: 70.45044185320536\n",
      "  num_agent_steps_sampled: 4285230\n",
      "  num_agent_steps_trained: 4285230\n",
      "  num_steps_sampled: 4285260\n",
      "  num_steps_trained: 4285260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 537\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.88\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.02000000000001\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.666666666666664\n",
      "  player_1: 32.0\n",
      "  player_2: 34.0\n",
      "policy_reward_mean:\n",
      "  player_0: -1.8433333333333328\n",
      "  player_1: -0.3033333333333331\n",
      "  player_2: 5.146666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -31.666666666666664\n",
      "  player_1: -26.0\n",
      "  player_2: -42.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08746186945334361\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3038947228748505\n",
      "  mean_inference_ms: 1.632271568945759\n",
      "  mean_raw_obs_processing_ms: 0.2123154286271982\n",
      "time_since_restore: 8651.040490865707\n",
      "time_this_iter_s: 16.029152631759644\n",
      "time_total_s: 8651.040490865707\n",
      "timers:\n",
      "  learn_throughput: 550.749\n",
      "  learn_time_ms: 14489.372\n",
      "  load_throughput: 912946.774\n",
      "  load_time_ms: 8.741\n",
      "  sample_throughput: 506.306\n",
      "  sample_time_ms: 15761.227\n",
      "  update_time_ms: 5.936\n",
      "timestamp: 1643544884\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4285260\n",
      "training_iteration: 537\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4301190\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-15-17\n",
      "done: false\n",
      "episode_len_mean: 133.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 58\n",
      "episodes_total: 21747\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4871286244690418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014778840344818795\n",
      "        policy_loss: -0.04392126352215807\n",
      "        total_loss: 63.489005637963615\n",
      "        vf_explained_var: 0.2553751544157664\n",
      "        vf_loss: 63.52295128107071\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5335787267486254\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015562357799281017\n",
      "        policy_loss: -0.036272141902397075\n",
      "        total_loss: 54.177920645078025\n",
      "        vf_explained_var: 0.2135374548037847\n",
      "        vf_loss: 54.20368810494741\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47375457604726157\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014795805377353114\n",
      "        policy_loss: -0.12174433146758626\n",
      "        total_loss: 59.23938747406006\n",
      "        vf_explained_var: 0.23235519766807555\n",
      "        vf_loss: 59.35114459673564\n",
      "  num_agent_steps_sampled: 4301190\n",
      "  num_agent_steps_trained: 4301190\n",
      "  num_steps_sampled: 4301220\n",
      "  num_steps_trained: 4301220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 539\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.77\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.02000000000001\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.333333333333336\n",
      "  player_1: 34.33333333333333\n",
      "  player_2: 35.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 1.6433333333333335\n",
      "  player_1: -3.2766666666666664\n",
      "  player_2: 4.633333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -62.66666666666667\n",
      "  player_1: -35.0\n",
      "  player_2: -51.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08754078349292758\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30390435050820214\n",
      "  mean_inference_ms: 1.6322703192595878\n",
      "  mean_raw_obs_processing_ms: 0.21231563713294727\n",
      "time_since_restore: 8683.301636695862\n",
      "time_this_iter_s: 15.792533159255981\n",
      "time_total_s: 8683.301636695862\n",
      "timers:\n",
      "  learn_throughput: 547.25\n",
      "  learn_time_ms: 14581.988\n",
      "  load_throughput: 883146.502\n",
      "  load_time_ms: 9.036\n",
      "  sample_throughput: 505.941\n",
      "  sample_time_ms: 15772.604\n",
      "  update_time_ms: 5.82\n",
      "timestamp: 1643544917\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4301220\n",
      "training_iteration: 539\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4317150\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-15-50\n",
      "done: false\n",
      "episode_len_mean: 126.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 59\n",
      "episodes_total: 21867\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4919615585108598\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016816453011048223\n",
      "        policy_loss: -0.07979126427322626\n",
      "        total_loss: 54.05319314320882\n",
      "        vf_explained_var: 0.37082106987635294\n",
      "        vf_loss: 54.12163332780202\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5374022502700487\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01507096095923032\n",
      "        policy_loss: -0.06788345319218933\n",
      "        total_loss: 67.27982377211252\n",
      "        vf_explained_var: 0.27557539800802866\n",
      "        vf_loss: 67.33753423849741\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47479035452008245\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015856068617285928\n",
      "        policy_loss: -0.08118831799676021\n",
      "        total_loss: 65.79948659578959\n",
      "        vf_explained_var: 0.36163160324096677\n",
      "        vf_loss: 65.86997212727864\n",
      "  num_agent_steps_sampled: 4317150\n",
      "  num_agent_steps_trained: 4317150\n",
      "  num_steps_sampled: 4317180\n",
      "  num_steps_trained: 4317180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 541\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.63809523809524\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.00952380952381\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 26.666666666666668\n",
      "  player_1: 33.333333333333336\n",
      "  player_2: 37.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -1.44\n",
      "  player_1: -2.87\n",
      "  player_2: 7.309999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -35.0\n",
      "  player_1: -44.666666666666664\n",
      "  player_2: -47.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08766113760988394\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30366405306658933\n",
      "  mean_inference_ms: 1.6301398173723578\n",
      "  mean_raw_obs_processing_ms: 0.21242578232641024\n",
      "time_since_restore: 8716.45896410942\n",
      "time_this_iter_s: 16.567182540893555\n",
      "time_total_s: 8716.45896410942\n",
      "timers:\n",
      "  learn_throughput: 537.246\n",
      "  learn_time_ms: 14853.532\n",
      "  load_throughput: 894260.101\n",
      "  load_time_ms: 8.924\n",
      "  sample_throughput: 500.195\n",
      "  sample_time_ms: 15953.781\n",
      "  update_time_ms: 5.52\n",
      "timestamp: 1643544950\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4317180\n",
      "training_iteration: 541\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4333110\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-16-21\n",
      "done: false\n",
      "episode_len_mean: 130.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 57\n",
      "episodes_total: 21987\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47018151337901753\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01656460909894425\n",
      "        policy_loss: -0.08750928167874615\n",
      "        total_loss: 47.514082752863565\n",
      "        vf_explained_var: 0.3292287372549375\n",
      "        vf_loss: 47.590411084493\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5237758866449197\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016180482882051497\n",
      "        policy_loss: -0.07399749954541525\n",
      "        total_loss: 35.10788921912511\n",
      "        vf_explained_var: 0.046852216223875684\n",
      "        vf_loss: 35.17096513271332\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.474102198779583\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01751579044692581\n",
      "        policy_loss: -0.08240990897951027\n",
      "        total_loss: 46.57941010713577\n",
      "        vf_explained_var: 0.11065545121828715\n",
      "        vf_loss: 46.649996940294905\n",
      "  num_agent_steps_sampled: 4333110\n",
      "  num_agent_steps_trained: 4333110\n",
      "  num_steps_sampled: 4333140\n",
      "  num_steps_trained: 4333140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 543\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.644444444444446\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.02222222222223\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.666666666666664\n",
      "  player_1: 42.0\n",
      "  player_2: 30.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.166666666666666\n",
      "  player_1: -3.593333333333334\n",
      "  player_2: 4.426666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -51.0\n",
      "  player_1: -27.0\n",
      "  player_2: -29.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0874776174018967\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30410868686930526\n",
      "  mean_inference_ms: 1.6333828307011573\n",
      "  mean_raw_obs_processing_ms: 0.21234860969277278\n",
      "time_since_restore: 8746.994347572327\n",
      "time_this_iter_s: 15.102003574371338\n",
      "time_total_s: 8746.994347572327\n",
      "timers:\n",
      "  learn_throughput: 539.623\n",
      "  learn_time_ms: 14788.113\n",
      "  load_throughput: 903839.864\n",
      "  load_time_ms: 8.829\n",
      "  sample_throughput: 494.803\n",
      "  sample_time_ms: 16127.619\n",
      "  update_time_ms: 5.582\n",
      "timestamp: 1643544981\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4333140\n",
      "training_iteration: 543\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4349070\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-16-52\n",
      "done: false\n",
      "episode_len_mean: 135.42\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 63\n",
      "episodes_total: 22110\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4843694666028023\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016309528176521062\n",
      "        policy_loss: -0.11116039718190829\n",
      "        total_loss: 61.4489443508784\n",
      "        vf_explained_var: 0.26836873024702074\n",
      "        vf_loss: 61.54909590244293\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.550959511846304\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015942310629587456\n",
      "        policy_loss: -0.05502047325484455\n",
      "        total_loss: 108.69865379015604\n",
      "        vf_explained_var: 0.15594304392735164\n",
      "        vf_loss: 108.74291298548381\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48635966191689173\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01664317521426104\n",
      "        policy_loss: -0.08152692314858238\n",
      "        total_loss: 80.71578604857127\n",
      "        vf_explained_var: 0.3309186531106631\n",
      "        vf_loss: 80.78607920646668\n",
      "  num_agent_steps_sampled: 4349070\n",
      "  num_agent_steps_trained: 4349070\n",
      "  num_steps_sampled: 4349100\n",
      "  num_steps_trained: 4349100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 545\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.709523809523805\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.03809523809524\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 40.666666666666664\n",
      "  player_1: 37.66666666666667\n",
      "  player_2: 39.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 3.5499999999999994\n",
      "  player_1: -4.56\n",
      "  player_2: 4.01\n",
      "policy_reward_min:\n",
      "  player_0: -41.666666666666664\n",
      "  player_1: -59.666666666666664\n",
      "  player_2: -51.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08753187755589137\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3038065762689064\n",
      "  mean_inference_ms: 1.631541242426791\n",
      "  mean_raw_obs_processing_ms: 0.21232003200002253\n",
      "time_since_restore: 8778.728307723999\n",
      "time_this_iter_s: 16.658167839050293\n",
      "time_total_s: 8778.728307723999\n",
      "timers:\n",
      "  learn_throughput: 539.11\n",
      "  learn_time_ms: 14802.18\n",
      "  load_throughput: 946725.856\n",
      "  load_time_ms: 8.429\n",
      "  sample_throughput: 498.198\n",
      "  sample_time_ms: 16017.73\n",
      "  update_time_ms: 5.587\n",
      "timestamp: 1643545012\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4349100\n",
      "training_iteration: 545\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4365030\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-17-24\n",
      "done: false\n",
      "episode_len_mean: 146.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 53\n",
      "episodes_total: 22219\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4735873578985532\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015909831022774674\n",
      "        policy_loss: -0.09154990661889315\n",
      "        total_loss: 45.602121191819506\n",
      "        vf_explained_var: 0.42417014251152674\n",
      "        vf_loss: 45.68293210109075\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5333266763885816\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01795027387115018\n",
      "        policy_loss: -0.09360418178141117\n",
      "        total_loss: 49.63833081563314\n",
      "        vf_explained_var: -0.013397796154022217\n",
      "        vf_loss: 49.719818379084266\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4579864983757337\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016811201058776818\n",
      "        policy_loss: -0.04372247586337229\n",
      "        total_loss: 51.98348303715388\n",
      "        vf_explained_var: 0.34909884760777155\n",
      "        vf_loss: 52.01585820913315\n",
      "  num_agent_steps_sampled: 4365030\n",
      "  num_agent_steps_trained: 4365030\n",
      "  num_steps_sampled: 4365060\n",
      "  num_steps_trained: 4365060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 547\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.715000000000003\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.02000000000001\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.666666666666664\n",
      "  player_1: 30.666666666666664\n",
      "  player_2: 33.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 1.096666666666667\n",
      "  player_1: -1.5733333333333333\n",
      "  player_2: 3.476666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -59.333333333333336\n",
      "  player_1: -38.333333333333336\n",
      "  player_2: -46.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08748453830663791\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30318738542347057\n",
      "  mean_inference_ms: 1.6277439665657556\n",
      "  mean_raw_obs_processing_ms: 0.2122244803805306\n",
      "time_since_restore: 8810.24287366867\n",
      "time_this_iter_s: 15.802544355392456\n",
      "time_total_s: 8810.24287366867\n",
      "timers:\n",
      "  learn_throughput: 542.657\n",
      "  learn_time_ms: 14705.422\n",
      "  load_throughput: 844880.841\n",
      "  load_time_ms: 9.445\n",
      "  sample_throughput: 499.71\n",
      "  sample_time_ms: 15969.26\n",
      "  update_time_ms: 5.491\n",
      "timestamp: 1643545044\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4365060\n",
      "training_iteration: 547\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4380990\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-17-55\n",
      "done: false\n",
      "episode_len_mean: 139.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 61\n",
      "episodes_total: 22336\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4679781817893187\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014954356603381408\n",
      "        policy_loss: -0.08730780352217456\n",
      "        total_loss: 58.24049324035644\n",
      "        vf_explained_var: 0.2210096021493276\n",
      "        vf_loss: 58.31770655314128\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5262098405261835\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01670490103695812\n",
      "        policy_loss: -0.11061218762770295\n",
      "        total_loss: 35.763399298985796\n",
      "        vf_explained_var: 0.1506148647268613\n",
      "        vf_loss: 35.86273574034373\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45781255458792053\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018277326030287592\n",
      "        policy_loss: -0.05247781003514926\n",
      "        total_loss: 51.83570450305939\n",
      "        vf_explained_var: 0.14594388633966446\n",
      "        vf_loss: 51.87584514935811\n",
      "  num_agent_steps_sampled: 4380990\n",
      "  num_agent_steps_trained: 4380990\n",
      "  num_steps_sampled: 4381020\n",
      "  num_steps_trained: 4381020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 549\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.788888888888888\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.02222222222223\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 25.0\n",
      "  player_1: 27.333333333333336\n",
      "  player_2: 28.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.04333333333333306\n",
      "  player_1: -2.073333333333333\n",
      "  player_2: 5.116666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -39.0\n",
      "  player_1: -24.333333333333332\n",
      "  player_2: -33.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08745384309261964\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3037061599687283\n",
      "  mean_inference_ms: 1.6309445936898543\n",
      "  mean_raw_obs_processing_ms: 0.21232385634895187\n",
      "time_since_restore: 8841.250413656235\n",
      "time_this_iter_s: 14.880245447158813\n",
      "time_total_s: 8841.250413656235\n",
      "timers:\n",
      "  learn_throughput: 547.137\n",
      "  learn_time_ms: 14585.023\n",
      "  load_throughput: 896653.368\n",
      "  load_time_ms: 8.9\n",
      "  sample_throughput: 501.54\n",
      "  sample_time_ms: 15910.984\n",
      "  update_time_ms: 5.509\n",
      "timestamp: 1643545075\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4381020\n",
      "training_iteration: 549\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4396952\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-18-25\n",
      "done: false\n",
      "episode_len_mean: 122.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 65\n",
      "episodes_total: 22468\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46390349060297015\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01580474566662815\n",
      "        policy_loss: -0.10024463122089704\n",
      "        total_loss: 61.098976248105366\n",
      "        vf_explained_var: 0.41092772791783017\n",
      "        vf_loss: 61.188552501996355\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5294386252264182\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0157325681504426\n",
      "        policy_loss: -0.0786859433228771\n",
      "        total_loss: 63.66389049530029\n",
      "        vf_explained_var: 0.21206343640883762\n",
      "        vf_loss: 63.7319570016861\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47433335686723394\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01630230190943877\n",
      "        policy_loss: -0.07079718369680146\n",
      "        total_loss: 99.7822443262736\n",
      "        vf_explained_var: 0.336523461441199\n",
      "        vf_loss: 99.84203749338786\n",
      "  num_agent_steps_sampled: 4396952\n",
      "  num_agent_steps_trained: 4396952\n",
      "  num_steps_sampled: 4396980\n",
      "  num_steps_trained: 4396980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 551\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.310526315789476\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.25263157894737\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.666666666666664\n",
      "  player_1: 39.0\n",
      "  player_2: 42.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.28\n",
      "  player_1: -3.6999999999999993\n",
      "  player_2: 5.419999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -40.0\n",
      "  player_1: -37.666666666666664\n",
      "  player_2: -57.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08734162153741924\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3035222160405138\n",
      "  mean_inference_ms: 1.631619799163129\n",
      "  mean_raw_obs_processing_ms: 0.21210870599983983\n",
      "time_since_restore: 8871.69567656517\n",
      "time_this_iter_s: 15.520604133605957\n",
      "time_total_s: 8871.69567656517\n",
      "timers:\n",
      "  learn_throughput: 557.487\n",
      "  learn_time_ms: 14314.237\n",
      "  load_throughput: 921175.571\n",
      "  load_time_ms: 8.663\n",
      "  sample_throughput: 509.805\n",
      "  sample_time_ms: 15653.032\n",
      "  update_time_ms: 5.448\n",
      "timestamp: 1643545105\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4396980\n",
      "training_iteration: 551\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4412915\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-18-58\n",
      "done: false\n",
      "episode_len_mean: 134.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 58\n",
      "episodes_total: 22585\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4782703523337841\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017086383603282324\n",
      "        policy_loss: -0.10008897020171086\n",
      "        total_loss: 42.37055942217509\n",
      "        vf_explained_var: 0.3810325586795807\n",
      "        vf_loss: 42.459115244547526\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5482437936464946\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015424895077994733\n",
      "        policy_loss: -0.06866285798450311\n",
      "        total_loss: 67.86578290462494\n",
      "        vf_explained_var: 0.08095389942328135\n",
      "        vf_loss: 67.92403414249421\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4752669924000899\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01599491861524257\n",
      "        policy_loss: -0.06330685689424476\n",
      "        total_loss: 62.09792383988698\n",
      "        vf_explained_var: 0.15861036509275436\n",
      "        vf_loss: 62.15043371359507\n",
      "  num_agent_steps_sampled: 4412915\n",
      "  num_agent_steps_trained: 4412915\n",
      "  num_steps_sampled: 4412940\n",
      "  num_steps_trained: 4412940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 553\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.55238095238095\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.01904761904763\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.0\n",
      "  player_1: 39.666666666666664\n",
      "  player_2: 44.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.346666666666667\n",
      "  player_1: -2.793333333333333\n",
      "  player_2: 4.446666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -51.333333333333336\n",
      "  player_1: -61.666666666666664\n",
      "  player_2: -47.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08749651996771317\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3031849120713856\n",
      "  mean_inference_ms: 1.6286721930177808\n",
      "  mean_raw_obs_processing_ms: 0.21228897578369693\n",
      "time_since_restore: 8904.52453327179\n",
      "time_this_iter_s: 16.985273361206055\n",
      "time_total_s: 8904.52453327179\n",
      "timers:\n",
      "  learn_throughput: 549.307\n",
      "  learn_time_ms: 14527.39\n",
      "  load_throughput: 836659.066\n",
      "  load_time_ms: 9.538\n",
      "  sample_throughput: 511.813\n",
      "  sample_time_ms: 15591.636\n",
      "  update_time_ms: 5.495\n",
      "timestamp: 1643545138\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4412940\n",
      "training_iteration: 553\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4428870\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-19-31\n",
      "done: false\n",
      "episode_len_mean: 126.6\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 62\n",
      "episodes_total: 22707\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47145281106233594\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015812064970537942\n",
      "        policy_loss: -0.05394442166512211\n",
      "        total_loss: 97.93806869983673\n",
      "        vf_explained_var: 0.3037344322601954\n",
      "        vf_loss: 97.98133927583694\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5127179544170698\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015689410885465806\n",
      "        policy_loss: -0.03723689609051992\n",
      "        total_loss: 61.893426875273384\n",
      "        vf_explained_var: 0.5189284500479698\n",
      "        vf_loss: 61.92007304509481\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47640992840131124\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01689581021227241\n",
      "        policy_loss: -0.12058616266896327\n",
      "        total_loss: 99.98366129875183\n",
      "        vf_explained_var: 0.19504204710324605\n",
      "        vf_loss: 100.09284276326497\n",
      "  num_agent_steps_sampled: 4428870\n",
      "  num_agent_steps_trained: 4428870\n",
      "  num_steps_sampled: 4428900\n",
      "  num_steps_trained: 4428900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 555\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.02\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.666666666666664\n",
      "  player_1: 43.0\n",
      "  player_2: 33.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.1933333333333334\n",
      "  player_1: -2.9266666666666667\n",
      "  player_2: 3.733333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -50.0\n",
      "  player_1: -36.0\n",
      "  player_2: -49.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08733825720617627\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3032836203354529\n",
      "  mean_inference_ms: 1.6298164143540737\n",
      "  mean_raw_obs_processing_ms: 0.21218985924672246\n",
      "time_since_restore: 8937.321615457535\n",
      "time_this_iter_s: 16.366482257843018\n",
      "time_total_s: 8937.321615457535\n",
      "timers:\n",
      "  learn_throughput: 545.459\n",
      "  learn_time_ms: 14629.878\n",
      "  load_throughput: 804638.458\n",
      "  load_time_ms: 9.917\n",
      "  sample_throughput: 501.109\n",
      "  sample_time_ms: 15924.693\n",
      "  update_time_ms: 5.419\n",
      "timestamp: 1643545171\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4428900\n",
      "training_iteration: 555\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4444830\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-20-03\n",
      "done: false\n",
      "episode_len_mean: 139.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 58\n",
      "episodes_total: 22820\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4833655904233456\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017736151103137178\n",
      "        policy_loss: -0.08405408749356866\n",
      "        total_loss: 57.570237304369606\n",
      "        vf_explained_var: 0.2178576985001564\n",
      "        vf_loss: 57.6423193470637\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5212834430734317\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01577682671181416\n",
      "        policy_loss: -0.06115745296701789\n",
      "        total_loss: 41.41065952936808\n",
      "        vf_explained_var: 0.14877648085355757\n",
      "        vf_loss: 41.461167618433635\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45959392239650093\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01726896621539576\n",
      "        policy_loss: -0.08649244574829935\n",
      "        total_loss: 59.59119861920674\n",
      "        vf_explained_var: 0.19265515039364497\n",
      "        vf_loss: 59.66603451728821\n",
      "  num_agent_steps_sampled: 4444830\n",
      "  num_agent_steps_trained: 4444830\n",
      "  num_steps_sampled: 4444860\n",
      "  num_steps_trained: 4444860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 557\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.926315789473682\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.02105263157895\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 42.66666666666667\n",
      "  player_1: 34.666666666666664\n",
      "  player_2: 30.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.9566666666666663\n",
      "  player_1: -0.5133333333333333\n",
      "  player_2: 1.5566666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -44.333333333333336\n",
      "  player_1: -25.666666666666664\n",
      "  player_2: -36.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08734727757918341\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.303010021960998\n",
      "  mean_inference_ms: 1.6283680325488006\n",
      "  mean_raw_obs_processing_ms: 0.2121273853104262\n",
      "time_since_restore: 8969.041081905365\n",
      "time_this_iter_s: 15.092782735824585\n",
      "time_total_s: 8969.041081905365\n",
      "timers:\n",
      "  learn_throughput: 544.736\n",
      "  learn_time_ms: 14649.307\n",
      "  load_throughput: 923248.998\n",
      "  load_time_ms: 8.643\n",
      "  sample_throughput: 499.174\n",
      "  sample_time_ms: 15986.405\n",
      "  update_time_ms: 5.557\n",
      "timestamp: 1643545203\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4444860\n",
      "training_iteration: 557\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4460790\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-20-33\n",
      "done: false\n",
      "episode_len_mean: 132.12\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 59\n",
      "episodes_total: 22942\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4654910853505135\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015945714828035116\n",
      "        policy_loss: -0.06934311108663678\n",
      "        total_loss: 80.56736607551575\n",
      "        vf_explained_var: 0.10969268987576167\n",
      "        vf_loss: 80.62594548543294\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5243015283842881\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015965054569275404\n",
      "        policy_loss: -0.09280706009827554\n",
      "        total_loss: 41.83263095378876\n",
      "        vf_explained_var: 0.1506748644510905\n",
      "        vf_loss: 41.91466151396433\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.457363872975111\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016190864545912596\n",
      "        policy_loss: -0.08979131882389386\n",
      "        total_loss: 66.221358071963\n",
      "        vf_explained_var: 0.2608312291900317\n",
      "        vf_loss: 66.3002208741506\n",
      "  num_agent_steps_sampled: 4460790\n",
      "  num_agent_steps_trained: 4460790\n",
      "  num_steps_sampled: 4460820\n",
      "  num_steps_trained: 4460820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 559\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.826315789473684\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.04736842105264\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.0\n",
      "  player_1: 30.0\n",
      "  player_2: 34.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -0.46\n",
      "  player_1: -3.3599999999999994\n",
      "  player_2: 6.82\n",
      "policy_reward_min:\n",
      "  player_0: -39.666666666666664\n",
      "  player_1: -51.333333333333336\n",
      "  player_2: -37.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08725914970385197\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30322030760716734\n",
      "  mean_inference_ms: 1.6304081225012215\n",
      "  mean_raw_obs_processing_ms: 0.21204782766314104\n",
      "time_since_restore: 8999.298132419586\n",
      "time_this_iter_s: 15.289143085479736\n",
      "time_total_s: 8999.298132419586\n",
      "timers:\n",
      "  learn_throughput: 547.509\n",
      "  learn_time_ms: 14575.101\n",
      "  load_throughput: 919389.151\n",
      "  load_time_ms: 8.68\n",
      "  sample_throughput: 505.153\n",
      "  sample_time_ms: 15797.185\n",
      "  update_time_ms: 5.528\n",
      "timestamp: 1643545233\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4460820\n",
      "training_iteration: 559\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4476750\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-21-05\n",
      "done: false\n",
      "episode_len_mean: 141.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 56\n",
      "episodes_total: 23051\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4597983900705973\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01674413913184897\n",
      "        policy_loss: -0.0708268052401642\n",
      "        total_loss: 54.62386678695679\n",
      "        vf_explained_var: 0.2438794152935346\n",
      "        vf_loss: 54.68339107354482\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5127270638942718\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01630046044563907\n",
      "        policy_loss: -0.0911696074041538\n",
      "        total_loss: 49.05175026655197\n",
      "        vf_explained_var: 0.1541113269329071\n",
      "        vf_loss: 49.131917108694715\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4683297524849574\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017193555100764493\n",
      "        policy_loss: -0.07466528026387095\n",
      "        total_loss: 55.06652614196142\n",
      "        vf_explained_var: 0.3240307309230169\n",
      "        vf_loss: 55.12958575328191\n",
      "  num_agent_steps_sampled: 4476750\n",
      "  num_agent_steps_trained: 4476750\n",
      "  num_steps_sampled: 4476780\n",
      "  num_steps_trained: 4476780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 561\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.844999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.025\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 29.333333333333336\n",
      "  player_2: 43.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.643333333333333\n",
      "  player_1: -2.776666666666667\n",
      "  player_2: 4.133333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -45.0\n",
      "  player_1: -40.333333333333336\n",
      "  player_2: -48.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08726351044397106\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30253911126098215\n",
      "  mean_inference_ms: 1.6268009940552537\n",
      "  mean_raw_obs_processing_ms: 0.21192574409996098\n",
      "time_since_restore: 9031.361755609512\n",
      "time_this_iter_s: 15.882607221603394\n",
      "time_total_s: 9031.361755609512\n",
      "timers:\n",
      "  learn_throughput: 541.578\n",
      "  learn_time_ms: 14734.731\n",
      "  load_throughput: 865059.753\n",
      "  load_time_ms: 9.225\n",
      "  sample_throughput: 499.809\n",
      "  sample_time_ms: 15966.109\n",
      "  update_time_ms: 5.612\n",
      "timestamp: 1643545265\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4476780\n",
      "training_iteration: 561\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4492710\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-21-36\n",
      "done: false\n",
      "episode_len_mean: 132.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 61\n",
      "episodes_total: 23168\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47512779677907624\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015200408803038006\n",
      "        policy_loss: -0.056658853319628784\n",
      "        total_loss: 68.58876979351044\n",
      "        vf_explained_var: 0.3126849873860677\n",
      "        vf_loss: 68.63516854286193\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5414701254665851\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01510602202619623\n",
      "        policy_loss: -0.07398085579276086\n",
      "        total_loss: 52.48589223225911\n",
      "        vf_explained_var: 0.14644116371870042\n",
      "        vf_loss: 52.54967666467031\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46485914503534637\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015507839248926417\n",
      "        policy_loss: -0.07686662293970585\n",
      "        total_loss: 74.32872299830119\n",
      "        vf_explained_var: 0.37307027608156207\n",
      "        vf_loss: 74.3951216729482\n",
      "  num_agent_steps_sampled: 4492710\n",
      "  num_agent_steps_trained: 4492710\n",
      "  num_steps_sampled: 4492740\n",
      "  num_steps_trained: 4492740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 563\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.694736842105263\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.05263157894737\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.0\n",
      "  player_1: 40.333333333333336\n",
      "  player_2: 31.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -0.5599999999999998\n",
      "  player_1: -1.71\n",
      "  player_2: 5.27\n",
      "policy_reward_min:\n",
      "  player_0: -54.333333333333336\n",
      "  player_1: -47.333333333333336\n",
      "  player_2: -43.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08732606561552478\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30248351797919165\n",
      "  mean_inference_ms: 1.6263945222397205\n",
      "  mean_raw_obs_processing_ms: 0.21198583401212956\n",
      "time_since_restore: 9061.802376031876\n",
      "time_this_iter_s: 15.51566195487976\n",
      "time_total_s: 9061.802376031876\n",
      "timers:\n",
      "  learn_throughput: 549.784\n",
      "  learn_time_ms: 14514.803\n",
      "  load_throughput: 922396.647\n",
      "  load_time_ms: 8.651\n",
      "  sample_throughput: 501.725\n",
      "  sample_time_ms: 15905.127\n",
      "  update_time_ms: 5.586\n",
      "timestamp: 1643545296\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4492740\n",
      "training_iteration: 563\n",
      "trial_id: default\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0130 13:21:53.724774188   17020 backup_poller.cc:134]       Run client channel backup poller: {\"created\":\"@1643545313.724738149\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1643545313.724730895\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 4508670\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-22-10\n",
      "done: false\n",
      "episode_len_mean: 131.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 55\n",
      "episodes_total: 23282\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46491757546861967\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014353664895806256\n",
      "        policy_loss: -0.06435167199621598\n",
      "        total_loss: 56.292750123341875\n",
      "        vf_explained_var: 0.306715066631635\n",
      "        vf_loss: 56.347413093249\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48484332422415416\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015735567664778502\n",
      "        policy_loss: -0.06866420689815035\n",
      "        total_loss: 37.70830111503601\n",
      "        vf_explained_var: 0.23120844691991807\n",
      "        vf_loss: 37.766343910694125\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.455696423103412\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015752730229905865\n",
      "        policy_loss: -0.09317147340625524\n",
      "        total_loss: 55.719617652098336\n",
      "        vf_explained_var: 0.1459977019826571\n",
      "        vf_loss: 55.80215621232986\n",
      "  num_agent_steps_sampled: 4508670\n",
      "  num_agent_steps_trained: 4508670\n",
      "  num_steps_sampled: 4508700\n",
      "  num_steps_trained: 4508700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 565\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.871428571428572\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.3714285714286\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 42.666666666666664\n",
      "  player_1: 35.0\n",
      "  player_2: 40.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 2.773333333333333\n",
      "  player_1: -2.6166666666666663\n",
      "  player_2: 2.8433333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -57.33333333333333\n",
      "  player_1: -43.666666666666664\n",
      "  player_2: -34.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08742740417777989\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3027944481722305\n",
      "  mean_inference_ms: 1.6270516545883555\n",
      "  mean_raw_obs_processing_ms: 0.2122079466346184\n",
      "time_since_restore: 9095.442741155624\n",
      "time_this_iter_s: 16.884572982788086\n",
      "time_total_s: 9095.442741155624\n",
      "timers:\n",
      "  learn_throughput: 546.504\n",
      "  learn_time_ms: 14601.904\n",
      "  load_throughput: 950381.791\n",
      "  load_time_ms: 8.397\n",
      "  sample_throughput: 505.536\n",
      "  sample_time_ms: 15785.236\n",
      "  update_time_ms: 5.779\n",
      "timestamp: 1643545330\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4508700\n",
      "training_iteration: 565\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4524630\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-22-40\n",
      "done: false\n",
      "episode_len_mean: 135.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 55\n",
      "episodes_total: 23396\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44341525860130787\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015959284612911896\n",
      "        policy_loss: -0.09775443825870753\n",
      "        total_loss: 55.12114871780078\n",
      "        vf_explained_var: 0.3003376329938571\n",
      "        vf_loss: 55.208130750854814\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5218946082890034\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016495709292154666\n",
      "        policy_loss: -0.04648100370308384\n",
      "        total_loss: 42.717425718307496\n",
      "        vf_explained_var: 0.10923700541257858\n",
      "        vf_loss: 42.75277209122976\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46454543362061185\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01695415322090317\n",
      "        policy_loss: -0.08091689381593217\n",
      "        total_loss: 49.98607237736384\n",
      "        vf_explained_var: 0.28282084902127586\n",
      "        vf_loss: 50.055545082887015\n",
      "  num_agent_steps_sampled: 4524630\n",
      "  num_agent_steps_trained: 4524630\n",
      "  num_steps_sampled: 4524660\n",
      "  num_steps_trained: 4524660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 567\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.52777777777778\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.02777777777777\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.333333333333336\n",
      "  player_1: 30.0\n",
      "  player_2: 38.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.6033333333333333\n",
      "  player_1: -3.0333333333333337\n",
      "  player_2: 6.636666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -36.333333333333336\n",
      "  player_1: -36.333333333333336\n",
      "  player_2: -48.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08730476996766415\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.302524753319428\n",
      "  mean_inference_ms: 1.626260589766507\n",
      "  mean_raw_obs_processing_ms: 0.21206603366253154\n",
      "time_since_restore: 9125.663362979889\n",
      "time_this_iter_s: 14.510430097579956\n",
      "time_total_s: 9125.663362979889\n",
      "timers:\n",
      "  learn_throughput: 551.974\n",
      "  learn_time_ms: 14457.196\n",
      "  load_throughput: 1002986.012\n",
      "  load_time_ms: 7.956\n",
      "  sample_throughput: 506.959\n",
      "  sample_time_ms: 15740.93\n",
      "  update_time_ms: 5.707\n",
      "timestamp: 1643545360\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4524660\n",
      "training_iteration: 567\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4540590\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-23-11\n",
      "done: false\n",
      "episode_len_mean: 146.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 54\n",
      "episodes_total: 23511\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44108464593688645\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015518947075336959\n",
      "        policy_loss: -0.07757940384559334\n",
      "        total_loss: 59.68486620744069\n",
      "        vf_explained_var: 0.2437542909383774\n",
      "        vf_loss: 59.75197035948435\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5233085570236047\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017047981585915483\n",
      "        policy_loss: -0.07019267904261749\n",
      "        total_loss: 69.28719286600749\n",
      "        vf_explained_var: 0.1569710369904836\n",
      "        vf_loss: 69.34587805906932\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4773291145265102\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018806955347657396\n",
      "        policy_loss: -0.08817795212691029\n",
      "        total_loss: 72.70767790476481\n",
      "        vf_explained_var: 0.2728815441330274\n",
      "        vf_loss: 72.7831611363093\n",
      "  num_agent_steps_sampled: 4540590\n",
      "  num_agent_steps_trained: 4540590\n",
      "  num_steps_sampled: 4540620\n",
      "  num_steps_trained: 4540620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 569\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.679999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.666666666666664\n",
      "  player_1: 27.333333333333336\n",
      "  player_2: 45.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.6466666666666665\n",
      "  player_1: -3.533333333333333\n",
      "  player_2: 4.886666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -34.0\n",
      "  player_1: -55.666666666666664\n",
      "  player_2: -28.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08722440648572816\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30251398676297736\n",
      "  mean_inference_ms: 1.6275945586649203\n",
      "  mean_raw_obs_processing_ms: 0.21187145271061378\n",
      "time_since_restore: 9156.458558559418\n",
      "time_this_iter_s: 16.12167453765869\n",
      "time_total_s: 9156.458558559418\n",
      "timers:\n",
      "  learn_throughput: 549.815\n",
      "  learn_time_ms: 14513.982\n",
      "  load_throughput: 1001167.938\n",
      "  load_time_ms: 7.971\n",
      "  sample_throughput: 509.715\n",
      "  sample_time_ms: 15655.817\n",
      "  update_time_ms: 5.812\n",
      "timestamp: 1643545391\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4540620\n",
      "training_iteration: 569\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4556550\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-23-45\n",
      "done: false\n",
      "episode_len_mean: 142.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 57\n",
      "episodes_total: 23621\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46406354049841564\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016344330126816735\n",
      "        policy_loss: -0.05999879869321982\n",
      "        total_loss: 44.38503295103709\n",
      "        vf_explained_var: 0.28954142073790234\n",
      "        vf_loss: 44.43399917761485\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.500836616307497\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017042718956060222\n",
      "        policy_loss: -0.06971273453906178\n",
      "        total_loss: 43.41083724339803\n",
      "        vf_explained_var: 0.0978644272685051\n",
      "        vf_loss: 43.46904621124268\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4479171533385913\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016901575169125257\n",
      "        policy_loss: -0.08699704344694813\n",
      "        total_loss: 59.62063823858897\n",
      "        vf_explained_var: 0.29001871665318807\n",
      "        vf_loss: 59.69622668663661\n",
      "  num_agent_steps_sampled: 4556550\n",
      "  num_agent_steps_trained: 4556550\n",
      "  num_steps_sampled: 4556580\n",
      "  num_steps_trained: 4556580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 571\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.485714285714288\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 34.0\n",
      "  player_2: 33.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.9666666666666663\n",
      "  player_1: -3.083333333333333\n",
      "  player_2: 3.1166666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -42.666666666666664\n",
      "  player_1: -37.333333333333336\n",
      "  player_2: -47.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08718262906532916\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3022461263824087\n",
      "  mean_inference_ms: 1.6244709151369954\n",
      "  mean_raw_obs_processing_ms: 0.21190854263341877\n",
      "time_since_restore: 9190.845529079437\n",
      "time_this_iter_s: 17.46828532218933\n",
      "time_total_s: 9190.845529079437\n",
      "timers:\n",
      "  learn_throughput: 540.908\n",
      "  learn_time_ms: 14752.983\n",
      "  load_throughput: 1086519.07\n",
      "  load_time_ms: 7.345\n",
      "  sample_throughput: 504.682\n",
      "  sample_time_ms: 15811.948\n",
      "  update_time_ms: 5.792\n",
      "timestamp: 1643545425\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4556580\n",
      "training_iteration: 571\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4572510\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-24-15\n",
      "done: false\n",
      "episode_len_mean: 144.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 58\n",
      "episodes_total: 23736\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4691700235505899\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016244820782402865\n",
      "        policy_loss: -0.08245673788400988\n",
      "        total_loss: 52.57554860273997\n",
      "        vf_explained_var: 0.27311092178026836\n",
      "        vf_loss: 52.647039903004966\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4995986714462439\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015052039214172813\n",
      "        policy_loss: -0.10563529439270497\n",
      "        total_loss: 48.205499936739606\n",
      "        vf_explained_var: 0.19145714263121286\n",
      "        vf_loss: 48.30097502231598\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4835509395599365\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01698663375717634\n",
      "        policy_loss: -0.04502724815703307\n",
      "        total_loss: 47.16862416267395\n",
      "        vf_explained_var: 0.15714026550451915\n",
      "        vf_loss: 47.20218520641327\n",
      "  num_agent_steps_sampled: 4572510\n",
      "  num_agent_steps_trained: 4572510\n",
      "  num_steps_sampled: 4572540\n",
      "  num_steps_trained: 4572540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 573\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.75\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.333333333333336\n",
      "  player_1: 31.666666666666664\n",
      "  player_2: 34.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 0.6833333333333331\n",
      "  player_1: -2.386666666666667\n",
      "  player_2: 4.703333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -36.333333333333336\n",
      "  player_1: -42.333333333333336\n",
      "  player_2: -34.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08718124201342135\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3024060874025796\n",
      "  mean_inference_ms: 1.62664631058169\n",
      "  mean_raw_obs_processing_ms: 0.21185143457549307\n",
      "time_since_restore: 9220.345776557922\n",
      "time_this_iter_s: 14.74068546295166\n",
      "time_total_s: 9220.345776557922\n",
      "timers:\n",
      "  learn_throughput: 544.41\n",
      "  learn_time_ms: 14658.075\n",
      "  load_throughput: 1136640.73\n",
      "  load_time_ms: 7.021\n",
      "  sample_throughput: 500.02\n",
      "  sample_time_ms: 15959.351\n",
      "  update_time_ms: 5.617\n",
      "timestamp: 1643545455\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4572540\n",
      "training_iteration: 573\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4588470\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-24-45\n",
      "done: false\n",
      "episode_len_mean: 131.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 64\n",
      "episodes_total: 23856\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4634503349661827\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016190737849198437\n",
      "        policy_loss: -0.05883903201048573\n",
      "        total_loss: 62.95431286970774\n",
      "        vf_explained_var: 0.4729203697045644\n",
      "        vf_loss: 63.00222335656484\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5161193457742532\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016198872236863813\n",
      "        policy_loss: -0.09933543129513661\n",
      "        total_loss: 47.51928705533346\n",
      "        vf_explained_var: 0.23835919698079427\n",
      "        vf_loss: 47.60768821398417\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4690030278265476\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01588356564590034\n",
      "        policy_loss: -0.06919320384350916\n",
      "        total_loss: 62.894240695635474\n",
      "        vf_explained_var: 0.5313488973180452\n",
      "        vf_loss: 62.95271230538686\n",
      "  num_agent_steps_sampled: 4588470\n",
      "  num_agent_steps_trained: 4588470\n",
      "  num_steps_sampled: 4588500\n",
      "  num_steps_trained: 4588500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 575\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.757894736842108\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0578947368421\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.0\n",
      "  player_1: 33.333333333333336\n",
      "  player_2: 41.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.6266666666666665\n",
      "  player_1: -2.8733333333333335\n",
      "  player_2: 3.246666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -42.666666666666664\n",
      "  player_1: -32.666666666666664\n",
      "  player_2: -37.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08724637826708728\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30248050126144455\n",
      "  mean_inference_ms: 1.6265221469058673\n",
      "  mean_raw_obs_processing_ms: 0.21197972386446973\n",
      "time_since_restore: 9250.758651971817\n",
      "time_this_iter_s: 15.434127569198608\n",
      "time_total_s: 9250.758651971817\n",
      "timers:\n",
      "  learn_throughput: 556.547\n",
      "  learn_time_ms: 14338.418\n",
      "  load_throughput: 1131797.893\n",
      "  load_time_ms: 7.051\n",
      "  sample_throughput: 508.096\n",
      "  sample_time_ms: 15705.68\n",
      "  update_time_ms: 5.6\n",
      "timestamp: 1643545485\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4588500\n",
      "training_iteration: 575\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4604431\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-25-17\n",
      "done: false\n",
      "episode_len_mean: 137.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 58\n",
      "episodes_total: 23973\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46636755575736366\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016591509093554085\n",
      "        policy_loss: -0.11519864197354764\n",
      "        total_loss: 51.042775843938195\n",
      "        vf_explained_var: 0.18175767749547958\n",
      "        vf_loss: 51.146775271097816\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5184052953124046\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016616086888137715\n",
      "        policy_loss: -0.047279463230321804\n",
      "        total_loss: 46.57882051785787\n",
      "        vf_explained_var: 0.19744257181882857\n",
      "        vf_loss: 46.6148842604955\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4788523251811663\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017187094353057545\n",
      "        policy_loss: -0.04303189990731577\n",
      "        total_loss: 63.27393856287003\n",
      "        vf_explained_var: 0.17624886284271876\n",
      "        vf_loss: 63.30536930322647\n",
      "  num_agent_steps_sampled: 4604431\n",
      "  num_agent_steps_trained: 4604431\n",
      "  num_steps_sampled: 4604460\n",
      "  num_steps_trained: 4604460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 577\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.31111111111111\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.03888888888889\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.33333333333333\n",
      "  player_1: 35.666666666666664\n",
      "  player_2: 42.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.8133333333333332\n",
      "  player_1: -3.9666666666666663\n",
      "  player_2: 5.153333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -29.666666666666664\n",
      "  player_1: -59.666666666666664\n",
      "  player_2: -33.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08723181986227059\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3027736878889325\n",
      "  mean_inference_ms: 1.6279330569393264\n",
      "  mean_raw_obs_processing_ms: 0.21201426515266344\n",
      "time_since_restore: 9282.435306549072\n",
      "time_this_iter_s: 14.845542192459106\n",
      "time_total_s: 9282.435306549072\n",
      "timers:\n",
      "  learn_throughput: 550.998\n",
      "  learn_time_ms: 14482.82\n",
      "  load_throughput: 1134056.581\n",
      "  load_time_ms: 7.037\n",
      "  sample_throughput: 509.238\n",
      "  sample_time_ms: 15670.463\n",
      "  update_time_ms: 5.606\n",
      "timestamp: 1643545517\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4604460\n",
      "training_iteration: 577\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4620390\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-25-50\n",
      "done: false\n",
      "episode_len_mean: 136.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 56\n",
      "episodes_total: 24087\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45698628877600034\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014590413261045872\n",
      "        policy_loss: -0.11911684986281519\n",
      "        total_loss: 48.71434735536575\n",
      "        vf_explained_var: 0.2734904032945633\n",
      "        vf_loss: 48.82361576159795\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5089132215082646\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016902310050429983\n",
      "        policy_loss: -0.06458659191305438\n",
      "        total_loss: 37.284161500930786\n",
      "        vf_explained_var: 0.06888846596082052\n",
      "        vf_loss: 37.33733898242315\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4706973234812419\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01697983416904549\n",
      "        policy_loss: -0.03630076869701346\n",
      "        total_loss: 52.58878277460734\n",
      "        vf_explained_var: 0.2681412105758985\n",
      "        vf_loss: 52.61362223943075\n",
      "  num_agent_steps_sampled: 4620390\n",
      "  num_agent_steps_trained: 4620390\n",
      "  num_steps_sampled: 4620420\n",
      "  num_steps_trained: 4620420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 579\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.885000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.05499999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 45.666666666666664\n",
      "  player_1: 30.666666666666664\n",
      "  player_2: 46.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.9033333333333333\n",
      "  player_1: -3.146666666666667\n",
      "  player_2: 4.243333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -57.666666666666664\n",
      "  player_1: -40.0\n",
      "  player_2: -37.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08730315873816004\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3024481626900178\n",
      "  mean_inference_ms: 1.6262410406809555\n",
      "  mean_raw_obs_processing_ms: 0.212055015741661\n",
      "time_since_restore: 9315.337961435318\n",
      "time_this_iter_s: 16.129209756851196\n",
      "time_total_s: 9315.337961435318\n",
      "timers:\n",
      "  learn_throughput: 543.13\n",
      "  learn_time_ms: 14692.608\n",
      "  load_throughput: 1130578.349\n",
      "  load_time_ms: 7.058\n",
      "  sample_throughput: 501.432\n",
      "  sample_time_ms: 15914.41\n",
      "  update_time_ms: 5.58\n",
      "timestamp: 1643545550\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4620420\n",
      "training_iteration: 579\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4636350\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-26-23\n",
      "done: false\n",
      "episode_len_mean: 139.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 57\n",
      "episodes_total: 24204\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4644153009355068\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01551957329476058\n",
      "        policy_loss: -0.08205888884762923\n",
      "        total_loss: 61.187528171539306\n",
      "        vf_explained_var: 0.2664553928375244\n",
      "        vf_loss: 61.25911169528961\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5088518224159877\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01672969891510282\n",
      "        policy_loss: -0.08838688857853412\n",
      "        total_loss: 47.31227017402649\n",
      "        vf_explained_var: 0.11398476968208948\n",
      "        vf_loss: 47.389364376068116\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47054113551974297\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01572876092818736\n",
      "        policy_loss: -0.07290514458902181\n",
      "        total_loss: 78.76432563145956\n",
      "        vf_explained_var: 0.18442176123460133\n",
      "        vf_loss: 78.82661329269409\n",
      "  num_agent_steps_sampled: 4636350\n",
      "  num_agent_steps_trained: 4636350\n",
      "  num_steps_sampled: 4636380\n",
      "  num_steps_trained: 4636380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 581\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.923809523809522\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0142857142857\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.0\n",
      "  player_1: 35.0\n",
      "  player_2: 32.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.6366666666666663\n",
      "  player_1: -2.6133333333333337\n",
      "  player_2: 1.9766666666666668\n",
      "policy_reward_min:\n",
      "  player_0: -28.0\n",
      "  player_1: -38.333333333333336\n",
      "  player_2: -52.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08730571119502858\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3025748368645925\n",
      "  mean_inference_ms: 1.6271458666602334\n",
      "  mean_raw_obs_processing_ms: 0.2120272973251315\n",
      "time_since_restore: 9348.661643505096\n",
      "time_this_iter_s: 17.140056848526\n",
      "time_total_s: 9348.661643505096\n",
      "timers:\n",
      "  learn_throughput: 547.088\n",
      "  learn_time_ms: 14586.33\n",
      "  load_throughput: 1107940.361\n",
      "  load_time_ms: 7.203\n",
      "  sample_throughput: 503.754\n",
      "  sample_time_ms: 15841.067\n",
      "  update_time_ms: 5.559\n",
      "timestamp: 1643545583\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4636380\n",
      "training_iteration: 581\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4652310\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-26-54\n",
      "done: false\n",
      "episode_len_mean: 137.84\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 57\n",
      "episodes_total: 24318\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4628212849299113\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01712480506010157\n",
      "        policy_loss: -0.08283340898031989\n",
      "        total_loss: 49.730839381217955\n",
      "        vf_explained_var: 0.2858935771385829\n",
      "        vf_loss: 49.80211346785227\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5058296154439449\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016838715987349248\n",
      "        policy_loss: -0.07191453454705576\n",
      "        total_loss: 27.85662046432495\n",
      "        vf_explained_var: 0.3952918953696887\n",
      "        vf_loss: 27.917168862819672\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4806123615304629\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017803849772323778\n",
      "        policy_loss: -0.08955740666016937\n",
      "        total_loss: 53.347233697573344\n",
      "        vf_explained_var: 0.389091424147288\n",
      "        vf_loss: 53.42477341175079\n",
      "  num_agent_steps_sampled: 4652310\n",
      "  num_agent_steps_trained: 4652310\n",
      "  num_steps_sampled: 4652340\n",
      "  num_steps_trained: 4652340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 583\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.81111111111111\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.66666666666667\n",
      "  player_1: 27.333333333333336\n",
      "  player_2: 33.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 5.546666666666665\n",
      "  player_1: -3.8433333333333337\n",
      "  player_2: 1.2966666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -29.333333333333336\n",
      "  player_1: -57.33333333333333\n",
      "  player_2: -34.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08722700129515715\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30227363875885965\n",
      "  mean_inference_ms: 1.6267615225367433\n",
      "  mean_raw_obs_processing_ms: 0.21190512553590893\n",
      "time_since_restore: 9379.737940788269\n",
      "time_this_iter_s: 15.05224871635437\n",
      "time_total_s: 9379.737940788269\n",
      "timers:\n",
      "  learn_throughput: 541.139\n",
      "  learn_time_ms: 14746.673\n",
      "  load_throughput: 1105200.198\n",
      "  load_time_ms: 7.22\n",
      "  sample_throughput: 500.874\n",
      "  sample_time_ms: 15932.146\n",
      "  update_time_ms: 5.653\n",
      "timestamp: 1643545614\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4652340\n",
      "training_iteration: 583\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4668270\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-27-27\n",
      "done: false\n",
      "episode_len_mean: 140.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 59\n",
      "episodes_total: 24435\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4737037208676338\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016140151220547714\n",
      "        policy_loss: -0.07210128257671992\n",
      "        total_loss: 70.02956297079722\n",
      "        vf_explained_var: 0.2305675709247589\n",
      "        vf_loss: 70.09076982339224\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5095109138886134\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01602884527167286\n",
      "        policy_loss: -0.08618840718641878\n",
      "        total_loss: 68.17144049962361\n",
      "        vf_explained_var: 0.3690341725945473\n",
      "        vf_loss: 68.24680915196737\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48865476628144583\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017081734670719016\n",
      "        policy_loss: -0.0696374094982942\n",
      "        total_loss: 68.14763144652049\n",
      "        vf_explained_var: 0.3689732780059179\n",
      "        vf_loss: 68.20573858261109\n",
      "  num_agent_steps_sampled: 4668270\n",
      "  num_agent_steps_trained: 4668270\n",
      "  num_steps_sampled: 4668300\n",
      "  num_steps_trained: 4668300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 585\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.985\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.07999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.0\n",
      "  player_1: 41.33333333333333\n",
      "  player_2: 41.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 2.2300000000000004\n",
      "  player_1: -0.30999999999999983\n",
      "  player_2: 1.08\n",
      "policy_reward_min:\n",
      "  player_0: -55.333333333333336\n",
      "  player_1: -57.66666666666667\n",
      "  player_2: -48.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0870931368741531\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30209670233992547\n",
      "  mean_inference_ms: 1.6247419420743705\n",
      "  mean_raw_obs_processing_ms: 0.2118302235568691\n",
      "time_since_restore: 9412.813229084015\n",
      "time_this_iter_s: 16.001442432403564\n",
      "time_total_s: 9412.813229084015\n",
      "timers:\n",
      "  learn_throughput: 531.507\n",
      "  learn_time_ms: 15013.915\n",
      "  load_throughput: 1085673.237\n",
      "  load_time_ms: 7.35\n",
      "  sample_throughput: 493.517\n",
      "  sample_time_ms: 16169.665\n",
      "  update_time_ms: 5.782\n",
      "timestamp: 1643545647\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4668300\n",
      "training_iteration: 585\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4684230\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-27-57\n",
      "done: false\n",
      "episode_len_mean: 145.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 58\n",
      "episodes_total: 24545\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44567043021321295\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016248435600301436\n",
      "        policy_loss: -0.08346659009034435\n",
      "        total_loss: 54.41754519462585\n",
      "        vf_explained_var: 0.25088876456022263\n",
      "        vf_loss: 54.490043959617616\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.519251022040844\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01675321281204333\n",
      "        policy_loss: -0.06540577037570378\n",
      "        total_loss: 39.01862249215444\n",
      "        vf_explained_var: 0.290170013209184\n",
      "        vf_loss: 39.07271967728933\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48566042800744375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0171308651908051\n",
      "        policy_loss: -0.08900214413491388\n",
      "        total_loss: 66.85871397495269\n",
      "        vf_explained_var: 0.3567911028862\n",
      "        vf_loss: 66.93615262349446\n",
      "  num_agent_steps_sampled: 4684230\n",
      "  num_agent_steps_trained: 4684230\n",
      "  num_steps_sampled: 4684260\n",
      "  num_steps_trained: 4684260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 587\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.483333333333333\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.07777777777775\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 38.333333333333336\n",
      "  player_2: 28.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.396666666666667\n",
      "  player_1: -2.423333333333333\n",
      "  player_2: 2.0266666666666673\n",
      "policy_reward_min:\n",
      "  player_0: -27.0\n",
      "  player_1: -39.666666666666664\n",
      "  player_2: -43.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08719712024326227\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3020299326544904\n",
      "  mean_inference_ms: 1.6243491843565585\n",
      "  mean_raw_obs_processing_ms: 0.21186694306433992\n",
      "time_since_restore: 9442.746819972992\n",
      "time_this_iter_s: 14.758845329284668\n",
      "time_total_s: 9442.746819972992\n",
      "timers:\n",
      "  learn_throughput: 537.709\n",
      "  learn_time_ms: 14840.751\n",
      "  load_throughput: 1070920.392\n",
      "  load_time_ms: 7.452\n",
      "  sample_throughput: 496.705\n",
      "  sample_time_ms: 16065.868\n",
      "  update_time_ms: 5.8\n",
      "timestamp: 1643545677\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4684260\n",
      "training_iteration: 587\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4700190\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-28-27\n",
      "done: false\n",
      "episode_len_mean: 131.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 59\n",
      "episodes_total: 24669\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45259354084730147\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016583533106738362\n",
      "        policy_loss: -0.09361297133068244\n",
      "        total_loss: 62.22222399711609\n",
      "        vf_explained_var: 0.1667217500011126\n",
      "        vf_loss: 62.304643074671425\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5007616834839185\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016272294436051122\n",
      "        policy_loss: -0.055317187315473956\n",
      "        total_loss: 34.34392999966939\n",
      "        vf_explained_var: 0.13546739429235458\n",
      "        vf_loss: 34.38826333204905\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46591496194402376\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016133096831874052\n",
      "        policy_loss: -0.06171166567287097\n",
      "        total_loss: 58.13636385281881\n",
      "        vf_explained_var: 0.2792426775892576\n",
      "        vf_loss: 58.187185567220055\n",
      "  num_agent_steps_sampled: 4700190\n",
      "  num_agent_steps_trained: 4700190\n",
      "  num_steps_sampled: 4700220\n",
      "  num_steps_trained: 4700220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 589\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.31578947368421\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09473684210525\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.666666666666664\n",
      "  player_1: 23.333333333333336\n",
      "  player_2: 31.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.563333333333333\n",
      "  player_1: -3.6366666666666663\n",
      "  player_2: 4.073333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -24.333333333333336\n",
      "  player_1: -43.333333333333336\n",
      "  player_2: -30.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08709627317484976\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30172102592443273\n",
      "  mean_inference_ms: 1.6243437287117082\n",
      "  mean_raw_obs_processing_ms: 0.21163477458020138\n",
      "time_since_restore: 9472.51433801651\n",
      "time_this_iter_s: 15.134762525558472\n",
      "time_total_s: 9472.51433801651\n",
      "timers:\n",
      "  learn_throughput: 549.405\n",
      "  learn_time_ms: 14524.813\n",
      "  load_throughput: 1075690.684\n",
      "  load_time_ms: 7.418\n",
      "  sample_throughput: 503.672\n",
      "  sample_time_ms: 15843.654\n",
      "  update_time_ms: 5.776\n",
      "timestamp: 1643545707\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4700220\n",
      "training_iteration: 589\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4716150\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-28-59\n",
      "done: false\n",
      "episode_len_mean: 128.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 59\n",
      "episodes_total: 24790\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.465438235749801\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01790393477453032\n",
      "        policy_loss: -0.08324170224989454\n",
      "        total_loss: 47.79969122171402\n",
      "        vf_explained_var: 0.30762153456608454\n",
      "        vf_loss: 47.87084761381149\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5081926630437374\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014617193635165032\n",
      "        policy_loss: -0.12427466957829893\n",
      "        total_loss: 40.70632791201274\n",
      "        vf_explained_var: 0.17274169355630875\n",
      "        vf_loss: 40.82073588569959\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48169803351163865\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017053481032948146\n",
      "        policy_loss: -0.025595932540794213\n",
      "        total_loss: 49.06575184027354\n",
      "        vf_explained_var: 0.3636169351140658\n",
      "        vf_loss: 49.079836685657504\n",
      "  num_agent_steps_sampled: 4716150\n",
      "  num_agent_steps_trained: 4716150\n",
      "  num_steps_sampled: 4716180\n",
      "  num_steps_trained: 4716180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 591\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.089473684210526\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09473684210525\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.666666666666664\n",
      "  player_1: 46.666666666666664\n",
      "  player_2: 33.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 5.926666666666668\n",
      "  player_1: -3.753333333333334\n",
      "  player_2: 0.8266666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -27.0\n",
      "  player_1: -51.666666666666664\n",
      "  player_2: -40.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0871677587355189\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30207972462684324\n",
      "  mean_inference_ms: 1.6260615974258519\n",
      "  mean_raw_obs_processing_ms: 0.21191547240511022\n",
      "time_since_restore: 9504.584498643875\n",
      "time_this_iter_s: 15.78580904006958\n",
      "time_total_s: 9504.584498643875\n",
      "timers:\n",
      "  learn_throughput: 554.932\n",
      "  learn_time_ms: 14380.134\n",
      "  load_throughput: 1099427.989\n",
      "  load_time_ms: 7.258\n",
      "  sample_throughput: 505.783\n",
      "  sample_time_ms: 15777.52\n",
      "  update_time_ms: 5.973\n",
      "timestamp: 1643545739\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4716180\n",
      "training_iteration: 591\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4732110\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-29-32\n",
      "done: false\n",
      "episode_len_mean: 137.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 60\n",
      "episodes_total: 24906\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4743297612667084\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01820791145628997\n",
      "        policy_loss: -0.07576250734428565\n",
      "        total_loss: 51.80950748443603\n",
      "        vf_explained_var: 0.3624875621994336\n",
      "        vf_loss: 51.87297963937124\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4946386486788591\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015409573582898399\n",
      "        policy_loss: -0.09038942373047272\n",
      "        total_loss: 39.294243590831755\n",
      "        vf_explained_var: 0.13487446000178654\n",
      "        vf_loss: 39.37423164685567\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46193295473853746\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016445017549970846\n",
      "        policy_loss: -0.07634676539028684\n",
      "        total_loss: 44.2049240175883\n",
      "        vf_explained_var: 0.2711798666914304\n",
      "        vf_loss: 44.270170319875085\n",
      "  num_agent_steps_sampled: 4732110\n",
      "  num_agent_steps_trained: 4732110\n",
      "  num_steps_sampled: 4732140\n",
      "  num_steps_trained: 4732140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 593\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.504761904761901\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 41.0\n",
      "  player_2: 47.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.42\n",
      "  player_1: -1.31\n",
      "  player_2: 1.8900000000000003\n",
      "policy_reward_min:\n",
      "  player_0: -54.0\n",
      "  player_1: -45.666666666666664\n",
      "  player_2: -38.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08710932249183027\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30124894576782096\n",
      "  mean_inference_ms: 1.6218092582561983\n",
      "  mean_raw_obs_processing_ms: 0.21167576413109257\n",
      "time_since_restore: 9537.135340690613\n",
      "time_this_iter_s: 16.513823747634888\n",
      "time_total_s: 9537.135340690613\n",
      "timers:\n",
      "  learn_throughput: 549.321\n",
      "  learn_time_ms: 14527.015\n",
      "  load_throughput: 1054971.268\n",
      "  load_time_ms: 7.564\n",
      "  sample_throughput: 510.688\n",
      "  sample_time_ms: 15625.983\n",
      "  update_time_ms: 6.057\n",
      "timestamp: 1643545772\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4732140\n",
      "training_iteration: 593\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4748071\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-30-03\n",
      "done: false\n",
      "episode_len_mean: 134.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 57\n",
      "episodes_total: 25023\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4572215464214484\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01592994334292598\n",
      "        policy_loss: -0.04424827290078004\n",
      "        total_loss: 57.09160984675089\n",
      "        vf_explained_var: 0.35074765970309574\n",
      "        vf_loss: 57.12510526657105\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5113549374540647\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016637081733343564\n",
      "        policy_loss: -0.10191651440225541\n",
      "        total_loss: 37.133469568888344\n",
      "        vf_explained_var: 0.22764779657125472\n",
      "        vf_loss: 37.22415600617727\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47958444356918334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01590884626112711\n",
      "        policy_loss: -0.07854609228670598\n",
      "        total_loss: 53.81100589911143\n",
      "        vf_explained_var: 0.2644187808036804\n",
      "        vf_loss: 53.87881330649058\n",
      "  num_agent_steps_sampled: 4748071\n",
      "  num_agent_steps_trained: 4748071\n",
      "  num_steps_sampled: 4748100\n",
      "  num_steps_trained: 4748100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 595\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.763157894736842\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.333333333333336\n",
      "  player_1: 33.666666666666664\n",
      "  player_2: 30.0\n",
      "policy_reward_mean:\n",
      "  player_0: 4.709999999999999\n",
      "  player_1: -1.4499999999999997\n",
      "  player_2: -0.25999999999999956\n",
      "policy_reward_min:\n",
      "  player_0: -38.333333333333336\n",
      "  player_1: -29.66666666666667\n",
      "  player_2: -47.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08705099337845454\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3017196510466819\n",
      "  mean_inference_ms: 1.6236914805669118\n",
      "  mean_raw_obs_processing_ms: 0.21171979621703393\n",
      "time_since_restore: 9568.022508382797\n",
      "time_this_iter_s: 15.369414806365967\n",
      "time_total_s: 9568.022508382797\n",
      "timers:\n",
      "  learn_throughput: 557.639\n",
      "  learn_time_ms: 14310.344\n",
      "  load_throughput: 1060368.95\n",
      "  load_time_ms: 7.526\n",
      "  sample_throughput: 510.941\n",
      "  sample_time_ms: 15618.251\n",
      "  update_time_ms: 5.915\n",
      "timestamp: 1643545803\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4748100\n",
      "training_iteration: 595\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4764031\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-30-36\n",
      "done: false\n",
      "episode_len_mean: 142.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 50\n",
      "episodes_total: 25125\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4247723193466663\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013755592736251099\n",
      "        policy_loss: -0.09403103990480303\n",
      "        total_loss: 37.76083361705144\n",
      "        vf_explained_var: 0.2521515601873398\n",
      "        vf_loss: 37.84557965755462\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4943235791971286\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015431754696872796\n",
      "        policy_loss: -0.035205468712374566\n",
      "        total_loss: 34.875585076014204\n",
      "        vf_explained_var: 0.22841747591892878\n",
      "        vf_loss: 34.90037411610285\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4487046546737353\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016063865872994257\n",
      "        policy_loss: -0.07974732080474496\n",
      "        total_loss: 42.74353979508082\n",
      "        vf_explained_var: 0.2996733277042707\n",
      "        vf_loss: 42.81244424978892\n",
      "  num_agent_steps_sampled: 4764031\n",
      "  num_agent_steps_trained: 4764031\n",
      "  num_steps_sampled: 4764060\n",
      "  num_steps_trained: 4764060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 597\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.522222222222224\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.0\n",
      "  player_1: 29.66666666666667\n",
      "  player_2: 33.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 0.9266666666666669\n",
      "  player_1: -1.8233333333333328\n",
      "  player_2: 3.8966666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -34.666666666666664\n",
      "  player_1: -27.666666666666664\n",
      "  player_2: -53.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08691693448172406\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3015008136627038\n",
      "  mean_inference_ms: 1.6222278329766424\n",
      "  mean_raw_obs_processing_ms: 0.21150617957824078\n",
      "time_since_restore: 9600.792209625244\n",
      "time_this_iter_s: 14.65424370765686\n",
      "time_total_s: 9600.792209625244\n",
      "timers:\n",
      "  learn_throughput: 546.783\n",
      "  learn_time_ms: 14594.446\n",
      "  load_throughput: 1070016.557\n",
      "  load_time_ms: 7.458\n",
      "  sample_throughput: 503.534\n",
      "  sample_time_ms: 15847.972\n",
      "  update_time_ms: 5.823\n",
      "timestamp: 1643545836\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4764060\n",
      "training_iteration: 597\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4779990\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-31-06\n",
      "done: false\n",
      "episode_len_mean: 149.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 58\n",
      "episodes_total: 25241\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4705138282974561\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01549547460188478\n",
      "        policy_loss: -0.083643008080932\n",
      "        total_loss: 57.5406857474645\n",
      "        vf_explained_var: 0.19655986895163854\n",
      "        vf_loss: 57.61386929194133\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4952528348068396\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01517909665546237\n",
      "        policy_loss: -0.07748173642282685\n",
      "        total_loss: 70.02140715281169\n",
      "        vf_explained_var: 0.30546391357978186\n",
      "        vf_loss: 70.08864251613618\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48249060720205306\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015617708278705322\n",
      "        policy_loss: -0.0557488897566994\n",
      "        total_loss: 77.26571009318035\n",
      "        vf_explained_var: 0.17508524000644685\n",
      "        vf_loss: 77.31091701189676\n",
      "  num_agent_steps_sampled: 4779990\n",
      "  num_agent_steps_trained: 4779990\n",
      "  num_steps_sampled: 4780020\n",
      "  num_steps_trained: 4780020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 599\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.700000000000005\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.33333333333333\n",
      "  player_1: 34.0\n",
      "  player_2: 31.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.6333333333333333\n",
      "  player_1: -0.9866666666666667\n",
      "  player_2: 0.35333333333333283\n",
      "policy_reward_min:\n",
      "  player_0: -35.66666666666667\n",
      "  player_1: -50.666666666666664\n",
      "  player_2: -53.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08718580736357963\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3015539407467185\n",
      "  mean_inference_ms: 1.6235279541841658\n",
      "  mean_raw_obs_processing_ms: 0.21177752567381666\n",
      "time_since_restore: 9631.361379146576\n",
      "time_this_iter_s: 14.749202966690063\n",
      "time_total_s: 9631.361379146576\n",
      "timers:\n",
      "  learn_throughput: 543.711\n",
      "  learn_time_ms: 14676.924\n",
      "  load_throughput: 1046693.787\n",
      "  load_time_ms: 7.624\n",
      "  sample_throughput: 500.149\n",
      "  sample_time_ms: 15955.257\n",
      "  update_time_ms: 5.802\n",
      "timestamp: 1643545866\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4780020\n",
      "training_iteration: 599\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4795951\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-31-37\n",
      "done: false\n",
      "episode_len_mean: 136.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 57\n",
      "episodes_total: 25359\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44471257959802946\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014913284656143637\n",
      "        policy_loss: -0.08471945771947503\n",
      "        total_loss: 57.044545316696166\n",
      "        vf_explained_var: 0.31544598092635473\n",
      "        vf_loss: 57.11919846375783\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4947503831485907\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015167489444726432\n",
      "        policy_loss: -0.1046237581782043\n",
      "        total_loss: 49.68939324537913\n",
      "        vf_explained_var: 0.15395122438669204\n",
      "        vf_loss: 49.783778834342954\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48218524212638536\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017628116542280924\n",
      "        policy_loss: -0.032024099612608554\n",
      "        total_loss: 53.62692202647527\n",
      "        vf_explained_var: 0.31427172938982645\n",
      "        vf_loss: 53.64704722245534\n",
      "  num_agent_steps_sampled: 4795951\n",
      "  num_agent_steps_trained: 4795951\n",
      "  num_steps_sampled: 4795980\n",
      "  num_steps_trained: 4795980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 601\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.094736842105261\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.0\n",
      "  player_1: 42.666666666666664\n",
      "  player_2: 39.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.003333333333334\n",
      "  player_1: -0.5966666666666666\n",
      "  player_2: 1.5933333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -37.333333333333336\n",
      "  player_1: -37.666666666666664\n",
      "  player_2: -41.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08709916845174648\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3009847492875686\n",
      "  mean_inference_ms: 1.6199975048406805\n",
      "  mean_raw_obs_processing_ms: 0.21157761011602516\n",
      "time_since_restore: 9661.406383037567\n",
      "time_this_iter_s: 14.972873449325562\n",
      "time_total_s: 9661.406383037567\n",
      "timers:\n",
      "  learn_throughput: 550.713\n",
      "  learn_time_ms: 14490.297\n",
      "  load_throughput: 1026190.767\n",
      "  load_time_ms: 7.776\n",
      "  sample_throughput: 505.885\n",
      "  sample_time_ms: 15774.345\n",
      "  update_time_ms: 5.608\n",
      "timestamp: 1643545897\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4795980\n",
      "training_iteration: 601\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4811910\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-32-07\n",
      "done: false\n",
      "episode_len_mean: 127.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 61\n",
      "episodes_total: 25480\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4751326237618923\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016473676564661824\n",
      "        policy_loss: -0.09934413646503042\n",
      "        total_loss: 55.09732820351918\n",
      "        vf_explained_var: 0.2806514362494151\n",
      "        vf_loss: 55.185552803675336\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4811053641140461\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01499608446317249\n",
      "        policy_loss: -0.09893473220368226\n",
      "        total_loss: 60.99204383691152\n",
      "        vf_explained_var: 0.22501958320538204\n",
      "        vf_loss: 61.08085605462392\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47118875314791997\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01794594402467093\n",
      "        policy_loss: -0.05857169892638922\n",
      "        total_loss: 82.3771180788676\n",
      "        vf_explained_var: 0.3384769731760025\n",
      "        vf_loss: 82.42357610384623\n",
      "  num_agent_steps_sampled: 4811910\n",
      "  num_agent_steps_trained: 4811910\n",
      "  num_steps_sampled: 4811940\n",
      "  num_steps_trained: 4811940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 603\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.473684210526315\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.05789473684209\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 42.666666666666664\n",
      "  player_1: 28.333333333333336\n",
      "  player_2: 29.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.7933333333333334\n",
      "  player_1: -0.26666666666666666\n",
      "  player_2: 1.4733333333333336\n",
      "policy_reward_min:\n",
      "  player_0: -42.33333333333333\n",
      "  player_1: -39.0\n",
      "  player_2: -40.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08702543796522351\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3012706849570587\n",
      "  mean_inference_ms: 1.622621692076808\n",
      "  mean_raw_obs_processing_ms: 0.21156993712831848\n",
      "time_since_restore: 9691.58337020874\n",
      "time_this_iter_s: 15.538430213928223\n",
      "time_total_s: 9691.58337020874\n",
      "timers:\n",
      "  learn_throughput: 559.892\n",
      "  learn_time_ms: 14252.75\n",
      "  load_throughput: 1063529.383\n",
      "  load_time_ms: 7.503\n",
      "  sample_throughput: 512.476\n",
      "  sample_time_ms: 15571.458\n",
      "  update_time_ms: 5.688\n",
      "timestamp: 1643545927\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4811940\n",
      "training_iteration: 603\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4827870\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-32-38\n",
      "done: false\n",
      "episode_len_mean: 134.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 58\n",
      "episodes_total: 25594\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49491776471336685\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018071427830334035\n",
      "        policy_loss: -0.023119023277734716\n",
      "        total_loss: 42.13735690116882\n",
      "        vf_explained_var: 0.23952860832214357\n",
      "        vf_loss: 42.14827752749125\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5015386065344015\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015848873979002937\n",
      "        policy_loss: -0.12458725696429611\n",
      "        total_loss: 56.22351203282674\n",
      "        vf_explained_var: 0.37203451454639436\n",
      "        vf_loss: 56.337401531537374\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4790110171834628\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016068324374215688\n",
      "        policy_loss: -0.0767021385828654\n",
      "        total_loss: 49.24361255645752\n",
      "        vf_explained_var: 0.19233274151881535\n",
      "        vf_loss: 49.30946873982747\n",
      "  num_agent_steps_sampled: 4827870\n",
      "  num_agent_steps_trained: 4827870\n",
      "  num_steps_sampled: 4827900\n",
      "  num_steps_trained: 4827900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 605\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.745\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 23.0\n",
      "  player_1: 32.333333333333336\n",
      "  player_2: 43.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 0.44666666666666643\n",
      "  player_1: -1.5333333333333337\n",
      "  player_2: 4.086666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -26.666666666666664\n",
      "  player_1: -44.666666666666664\n",
      "  player_2: -34.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08707080808498727\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3012988388382104\n",
      "  mean_inference_ms: 1.622185425967627\n",
      "  mean_raw_obs_processing_ms: 0.21162683729195514\n",
      "time_since_restore: 9722.329488515854\n",
      "time_this_iter_s: 15.863193035125732\n",
      "time_total_s: 9722.329488515854\n",
      "timers:\n",
      "  learn_throughput: 560.688\n",
      "  learn_time_ms: 14232.502\n",
      "  load_throughput: 985938.627\n",
      "  load_time_ms: 8.094\n",
      "  sample_throughput: 517.802\n",
      "  sample_time_ms: 15411.307\n",
      "  update_time_ms: 7.978\n",
      "timestamp: 1643545958\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4827900\n",
      "training_iteration: 605\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4843830\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-33-10\n",
      "done: false\n",
      "episode_len_mean: 137.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 59\n",
      "episodes_total: 25709\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4477590962747733\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015549214465145268\n",
      "        policy_loss: -0.07729667559886973\n",
      "        total_loss: 48.736648008028666\n",
      "        vf_explained_var: 0.444111775457859\n",
      "        vf_loss: 48.803448922634125\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5133832064270973\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016178763935440658\n",
      "        policy_loss: -0.08519254689260075\n",
      "        total_loss: 81.88222413698833\n",
      "        vf_explained_var: 0.2508094562093417\n",
      "        vf_loss: 81.95649581432343\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4720322926839193\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016550580822031407\n",
      "        policy_loss: -0.05140724983687202\n",
      "        total_loss: 73.39554691155752\n",
      "        vf_explained_var: 0.5363258742292722\n",
      "        vf_loss: 73.43578262488047\n",
      "  num_agent_steps_sampled: 4843830\n",
      "  num_agent_steps_trained: 4843830\n",
      "  num_steps_sampled: 4843860\n",
      "  num_steps_trained: 4843860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 607\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.729999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.66666666666667\n",
      "  player_1: 44.33333333333333\n",
      "  player_2: 40.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -0.016666666666666642\n",
      "  player_1: 0.013333333333333109\n",
      "  player_2: 3.003333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -37.666666666666664\n",
      "  player_1: -40.666666666666664\n",
      "  player_2: -54.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0870694823842485\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30141342052863745\n",
      "  mean_inference_ms: 1.6234376951432141\n",
      "  mean_raw_obs_processing_ms: 0.21159993511509165\n",
      "time_since_restore: 9754.606622457504\n",
      "time_this_iter_s: 16.79346203804016\n",
      "time_total_s: 9754.606622457504\n",
      "timers:\n",
      "  learn_throughput: 562.613\n",
      "  learn_time_ms: 14183.823\n",
      "  load_throughput: 928950.359\n",
      "  load_time_ms: 8.59\n",
      "  sample_throughput: 525.0\n",
      "  sample_time_ms: 15199.992\n",
      "  update_time_ms: 8.048\n",
      "timestamp: 1643545990\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4843860\n",
      "training_iteration: 607\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4859790\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-33-40\n",
      "done: false\n",
      "episode_len_mean: 124.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 61\n",
      "episodes_total: 25832\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47557602390646936\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015188952896668297\n",
      "        policy_loss: -0.08225094650561611\n",
      "        total_loss: 45.03116284370422\n",
      "        vf_explained_var: 0.28704709937175116\n",
      "        vf_loss: 45.10316129525503\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5088870017230511\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01639332393660387\n",
      "        policy_loss: -0.06237059590096275\n",
      "        total_loss: 49.039903349876404\n",
      "        vf_explained_var: 0.22640710482994716\n",
      "        vf_loss: 49.09120838960012\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4810446538031101\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016915145528226958\n",
      "        policy_loss: -0.07605567618583639\n",
      "        total_loss: 64.3245868841807\n",
      "        vf_explained_var: 0.21408688594897587\n",
      "        vf_loss: 64.38922490437825\n",
      "  num_agent_steps_sampled: 4859790\n",
      "  num_agent_steps_trained: 4859790\n",
      "  num_steps_sampled: 4859820\n",
      "  num_steps_trained: 4859820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 609\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.972222222222221\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.333333333333336\n",
      "  player_1: 28.666666666666664\n",
      "  player_2: 33.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.7266666666666672\n",
      "  player_1: -0.4133333333333332\n",
      "  player_2: 1.6866666666666663\n",
      "policy_reward_min:\n",
      "  player_0: -41.0\n",
      "  player_1: -28.666666666666664\n",
      "  player_2: -47.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0869128970658257\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3003981107132633\n",
      "  mean_inference_ms: 1.6185844851452975\n",
      "  mean_raw_obs_processing_ms: 0.2112928319315791\n",
      "time_since_restore: 9784.775523424149\n",
      "time_this_iter_s: 15.108614683151245\n",
      "time_total_s: 9784.775523424149\n",
      "timers:\n",
      "  learn_throughput: 564.305\n",
      "  learn_time_ms: 14141.279\n",
      "  load_throughput: 883967.513\n",
      "  load_time_ms: 9.027\n",
      "  sample_throughput: 520.21\n",
      "  sample_time_ms: 15339.945\n",
      "  update_time_ms: 8.042\n",
      "timestamp: 1643546020\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4859820\n",
      "training_iteration: 609\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4875750\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-34-12\n",
      "done: false\n",
      "episode_len_mean: 130.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 58\n",
      "episodes_total: 25957\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.466565930445989\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016127761039070567\n",
      "        policy_loss: -0.053117379918694495\n",
      "        total_loss: 65.23430758317312\n",
      "        vf_explained_var: 0.20008920699357988\n",
      "        vf_loss: 65.27653917789459\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49205066631237665\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01593084602829549\n",
      "        policy_loss: -0.1049397281681498\n",
      "        total_loss: 49.578000875314075\n",
      "        vf_explained_var: 0.284909986158212\n",
      "        vf_loss: 49.672187340259555\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49573903933167457\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017098347771385913\n",
      "        policy_loss: -0.0928941480986153\n",
      "        total_loss: 56.20308065891266\n",
      "        vf_explained_var: 0.2674848442276319\n",
      "        vf_loss: 56.28443327109019\n",
      "  num_agent_steps_sampled: 4875750\n",
      "  num_agent_steps_trained: 4875750\n",
      "  num_steps_sampled: 4875780\n",
      "  num_steps_trained: 4875780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 611\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.054999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.12999999999997\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 39.666666666666664\n",
      "  player_2: 50.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.3133333333333334\n",
      "  player_1: -0.4266666666666666\n",
      "  player_2: 3.1133333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -66.0\n",
      "  player_1: -30.666666666666664\n",
      "  player_2: -43.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08697624887405529\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30061340059002356\n",
      "  mean_inference_ms: 1.6200097284134432\n",
      "  mean_raw_obs_processing_ms: 0.21136674344978007\n",
      "time_since_restore: 9816.060600042343\n",
      "time_this_iter_s: 16.20335292816162\n",
      "time_total_s: 9816.060600042343\n",
      "timers:\n",
      "  learn_throughput: 559.497\n",
      "  learn_time_ms: 14262.812\n",
      "  load_throughput: 891297.698\n",
      "  load_time_ms: 8.953\n",
      "  sample_throughput: 519.038\n",
      "  sample_time_ms: 15374.595\n",
      "  update_time_ms: 10.44\n",
      "timestamp: 1643546052\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4875780\n",
      "training_iteration: 611\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4891710\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-34-45\n",
      "done: false\n",
      "episode_len_mean: 128.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 63\n",
      "episodes_total: 26079\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47471100295583407\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016911232049630295\n",
      "        policy_loss: -0.10516185116333267\n",
      "        total_loss: 43.980825703938805\n",
      "        vf_explained_var: 0.36579817076524096\n",
      "        vf_loss: 44.07457262674968\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4951595106224219\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017538378614392135\n",
      "        policy_loss: -0.05343228679150343\n",
      "        total_loss: 47.42449082692464\n",
      "        vf_explained_var: 0.19569549341996512\n",
      "        vf_loss: 47.46608462969462\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48185878336429594\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01688111735365055\n",
      "        policy_loss: -0.078582012715439\n",
      "        total_loss: 61.1652900091807\n",
      "        vf_explained_var: 0.3470544376969337\n",
      "        vf_loss: 61.232477502822874\n",
      "  num_agent_steps_sampled: 4891710\n",
      "  num_agent_steps_trained: 4891710\n",
      "  num_steps_sampled: 4891740\n",
      "  num_steps_trained: 4891740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 613\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.939999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 47.33333333333333\n",
      "  player_1: 31.666666666666664\n",
      "  player_2: 34.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 0.7199999999999999\n",
      "  player_1: -0.7300000000000004\n",
      "  player_2: 3.01\n",
      "policy_reward_min:\n",
      "  player_0: -56.0\n",
      "  player_1: -30.0\n",
      "  player_2: -63.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08694908948082265\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.300456553456323\n",
      "  mean_inference_ms: 1.6197479308472333\n",
      "  mean_raw_obs_processing_ms: 0.21139597493705606\n",
      "time_since_restore: 9848.981521606445\n",
      "time_this_iter_s: 16.57737445831299\n",
      "time_total_s: 9848.981521606445\n",
      "timers:\n",
      "  learn_throughput: 548.879\n",
      "  learn_time_ms: 14538.724\n",
      "  load_throughput: 853696.722\n",
      "  load_time_ms: 9.348\n",
      "  sample_throughput: 509.165\n",
      "  sample_time_ms: 15672.707\n",
      "  update_time_ms: 10.206\n",
      "timestamp: 1643546085\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4891740\n",
      "training_iteration: 613\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4907670\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-35-17\n",
      "done: false\n",
      "episode_len_mean: 125.12\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 61\n",
      "episodes_total: 26200\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4621396306157112\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015756393248395662\n",
      "        policy_loss: -0.08756744870916008\n",
      "        total_loss: 74.65740605195363\n",
      "        vf_explained_var: 0.3550464752316475\n",
      "        vf_loss: 74.73433827559153\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5035137721399466\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01585095940874794\n",
      "        policy_loss: -0.08676000227530797\n",
      "        total_loss: 83.49849074999491\n",
      "        vf_explained_var: 0.3062469001611074\n",
      "        vf_loss: 83.57455138524374\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4971367413798968\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017312971426317745\n",
      "        policy_loss: -0.05769881155341863\n",
      "        total_loss: 60.65004868030548\n",
      "        vf_explained_var: 0.2108236273129781\n",
      "        vf_loss: 60.69606113433838\n",
      "  num_agent_steps_sampled: 4907670\n",
      "  num_agent_steps_trained: 4907670\n",
      "  num_steps_sampled: 4907700\n",
      "  num_steps_trained: 4907700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 615\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.855\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 44.666666666666664\n",
      "  player_1: 37.0\n",
      "  player_2: 30.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 3.253333333333333\n",
      "  player_1: -2.8666666666666667\n",
      "  player_2: 2.6133333333333337\n",
      "policy_reward_min:\n",
      "  player_0: -36.0\n",
      "  player_1: -43.666666666666664\n",
      "  player_2: -35.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0869477242984063\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3005372347335146\n",
      "  mean_inference_ms: 1.6193304595711395\n",
      "  mean_raw_obs_processing_ms: 0.2113887173693248\n",
      "time_since_restore: 9881.393827199936\n",
      "time_this_iter_s: 15.905050277709961\n",
      "time_total_s: 9881.393827199936\n",
      "timers:\n",
      "  learn_throughput: 542.51\n",
      "  learn_time_ms: 14709.395\n",
      "  load_throughput: 942888.056\n",
      "  load_time_ms: 8.463\n",
      "  sample_throughput: 500.681\n",
      "  sample_time_ms: 15938.3\n",
      "  update_time_ms: 7.788\n",
      "timestamp: 1643546117\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4907700\n",
      "training_iteration: 615\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4923634\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-35-48\n",
      "done: false\n",
      "episode_len_mean: 134.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 61\n",
      "episodes_total: 26322\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46701318432887395\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016759105029717225\n",
      "        policy_loss: -0.07061670443043112\n",
      "        total_loss: 54.810430959065755\n",
      "        vf_explained_var: 0.2839514371752739\n",
      "        vf_loss: 54.86973534584045\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5014119768639406\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01675426141614442\n",
      "        policy_loss: -0.07402438374857108\n",
      "        total_loss: 36.44692317167918\n",
      "        vf_explained_var: 0.15712471008300782\n",
      "        vf_loss: 36.509638391335805\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4845029833416144\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01725523850767426\n",
      "        policy_loss: -0.09476417898510893\n",
      "        total_loss: 61.28392930348714\n",
      "        vf_explained_var: 0.3406321607033412\n",
      "        vf_loss: 61.367046155929565\n",
      "  num_agent_steps_sampled: 4923634\n",
      "  num_agent_steps_trained: 4923634\n",
      "  num_steps_sampled: 4923660\n",
      "  num_steps_trained: 4923660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 617\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.284210526315789\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 44.666666666666664\n",
      "  player_1: 34.0\n",
      "  player_2: 28.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.3866666666666667\n",
      "  player_1: 1.116666666666667\n",
      "  player_2: -1.5033333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -31.0\n",
      "  player_1: -32.0\n",
      "  player_2: -48.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08695393860349815\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30084099537694514\n",
      "  mean_inference_ms: 1.6201217145422944\n",
      "  mean_raw_obs_processing_ms: 0.21143885182673994\n",
      "time_since_restore: 9912.033452510834\n",
      "time_this_iter_s: 15.216837167739868\n",
      "time_total_s: 9912.033452510834\n",
      "timers:\n",
      "  learn_throughput: 548.625\n",
      "  learn_time_ms: 14545.443\n",
      "  load_throughput: 995199.391\n",
      "  load_time_ms: 8.018\n",
      "  sample_throughput: 500.898\n",
      "  sample_time_ms: 15931.375\n",
      "  update_time_ms: 7.907\n",
      "timestamp: 1643546148\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4923660\n",
      "training_iteration: 617\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4939592\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-36-18\n",
      "done: false\n",
      "episode_len_mean: 133.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 55\n",
      "episodes_total: 26431\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46087231904268267\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015949189787198368\n",
      "        policy_loss: -0.0861755927093327\n",
      "        total_loss: 53.609292948246\n",
      "        vf_explained_var: 0.25998519678910575\n",
      "        vf_loss: 53.684702904224395\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4963671202460925\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017302288931470382\n",
      "        policy_loss: -0.0924927750788629\n",
      "        total_loss: 55.932024281819665\n",
      "        vf_explained_var: 0.3431011422475179\n",
      "        vf_loss: 56.01283803621928\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.482081792751948\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01678942885628203\n",
      "        policy_loss: -0.048539712131023406\n",
      "        total_loss: 58.17697710355123\n",
      "        vf_explained_var: 0.28456630289554596\n",
      "        vf_loss: 58.214183888435365\n",
      "  num_agent_steps_sampled: 4939592\n",
      "  num_agent_steps_trained: 4939592\n",
      "  num_steps_sampled: 4939620\n",
      "  num_steps_trained: 4939620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 619\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.488888888888887\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.14999999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.333333333333332\n",
      "  player_1: 34.0\n",
      "  player_2: 30.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.2799999999999996\n",
      "  player_1: -0.92\n",
      "  player_2: 4.2\n",
      "policy_reward_min:\n",
      "  player_0: -46.0\n",
      "  player_1: -33.66666666666667\n",
      "  player_2: -35.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08701661904513408\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3002641184731285\n",
      "  mean_inference_ms: 1.615949124592965\n",
      "  mean_raw_obs_processing_ms: 0.21148986037486864\n",
      "time_since_restore: 9942.612432956696\n",
      "time_this_iter_s: 14.699998140335083\n",
      "time_total_s: 9942.612432956696\n",
      "timers:\n",
      "  learn_throughput: 547.033\n",
      "  learn_time_ms: 14587.795\n",
      "  load_throughput: 1017793.486\n",
      "  load_time_ms: 7.84\n",
      "  sample_throughput: 503.262\n",
      "  sample_time_ms: 15856.545\n",
      "  update_time_ms: 7.926\n",
      "timestamp: 1643546178\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4939620\n",
      "training_iteration: 619\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4955550\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-36-49\n",
      "done: false\n",
      "episode_len_mean: 144.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 61\n",
      "episodes_total: 26552\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4687831196188927\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016719828963208175\n",
      "        policy_loss: -0.07641440767173965\n",
      "        total_loss: 75.98874367237092\n",
      "        vf_explained_var: 0.007990079621473948\n",
      "        vf_loss: 76.05387184937796\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5017344438532988\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01456519743707986\n",
      "        policy_loss: -0.10797537812342246\n",
      "        total_loss: 51.65106450239817\n",
      "        vf_explained_var: 0.28637128323316574\n",
      "        vf_loss: 51.74920837640762\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.470419268955787\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015990356103698863\n",
      "        policy_loss: -0.05328794987251361\n",
      "        total_loss: 87.8345449590683\n",
      "        vf_explained_var: 0.1810303395986557\n",
      "        vf_loss: 87.87703932762146\n",
      "  num_agent_steps_sampled: 4955550\n",
      "  num_agent_steps_trained: 4955550\n",
      "  num_steps_sampled: 4955580\n",
      "  num_steps_trained: 4955580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 621\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.305263157894734\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.666666666666664\n",
      "  player_1: 32.333333333333336\n",
      "  player_2: 37.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 1.4099999999999997\n",
      "  player_1: -0.0300000000000005\n",
      "  player_2: 1.6199999999999997\n",
      "policy_reward_min:\n",
      "  player_0: -32.333333333333336\n",
      "  player_1: -44.66666666666667\n",
      "  player_2: -53.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08696681367301563\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30037166027022555\n",
      "  mean_inference_ms: 1.6179520667099316\n",
      "  mean_raw_obs_processing_ms: 0.21145469762049074\n",
      "time_since_restore: 9972.947155475616\n",
      "time_this_iter_s: 15.159850358963013\n",
      "time_total_s: 9972.947155475616\n",
      "timers:\n",
      "  learn_throughput: 550.501\n",
      "  learn_time_ms: 14495.896\n",
      "  load_throughput: 1017920.396\n",
      "  load_time_ms: 7.84\n",
      "  sample_throughput: 504.299\n",
      "  sample_time_ms: 15823.944\n",
      "  update_time_ms: 5.536\n",
      "timestamp: 1643546209\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4955580\n",
      "training_iteration: 621\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4971514\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-37-19\n",
      "done: false\n",
      "episode_len_mean: 124.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 57\n",
      "episodes_total: 26676\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46536671991149586\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01700278889435519\n",
      "        policy_loss: -0.0517517421549807\n",
      "        total_loss: 72.88649755477906\n",
      "        vf_explained_var: 0.27821447134017946\n",
      "        vf_loss: 72.9267727613449\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47845140516757967\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013550930917899677\n",
      "        policy_loss: -0.07510324362975855\n",
      "        total_loss: 50.45383492469787\n",
      "        vf_explained_var: 0.1550425154964129\n",
      "        vf_loss: 50.519791356722514\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4886888619760672\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015603091223407925\n",
      "        policy_loss: -0.08942678023440143\n",
      "        total_loss: 58.8565479628245\n",
      "        vf_explained_var: 0.32859859267870584\n",
      "        vf_loss: 58.93544275442759\n",
      "  num_agent_steps_sampled: 4971514\n",
      "  num_agent_steps_trained: 4971514\n",
      "  num_steps_sampled: 4971540\n",
      "  num_steps_trained: 4971540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 623\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.766666666666667\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.333333333333336\n",
      "  player_1: 50.0\n",
      "  player_2: 32.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.6233333333333335\n",
      "  player_1: -2.856666666666667\n",
      "  player_2: 3.2333333333333325\n",
      "policy_reward_min:\n",
      "  player_0: -50.0\n",
      "  player_1: -37.333333333333336\n",
      "  player_2: -63.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08688452893617787\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2995897674289185\n",
      "  mean_inference_ms: 1.6155288697309684\n",
      "  mean_raw_obs_processing_ms: 0.21112641401200655\n",
      "time_since_restore: 10003.091627120972\n",
      "time_this_iter_s: 14.592148542404175\n",
      "time_total_s: 10003.091627120972\n",
      "timers:\n",
      "  learn_throughput: 561.332\n",
      "  learn_time_ms: 14216.188\n",
      "  load_throughput: 1074264.794\n",
      "  load_time_ms: 7.428\n",
      "  sample_throughput: 510.441\n",
      "  sample_time_ms: 15633.555\n",
      "  update_time_ms: 5.651\n",
      "timestamp: 1643546239\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4971540\n",
      "training_iteration: 623\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 4987470\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-37-51\n",
      "done: false\n",
      "episode_len_mean: 132.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 57\n",
      "episodes_total: 26795\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.444457213083903\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016060226939367414\n",
      "        policy_loss: -0.06682771837959688\n",
      "        total_loss: 54.71265197277069\n",
      "        vf_explained_var: 0.29725313911835355\n",
      "        vf_loss: 54.76863906860351\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5052748222152392\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018004640679209843\n",
      "        policy_loss: -0.08892827367410064\n",
      "        total_loss: 47.23595232407252\n",
      "        vf_explained_var: 0.03670263995726903\n",
      "        vf_loss: 47.31272769371669\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4823706882695357\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014950511099290414\n",
      "        policy_loss: -0.057850439820128184\n",
      "        total_loss: 81.95998299439748\n",
      "        vf_explained_var: 0.1890709136923154\n",
      "        vf_loss: 82.00774122556051\n",
      "  num_agent_steps_sampled: 4987470\n",
      "  num_agent_steps_trained: 4987470\n",
      "  num_steps_sampled: 4987500\n",
      "  num_steps_trained: 4987500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 625\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.959999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.005\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 45.0\n",
      "  player_1: 29.0\n",
      "  player_2: 33.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 4.076666666666667\n",
      "  player_1: -1.6633333333333333\n",
      "  player_2: 0.5866666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -24.0\n",
      "  player_1: -32.333333333333336\n",
      "  player_2: -57.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08700729384264337\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.30013662134955493\n",
      "  mean_inference_ms: 1.6171780238789557\n",
      "  mean_raw_obs_processing_ms: 0.21141543774346247\n",
      "time_since_restore: 10034.675981760025\n",
      "time_this_iter_s: 16.580341577529907\n",
      "time_total_s: 10034.675981760025\n",
      "timers:\n",
      "  learn_throughput: 564.575\n",
      "  learn_time_ms: 14134.521\n",
      "  load_throughput: 1010672.611\n",
      "  load_time_ms: 7.896\n",
      "  sample_throughput: 522.062\n",
      "  sample_time_ms: 15285.553\n",
      "  update_time_ms: 5.76\n",
      "timestamp: 1643546271\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4987500\n",
      "training_iteration: 625\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5003432\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-38-24\n",
      "done: false\n",
      "episode_len_mean: 133.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 62\n",
      "episodes_total: 26916\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4719035315513611\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016313174756420683\n",
      "        policy_loss: -0.0367404075494657\n",
      "        total_loss: 55.827425696055094\n",
      "        vf_explained_var: 0.0958434001604716\n",
      "        vf_loss: 55.85315473079682\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49391864488522214\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016277665374204085\n",
      "        policy_loss: -0.10763775855302811\n",
      "        total_loss: 44.47068247715632\n",
      "        vf_explained_var: 0.3134234134356181\n",
      "        vf_loss: 44.567332755724586\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49856884668270746\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015702438103419734\n",
      "        policy_loss: -0.08174827087360124\n",
      "        total_loss: 49.27129085381826\n",
      "        vf_explained_var: 0.16592552095651628\n",
      "        vf_loss: 49.34243991851807\n",
      "  num_agent_steps_sampled: 5003432\n",
      "  num_agent_steps_trained: 5003432\n",
      "  num_steps_sampled: 5003460\n",
      "  num_steps_trained: 5003460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 627\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.895\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.0\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.333333333333336\n",
      "  player_1: 32.333333333333336\n",
      "  player_2: 33.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.1033333333333335\n",
      "  player_1: 0.36333333333333323\n",
      "  player_2: 1.5333333333333337\n",
      "policy_reward_min:\n",
      "  player_0: -44.666666666666664\n",
      "  player_1: -30.0\n",
      "  player_2: -34.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08692943954637938\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.3005204708324696\n",
      "  mean_inference_ms: 1.6196030040232687\n",
      "  mean_raw_obs_processing_ms: 0.21131422175225376\n",
      "time_since_restore: 10067.841670274734\n",
      "time_this_iter_s: 16.38214945793152\n",
      "time_total_s: 10067.841670274734\n",
      "timers:\n",
      "  learn_throughput: 554.828\n",
      "  learn_time_ms: 14382.842\n",
      "  load_throughput: 945378.143\n",
      "  load_time_ms: 8.441\n",
      "  sample_throughput: 515.128\n",
      "  sample_time_ms: 15491.282\n",
      "  update_time_ms: 5.652\n",
      "timestamp: 1643546304\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5003460\n",
      "training_iteration: 627\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5019390\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-38-55\n",
      "done: false\n",
      "episode_len_mean: 138.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 61\n",
      "episodes_total: 27035\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4771780698001385\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01624636031119735\n",
      "        policy_loss: -0.07877145989177127\n",
      "        total_loss: 54.778447001775106\n",
      "        vf_explained_var: 0.3705749510725339\n",
      "        vf_loss: 54.84625202337901\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4907382535934448\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014455291907058078\n",
      "        policy_loss: -0.10314369652420283\n",
      "        total_loss: 45.61494341532389\n",
      "        vf_explained_var: 0.23490343739589056\n",
      "        vf_loss: 45.708329780896506\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4952133440474669\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016124737593828323\n",
      "        policy_loss: -0.056142771877348424\n",
      "        total_loss: 58.06773867766062\n",
      "        vf_explained_var: 0.32936166514952975\n",
      "        vf_loss: 58.11299716154734\n",
      "  num_agent_steps_sampled: 5019390\n",
      "  num_agent_steps_trained: 5019390\n",
      "  num_steps_sampled: 5019420\n",
      "  num_steps_trained: 5019420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 629\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.947368421052632\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09473684210525\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.0\n",
      "  player_1: 23.666666666666668\n",
      "  player_2: 34.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.13\n",
      "  player_1: -1.0400000000000003\n",
      "  player_2: 1.91\n",
      "policy_reward_min:\n",
      "  player_0: -37.666666666666664\n",
      "  player_1: -33.333333333333336\n",
      "  player_2: -36.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08685648873320222\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29999346431460516\n",
      "  mean_inference_ms: 1.617346083289535\n",
      "  mean_raw_obs_processing_ms: 0.21121393427450796\n",
      "time_since_restore: 10098.663017511368\n",
      "time_this_iter_s: 15.371150732040405\n",
      "time_total_s: 10098.663017511368\n",
      "timers:\n",
      "  learn_throughput: 554.627\n",
      "  learn_time_ms: 14388.048\n",
      "  load_throughput: 932441.467\n",
      "  load_time_ms: 8.558\n",
      "  sample_throughput: 512.128\n",
      "  sample_time_ms: 15582.029\n",
      "  update_time_ms: 5.646\n",
      "timestamp: 1643546335\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5019420\n",
      "training_iteration: 629\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5035352\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-39-28\n",
      "done: false\n",
      "episode_len_mean: 123.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 65\n",
      "episodes_total: 27161\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46409284844994547\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015016342974466758\n",
      "        policy_loss: -0.04331797691062093\n",
      "        total_loss: 42.63597881158193\n",
      "        vf_explained_var: 0.28082086980342863\n",
      "        vf_loss: 42.669160876274105\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4938578252494335\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016164256231680458\n",
      "        policy_loss: -0.06365444122658422\n",
      "        total_loss: 59.3898458703359\n",
      "        vf_explained_var: 0.33973490963379543\n",
      "        vf_loss: 59.44258953730265\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49552696029345195\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016613017487055213\n",
      "        policy_loss: -0.11400852526227633\n",
      "        total_loss: 57.19092303276062\n",
      "        vf_explained_var: 0.3331126792232196\n",
      "        vf_loss: 57.293717885017394\n",
      "  num_agent_steps_sampled: 5035352\n",
      "  num_agent_steps_trained: 5035352\n",
      "  num_steps_sampled: 5035380\n",
      "  num_steps_trained: 5035380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 631\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.231578947368423\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.333333333333336\n",
      "  player_1: 34.666666666666664\n",
      "  player_2: 33.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.94\n",
      "  player_1: -2.36\n",
      "  player_2: 1.4199999999999997\n",
      "policy_reward_min:\n",
      "  player_0: -35.666666666666664\n",
      "  player_1: -40.0\n",
      "  player_2: -39.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08681739426258883\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29989282046415244\n",
      "  mean_inference_ms: 1.6172747788300648\n",
      "  mean_raw_obs_processing_ms: 0.21118893044264309\n",
      "time_since_restore: 10131.516566038132\n",
      "time_this_iter_s: 15.56930923461914\n",
      "time_total_s: 10131.516566038132\n",
      "timers:\n",
      "  learn_throughput: 545.057\n",
      "  learn_time_ms: 14640.677\n",
      "  load_throughput: 826335.297\n",
      "  load_time_ms: 9.657\n",
      "  sample_throughput: 503.763\n",
      "  sample_time_ms: 15840.79\n",
      "  update_time_ms: 5.627\n",
      "timestamp: 1643546368\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5035380\n",
      "training_iteration: 631\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5051310\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-39-58\n",
      "done: false\n",
      "episode_len_mean: 119.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 66\n",
      "episodes_total: 27291\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4805985360344251\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016037530708447035\n",
      "        policy_loss: -0.06171045633032918\n",
      "        total_loss: 60.808181835810345\n",
      "        vf_explained_var: 0.38492602547009785\n",
      "        vf_loss: 60.85906718889872\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5166106478869915\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015653057021930483\n",
      "        policy_loss: -0.09047522988170385\n",
      "        total_loss: 58.136100805600485\n",
      "        vf_explained_var: 0.45691962391138075\n",
      "        vf_loss: 58.21601012388865\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4957918423910936\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01746456371321481\n",
      "        policy_loss: -0.10426333864529927\n",
      "        total_loss: 83.30389496167501\n",
      "        vf_explained_var: 0.2226498211423556\n",
      "        vf_loss: 83.39636989275614\n",
      "  num_agent_steps_sampled: 5051310\n",
      "  num_agent_steps_trained: 5051310\n",
      "  num_steps_sampled: 5051340\n",
      "  num_steps_trained: 5051340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 633\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.205263157894736\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.66666666666667\n",
      "  player_1: 32.666666666666664\n",
      "  player_2: 48.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.466666666666667\n",
      "  player_1: -0.5733333333333334\n",
      "  player_2: 1.1066666666666671\n",
      "policy_reward_min:\n",
      "  player_0: -33.333333333333336\n",
      "  player_1: -70.66666666666666\n",
      "  player_2: -47.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08677138497706714\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2998425345734108\n",
      "  mean_inference_ms: 1.6178394716568405\n",
      "  mean_raw_obs_processing_ms: 0.21108182528860645\n",
      "time_since_restore: 10161.782895088196\n",
      "time_this_iter_s: 14.940306425094604\n",
      "time_total_s: 10161.782895088196\n",
      "timers:\n",
      "  learn_throughput: 544.59\n",
      "  learn_time_ms: 14653.223\n",
      "  load_throughput: 803821.052\n",
      "  load_time_ms: 9.928\n",
      "  sample_throughput: 503.085\n",
      "  sample_time_ms: 15862.135\n",
      "  update_time_ms: 5.53\n",
      "timestamp: 1643546398\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5051340\n",
      "training_iteration: 633\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5067273\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-40-29\n",
      "done: false\n",
      "episode_len_mean: 130.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 65\n",
      "episodes_total: 27416\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4820938685039679\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016845276598284425\n",
      "        policy_loss: -0.06545874581982693\n",
      "        total_loss: 88.65143422444662\n",
      "        vf_explained_var: 0.28898707777261734\n",
      "        vf_loss: 88.7055222304662\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4940526497364044\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01693638930667324\n",
      "        policy_loss: -0.10343100753923257\n",
      "        total_loss: 53.126357191403706\n",
      "        vf_explained_var: 0.16621950546900432\n",
      "        vf_loss: 53.21835601170858\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48805667395393054\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017821501120834\n",
      "        policy_loss: -0.08484054441253344\n",
      "        total_loss: 88.3704754447937\n",
      "        vf_explained_var: 0.21942191153764726\n",
      "        vf_loss: 88.4432859134674\n",
      "  num_agent_steps_sampled: 5067273\n",
      "  num_agent_steps_trained: 5067273\n",
      "  num_steps_sampled: 5067300\n",
      "  num_steps_trained: 5067300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 635\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.026315789473685\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.666666666666664\n",
      "  player_1: 34.0\n",
      "  player_2: 35.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.6866666666666672\n",
      "  player_1: -0.3133333333333334\n",
      "  player_2: 1.6266666666666671\n",
      "policy_reward_min:\n",
      "  player_0: -46.0\n",
      "  player_1: -39.333333333333336\n",
      "  player_2: -37.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08669375516750914\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2995044713030344\n",
      "  mean_inference_ms: 1.6155374825276632\n",
      "  mean_raw_obs_processing_ms: 0.21105713694397044\n",
      "time_since_restore: 10192.96634888649\n",
      "time_this_iter_s: 15.098496198654175\n",
      "time_total_s: 10192.96634888649\n",
      "timers:\n",
      "  learn_throughput: 546.095\n",
      "  learn_time_ms: 14612.84\n",
      "  load_throughput: 843065.563\n",
      "  load_time_ms: 9.465\n",
      "  sample_throughput: 498.501\n",
      "  sample_time_ms: 16007.985\n",
      "  update_time_ms: 5.501\n",
      "timestamp: 1643546429\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5067300\n",
      "training_iteration: 635\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5083230\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-41-01\n",
      "done: false\n",
      "episode_len_mean: 137.82\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 55\n",
      "episodes_total: 27529\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47329480851689976\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015974060043190547\n",
      "        policy_loss: -0.04952328863243262\n",
      "        total_loss: 54.32743179162343\n",
      "        vf_explained_var: 0.241326402425766\n",
      "        vf_loss: 54.36617257197698\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47281982252995175\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015649403082964756\n",
      "        policy_loss: -0.11913296258077026\n",
      "        total_loss: 60.59011411984761\n",
      "        vf_explained_var: 0.22620340555906296\n",
      "        vf_loss: 60.69868400414785\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.478094515701135\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01496045273912254\n",
      "        policy_loss: -0.05164235237675408\n",
      "        total_loss: 65.49194121837616\n",
      "        vf_explained_var: 0.4236583043138186\n",
      "        vf_loss: 65.53348554611206\n",
      "  num_agent_steps_sampled: 5083230\n",
      "  num_agent_steps_trained: 5083230\n",
      "  num_steps_sampled: 5083260\n",
      "  num_steps_trained: 5083260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 637\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.677777777777777\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.1111111111111\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 42.0\n",
      "  player_1: 47.0\n",
      "  player_2: 35.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.1700000000000004\n",
      "  player_1: 2.63\n",
      "  player_2: -1.7999999999999998\n",
      "policy_reward_min:\n",
      "  player_0: -33.33333333333333\n",
      "  player_1: -74.0\n",
      "  player_2: -47.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08683981061686605\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2996430749650853\n",
      "  mean_inference_ms: 1.6147525169984516\n",
      "  mean_raw_obs_processing_ms: 0.21121610010859024\n",
      "time_since_restore: 10224.87023472786\n",
      "time_this_iter_s: 15.165764093399048\n",
      "time_total_s: 10224.87023472786\n",
      "timers:\n",
      "  learn_throughput: 550.7\n",
      "  learn_time_ms: 14490.648\n",
      "  load_throughput: 880393.762\n",
      "  load_time_ms: 9.064\n",
      "  sample_throughput: 503.298\n",
      "  sample_time_ms: 15855.423\n",
      "  update_time_ms: 5.476\n",
      "timestamp: 1643546461\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5083260\n",
      "training_iteration: 637\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5099190\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-41-32\n",
      "done: false\n",
      "episode_len_mean: 136.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 62\n",
      "episodes_total: 27649\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.461623708208402\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015833092289952996\n",
      "        policy_loss: -0.11313953681848943\n",
      "        total_loss: 51.57658664703369\n",
      "        vf_explained_var: 0.2371681703130404\n",
      "        vf_loss: 51.67903861840566\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4919832010070483\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015799223605966594\n",
      "        policy_loss: -0.02843450990660737\n",
      "        total_loss: 49.4810892645518\n",
      "        vf_explained_var: 0.39257072399059934\n",
      "        vf_loss: 49.49885957876841\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46913882235685983\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016050519583748535\n",
      "        policy_loss: -0.085811266504849\n",
      "        total_loss: 53.30621419906616\n",
      "        vf_explained_var: 0.1830224174261093\n",
      "        vf_loss: 53.38119128863017\n",
      "  num_agent_steps_sampled: 5099190\n",
      "  num_agent_steps_trained: 5099190\n",
      "  num_steps_sampled: 5099220\n",
      "  num_steps_trained: 5099220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 639\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.278947368421052\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.333333333333336\n",
      "  player_1: 37.0\n",
      "  player_2: 43.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 1.9833333333333332\n",
      "  player_1: -0.376666666666667\n",
      "  player_2: 1.3933333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -36.66666666666667\n",
      "  player_1: -28.0\n",
      "  player_2: -55.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08684535927297475\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2996213728813008\n",
      "  mean_inference_ms: 1.6147713611608108\n",
      "  mean_raw_obs_processing_ms: 0.21113701209299676\n",
      "time_since_restore: 10256.146206855774\n",
      "time_this_iter_s: 15.263487577438354\n",
      "time_total_s: 10256.146206855774\n",
      "timers:\n",
      "  learn_throughput: 548.249\n",
      "  learn_time_ms: 14555.422\n",
      "  load_throughput: 896946.517\n",
      "  load_time_ms: 8.897\n",
      "  sample_throughput: 506.038\n",
      "  sample_time_ms: 15769.552\n",
      "  update_time_ms: 5.427\n",
      "timestamp: 1643546492\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5099220\n",
      "training_iteration: 639\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5115151\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-42-05\n",
      "done: false\n",
      "episode_len_mean: 127.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 62\n",
      "episodes_total: 27771\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4725795784095923\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016508790861438076\n",
      "        policy_loss: -0.08837171100080013\n",
      "        total_loss: 62.37448930581411\n",
      "        vf_explained_var: 0.2640634773174922\n",
      "        vf_loss: 62.45171786467234\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5074307491878668\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01690125009476257\n",
      "        policy_loss: -0.07547376440217098\n",
      "        total_loss: 57.297339510917666\n",
      "        vf_explained_var: 0.237009124259154\n",
      "        vf_loss: 57.36140473206838\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.493433673431476\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01764689987796809\n",
      "        policy_loss: -0.08603081116452813\n",
      "        total_loss: 60.17972470442454\n",
      "        vf_explained_var: 0.3322215978304545\n",
      "        vf_loss: 60.253843530019125\n",
      "  num_agent_steps_sampled: 5115151\n",
      "  num_agent_steps_trained: 5115151\n",
      "  num_steps_sampled: 5115180\n",
      "  num_steps_trained: 5115180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 641\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.228571428571431\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09523809523807\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.0\n",
      "  player_1: 36.0\n",
      "  player_2: 35.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 0.40000000000000013\n",
      "  player_1: 0.27000000000000013\n",
      "  player_2: 2.3299999999999996\n",
      "policy_reward_min:\n",
      "  player_0: -34.666666666666664\n",
      "  player_1: -37.666666666666664\n",
      "  player_2: -30.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08678400411630681\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29971623591148844\n",
      "  mean_inference_ms: 1.6160163633522067\n",
      "  mean_raw_obs_processing_ms: 0.21115253405632228\n",
      "time_since_restore: 10289.070729017258\n",
      "time_this_iter_s: 17.178444385528564\n",
      "time_total_s: 10289.070729017258\n",
      "timers:\n",
      "  learn_throughput: 548.072\n",
      "  learn_time_ms: 14560.139\n",
      "  load_throughput: 1012154.912\n",
      "  load_time_ms: 7.884\n",
      "  sample_throughput: 510.568\n",
      "  sample_time_ms: 15629.641\n",
      "  update_time_ms: 5.549\n",
      "timestamp: 1643546525\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5115180\n",
      "training_iteration: 641\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5131110\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-42-35\n",
      "done: false\n",
      "episode_len_mean: 123.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 66\n",
      "episodes_total: 27904\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4835056861738364\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017813272693318064\n",
      "        policy_loss: -0.0754040140658617\n",
      "        total_loss: 78.53091511090597\n",
      "        vf_explained_var: 0.2665325235327085\n",
      "        vf_loss: 78.59429515043894\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5241493552426497\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017822857143582477\n",
      "        policy_loss: -0.07532712304499001\n",
      "        total_loss: 74.86812800884246\n",
      "        vf_explained_var: 0.2044971885283788\n",
      "        vf_loss: 74.93142477035522\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.492298015554746\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016411434013170474\n",
      "        policy_loss: -0.10294418944045901\n",
      "        total_loss: 51.309036315282185\n",
      "        vf_explained_var: 0.19028470307588577\n",
      "        vf_loss: 51.40090270837148\n",
      "  num_agent_steps_sampled: 5131110\n",
      "  num_agent_steps_trained: 5131110\n",
      "  num_steps_sampled: 5131140\n",
      "  num_steps_trained: 5131140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 643\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.788888888888888\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.333333333333336\n",
      "  player_1: 33.33333333333333\n",
      "  player_2: 29.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.4600000000000003\n",
      "  player_1: -2.2899999999999996\n",
      "  player_2: 4.829999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -35.0\n",
      "  player_1: -47.666666666666664\n",
      "  player_2: -35.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08678266744629214\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2999848389575923\n",
      "  mean_inference_ms: 1.618044650992295\n",
      "  mean_raw_obs_processing_ms: 0.21119751955994245\n",
      "time_since_restore: 10318.70754647255\n",
      "time_this_iter_s: 14.655416488647461\n",
      "time_total_s: 10318.70754647255\n",
      "timers:\n",
      "  learn_throughput: 550.466\n",
      "  learn_time_ms: 14496.802\n",
      "  load_throughput: 988147.907\n",
      "  load_time_ms: 8.076\n",
      "  sample_throughput: 506.671\n",
      "  sample_time_ms: 15749.863\n",
      "  update_time_ms: 5.686\n",
      "timestamp: 1643546555\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5131140\n",
      "training_iteration: 643\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5147072\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-43-08\n",
      "done: false\n",
      "episode_len_mean: 124.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 61\n",
      "episodes_total: 28030\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4843854101995627\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01780144371813075\n",
      "        policy_loss: -0.07166238315713902\n",
      "        total_loss: 59.256104440689086\n",
      "        vf_explained_var: 0.3137384823958079\n",
      "        vf_loss: 59.315750713348386\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5094997978210449\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017240785251874512\n",
      "        policy_loss: -0.0670193869775782\n",
      "        total_loss: 41.777799730300906\n",
      "        vf_explained_var: 0.25482561031977335\n",
      "        vf_loss: 41.83318168004354\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4819061929980914\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015996592354416254\n",
      "        policy_loss: -0.09523225404166927\n",
      "        total_loss: 57.73823650201162\n",
      "        vf_explained_var: 0.3031964100400607\n",
      "        vf_loss: 57.82267083803813\n",
      "  num_agent_steps_sampled: 5147072\n",
      "  num_agent_steps_trained: 5147072\n",
      "  num_steps_sampled: 5147100\n",
      "  num_steps_trained: 5147100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 645\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.619047619047619\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 43.333333333333336\n",
      "  player_1: 33.666666666666664\n",
      "  player_2: 28.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.2633333333333334\n",
      "  player_1: -0.1366666666666665\n",
      "  player_2: 1.8733333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -28.0\n",
      "  player_1: -25.666666666666668\n",
      "  player_2: -41.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0865621829508957\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29932784112471816\n",
      "  mean_inference_ms: 1.615336689394328\n",
      "  mean_raw_obs_processing_ms: 0.21089343199665475\n",
      "time_since_restore: 10351.89365696907\n",
      "time_this_iter_s: 16.859713077545166\n",
      "time_total_s: 10351.89365696907\n",
      "timers:\n",
      "  learn_throughput: 542.913\n",
      "  learn_time_ms: 14698.485\n",
      "  load_throughput: 973119.599\n",
      "  load_time_ms: 8.2\n",
      "  sample_throughput: 506.986\n",
      "  sample_time_ms: 15740.089\n",
      "  update_time_ms: 5.649\n",
      "timestamp: 1643546588\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5147100\n",
      "training_iteration: 645\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5163032\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-43-40\n",
      "done: false\n",
      "episode_len_mean: 119.12\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 67\n",
      "episodes_total: 28160\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4738413499792417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016840609624836512\n",
      "        policy_loss: -0.10439298959914595\n",
      "        total_loss: 55.16085851987203\n",
      "        vf_explained_var: 0.2909154078364372\n",
      "        vf_loss: 55.25388415336609\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5140017314255237\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015662128349297103\n",
      "        policy_loss: -0.058315417257448036\n",
      "        total_loss: 63.66365000804265\n",
      "        vf_explained_var: 0.11616646031538645\n",
      "        vf_loss: 63.71139373699824\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48582777271668115\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017210665103820588\n",
      "        policy_loss: -0.08989680129413803\n",
      "        total_loss: 60.9126291624705\n",
      "        vf_explained_var: 0.24915890713532765\n",
      "        vf_loss: 60.99090913454692\n",
      "  num_agent_steps_sampled: 5163032\n",
      "  num_agent_steps_trained: 5163032\n",
      "  num_steps_sampled: 5163060\n",
      "  num_steps_trained: 5163060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 647\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.973684210526319\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.333333333333336\n",
      "  player_1: 34.0\n",
      "  player_2: 33.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 2.746666666666666\n",
      "  player_1: -2.043333333333334\n",
      "  player_2: 2.2966666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -48.666666666666664\n",
      "  player_1: -46.66666666666667\n",
      "  player_2: -33.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08659640118799417\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2989092285725794\n",
      "  mean_inference_ms: 1.6136280323907966\n",
      "  mean_raw_obs_processing_ms: 0.21087060787647632\n",
      "time_since_restore: 10383.80670428276\n",
      "time_this_iter_s: 15.293738603591919\n",
      "time_total_s: 10383.80670428276\n",
      "timers:\n",
      "  learn_throughput: 543.023\n",
      "  learn_time_ms: 14695.517\n",
      "  load_throughput: 930647.353\n",
      "  load_time_ms: 8.575\n",
      "  sample_throughput: 501.644\n",
      "  sample_time_ms: 15907.697\n",
      "  update_time_ms: 5.616\n",
      "timestamp: 1643546620\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5163060\n",
      "training_iteration: 647\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5178990\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-44-11\n",
      "done: false\n",
      "episode_len_mean: 127.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 58\n",
      "episodes_total: 28283\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4341500679651896\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015676069251465304\n",
      "        policy_loss: -0.07544837483515342\n",
      "        total_loss: 72.66620811065037\n",
      "        vf_explained_var: 0.07484332819779714\n",
      "        vf_loss: 72.73107497930526\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5001312929888566\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016797800040167205\n",
      "        policy_loss: -0.07790180277312174\n",
      "        total_loss: 63.02953988869985\n",
      "        vf_explained_var: 0.322653731405735\n",
      "        vf_loss: 63.09610319137573\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47170427863796555\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015944722681661007\n",
      "        policy_loss: -0.07325131309529145\n",
      "        total_loss: 55.04145931243897\n",
      "        vf_explained_var: 0.11869209855794907\n",
      "        vf_loss: 55.10394787152608\n",
      "  num_agent_steps_sampled: 5178990\n",
      "  num_agent_steps_trained: 5178990\n",
      "  num_steps_sampled: 5179020\n",
      "  num_steps_trained: 5179020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 649\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.205263157894736\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.333333333333336\n",
      "  player_1: 41.0\n",
      "  player_2: 30.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.3733333333333335\n",
      "  player_1: -0.8566666666666666\n",
      "  player_2: 2.483333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -39.333333333333336\n",
      "  player_1: -53.33333333333333\n",
      "  player_2: -34.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08669254579664536\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.299185351001047\n",
      "  mean_inference_ms: 1.6136653379723385\n",
      "  mean_raw_obs_processing_ms: 0.21105291118668185\n",
      "time_since_restore: 10414.55573630333\n",
      "time_this_iter_s: 15.124497413635254\n",
      "time_total_s: 10414.55573630333\n",
      "timers:\n",
      "  learn_throughput: 545.102\n",
      "  learn_time_ms: 14639.459\n",
      "  load_throughput: 919571.018\n",
      "  load_time_ms: 8.678\n",
      "  sample_throughput: 502.425\n",
      "  sample_time_ms: 15882.97\n",
      "  update_time_ms: 5.581\n",
      "timestamp: 1643546651\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5179020\n",
      "training_iteration: 649\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5194950\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-44-43\n",
      "done: false\n",
      "episode_len_mean: 126.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 67\n",
      "episodes_total: 28413\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48606882547338803\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01669994134981607\n",
      "        policy_loss: -0.06656817320423822\n",
      "        total_loss: 53.96945700645447\n",
      "        vf_explained_var: 0.37363499879837037\n",
      "        vf_loss: 54.024752736091614\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5110779250164826\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01653266258405286\n",
      "        policy_loss: -0.11646448097502192\n",
      "        total_loss: 58.65340026219686\n",
      "        vf_explained_var: 0.2843297325571378\n",
      "        vf_loss: 58.75870544433594\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49367602114876113\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016896958857529776\n",
      "        policy_loss: -0.06564030768349767\n",
      "        total_loss: 72.27498445510864\n",
      "        vf_explained_var: 0.2837041211128235\n",
      "        vf_loss: 72.32921921730042\n",
      "  num_agent_steps_sampled: 5194950\n",
      "  num_agent_steps_trained: 5194950\n",
      "  num_steps_sampled: 5194980\n",
      "  num_steps_trained: 5194980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 651\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.628571428571428\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.666666666666664\n",
      "  player_1: 43.33333333333333\n",
      "  player_2: 32.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.5800000000000001\n",
      "  player_1: -2.18\n",
      "  player_2: 4.6\n",
      "policy_reward_min:\n",
      "  player_0: -40.33333333333333\n",
      "  player_1: -36.66666666666667\n",
      "  player_2: -54.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08681067072907203\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2993924515122398\n",
      "  mean_inference_ms: 1.6145129226173165\n",
      "  mean_raw_obs_processing_ms: 0.21102184981772878\n",
      "time_since_restore: 10446.039186477661\n",
      "time_this_iter_s: 16.779343366622925\n",
      "time_total_s: 10446.039186477661\n",
      "timers:\n",
      "  learn_throughput: 550.444\n",
      "  learn_time_ms: 14497.386\n",
      "  load_throughput: 920175.232\n",
      "  load_time_ms: 8.672\n",
      "  sample_throughput: 506.337\n",
      "  sample_time_ms: 15760.268\n",
      "  update_time_ms: 5.459\n",
      "timestamp: 1643546683\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5194980\n",
      "training_iteration: 651\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5210910\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-45-13\n",
      "done: false\n",
      "episode_len_mean: 132.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 63\n",
      "episodes_total: 28533\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46187158857782684\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01610468224152991\n",
      "        policy_loss: -0.0752181235079964\n",
      "        total_loss: 90.52873215675353\n",
      "        vf_explained_var: 0.2540480327606201\n",
      "        vf_loss: 90.59307981332144\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5033197729289531\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016237981635703894\n",
      "        policy_loss: -0.101928255155993\n",
      "        total_loss: 56.966219573020936\n",
      "        vf_explained_var: 0.2206554454565048\n",
      "        vf_loss: 57.05718723932902\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48720691427588464\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016203226044384945\n",
      "        policy_loss: -0.05440872981057813\n",
      "        total_loss: 96.18337670644124\n",
      "        vf_explained_var: 0.32892620344956713\n",
      "        vf_loss: 96.22684813499451\n",
      "  num_agent_steps_sampled: 5210910\n",
      "  num_agent_steps_trained: 5210910\n",
      "  num_steps_sampled: 5210940\n",
      "  num_steps_trained: 5210940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 653\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.75\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.17222222222222\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 41.0\n",
      "  player_1: 27.666666666666664\n",
      "  player_2: 30.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 1.6466666666666665\n",
      "  player_1: -0.913333333333333\n",
      "  player_2: 2.2666666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -30.333333333333336\n",
      "  player_1: -41.333333333333336\n",
      "  player_2: -54.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08675534551131982\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29959970865407953\n",
      "  mean_inference_ms: 1.6153120720522247\n",
      "  mean_raw_obs_processing_ms: 0.21111406906980829\n",
      "time_since_restore: 10476.311308860779\n",
      "time_this_iter_s: 15.010387182235718\n",
      "time_total_s: 10476.311308860779\n",
      "timers:\n",
      "  learn_throughput: 547.976\n",
      "  learn_time_ms: 14562.686\n",
      "  load_throughput: 975649.842\n",
      "  load_time_ms: 8.179\n",
      "  sample_throughput: 506.582\n",
      "  sample_time_ms: 15752.645\n",
      "  update_time_ms: 5.352\n",
      "timestamp: 1643546713\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5210940\n",
      "training_iteration: 653\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5226870\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-45-43\n",
      "done: false\n",
      "episode_len_mean: 128.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 61\n",
      "episodes_total: 28663\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47792194242278735\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018098831531967032\n",
      "        policy_loss: -0.09208681085457404\n",
      "        total_loss: 54.424093074798584\n",
      "        vf_explained_var: 0.20014827966690063\n",
      "        vf_loss: 54.50396320501963\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48760622332493464\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016186239919428167\n",
      "        policy_loss: -0.09627640463411807\n",
      "        total_loss: 51.06454689582189\n",
      "        vf_explained_var: 0.2539712928732236\n",
      "        vf_loss: 51.14989733695984\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4792385370532672\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0165675103680951\n",
      "        policy_loss: -0.056371718955536686\n",
      "        total_loss: 71.58592040220897\n",
      "        vf_explained_var: 0.10613334427277248\n",
      "        vf_loss: 71.63110903422037\n",
      "  num_agent_steps_sampled: 5226870\n",
      "  num_agent_steps_trained: 5226870\n",
      "  num_steps_sampled: 5226900\n",
      "  num_steps_trained: 5226900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 655\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.472222222222221\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.0\n",
      "  player_1: 37.666666666666664\n",
      "  player_2: 31.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.2300000000000006\n",
      "  player_1: 1.7300000000000002\n",
      "  player_2: 0.04000000000000032\n",
      "policy_reward_min:\n",
      "  player_0: -25.0\n",
      "  player_1: -33.666666666666664\n",
      "  player_2: -46.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08669024383924588\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29936924500899265\n",
      "  mean_inference_ms: 1.6149255804220572\n",
      "  mean_raw_obs_processing_ms: 0.21106457019576066\n",
      "time_since_restore: 10506.25948214531\n",
      "time_this_iter_s: 14.721796751022339\n",
      "time_total_s: 10506.25948214531\n",
      "timers:\n",
      "  learn_throughput: 560.549\n",
      "  learn_time_ms: 14236.032\n",
      "  load_throughput: 980933.965\n",
      "  load_time_ms: 8.135\n",
      "  sample_throughput: 509.011\n",
      "  sample_time_ms: 15677.466\n",
      "  update_time_ms: 5.376\n",
      "timestamp: 1643546743\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5226900\n",
      "training_iteration: 655\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5242833\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-46-12\n",
      "done: false\n",
      "episode_len_mean: 130.54\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 63\n",
      "episodes_total: 28785\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4803467603524526\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017953501154782617\n",
      "        policy_loss: -0.0671218018575261\n",
      "        total_loss: 51.85769193649292\n",
      "        vf_explained_var: 0.2832243733604749\n",
      "        vf_loss: 51.91269500255585\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5055389791727066\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016207839988449376\n",
      "        policy_loss: -0.08371068487564723\n",
      "        total_loss: 52.08239828507106\n",
      "        vf_explained_var: 0.1430861928065618\n",
      "        vf_loss: 52.15516886393229\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.491077099442482\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01667702547357597\n",
      "        policy_loss: -0.08034980933337162\n",
      "        total_loss: 57.10924042860667\n",
      "        vf_explained_var: 0.35503150333960853\n",
      "        vf_loss: 57.17833303610484\n",
      "  num_agent_steps_sampled: 5242833\n",
      "  num_agent_steps_trained: 5242833\n",
      "  num_steps_sampled: 5242860\n",
      "  num_steps_trained: 5242860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 657\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.366666666666665\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.15555555555554\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.333333333333336\n",
      "  player_1: 30.666666666666664\n",
      "  player_2: 35.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.5133333333333332\n",
      "  player_1: 0.38333333333333336\n",
      "  player_2: 1.1033333333333337\n",
      "policy_reward_min:\n",
      "  player_0: -30.333333333333336\n",
      "  player_1: -50.333333333333336\n",
      "  player_2: -31.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08666873323785972\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29924072272163843\n",
      "  mean_inference_ms: 1.6144670622716877\n",
      "  mean_raw_obs_processing_ms: 0.21098523193482382\n",
      "time_since_restore: 10535.293587684631\n",
      "time_this_iter_s: 14.444031715393066\n",
      "time_total_s: 10535.293587684631\n",
      "timers:\n",
      "  learn_throughput: 572.091\n",
      "  learn_time_ms: 13948.828\n",
      "  load_throughput: 1043167.347\n",
      "  load_time_ms: 7.65\n",
      "  sample_throughput: 522.948\n",
      "  sample_time_ms: 15259.635\n",
      "  update_time_ms: 5.397\n",
      "timestamp: 1643546772\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5242860\n",
      "training_iteration: 657\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5258792\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-46-44\n",
      "done: false\n",
      "episode_len_mean: 131.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 64\n",
      "episodes_total: 28911\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4801258321106434\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01689692264078379\n",
      "        policy_loss: -0.03429938187512259\n",
      "        total_loss: 47.7542058579127\n",
      "        vf_explained_var: 0.2855885225534439\n",
      "        vf_loss: 47.77710007826487\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5070271402100722\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015258580295896423\n",
      "        policy_loss: -0.07277405850589275\n",
      "        total_loss: 55.5565372800827\n",
      "        vf_explained_var: 0.23182925721009573\n",
      "        vf_loss: 55.61901212851206\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4906647759179274\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015577333113972903\n",
      "        policy_loss: -0.10983051173699399\n",
      "        total_loss: 84.81806455930074\n",
      "        vf_explained_var: 0.1625777460138003\n",
      "        vf_loss: 84.91738044420877\n",
      "  num_agent_steps_sampled: 5258792\n",
      "  num_agent_steps_trained: 5258792\n",
      "  num_steps_sampled: 5258820\n",
      "  num_steps_trained: 5258820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 659\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.733333333333329\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.1285714285714\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.666666666666664\n",
      "  player_1: 41.333333333333336\n",
      "  player_2: 32.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 4.753333333333334\n",
      "  player_1: -2.656666666666666\n",
      "  player_2: 0.9033333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -32.666666666666664\n",
      "  player_1: -37.666666666666664\n",
      "  player_2: -52.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08661442244058268\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29899185797314964\n",
      "  mean_inference_ms: 1.6132704422439452\n",
      "  mean_raw_obs_processing_ms: 0.21092946731918327\n",
      "time_since_restore: 10567.344285726547\n",
      "time_this_iter_s: 16.489203453063965\n",
      "time_total_s: 10567.344285726547\n",
      "timers:\n",
      "  learn_throughput: 566.636\n",
      "  learn_time_ms: 14083.113\n",
      "  load_throughput: 1120641.836\n",
      "  load_time_ms: 7.121\n",
      "  sample_throughput: 526.172\n",
      "  sample_time_ms: 15166.138\n",
      "  update_time_ms: 5.501\n",
      "timestamp: 1643546804\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5258820\n",
      "training_iteration: 659\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5274750\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-47-17\n",
      "done: false\n",
      "episode_len_mean: 122.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 61\n",
      "episodes_total: 29038\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4669458439449469\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016549180236540718\n",
      "        policy_loss: -0.08842469417645285\n",
      "        total_loss: 65.13827335993449\n",
      "        vf_explained_var: 0.2295406009753545\n",
      "        vf_loss: 65.21552774747212\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5037399024267991\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018873003395153017\n",
      "        policy_loss: -0.06664258412085473\n",
      "        total_loss: 50.916657474835716\n",
      "        vf_explained_var: 0.1316904033223788\n",
      "        vf_loss: 50.970560863812764\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48782772466540336\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016987415539554906\n",
      "        policy_loss: -0.1008137703469644\n",
      "        total_loss: 49.764421849250795\n",
      "        vf_explained_var: 0.2917971224586169\n",
      "        vf_loss: 49.85376902103424\n",
      "  num_agent_steps_sampled: 5274750\n",
      "  num_agent_steps_trained: 5274750\n",
      "  num_steps_sampled: 5274780\n",
      "  num_steps_trained: 5274780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 661\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.964999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.11999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.666666666666664\n",
      "  player_1: 36.0\n",
      "  player_2: 41.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.746666666666667\n",
      "  player_1: 1.636666666666667\n",
      "  player_2: 0.6166666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -46.666666666666664\n",
      "  player_1: -27.333333333333336\n",
      "  player_2: -54.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08659036673516296\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29885691001117926\n",
      "  mean_inference_ms: 1.612818649636493\n",
      "  mean_raw_obs_processing_ms: 0.21097677135007742\n",
      "time_since_restore: 10600.150081157684\n",
      "time_this_iter_s: 16.430339813232422\n",
      "time_total_s: 10600.150081157684\n",
      "timers:\n",
      "  learn_throughput: 561.304\n",
      "  learn_time_ms: 14216.885\n",
      "  load_throughput: 1125158.7\n",
      "  load_time_ms: 7.092\n",
      "  sample_throughput: 515.781\n",
      "  sample_time_ms: 15471.693\n",
      "  update_time_ms: 5.575\n",
      "timestamp: 1643546837\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5274780\n",
      "training_iteration: 661\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5290713\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-47-49\n",
      "done: false\n",
      "episode_len_mean: 126.12\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 66\n",
      "episodes_total: 29172\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4461397140224775\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015697624180265848\n",
      "        policy_loss: -0.08603837948913376\n",
      "        total_loss: 51.09023569424947\n",
      "        vf_explained_var: 0.3025449890891711\n",
      "        vf_loss: 51.16567796707153\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47298459614316624\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01689111464437133\n",
      "        policy_loss: -0.06556069149635732\n",
      "        total_loss: 49.67522081851959\n",
      "        vf_explained_var: 0.33161247114340464\n",
      "        vf_loss: 49.72938018798828\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4804097412526607\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016737428151388182\n",
      "        policy_loss: -0.0792708528538545\n",
      "        total_loss: 71.94053832530976\n",
      "        vf_explained_var: 0.26322882185379665\n",
      "        vf_loss: 72.00851176102957\n",
      "  num_agent_steps_sampled: 5290713\n",
      "  num_agent_steps_trained: 5290713\n",
      "  num_steps_sampled: 5290740\n",
      "  num_steps_trained: 5290740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 663\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.754999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.0\n",
      "  player_1: 29.666666666666664\n",
      "  player_2: 30.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.8533333333333328\n",
      "  player_1: 0.5033333333333334\n",
      "  player_2: 1.643333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -37.333333333333336\n",
      "  player_1: -29.0\n",
      "  player_2: -42.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0864909641245609\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2985371720943098\n",
      "  mean_inference_ms: 1.6115364216622583\n",
      "  mean_raw_obs_processing_ms: 0.2107033291364091\n",
      "time_since_restore: 10632.267495632172\n",
      "time_this_iter_s: 16.827141046524048\n",
      "time_total_s: 10632.267495632172\n",
      "timers:\n",
      "  learn_throughput: 554.134\n",
      "  learn_time_ms: 14400.858\n",
      "  load_throughput: 1099648.326\n",
      "  load_time_ms: 7.257\n",
      "  sample_throughput: 516.845\n",
      "  sample_time_ms: 15439.833\n",
      "  update_time_ms: 5.652\n",
      "timestamp: 1643546869\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5290740\n",
      "training_iteration: 663\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5306674\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-48-22\n",
      "done: false\n",
      "episode_len_mean: 123.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 60\n",
      "episodes_total: 29296\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4505059576531251\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017709270562920664\n",
      "        policy_loss: -0.07755584193704029\n",
      "        total_loss: 43.83131196022034\n",
      "        vf_explained_var: 0.18004926910003027\n",
      "        vf_loss: 43.89691390832265\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47391660531361895\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016277078128431034\n",
      "        policy_loss: -0.052154344809241596\n",
      "        total_loss: 50.67567992210388\n",
      "        vf_explained_var: 0.08058600684007008\n",
      "        vf_loss: 50.716847430864966\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47526847446958226\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016252193170338766\n",
      "        policy_loss: -0.09564226480821768\n",
      "        total_loss: 53.71511226892471\n",
      "        vf_explained_var: 0.1509599424401919\n",
      "        vf_loss: 53.79978411753972\n",
      "  num_agent_steps_sampled: 5306674\n",
      "  num_agent_steps_trained: 5306674\n",
      "  num_steps_sampled: 5306700\n",
      "  num_steps_trained: 5306700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 665\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.37368421052632\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.666666666666664\n",
      "  player_1: 47.33333333333333\n",
      "  player_2: 30.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.9466666666666668\n",
      "  player_1: -0.7633333333333332\n",
      "  player_2: -0.18333333333333357\n",
      "policy_reward_min:\n",
      "  player_0: -52.666666666666664\n",
      "  player_1: -32.33333333333333\n",
      "  player_2: -60.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08667655514093939\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29877602877210974\n",
      "  mean_inference_ms: 1.612030444352605\n",
      "  mean_raw_obs_processing_ms: 0.21092561158031065\n",
      "time_since_restore: 10664.752667427063\n",
      "time_this_iter_s: 15.929554462432861\n",
      "time_total_s: 10664.752667427063\n",
      "timers:\n",
      "  learn_throughput: 544.495\n",
      "  learn_time_ms: 14655.778\n",
      "  load_throughput: 1106768.003\n",
      "  load_time_ms: 7.21\n",
      "  sample_throughput: 506.47\n",
      "  sample_time_ms: 15756.12\n",
      "  update_time_ms: 5.84\n",
      "timestamp: 1643546902\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5306700\n",
      "training_iteration: 665\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5322630\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-48-55\n",
      "done: false\n",
      "episode_len_mean: 125.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 60\n",
      "episodes_total: 29427\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4554376292228699\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015553466920243675\n",
      "        policy_loss: -0.11608877466370662\n",
      "        total_loss: 39.40510488033295\n",
      "        vf_explained_var: 0.4069567974408468\n",
      "        vf_loss: 39.51069514115652\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4905081109702587\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017249767166259176\n",
      "        policy_loss: -0.04545836714717249\n",
      "        total_loss: 42.861878900527955\n",
      "        vf_explained_var: 0.1169586714108785\n",
      "        vf_loss: 42.89569355328878\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4711406451960405\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016939160485728355\n",
      "        policy_loss: -0.06815584770093362\n",
      "        total_loss: 49.56906227588654\n",
      "        vf_explained_var: 0.12390081842740376\n",
      "        vf_loss: 49.625784001350404\n",
      "  num_agent_steps_sampled: 5322630\n",
      "  num_agent_steps_trained: 5322630\n",
      "  num_steps_sampled: 5322660\n",
      "  num_steps_trained: 5322660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 667\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.623809523809522\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.333333333333336\n",
      "  player_1: 37.666666666666664\n",
      "  player_2: 35.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.3166666666666664\n",
      "  player_1: -1.3433333333333335\n",
      "  player_2: 1.0266666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -45.0\n",
      "  player_1: -37.0\n",
      "  player_2: -47.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08660010551939447\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2987084917612804\n",
      "  mean_inference_ms: 1.6125456795141866\n",
      "  mean_raw_obs_processing_ms: 0.2108615622221973\n",
      "time_since_restore: 10697.69107246399\n",
      "time_this_iter_s: 17.12744927406311\n",
      "time_total_s: 10697.69107246399\n",
      "timers:\n",
      "  learn_throughput: 530.283\n",
      "  learn_time_ms: 15048.562\n",
      "  load_throughput: 1086251.081\n",
      "  load_time_ms: 7.346\n",
      "  sample_throughput: 498.793\n",
      "  sample_time_ms: 15998.614\n",
      "  update_time_ms: 5.83\n",
      "timestamp: 1643546935\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5322660\n",
      "training_iteration: 667\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5338594\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-49-26\n",
      "done: false\n",
      "episode_len_mean: 124.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 65\n",
      "episodes_total: 29560\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4636374951402346\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01659455116650539\n",
      "        policy_loss: -0.11721676883132508\n",
      "        total_loss: 55.71189354499181\n",
      "        vf_explained_var: 0.4205997771024704\n",
      "        vf_loss: 55.817908948262534\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4673745079835256\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01605361667695244\n",
      "        policy_loss: -0.05738073713767032\n",
      "        total_loss: 64.19118486404419\n",
      "        vf_explained_var: 0.23056020667155583\n",
      "        vf_loss: 64.23772932688395\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4737827264269193\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014994541538521276\n",
      "        policy_loss: -0.0584646674649169\n",
      "        total_loss: 65.65168536027272\n",
      "        vf_explained_var: 0.2820619448026021\n",
      "        vf_loss: 65.70002876440684\n",
      "  num_agent_steps_sampled: 5338594\n",
      "  num_agent_steps_trained: 5338594\n",
      "  num_steps_sampled: 5338620\n",
      "  num_steps_trained: 5338620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 669\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.273684210526316\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 38.0\n",
      "  player_2: 28.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 0.25000000000000017\n",
      "  player_1: -0.08000000000000028\n",
      "  player_2: 2.8300000000000005\n",
      "policy_reward_min:\n",
      "  player_0: -32.0\n",
      "  player_1: -34.333333333333336\n",
      "  player_2: -55.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08645821832446814\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29836962247057375\n",
      "  mean_inference_ms: 1.6110544398626958\n",
      "  mean_raw_obs_processing_ms: 0.21070133149556994\n",
      "time_since_restore: 10728.705431461334\n",
      "time_this_iter_s: 15.529501914978027\n",
      "time_total_s: 10728.705431461334\n",
      "timers:\n",
      "  learn_throughput: 533.888\n",
      "  learn_time_ms: 14946.945\n",
      "  load_throughput: 1066450.404\n",
      "  load_time_ms: 7.483\n",
      "  sample_throughput: 490.723\n",
      "  sample_time_ms: 16261.709\n",
      "  update_time_ms: 5.95\n",
      "timestamp: 1643546966\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5338620\n",
      "training_iteration: 669\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5354551\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-49-56\n",
      "done: false\n",
      "episode_len_mean: 130.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 59\n",
      "episodes_total: 29678\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4874893112977346\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016909595730758155\n",
      "        policy_loss: -0.08336888713762164\n",
      "        total_loss: 71.55679101149241\n",
      "        vf_explained_var: 0.1757807767391205\n",
      "        vf_loss: 71.62874586900075\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47820495928327245\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015958161052084942\n",
      "        policy_loss: -0.10867845200312634\n",
      "        total_loss: 44.74512107849121\n",
      "        vf_explained_var: 0.26587919662396114\n",
      "        vf_loss: 44.84302782773972\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4838833771646023\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015752143783984898\n",
      "        policy_loss: -0.05054610339303811\n",
      "        total_loss: 67.52261649449666\n",
      "        vf_explained_var: 0.2868818981448809\n",
      "        vf_loss: 67.56253020127615\n",
      "  num_agent_steps_sampled: 5354551\n",
      "  num_agent_steps_trained: 5354551\n",
      "  num_steps_sampled: 5354580\n",
      "  num_steps_trained: 5354580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 671\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.221052631578948\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.333333333333336\n",
      "  player_1: 33.666666666666664\n",
      "  player_2: 38.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.1500000000000004\n",
      "  player_1: -1.8899999999999997\n",
      "  player_2: 3.74\n",
      "policy_reward_min:\n",
      "  player_0: -30.666666666666664\n",
      "  player_1: -41.0\n",
      "  player_2: -49.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08675776309992948\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2990612832097619\n",
      "  mean_inference_ms: 1.6134282504741584\n",
      "  mean_raw_obs_processing_ms: 0.2108932211858654\n",
      "time_since_restore: 10759.039772510529\n",
      "time_this_iter_s: 15.294711828231812\n",
      "time_total_s: 10759.039772510529\n",
      "timers:\n",
      "  learn_throughput: 542.834\n",
      "  learn_time_ms: 14700.627\n",
      "  load_throughput: 1066759.708\n",
      "  load_time_ms: 7.481\n",
      "  sample_throughput: 497.805\n",
      "  sample_time_ms: 16030.376\n",
      "  update_time_ms: 6.014\n",
      "timestamp: 1643546996\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5354580\n",
      "training_iteration: 671\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5370510\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-50-28\n",
      "done: false\n",
      "episode_len_mean: 128.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 64\n",
      "episodes_total: 29808\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4888667362431685\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015867636758650102\n",
      "        policy_loss: -0.09510029340162873\n",
      "        total_loss: 58.08010850270589\n",
      "        vf_explained_var: 0.29260565121968585\n",
      "        vf_loss: 58.16449801603953\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5037484335899353\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015168458332321014\n",
      "        policy_loss: -0.06079054681273798\n",
      "        total_loss: 65.22635974407196\n",
      "        vf_explained_var: 0.2582361835241318\n",
      "        vf_loss: 65.27691194375356\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47122947057088216\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015961418721266607\n",
      "        policy_loss: -0.08735767432798942\n",
      "        total_loss: 62.43173467318217\n",
      "        vf_explained_var: 0.2811596888303757\n",
      "        vf_loss: 62.508318079312644\n",
      "  num_agent_steps_sampled: 5370510\n",
      "  num_agent_steps_trained: 5370510\n",
      "  num_steps_sampled: 5370540\n",
      "  num_steps_trained: 5370540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 673\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.315789473684214\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.13684210526314\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.333333333333336\n",
      "  player_1: 35.0\n",
      "  player_2: 35.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.41\n",
      "  player_1: 0.26000000000000006\n",
      "  player_2: 2.3299999999999996\n",
      "policy_reward_min:\n",
      "  player_0: -37.0\n",
      "  player_1: -44.666666666666664\n",
      "  player_2: -52.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08662510944656115\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29830740126306393\n",
      "  mean_inference_ms: 1.6106581075797295\n",
      "  mean_raw_obs_processing_ms: 0.21087108696220716\n",
      "time_since_restore: 10791.166050195694\n",
      "time_this_iter_s: 15.843537092208862\n",
      "time_total_s: 10791.166050195694\n",
      "timers:\n",
      "  learn_throughput: 542.796\n",
      "  learn_time_ms: 14701.644\n",
      "  load_throughput: 1067562.696\n",
      "  load_time_ms: 7.475\n",
      "  sample_throughput: 498.173\n",
      "  sample_time_ms: 16018.544\n",
      "  update_time_ms: 5.967\n",
      "timestamp: 1643547028\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5370540\n",
      "training_iteration: 673\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5386472\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-50-59\n",
      "done: false\n",
      "episode_len_mean: 125.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 70\n",
      "episodes_total: 29940\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45920119966069856\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016850060007205913\n",
      "        policy_loss: -0.036935403076931834\n",
      "        total_loss: 49.02364560286204\n",
      "        vf_explained_var: 0.3350589284300804\n",
      "        vf_loss: 49.04920725186666\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48060547798871994\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014893782531765586\n",
      "        policy_loss: -0.033349822362264\n",
      "        total_loss: 45.501215678056084\n",
      "        vf_explained_var: 0.4100755717357\n",
      "        vf_loss: 45.52451227347056\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48580898488561314\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01639954941889016\n",
      "        policy_loss: -0.1382258186986049\n",
      "        total_loss: 53.78523631572723\n",
      "        vf_explained_var: 0.39816512842973073\n",
      "        vf_loss: 53.91239229838053\n",
      "  num_agent_steps_sampled: 5386472\n",
      "  num_agent_steps_trained: 5386472\n",
      "  num_steps_sampled: 5386500\n",
      "  num_steps_trained: 5386500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 675\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.063157894736841\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.666666666666664\n",
      "  player_1: 25.333333333333332\n",
      "  player_2: 32.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.28\n",
      "  player_1: -0.29999999999999977\n",
      "  player_2: 1.0200000000000005\n",
      "policy_reward_min:\n",
      "  player_0: -29.666666666666664\n",
      "  player_1: -40.666666666666664\n",
      "  player_2: -40.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08651621043234503\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2986005834789634\n",
      "  mean_inference_ms: 1.612087969703965\n",
      "  mean_raw_obs_processing_ms: 0.21068160426935278\n",
      "time_since_restore: 10821.66723704338\n",
      "time_this_iter_s: 15.719778060913086\n",
      "time_total_s: 10821.66723704338\n",
      "timers:\n",
      "  learn_throughput: 550.254\n",
      "  learn_time_ms: 14502.397\n",
      "  load_throughput: 1062152.384\n",
      "  load_time_ms: 7.513\n",
      "  sample_throughput: 506.935\n",
      "  sample_time_ms: 15741.678\n",
      "  update_time_ms: 5.828\n",
      "timestamp: 1643547059\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5386500\n",
      "training_iteration: 675\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5402432\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-51-29\n",
      "done: false\n",
      "episode_len_mean: 123.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 67\n",
      "episodes_total: 30070\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47406845505038897\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01606286602009902\n",
      "        policy_loss: -0.08804903948679567\n",
      "        total_loss: 43.76088921070099\n",
      "        vf_explained_var: 0.2810926521817843\n",
      "        vf_loss: 43.83809577782949\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49977289418379467\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017581672150929536\n",
      "        policy_loss: -0.08454854836842667\n",
      "        total_loss: 41.05923196633657\n",
      "        vf_explained_var: 0.22245773325363796\n",
      "        vf_loss: 41.13191284656525\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4930861581861973\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016680648877746004\n",
      "        policy_loss: -0.07421741181984544\n",
      "        total_loss: 50.12516283512115\n",
      "        vf_explained_var: 0.31228404223918915\n",
      "        vf_loss: 50.18812092145284\n",
      "  num_agent_steps_sampled: 5402432\n",
      "  num_agent_steps_trained: 5402432\n",
      "  num_steps_sampled: 5402460\n",
      "  num_steps_trained: 5402460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 677\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.105263157894736\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.666666666666664\n",
      "  player_1: 26.666666666666664\n",
      "  player_2: 37.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 1.2000000000000002\n",
      "  player_1: -1.4210854715202004e-16\n",
      "  player_2: 1.8\n",
      "policy_reward_min:\n",
      "  player_0: -30.333333333333336\n",
      "  player_1: -54.66666666666667\n",
      "  player_2: -29.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08645120601335723\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2984760985947828\n",
      "  mean_inference_ms: 1.612311717867364\n",
      "  mean_raw_obs_processing_ms: 0.21068888277044934\n",
      "time_since_restore: 10852.122526407242\n",
      "time_this_iter_s: 15.198701620101929\n",
      "time_total_s: 10852.122526407242\n",
      "timers:\n",
      "  learn_throughput: 559.807\n",
      "  learn_time_ms: 14254.916\n",
      "  load_throughput: 1099915.739\n",
      "  load_time_ms: 7.255\n",
      "  sample_throughput: 509.49\n",
      "  sample_time_ms: 15662.729\n",
      "  update_time_ms: 6.237\n",
      "timestamp: 1643547089\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5402460\n",
      "training_iteration: 677\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5418391\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-51-59\n",
      "done: false\n",
      "episode_len_mean: 125.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 63\n",
      "episodes_total: 30198\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4700842058161894\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01661464080931078\n",
      "        policy_loss: -0.08317360276977222\n",
      "        total_loss: 66.34535835901896\n",
      "        vf_explained_var: 0.2992255411545436\n",
      "        vf_loss: 66.41731714248657\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.483748758683602\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017017094985276343\n",
      "        policy_loss: -0.08321379553837081\n",
      "        total_loss: 50.20908498922984\n",
      "        vf_explained_var: 0.26474448323249816\n",
      "        vf_loss: 50.280812039375306\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5050962233046691\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01762849511284685\n",
      "        policy_loss: -0.09020167380571366\n",
      "        total_loss: 63.97805271466573\n",
      "        vf_explained_var: 0.22549903164307275\n",
      "        vf_loss: 64.05635537306468\n",
      "  num_agent_steps_sampled: 5418391\n",
      "  num_agent_steps_trained: 5418391\n",
      "  num_steps_sampled: 5418420\n",
      "  num_steps_trained: 5418420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 679\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.727777777777776\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.0\n",
      "  player_1: 33.0\n",
      "  player_2: 33.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 3.106666666666667\n",
      "  player_1: -0.3633333333333335\n",
      "  player_2: 0.2566666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -37.333333333333336\n",
      "  player_1: -43.333333333333336\n",
      "  player_2: -55.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08658135969143906\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2985331837548775\n",
      "  mean_inference_ms: 1.6112393861959335\n",
      "  mean_raw_obs_processing_ms: 0.2108138067523724\n",
      "time_since_restore: 10881.551661491394\n",
      "time_this_iter_s: 14.762624263763428\n",
      "time_total_s: 10881.551661491394\n",
      "timers:\n",
      "  learn_throughput: 566.116\n",
      "  learn_time_ms: 14096.063\n",
      "  load_throughput: 1112004.13\n",
      "  load_time_ms: 7.176\n",
      "  sample_throughput: 518.599\n",
      "  sample_time_ms: 15387.617\n",
      "  update_time_ms: 6.131\n",
      "timestamp: 1643547119\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5418420\n",
      "training_iteration: 679\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5434350\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-52-29\n",
      "done: false\n",
      "episode_len_mean: 118.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 69\n",
      "episodes_total: 30330\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48196755776802697\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016279365970445762\n",
      "        policy_loss: -0.07901684202874701\n",
      "        total_loss: 54.88362165768941\n",
      "        vf_explained_var: 0.34679888039827345\n",
      "        vf_loss: 54.9516499598821\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4826061813533306\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015806591797299915\n",
      "        policy_loss: -0.09837759980931877\n",
      "        total_loss: 70.74026135444642\n",
      "        vf_explained_var: 0.1860663150747617\n",
      "        vf_loss: 70.8279692586263\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49191723962624867\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016783564490241362\n",
      "        policy_loss: -0.0625131750991568\n",
      "        total_loss: 86.21782434781393\n",
      "        vf_explained_var: 0.3543453807632128\n",
      "        vf_loss: 86.26900859514872\n",
      "  num_agent_steps_sampled: 5434350\n",
      "  num_agent_steps_trained: 5434350\n",
      "  num_steps_sampled: 5434380\n",
      "  num_steps_trained: 5434380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 681\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.684999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.0\n",
      "  player_1: 37.666666666666664\n",
      "  player_2: 33.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.0533333333333332\n",
      "  player_1: -2.196666666666667\n",
      "  player_2: 2.1433333333333326\n",
      "policy_reward_min:\n",
      "  player_0: -22.333333333333336\n",
      "  player_1: -42.333333333333336\n",
      "  player_2: -41.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08639669892207724\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2978289634015583\n",
      "  mean_inference_ms: 1.6085345198848529\n",
      "  mean_raw_obs_processing_ms: 0.21059026338518547\n",
      "time_since_restore: 10911.94246339798\n",
      "time_this_iter_s: 15.615315198898315\n",
      "time_total_s: 10911.94246339798\n",
      "timers:\n",
      "  learn_throughput: 565.999\n",
      "  learn_time_ms: 14098.972\n",
      "  load_throughput: 1122223.687\n",
      "  load_time_ms: 7.111\n",
      "  sample_throughput: 522.025\n",
      "  sample_time_ms: 15286.629\n",
      "  update_time_ms: 6.099\n",
      "timestamp: 1643547149\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5434380\n",
      "training_iteration: 681\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5450310\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-53-00\n",
      "done: false\n",
      "episode_len_mean: 120.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 65\n",
      "episodes_total: 30459\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46420744612813\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01600276657325556\n",
      "        policy_loss: -0.08155005410313607\n",
      "        total_loss: 69.05595677375794\n",
      "        vf_explained_var: 0.35791468461354575\n",
      "        vf_loss: 69.12670454661051\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49654087339838343\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017623700414465968\n",
      "        policy_loss: -0.08100935209542513\n",
      "        total_loss: 67.58531209468842\n",
      "        vf_explained_var: 0.15398478865623474\n",
      "        vf_loss: 67.65442529996236\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49510142927368483\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016720912499111288\n",
      "        policy_loss: -0.07902612946306665\n",
      "        total_loss: 55.838642218112945\n",
      "        vf_explained_var: 0.41930802752574287\n",
      "        vf_loss: 55.90638168334961\n",
      "  num_agent_steps_sampled: 5450310\n",
      "  num_agent_steps_trained: 5450310\n",
      "  num_steps_sampled: 5450340\n",
      "  num_steps_trained: 5450340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 683\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.273684210526316\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.666666666666664\n",
      "  player_1: 29.0\n",
      "  player_2: 28.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.206666666666666\n",
      "  player_1: -0.7033333333333336\n",
      "  player_2: 1.496666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -31.666666666666664\n",
      "  player_1: -48.0\n",
      "  player_2: -48.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08649333411407416\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29816026486693675\n",
      "  mean_inference_ms: 1.610833759730516\n",
      "  mean_raw_obs_processing_ms: 0.21073081043565012\n",
      "time_since_restore: 10942.759024143219\n",
      "time_this_iter_s: 15.367257833480835\n",
      "time_total_s: 10942.759024143219\n",
      "timers:\n",
      "  learn_throughput: 571.356\n",
      "  learn_time_ms: 13966.773\n",
      "  load_throughput: 1075072.219\n",
      "  load_time_ms: 7.423\n",
      "  sample_throughput: 523.766\n",
      "  sample_time_ms: 15235.806\n",
      "  update_time_ms: 6.04\n",
      "timestamp: 1643547180\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5450340\n",
      "training_iteration: 683\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5466271\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-53-30\n",
      "done: false\n",
      "episode_len_mean: 124.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 64\n",
      "episodes_total: 30590\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47149159833788873\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016841167158381437\n",
      "        policy_loss: -0.060378064193452395\n",
      "        total_loss: 36.18570017019908\n",
      "        vf_explained_var: 0.3114020632704099\n",
      "        vf_loss: 36.234710369110104\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.489528155674537\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016678476621097314\n",
      "        policy_loss: -0.04684725649654865\n",
      "        total_loss: 65.77021723429363\n",
      "        vf_explained_var: 0.19679058571656546\n",
      "        vf_loss: 65.8058065144221\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4890983738998572\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015164206962481141\n",
      "        policy_loss: -0.12098399123176933\n",
      "        total_loss: 59.4481573009491\n",
      "        vf_explained_var: 0.41533465613921483\n",
      "        vf_loss: 59.55890540917714\n",
      "  num_agent_steps_sampled: 5466271\n",
      "  num_agent_steps_trained: 5466271\n",
      "  num_steps_sampled: 5466300\n",
      "  num_steps_trained: 5466300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 685\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.983333333333333\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.0\n",
      "  player_1: 32.666666666666664\n",
      "  player_2: 36.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.2299999999999995\n",
      "  player_1: 0.7999999999999997\n",
      "  player_2: -0.030000000000000426\n",
      "policy_reward_min:\n",
      "  player_0: -28.666666666666664\n",
      "  player_1: -41.333333333333336\n",
      "  player_2: -44.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08639321387558574\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2979344402991258\n",
      "  mean_inference_ms: 1.6103007643895544\n",
      "  mean_raw_obs_processing_ms: 0.2105664919923651\n",
      "time_since_restore: 10972.762502193451\n",
      "time_this_iter_s: 15.098075866699219\n",
      "time_total_s: 10972.762502193451\n",
      "timers:\n",
      "  learn_throughput: 573.344\n",
      "  learn_time_ms: 13918.355\n",
      "  load_throughput: 1080182.854\n",
      "  load_time_ms: 7.388\n",
      "  sample_throughput: 525.003\n",
      "  sample_time_ms: 15199.917\n",
      "  update_time_ms: 6.01\n",
      "timestamp: 1643547210\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5466300\n",
      "training_iteration: 685\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5482230\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-54-02\n",
      "done: false\n",
      "episode_len_mean: 125.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 62\n",
      "episodes_total: 30712\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4919759739935398\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016420327645015883\n",
      "        policy_loss: -0.06748499294742942\n",
      "        total_loss: 56.73745444615682\n",
      "        vf_explained_var: 0.188124830921491\n",
      "        vf_loss: 56.79385577519735\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49839951972166696\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016628397380845854\n",
      "        policy_loss: -0.1095910395681858\n",
      "        total_loss: 76.8232969379425\n",
      "        vf_explained_var: 0.2772133247057597\n",
      "        vf_loss: 76.92166393915812\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4790907301008701\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017086632161590388\n",
      "        policy_loss: -0.07309049270115793\n",
      "        total_loss: 47.311413391431174\n",
      "        vf_explained_var: 0.4044279146194458\n",
      "        vf_loss: 47.37297046661377\n",
      "  num_agent_steps_sampled: 5482230\n",
      "  num_agent_steps_trained: 5482230\n",
      "  num_steps_sampled: 5482260\n",
      "  num_steps_trained: 5482260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 687\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.809999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.0\n",
      "  player_1: 26.666666666666664\n",
      "  player_2: 27.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.706666666666667\n",
      "  player_1: -1.553333333333334\n",
      "  player_2: 1.8466666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -39.333333333333336\n",
      "  player_1: -39.666666666666664\n",
      "  player_2: -43.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08646600321473435\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29774590162628434\n",
      "  mean_inference_ms: 1.6081707718188278\n",
      "  mean_raw_obs_processing_ms: 0.2106333036895289\n",
      "time_since_restore: 11004.435584068298\n",
      "time_this_iter_s: 16.25971531867981\n",
      "time_total_s: 11004.435584068298\n",
      "timers:\n",
      "  learn_throughput: 568.475\n",
      "  learn_time_ms: 14037.561\n",
      "  load_throughput: 1084849.816\n",
      "  load_time_ms: 7.356\n",
      "  sample_throughput: 526.593\n",
      "  sample_time_ms: 15154.02\n",
      "  update_time_ms: 5.582\n",
      "timestamp: 1643547242\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5482260\n",
      "training_iteration: 687\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5498194\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-54-34\n",
      "done: false\n",
      "episode_len_mean: 123.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 66\n",
      "episodes_total: 30847\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47621328085660936\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01641086709474621\n",
      "        policy_loss: -0.08724783771050473\n",
      "        total_loss: 57.05187714417775\n",
      "        vf_explained_var: 0.1741432300209999\n",
      "        vf_loss: 57.128047421773275\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4852126358449459\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016933519555944562\n",
      "        policy_loss: -0.06885704891756177\n",
      "        total_loss: 49.64643706798554\n",
      "        vf_explained_var: -0.025793624818325044\n",
      "        vf_loss: 49.70386418263117\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49084005658825236\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015619828760164864\n",
      "        policy_loss: -0.073799728838106\n",
      "        total_loss: 70.85617036501567\n",
      "        vf_explained_var: 0.23182778000831605\n",
      "        vf_loss: 70.91942646662395\n",
      "  num_agent_steps_sampled: 5498194\n",
      "  num_agent_steps_trained: 5498194\n",
      "  num_steps_sampled: 5498220\n",
      "  num_steps_trained: 5498220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 689\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.06842105263158\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.14736842105262\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 46.0\n",
      "  player_1: 30.666666666666664\n",
      "  player_2: 27.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.6533333333333338\n",
      "  player_1: -1.7066666666666663\n",
      "  player_2: 3.0533333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -34.0\n",
      "  player_1: -32.666666666666664\n",
      "  player_2: -55.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08642974243062683\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2979730115542059\n",
      "  mean_inference_ms: 1.6108883534501681\n",
      "  mean_raw_obs_processing_ms: 0.21059497191707788\n",
      "time_since_restore: 11035.896514892578\n",
      "time_this_iter_s: 14.914496660232544\n",
      "time_total_s: 11035.896514892578\n",
      "timers:\n",
      "  learn_throughput: 560.584\n",
      "  learn_time_ms: 14235.144\n",
      "  load_throughput: 1082985.909\n",
      "  load_time_ms: 7.369\n",
      "  sample_throughput: 516.537\n",
      "  sample_time_ms: 15449.024\n",
      "  update_time_ms: 5.581\n",
      "timestamp: 1643547274\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5498220\n",
      "training_iteration: 689\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5514151\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-55-05\n",
      "done: false\n",
      "episode_len_mean: 125.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 65\n",
      "episodes_total: 30975\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4763728836675485\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01675566592787921\n",
      "        policy_loss: -0.08353628745147337\n",
      "        total_loss: 63.3587789106369\n",
      "        vf_explained_var: 0.1593984743952751\n",
      "        vf_loss: 63.43100489139557\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48926977008581163\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01758191686580176\n",
      "        policy_loss: -0.09652205819264054\n",
      "        total_loss: 49.36139911015828\n",
      "        vf_explained_var: 0.3656324937939644\n",
      "        vf_loss: 49.44605342070262\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4838451720774174\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017275690251553896\n",
      "        policy_loss: -0.06962600075018903\n",
      "        total_loss: 62.94262435595194\n",
      "        vf_explained_var: 0.3942055583993594\n",
      "        vf_loss: 63.000589658419294\n",
      "  num_agent_steps_sampled: 5514151\n",
      "  num_agent_steps_trained: 5514151\n",
      "  num_steps_sampled: 5514180\n",
      "  num_steps_trained: 5514180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 691\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.720000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.10999999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.0\n",
      "  player_1: 35.333333333333336\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: -1.1500000000000006\n",
      "  player_1: 0.48999999999999944\n",
      "  player_2: 3.6599999999999993\n",
      "policy_reward_min:\n",
      "  player_0: -68.0\n",
      "  player_1: -32.66666666666667\n",
      "  player_2: -50.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08646447590311249\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2982314526250091\n",
      "  mean_inference_ms: 1.610374893295903\n",
      "  mean_raw_obs_processing_ms: 0.21068891188669298\n",
      "time_since_restore: 11067.285359859467\n",
      "time_this_iter_s: 16.251651525497437\n",
      "time_total_s: 11067.285359859467\n",
      "timers:\n",
      "  learn_throughput: 556.791\n",
      "  learn_time_ms: 14332.121\n",
      "  load_throughput: 1030820.822\n",
      "  load_time_ms: 7.741\n",
      "  sample_throughput: 514.859\n",
      "  sample_time_ms: 15499.396\n",
      "  update_time_ms: 5.5\n",
      "timestamp: 1643547305\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5514180\n",
      "training_iteration: 691\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5530112\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-55-36\n",
      "done: false\n",
      "episode_len_mean: 127.54\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 66\n",
      "episodes_total: 31099\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46701957608262695\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016081665026922563\n",
      "        policy_loss: -0.07778053938100736\n",
      "        total_loss: 52.48513041496277\n",
      "        vf_explained_var: 0.0989623204867045\n",
      "        vf_loss: 52.5520558087031\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4824284415443738\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01768008812196778\n",
      "        policy_loss: -0.07448764660085241\n",
      "        total_loss: 60.80677520751953\n",
      "        vf_explained_var: 0.17753862341245016\n",
      "        vf_loss: 60.86932895342509\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48772233292460443\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016543005631556296\n",
      "        policy_loss: -0.0952657831289495\n",
      "        total_loss: 60.1499226140976\n",
      "        vf_explained_var: 0.16493611852327983\n",
      "        vf_loss: 60.23402179876963\n",
      "  num_agent_steps_sampled: 5530112\n",
      "  num_agent_steps_trained: 5530112\n",
      "  num_steps_sampled: 5530140\n",
      "  num_steps_trained: 5530140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 693\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.21578947368421\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.17894736842105\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.666666666666664\n",
      "  player_1: 35.0\n",
      "  player_2: 45.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.96\n",
      "  player_1: -1.15\n",
      "  player_2: 2.1899999999999995\n",
      "policy_reward_min:\n",
      "  player_0: -48.333333333333336\n",
      "  player_1: -40.0\n",
      "  player_2: -48.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08646807754053779\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2979440019947438\n",
      "  mean_inference_ms: 1.6090899183114107\n",
      "  mean_raw_obs_processing_ms: 0.2107482888131231\n",
      "time_since_restore: 11098.056530475616\n",
      "time_this_iter_s: 15.476903200149536\n",
      "time_total_s: 11098.056530475616\n",
      "timers:\n",
      "  learn_throughput: 557.086\n",
      "  learn_time_ms: 14324.527\n",
      "  load_throughput: 1026376.43\n",
      "  load_time_ms: 7.775\n",
      "  sample_throughput: 513.392\n",
      "  sample_time_ms: 15543.665\n",
      "  update_time_ms: 5.577\n",
      "timestamp: 1643547336\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5530140\n",
      "training_iteration: 693\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5546074\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-56-07\n",
      "done: false\n",
      "episode_len_mean: 119.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 69\n",
      "episodes_total: 31232\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47194090237220127\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017360947862738008\n",
      "        policy_loss: -0.09373256246248882\n",
      "        total_loss: 65.84641704241434\n",
      "        vf_explained_var: 0.3431719903151194\n",
      "        vf_loss: 65.9284307384491\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5006697563330332\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01624511308175632\n",
      "        policy_loss: -0.10201392984949052\n",
      "        total_loss: 63.99361290454865\n",
      "        vf_explained_var: 0.43847046593825023\n",
      "        vf_loss: 64.08466135978699\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4894917466739814\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0150143672626776\n",
      "        policy_loss: -0.035308170846352974\n",
      "        total_loss: 60.884580928484596\n",
      "        vf_explained_var: 0.31816527555386226\n",
      "        vf_loss: 60.90975439230601\n",
      "  num_agent_steps_sampled: 5546074\n",
      "  num_agent_steps_trained: 5546074\n",
      "  num_steps_sampled: 5546100\n",
      "  num_steps_trained: 5546100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 695\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.468421052631578\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.23157894736843\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.0\n",
      "  player_1: 30.0\n",
      "  player_2: 43.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 3.82\n",
      "  player_1: -3.6100000000000008\n",
      "  player_2: 2.79\n",
      "policy_reward_min:\n",
      "  player_0: -31.0\n",
      "  player_1: -38.0\n",
      "  player_2: -45.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0863853623200077\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2975920251323514\n",
      "  mean_inference_ms: 1.6077065040555096\n",
      "  mean_raw_obs_processing_ms: 0.2105653426650815\n",
      "time_since_restore: 11129.488815069199\n",
      "time_this_iter_s: 15.459280729293823\n",
      "time_total_s: 11129.488815069199\n",
      "timers:\n",
      "  learn_throughput: 551.596\n",
      "  learn_time_ms: 14467.102\n",
      "  load_throughput: 1022269.846\n",
      "  load_time_ms: 7.806\n",
      "  sample_throughput: 509.547\n",
      "  sample_time_ms: 15660.962\n",
      "  update_time_ms: 5.528\n",
      "timestamp: 1643547367\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5546100\n",
      "training_iteration: 695\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5562031\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-56-38\n",
      "done: false\n",
      "episode_len_mean: 118.28\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 66\n",
      "episodes_total: 31363\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48324065953493117\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016201406253512688\n",
      "        policy_loss: -0.08173262588058909\n",
      "        total_loss: 68.66899829864502\n",
      "        vf_explained_var: 0.29730470856030783\n",
      "        vf_loss: 68.7397949886322\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4993401588002841\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015055419641636642\n",
      "        policy_loss: -0.0713409590907395\n",
      "        total_loss: 85.92718396345775\n",
      "        vf_explained_var: 0.29579639703035354\n",
      "        vf_loss: 85.98836278120676\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48256627773245175\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01625792647621059\n",
      "        policy_loss: -0.08338222845767936\n",
      "        total_loss: 74.80950156529744\n",
      "        vf_explained_var: 0.02426933705806732\n",
      "        vf_loss: 74.88190961519877\n",
      "  num_agent_steps_sampled: 5562031\n",
      "  num_agent_steps_trained: 5562031\n",
      "  num_steps_sampled: 5562060\n",
      "  num_steps_trained: 5562060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 697\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.377777777777778\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 41.33333333333333\n",
      "  player_1: 38.0\n",
      "  player_2: 48.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -2.0466666666666664\n",
      "  player_1: 0.4933333333333334\n",
      "  player_2: 4.553333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -44.0\n",
      "  player_1: -57.666666666666664\n",
      "  player_2: -29.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08645726041093178\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29788379086325467\n",
      "  mean_inference_ms: 1.6089447114135689\n",
      "  mean_raw_obs_processing_ms: 0.21069669165327246\n",
      "time_since_restore: 11159.820719003677\n",
      "time_this_iter_s: 14.733309030532837\n",
      "time_total_s: 11159.820719003677\n",
      "timers:\n",
      "  learn_throughput: 556.738\n",
      "  learn_time_ms: 14333.504\n",
      "  load_throughput: 944881.741\n",
      "  load_time_ms: 8.446\n",
      "  sample_throughput: 507.703\n",
      "  sample_time_ms: 15717.85\n",
      "  update_time_ms: 5.507\n",
      "timestamp: 1643547398\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5562060\n",
      "training_iteration: 697\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5577990\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-57-10\n",
      "done: false\n",
      "episode_len_mean: 115.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 67\n",
      "episodes_total: 31500\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48504485974709194\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016679366116983375\n",
      "        policy_loss: -0.1039140670063595\n",
      "        total_loss: 48.36699323018392\n",
      "        vf_explained_var: 0.3163409542044004\n",
      "        vf_loss: 48.459648880958554\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5035919753710428\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01638054047132755\n",
      "        policy_loss: -0.034190749634678164\n",
      "        total_loss: 50.03565163612366\n",
      "        vf_explained_var: 0.19761665612459184\n",
      "        vf_loss: 50.058785619735715\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4786781464020411\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016452276566760324\n",
      "        policy_loss: -0.10513754267555972\n",
      "        total_loss: 54.12300909042359\n",
      "        vf_explained_var: 0.1744305244088173\n",
      "        vf_loss: 54.217041165033976\n",
      "  num_agent_steps_sampled: 5577990\n",
      "  num_agent_steps_trained: 5577990\n",
      "  num_steps_sampled: 5578020\n",
      "  num_steps_trained: 5578020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 699\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.895000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.0\n",
      "  player_1: 38.333333333333336\n",
      "  player_2: 27.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -0.02999999999999993\n",
      "  player_1: -1.22\n",
      "  player_2: 4.25\n",
      "policy_reward_min:\n",
      "  player_0: -29.333333333333336\n",
      "  player_1: -30.666666666666664\n",
      "  player_2: -45.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08648483000068971\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2978341213701628\n",
      "  mean_inference_ms: 1.6086854307003886\n",
      "  mean_raw_obs_processing_ms: 0.21070818673201536\n",
      "time_since_restore: 11191.93024110794\n",
      "time_this_iter_s: 16.093653202056885\n",
      "time_total_s: 11191.93024110794\n",
      "timers:\n",
      "  learn_throughput: 554.223\n",
      "  learn_time_ms: 14398.545\n",
      "  load_throughput: 774096.653\n",
      "  load_time_ms: 10.309\n",
      "  sample_throughput: 514.498\n",
      "  sample_time_ms: 15510.258\n",
      "  update_time_ms: 5.431\n",
      "timestamp: 1643547430\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5578020\n",
      "training_iteration: 699\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5593953\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-57-40\n",
      "done: false\n",
      "episode_len_mean: 118.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 69\n",
      "episodes_total: 31638\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4719376396139463\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015699008933111144\n",
      "        policy_loss: -0.11491969528918465\n",
      "        total_loss: 53.94088966051738\n",
      "        vf_explained_var: 0.37811719795068105\n",
      "        vf_loss: 54.0452126010259\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4959710096319517\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015802716631116785\n",
      "        policy_loss: -0.06391793175290028\n",
      "        total_loss: 76.82174348513286\n",
      "        vf_explained_var: 0.3042081547776858\n",
      "        vf_loss: 76.87499478022258\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.485468037823836\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01649957939195398\n",
      "        policy_loss: -0.0571592964604497\n",
      "        total_loss: 74.72949857076009\n",
      "        vf_explained_var: 0.21968567510445913\n",
      "        vf_loss: 74.77552064259847\n",
      "  num_agent_steps_sampled: 5593953\n",
      "  num_agent_steps_trained: 5593953\n",
      "  num_steps_sampled: 5593980\n",
      "  num_steps_trained: 5593980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 701\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.705555555555557\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.21111111111112\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.0\n",
      "  player_1: 33.66666666666667\n",
      "  player_2: 42.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.7799999999999998\n",
      "  player_1: -2.4800000000000004\n",
      "  player_2: 3.7\n",
      "policy_reward_min:\n",
      "  player_0: -27.666666666666664\n",
      "  player_1: -47.0\n",
      "  player_2: -31.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08639459496377064\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29817357166946334\n",
      "  mean_inference_ms: 1.6103420678835465\n",
      "  mean_raw_obs_processing_ms: 0.21066866076966595\n",
      "time_since_restore: 11222.015547037125\n",
      "time_this_iter_s: 14.82047986984253\n",
      "time_total_s: 11222.015547037125\n",
      "timers:\n",
      "  learn_throughput: 559.126\n",
      "  learn_time_ms: 14272.272\n",
      "  load_throughput: 805484.665\n",
      "  load_time_ms: 9.907\n",
      "  sample_throughput: 510.247\n",
      "  sample_time_ms: 15639.485\n",
      "  update_time_ms: 5.529\n",
      "timestamp: 1643547460\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5593980\n",
      "training_iteration: 701\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5609914\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-58-10\n",
      "done: false\n",
      "episode_len_mean: 116.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 31777\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.476224319289128\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016395569546973167\n",
      "        policy_loss: -0.08658846196408074\n",
      "        total_loss: 57.09223537445068\n",
      "        vf_explained_var: 0.409007447262605\n",
      "        vf_loss: 57.167756916681924\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4999335617820422\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015516030568372419\n",
      "        policy_loss: -0.11022853335365652\n",
      "        total_loss: 48.912227489153544\n",
      "        vf_explained_var: 0.17870308727025985\n",
      "        vf_loss: 49.01198273181915\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48443488890926045\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016690334079636767\n",
      "        policy_loss: -0.024799382040898005\n",
      "        total_loss: 48.38599939028422\n",
      "        vf_explained_var: 0.26619196981191634\n",
      "        vf_loss: 48.39953283468882\n",
      "  num_agent_steps_sampled: 5609914\n",
      "  num_agent_steps_trained: 5609914\n",
      "  num_steps_sampled: 5609940\n",
      "  num_steps_trained: 5609940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 703\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.294736842105264\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.09999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.33333333333333\n",
      "  player_1: 31.666666666666664\n",
      "  player_2: 32.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -0.1266666666666667\n",
      "  player_1: -0.5766666666666668\n",
      "  player_2: 3.703333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -37.666666666666664\n",
      "  player_1: -41.666666666666664\n",
      "  player_2: -31.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08641743351338035\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29735775430004696\n",
      "  mean_inference_ms: 1.6066933424021699\n",
      "  mean_raw_obs_processing_ms: 0.21066304202969233\n",
      "time_since_restore: 11251.603941202164\n",
      "time_this_iter_s: 14.857878684997559\n",
      "time_total_s: 11251.603941202164\n",
      "timers:\n",
      "  learn_throughput: 563.786\n",
      "  learn_time_ms: 14154.295\n",
      "  load_throughput: 843061.316\n",
      "  load_time_ms: 9.466\n",
      "  sample_throughput: 516.744\n",
      "  sample_time_ms: 15442.844\n",
      "  update_time_ms: 5.533\n",
      "timestamp: 1643547490\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5609940\n",
      "training_iteration: 703\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5625872\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-58-40\n",
      "done: false\n",
      "episode_len_mean: 122.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 67\n",
      "episodes_total: 31914\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4800689434508483\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015804155330431512\n",
      "        policy_loss: -0.09149248133103052\n",
      "        total_loss: 92.21125429471334\n",
      "        vf_explained_var: 0.29196201235055924\n",
      "        vf_loss: 92.2920791053772\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49607920308907827\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016943196360552974\n",
      "        policy_loss: -0.06394495545886457\n",
      "        total_loss: 69.46448653062184\n",
      "        vf_explained_var: 0.41353191882371904\n",
      "        vf_loss: 69.51699502944946\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4993423244357109\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018764384028084655\n",
      "        policy_loss: -0.0806041319295764\n",
      "        total_loss: 80.75748380502066\n",
      "        vf_explained_var: 0.32389771233002346\n",
      "        vf_loss: 80.82542208194732\n",
      "  num_agent_steps_sampled: 5625872\n",
      "  num_agent_steps_trained: 5625872\n",
      "  num_steps_sampled: 5625900\n",
      "  num_steps_trained: 5625900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 705\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.021052631578947\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.16842105263157\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.33333333333333\n",
      "  player_1: 39.0\n",
      "  player_2: 33.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.12666666666666665\n",
      "  player_1: 2.216666666666667\n",
      "  player_2: 0.6566666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -51.0\n",
      "  player_1: -39.0\n",
      "  player_2: -37.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08631717632015386\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29745458085187865\n",
      "  mean_inference_ms: 1.6078576424900362\n",
      "  mean_raw_obs_processing_ms: 0.2104768619829197\n",
      "time_since_restore: 11282.396473646164\n",
      "time_this_iter_s: 14.859943389892578\n",
      "time_total_s: 11282.396473646164\n",
      "timers:\n",
      "  learn_throughput: 566.468\n",
      "  learn_time_ms: 14087.302\n",
      "  load_throughput: 834354.45\n",
      "  load_time_ms: 9.564\n",
      "  sample_throughput: 518.954\n",
      "  sample_time_ms: 15377.098\n",
      "  update_time_ms: 7.763\n",
      "timestamp: 1643547520\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5625900\n",
      "training_iteration: 705\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5641830\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-59-15\n",
      "done: false\n",
      "episode_len_mean: 121.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 65\n",
      "episodes_total: 32043\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4828591027855873\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017466444262436768\n",
      "        policy_loss: -0.07765252949049076\n",
      "        total_loss: 61.25943850517273\n",
      "        vf_explained_var: 0.19312470813592275\n",
      "        vf_loss: 61.325301036834716\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5000877426564694\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016129521844635234\n",
      "        policy_loss: -0.07322954500404497\n",
      "        total_loss: 80.87550888379415\n",
      "        vf_explained_var: 0.1410495991508166\n",
      "        vf_loss: 80.93785119533538\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48260693952441214\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015601446431783189\n",
      "        policy_loss: -0.10678454772258798\n",
      "        total_loss: 51.22529027620951\n",
      "        vf_explained_var: 0.046242747902870175\n",
      "        vf_loss: 51.32154390335083\n",
      "  num_agent_steps_sampled: 5641830\n",
      "  num_agent_steps_trained: 5641830\n",
      "  num_steps_sampled: 5641860\n",
      "  num_steps_trained: 5641860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 707\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.559999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.20000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 27.333333333333336\n",
      "  player_1: 36.0\n",
      "  player_2: 32.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.57\n",
      "  player_1: -2.619999999999999\n",
      "  player_2: 4.05\n",
      "policy_reward_min:\n",
      "  player_0: -43.0\n",
      "  player_1: -53.666666666666664\n",
      "  player_2: -34.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0863183071821006\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29765611551284266\n",
      "  mean_inference_ms: 1.608763348179079\n",
      "  mean_raw_obs_processing_ms: 0.21055578882355067\n",
      "time_since_restore: 11316.36578464508\n",
      "time_this_iter_s: 16.838123559951782\n",
      "time_total_s: 11316.36578464508\n",
      "timers:\n",
      "  learn_throughput: 552.133\n",
      "  learn_time_ms: 14453.03\n",
      "  load_throughput: 850914.354\n",
      "  load_time_ms: 9.378\n",
      "  sample_throughput: 515.733\n",
      "  sample_time_ms: 15473.112\n",
      "  update_time_ms: 7.857\n",
      "timestamp: 1643547555\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5641860\n",
      "training_iteration: 707\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5657790\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_13-59-45\n",
      "done: false\n",
      "episode_len_mean: 115.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 67\n",
      "episodes_total: 32178\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47981589739521346\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01529025638052417\n",
      "        policy_loss: -0.09437564270260433\n",
      "        total_loss: 74.91312296231588\n",
      "        vf_explained_var: 0.41065923909346264\n",
      "        vf_loss: 74.99717768351238\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49872503583629924\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01626885256702053\n",
      "        policy_loss: -0.0754038929566741\n",
      "        total_loss: 62.12519986152649\n",
      "        vf_explained_var: 0.37474581569433213\n",
      "        vf_loss: 62.1896223894755\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5050435853501161\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016120494912654996\n",
      "        policy_loss: -0.07445372932900986\n",
      "        total_loss: 81.19227565606435\n",
      "        vf_explained_var: 0.4921544434626897\n",
      "        vf_loss: 81.25584854761759\n",
      "  num_agent_steps_sampled: 5657790\n",
      "  num_agent_steps_trained: 5657790\n",
      "  num_steps_sampled: 5657820\n",
      "  num_steps_trained: 5657820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 709\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.415789473684212\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 43.66666666666667\n",
      "  player_1: 44.0\n",
      "  player_2: 31.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 0.03333333333333286\n",
      "  player_1: 1.4733333333333332\n",
      "  player_2: 1.4933333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -67.0\n",
      "  player_1: -44.66666666666667\n",
      "  player_2: -46.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08628084061840703\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29740320854734187\n",
      "  mean_inference_ms: 1.607618183515142\n",
      "  mean_raw_obs_processing_ms: 0.21050265269903354\n",
      "time_since_restore: 11347.230910301208\n",
      "time_this_iter_s: 15.165581464767456\n",
      "time_total_s: 11347.230910301208\n",
      "timers:\n",
      "  learn_throughput: 556.782\n",
      "  learn_time_ms: 14332.364\n",
      "  load_throughput: 1045281.66\n",
      "  load_time_ms: 7.634\n",
      "  sample_throughput: 509.73\n",
      "  sample_time_ms: 15655.353\n",
      "  update_time_ms: 8.014\n",
      "timestamp: 1643547585\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5657820\n",
      "training_iteration: 709\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5673750\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-00-18\n",
      "done: false\n",
      "episode_len_mean: 123.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 64\n",
      "episodes_total: 32313\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4698315054674943\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016290077316131336\n",
      "        policy_loss: -0.07651559139601886\n",
      "        total_loss: 72.47574608802796\n",
      "        vf_explained_var: 0.20525770425796508\n",
      "        vf_loss: 72.54126610914867\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48519721696774165\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017141185656140957\n",
      "        policy_loss: -0.0834576748808225\n",
      "        total_loss: 86.28729412078857\n",
      "        vf_explained_var: 0.12112610787153244\n",
      "        vf_loss: 86.3591812992096\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4825083087881406\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01508491137756702\n",
      "        policy_loss: -0.06737326253205538\n",
      "        total_loss: 82.96818461418152\n",
      "        vf_explained_var: 0.13552602440118788\n",
      "        vf_loss: 83.02537543773651\n",
      "  num_agent_steps_sampled: 5673750\n",
      "  num_agent_steps_trained: 5673750\n",
      "  num_steps_sampled: 5673780\n",
      "  num_steps_trained: 5673780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 711\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.310526315789472\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.21052631578948\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.66666666666667\n",
      "  player_1: 33.333333333333336\n",
      "  player_2: 44.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 3.4466666666666663\n",
      "  player_1: -0.6633333333333343\n",
      "  player_2: 0.21666666666666642\n",
      "policy_reward_min:\n",
      "  player_0: -48.0\n",
      "  player_1: -49.0\n",
      "  player_2: -53.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08626065422146184\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29722911727868373\n",
      "  mean_inference_ms: 1.607502264533877\n",
      "  mean_raw_obs_processing_ms: 0.21042851105488453\n",
      "time_since_restore: 11379.985735654831\n",
      "time_this_iter_s: 16.087932109832764\n",
      "time_total_s: 11379.985735654831\n",
      "timers:\n",
      "  learn_throughput: 546.667\n",
      "  learn_time_ms: 14597.54\n",
      "  load_throughput: 851921.461\n",
      "  load_time_ms: 9.367\n",
      "  sample_throughput: 508.167\n",
      "  sample_time_ms: 15703.494\n",
      "  update_time_ms: 7.911\n",
      "timestamp: 1643547618\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5673780\n",
      "training_iteration: 711\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5689713\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-00-49\n",
      "done: false\n",
      "episode_len_mean: 118.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 70\n",
      "episodes_total: 32450\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46554364611705146\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01752589241189014\n",
      "        policy_loss: -0.07043536353856325\n",
      "        total_loss: 51.30206529935201\n",
      "        vf_explained_var: 0.3491064041852951\n",
      "        vf_loss: 51.360670731862385\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.501402187893788\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016578600291417635\n",
      "        policy_loss: -0.039228864138325055\n",
      "        total_loss: 63.29822570164998\n",
      "        vf_explained_var: 0.26258025149504344\n",
      "        vf_loss: 63.326263952255246\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48271753658850985\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015967089917047303\n",
      "        policy_loss: -0.10716931021461884\n",
      "        total_loss: 68.17270858605703\n",
      "        vf_explained_var: 0.33431500057379404\n",
      "        vf_loss: 68.2691001145045\n",
      "  num_agent_steps_sampled: 5689713\n",
      "  num_agent_steps_trained: 5689713\n",
      "  num_steps_sampled: 5689740\n",
      "  num_steps_trained: 5689740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 713\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.684999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.24000000000001\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.666666666666664\n",
      "  player_1: 33.0\n",
      "  player_2: 31.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.0933333333333333\n",
      "  player_1: 0.30333333333333357\n",
      "  player_2: -0.39666666666666656\n",
      "policy_reward_min:\n",
      "  player_0: -29.666666666666664\n",
      "  player_1: -63.33333333333334\n",
      "  player_2: -53.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08635355959620351\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29730914464687414\n",
      "  mean_inference_ms: 1.606864758604794\n",
      "  mean_raw_obs_processing_ms: 0.21055208764694974\n",
      "time_since_restore: 11410.958445310593\n",
      "time_this_iter_s: 16.20599603652954\n",
      "time_total_s: 11410.958445310593\n",
      "timers:\n",
      "  learn_throughput: 541.479\n",
      "  learn_time_ms: 14737.413\n",
      "  load_throughput: 840721.344\n",
      "  load_time_ms: 9.492\n",
      "  sample_throughput: 504.029\n",
      "  sample_time_ms: 15832.426\n",
      "  update_time_ms: 7.813\n",
      "timestamp: 1643547649\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5689740\n",
      "training_iteration: 713\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5705670\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-01-20\n",
      "done: false\n",
      "episode_len_mean: 124.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 61\n",
      "episodes_total: 32577\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48541073088844616\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016003963914821118\n",
      "        policy_loss: -0.024110372544576725\n",
      "        total_loss: 75.3974960009257\n",
      "        vf_explained_var: 0.3363500264286995\n",
      "        vf_loss: 75.41080392837524\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49228685438632963\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016086199385238444\n",
      "        policy_loss: -0.1389228681878497\n",
      "        total_loss: 54.41929574966431\n",
      "        vf_explained_var: 0.3627613001068433\n",
      "        vf_loss: 54.54736061414083\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4879438210527102\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01592711473271417\n",
      "        policy_loss: -0.08179205092135816\n",
      "        total_loss: 42.138237880071\n",
      "        vf_explained_var: 0.3342305525143941\n",
      "        vf_loss: 42.20927926063538\n",
      "  num_agent_steps_sampled: 5705670\n",
      "  num_agent_steps_trained: 5705670\n",
      "  num_steps_sampled: 5705700\n",
      "  num_steps_trained: 5705700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 715\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.473684210526313\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.21052631578948\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.333333333333336\n",
      "  player_1: 36.0\n",
      "  player_2: 43.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.02000000000000007\n",
      "  player_1: -0.010000000000000356\n",
      "  player_2: 2.9899999999999993\n",
      "policy_reward_min:\n",
      "  player_0: -41.0\n",
      "  player_1: -39.0\n",
      "  player_2: -24.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0862925555040233\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2971663788145746\n",
      "  mean_inference_ms: 1.6064562141374659\n",
      "  mean_raw_obs_processing_ms: 0.21050977569845256\n",
      "time_since_restore: 11441.350242853165\n",
      "time_this_iter_s: 14.806605100631714\n",
      "time_total_s: 11441.350242853165\n",
      "timers:\n",
      "  learn_throughput: 542.933\n",
      "  learn_time_ms: 14697.947\n",
      "  load_throughput: 808428.238\n",
      "  load_time_ms: 9.871\n",
      "  sample_throughput: 500.841\n",
      "  sample_time_ms: 15933.214\n",
      "  update_time_ms: 5.758\n",
      "timestamp: 1643547680\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5705700\n",
      "training_iteration: 715\n",
      "trial_id: default\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0130 14:01:38.851748218    6039 backup_poller.cc:134]       Run client channel backup poller: {\"created\":\"@1643547698.851701780\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1643547698.851694165\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 5721630\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-01-54\n",
      "done: false\n",
      "episode_len_mean: 116.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 67\n",
      "episodes_total: 32717\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47878475606441495\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01658750433445827\n",
      "        policy_loss: -0.042290535749246676\n",
      "        total_loss: 46.93493609746297\n",
      "        vf_explained_var: 0.298561885257562\n",
      "        vf_loss: 46.96603018124898\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5004282867908478\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016380313561950476\n",
      "        policy_loss: -0.08390791020666559\n",
      "        total_loss: 37.23100835720698\n",
      "        vf_explained_var: 0.31523970464865364\n",
      "        vf_loss: 37.30385938167572\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47946124280492464\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016317635873704908\n",
      "        policy_loss: -0.10315793773159385\n",
      "        total_loss: 62.00779568831126\n",
      "        vf_explained_var: 0.17484644244114558\n",
      "        vf_loss: 62.09993915875753\n",
      "  num_agent_steps_sampled: 5721630\n",
      "  num_agent_steps_trained: 5721630\n",
      "  num_steps_sampled: 5721660\n",
      "  num_steps_trained: 5721660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 717\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.239999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.66000000000001\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.333333333333336\n",
      "  player_1: 21.666666666666668\n",
      "  player_2: 34.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.8600000000000008\n",
      "  player_1: -3.42\n",
      "  player_2: 2.56\n",
      "policy_reward_min:\n",
      "  player_0: -31.666666666666664\n",
      "  player_1: -44.666666666666664\n",
      "  player_2: -24.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08629198826149272\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29694663495104867\n",
      "  mean_inference_ms: 1.6054221349658806\n",
      "  mean_raw_obs_processing_ms: 0.21041692953708008\n",
      "time_since_restore: 11475.546339273453\n",
      "time_this_iter_s: 16.02305817604065\n",
      "time_total_s: 11475.546339273453\n",
      "timers:\n",
      "  learn_throughput: 542.4\n",
      "  learn_time_ms: 14712.394\n",
      "  load_throughput: 697531.191\n",
      "  load_time_ms: 11.44\n",
      "  sample_throughput: 497.755\n",
      "  sample_time_ms: 16031.968\n",
      "  update_time_ms: 5.629\n",
      "timestamp: 1643547714\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5721660\n",
      "training_iteration: 717\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5737590\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-02-28\n",
      "done: false\n",
      "episode_len_mean: 117.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 66\n",
      "episodes_total: 32854\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4848565108080705\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016630007988769648\n",
      "        policy_loss: -0.06198321868355076\n",
      "        total_loss: 56.88796876589457\n",
      "        vf_explained_var: 0.3012598584095637\n",
      "        vf_loss: 56.938726728757224\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49107163240512214\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014871510083823787\n",
      "        policy_loss: -0.1187782471254468\n",
      "        total_loss: 63.77239974975586\n",
      "        vf_explained_var: 0.30302050123612084\n",
      "        vf_loss: 63.88113996505737\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48800987203915913\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016215442391120025\n",
      "        policy_loss: -0.04543336340536674\n",
      "        total_loss: 77.68881382147471\n",
      "        vf_explained_var: 0.32991083989540737\n",
      "        vf_loss: 77.72330147743224\n",
      "  num_agent_steps_sampled: 5737590\n",
      "  num_agent_steps_trained: 5737590\n",
      "  num_steps_sampled: 5737620\n",
      "  num_steps_trained: 5737620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 719\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.386363636363637\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.20909090909092\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.333333333333336\n",
      "  player_1: 29.666666666666668\n",
      "  player_2: 55.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -0.536666666666666\n",
      "  player_1: -1.3566666666666665\n",
      "  player_2: 4.8933333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -35.333333333333336\n",
      "  player_1: -50.0\n",
      "  player_2: -33.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08628730582526249\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2970589418187907\n",
      "  mean_inference_ms: 1.60556393846936\n",
      "  mean_raw_obs_processing_ms: 0.21043330367406143\n",
      "time_since_restore: 11509.356032133102\n",
      "time_this_iter_s: 17.335957050323486\n",
      "time_total_s: 11509.356032133102\n",
      "timers:\n",
      "  learn_throughput: 531.865\n",
      "  learn_time_ms: 15003.82\n",
      "  load_throughput: 679180.247\n",
      "  load_time_ms: 11.749\n",
      "  sample_throughput: 498.036\n",
      "  sample_time_ms: 16022.952\n",
      "  update_time_ms: 5.649\n",
      "timestamp: 1643547748\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5737620\n",
      "training_iteration: 719\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5753550\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-02-59\n",
      "done: false\n",
      "episode_len_mean: 118.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 67\n",
      "episodes_total: 32987\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4864316537976265\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01627579304498795\n",
      "        policy_loss: -0.07216667142541458\n",
      "        total_loss: 77.38271457195282\n",
      "        vf_explained_var: 0.2795482523242633\n",
      "        vf_loss: 77.44389490604401\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4889054852724075\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017999229878650266\n",
      "        policy_loss: -0.11348711336031556\n",
      "        total_loss: 55.106278745333356\n",
      "        vf_explained_var: 0.23341826796531678\n",
      "        vf_loss: 55.207616322835285\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48765149647990863\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01725144450866537\n",
      "        policy_loss: -0.06037356325114767\n",
      "        total_loss: 55.72613643248876\n",
      "        vf_explained_var: 0.19456777820984522\n",
      "        vf_loss: 55.77486500263214\n",
      "  num_agent_steps_sampled: 5753550\n",
      "  num_agent_steps_trained: 5753550\n",
      "  num_steps_sampled: 5753580\n",
      "  num_steps_trained: 5753580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 721\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.327777777777776\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.333333333333336\n",
      "  player_1: 41.0\n",
      "  player_2: 33.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.64\n",
      "  player_1: 0.6000000000000001\n",
      "  player_2: 1.7599999999999998\n",
      "policy_reward_min:\n",
      "  player_0: -32.666666666666664\n",
      "  player_1: -32.333333333333336\n",
      "  player_2: -50.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08624365107969169\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2972363653123591\n",
      "  mean_inference_ms: 1.6076330950852453\n",
      "  mean_raw_obs_processing_ms: 0.21043275358026875\n",
      "time_since_restore: 11540.374319791794\n",
      "time_this_iter_s: 14.396708726882935\n",
      "time_total_s: 11540.374319791794\n",
      "timers:\n",
      "  learn_throughput: 538.056\n",
      "  learn_time_ms: 14831.172\n",
      "  load_throughput: 786147.539\n",
      "  load_time_ms: 10.151\n",
      "  sample_throughput: 491.564\n",
      "  sample_time_ms: 16233.894\n",
      "  update_time_ms: 5.728\n",
      "timestamp: 1643547779\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5753580\n",
      "training_iteration: 721\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5769514\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-03-30\n",
      "done: false\n",
      "episode_len_mean: 116.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 69\n",
      "episodes_total: 33127\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4758734156688054\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01568629564449414\n",
      "        policy_loss: -0.07744257724223037\n",
      "        total_loss: 66.78769011497498\n",
      "        vf_explained_var: 0.1583443166812261\n",
      "        vf_loss: 66.85454462687174\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.487223516056935\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016413261026371327\n",
      "        policy_loss: -0.08164743778606255\n",
      "        total_loss: 83.57241188049316\n",
      "        vf_explained_var: 0.2579546975096067\n",
      "        vf_loss: 83.64298018614451\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5030895872910818\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016598506884669177\n",
      "        policy_loss: -0.06026508040105303\n",
      "        total_loss: 65.34711981455484\n",
      "        vf_explained_var: 0.008250189026196798\n",
      "        vf_loss: 65.39618113835652\n",
      "  num_agent_steps_sampled: 5769514\n",
      "  num_agent_steps_trained: 5769514\n",
      "  num_steps_sampled: 5769540\n",
      "  num_steps_trained: 5769540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 723\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.731578947368419\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.0\n",
      "  player_1: 41.0\n",
      "  player_2: 34.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 2.6533333333333338\n",
      "  player_1: -0.09666666666666675\n",
      "  player_2: 0.4433333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -28.666666666666664\n",
      "  player_1: -44.666666666666664\n",
      "  player_2: -68.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08634393508057801\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2971680038184653\n",
      "  mean_inference_ms: 1.6056886340919914\n",
      "  mean_raw_obs_processing_ms: 0.21057035423407452\n",
      "time_since_restore: 11570.943068027496\n",
      "time_this_iter_s: 15.058120250701904\n",
      "time_total_s: 11570.943068027496\n",
      "timers:\n",
      "  learn_throughput: 539.429\n",
      "  learn_time_ms: 14793.411\n",
      "  load_throughput: 792783.958\n",
      "  load_time_ms: 10.066\n",
      "  sample_throughput: 494.495\n",
      "  sample_time_ms: 16137.66\n",
      "  update_time_ms: 5.738\n",
      "timestamp: 1643547810\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5769540\n",
      "training_iteration: 723\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5785470\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-04-02\n",
      "done: false\n",
      "episode_len_mean: 119.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 62\n",
      "episodes_total: 33255\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5054646103580793\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01756599063506921\n",
      "        policy_loss: -0.09337825548214217\n",
      "        total_loss: 51.14841722011566\n",
      "        vf_explained_var: 0.23422650903463363\n",
      "        vf_loss: 51.22993865172068\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5067452267805735\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01767668820123769\n",
      "        policy_loss: -0.09016227939476569\n",
      "        total_loss: 39.73863834857941\n",
      "        vf_explained_var: 0.2370877714951833\n",
      "        vf_loss: 39.81686898390452\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4847868595520655\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016469298567050525\n",
      "        policy_loss: -0.07597646972785393\n",
      "        total_loss: 52.19915814240773\n",
      "        vf_explained_var: 0.31432052711645764\n",
      "        vf_loss: 52.2640178378423\n",
      "  num_agent_steps_sampled: 5785470\n",
      "  num_agent_steps_trained: 5785470\n",
      "  num_steps_sampled: 5785500\n",
      "  num_steps_trained: 5785500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 725\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.552631578947368\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.333333333333336\n",
      "  player_1: 36.0\n",
      "  player_2: 38.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.09\n",
      "  player_1: 0.25999999999999984\n",
      "  player_2: 0.65\n",
      "policy_reward_min:\n",
      "  player_0: -25.333333333333336\n",
      "  player_1: -38.666666666666664\n",
      "  player_2: -33.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.086155102304771\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29667558509042274\n",
      "  mean_inference_ms: 1.6051425987169745\n",
      "  mean_raw_obs_processing_ms: 0.2103010213078482\n",
      "time_since_restore: 11603.298980474472\n",
      "time_this_iter_s: 15.367637157440186\n",
      "time_total_s: 11603.298980474472\n",
      "timers:\n",
      "  learn_throughput: 532.27\n",
      "  learn_time_ms: 14992.392\n",
      "  load_throughput: 801664.761\n",
      "  load_time_ms: 9.954\n",
      "  sample_throughput: 493.699\n",
      "  sample_time_ms: 16163.698\n",
      "  update_time_ms: 5.6\n",
      "timestamp: 1643547842\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5785500\n",
      "training_iteration: 725\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5801430\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-04-33\n",
      "done: false\n",
      "episode_len_mean: 114.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 68\n",
      "episodes_total: 33397\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4815685447057088\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01636745276172822\n",
      "        policy_loss: -0.10159145750047173\n",
      "        total_loss: 68.55942600886027\n",
      "        vf_explained_var: 0.2419669124484062\n",
      "        vf_loss: 68.64996918678284\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4924374564985434\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015510500618577226\n",
      "        policy_loss: -0.08848641484975815\n",
      "        total_loss: 68.27541478792827\n",
      "        vf_explained_var: 0.20603600809971492\n",
      "        vf_loss: 68.3534311580658\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5056903714934985\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015985814938856226\n",
      "        policy_loss: -0.048732931508372226\n",
      "        total_loss: 63.689479301770525\n",
      "        vf_explained_var: 0.4902991385261218\n",
      "        vf_loss: 63.72742180983226\n",
      "  num_agent_steps_sampled: 5801430\n",
      "  num_agent_steps_trained: 5801430\n",
      "  num_steps_sampled: 5801460\n",
      "  num_steps_trained: 5801460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 727\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.110526315789475\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.333333333333336\n",
      "  player_1: 35.666666666666664\n",
      "  player_2: 31.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.4433333333333334\n",
      "  player_1: -0.7966666666666667\n",
      "  player_2: 0.3533333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -35.0\n",
      "  player_1: -46.0\n",
      "  player_2: -41.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08623876149572336\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2969631931722727\n",
      "  mean_inference_ms: 1.6052135393150322\n",
      "  mean_raw_obs_processing_ms: 0.21049063340159307\n",
      "time_since_restore: 11634.383102178574\n",
      "time_this_iter_s: 15.05264663696289\n",
      "time_total_s: 11634.383102178574\n",
      "timers:\n",
      "  learn_throughput: 543.327\n",
      "  learn_time_ms: 14687.287\n",
      "  load_throughput: 981483.371\n",
      "  load_time_ms: 8.131\n",
      "  sample_throughput: 498.681\n",
      "  sample_time_ms: 16002.209\n",
      "  update_time_ms: 5.664\n",
      "timestamp: 1643547873\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5801460\n",
      "training_iteration: 727\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5817391\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-05-04\n",
      "done: false\n",
      "episode_len_mean: 119.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 68\n",
      "episodes_total: 33537\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4880630416671435\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017202160016900052\n",
      "        policy_loss: -0.1088399000869443\n",
      "        total_loss: 58.86728022257487\n",
      "        vf_explained_var: 0.21064716666936875\n",
      "        vf_loss: 58.96450896898905\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49070864463845887\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0172171165435221\n",
      "        policy_loss: -0.0666333406449606\n",
      "        total_loss: 55.69916352589925\n",
      "        vf_explained_var: 0.33825105786323545\n",
      "        vf_loss: 55.75417514324188\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48153126835823057\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017523963666967575\n",
      "        policy_loss: -0.0672815747000277\n",
      "        total_loss: 55.0769012260437\n",
      "        vf_explained_var: 0.35837704261144004\n",
      "        vf_loss: 55.13235410690308\n",
      "  num_agent_steps_sampled: 5817391\n",
      "  num_agent_steps_trained: 5817391\n",
      "  num_steps_sampled: 5817420\n",
      "  num_steps_trained: 5817420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 729\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.66111111111111\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 33.33333333333333\n",
      "  player_2: 38.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.53\n",
      "  player_1: 0.3199999999999997\n",
      "  player_2: 0.1499999999999998\n",
      "policy_reward_min:\n",
      "  player_0: -30.0\n",
      "  player_1: -49.666666666666664\n",
      "  player_2: -43.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08624883066364329\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2967606292707011\n",
      "  mean_inference_ms: 1.6053777578183988\n",
      "  mean_raw_obs_processing_ms: 0.21031850015831188\n",
      "time_since_restore: 11665.303364276886\n",
      "time_this_iter_s: 14.770903587341309\n",
      "time_total_s: 11665.303364276886\n",
      "timers:\n",
      "  learn_throughput: 554.049\n",
      "  learn_time_ms: 14403.056\n",
      "  load_throughput: 997239.415\n",
      "  load_time_ms: 8.002\n",
      "  sample_throughput: 502.741\n",
      "  sample_time_ms: 15872.997\n",
      "  update_time_ms: 5.471\n",
      "timestamp: 1643547904\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5817420\n",
      "training_iteration: 729\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5833350\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-05-37\n",
      "done: false\n",
      "episode_len_mean: 119.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 64\n",
      "episodes_total: 33669\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4826724365353584\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017516472576702427\n",
      "        policy_loss: -0.10083061256290723\n",
      "        total_loss: 48.420318857828775\n",
      "        vf_explained_var: 0.22445461690425872\n",
      "        vf_loss: 48.50932585398356\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4899332146346569\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016123889136325717\n",
      "        policy_loss: -0.06991551159570615\n",
      "        total_loss: 54.109092860221864\n",
      "        vf_explained_var: 0.39248482127984363\n",
      "        vf_loss: 54.168124748865765\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47469833453496296\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016734891747261334\n",
      "        policy_loss: -0.0888191835070029\n",
      "        total_loss: 58.59221477667491\n",
      "        vf_explained_var: 0.3073356490333875\n",
      "        vf_loss: 58.66973780790965\n",
      "  num_agent_steps_sampled: 5833350\n",
      "  num_agent_steps_trained: 5833350\n",
      "  num_steps_sampled: 5833380\n",
      "  num_steps_trained: 5833380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 731\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.668421052631581\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.21052631578948\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.0\n",
      "  player_1: 33.666666666666664\n",
      "  player_2: 32.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -0.05000000000000005\n",
      "  player_1: -0.62\n",
      "  player_2: 3.6700000000000013\n",
      "policy_reward_min:\n",
      "  player_0: -44.333333333333336\n",
      "  player_1: -42.333333333333336\n",
      "  player_2: -40.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08616849979967732\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29691526974834487\n",
      "  mean_inference_ms: 1.6056926030843885\n",
      "  mean_raw_obs_processing_ms: 0.21039439616826447\n",
      "time_since_restore: 11697.981427192688\n",
      "time_this_iter_s: 15.948269844055176\n",
      "time_total_s: 11697.981427192688\n",
      "timers:\n",
      "  learn_throughput: 547.791\n",
      "  learn_time_ms: 14567.601\n",
      "  load_throughput: 956172.912\n",
      "  load_time_ms: 8.346\n",
      "  sample_throughput: 510.579\n",
      "  sample_time_ms: 15629.3\n",
      "  update_time_ms: 5.471\n",
      "timestamp: 1643547937\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5833380\n",
      "training_iteration: 731\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5849312\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-06-09\n",
      "done: false\n",
      "episode_len_mean: 121.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 67\n",
      "episodes_total: 33802\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47451126128435134\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016950312998728047\n",
      "        policy_loss: -0.05789777863770723\n",
      "        total_loss: 53.145379603703816\n",
      "        vf_explained_var: 0.3187060758471489\n",
      "        vf_loss: 53.19183595816294\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47634155362844466\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015715404724805163\n",
      "        policy_loss: -0.1043057547758023\n",
      "        total_loss: 59.23087131341298\n",
      "        vf_explained_var: 0.10472099592288335\n",
      "        vf_loss: 59.324568991661074\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49145126322905225\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016984554708485575\n",
      "        policy_loss: -0.07701371061926086\n",
      "        total_loss: 84.94914006551107\n",
      "        vf_explained_var: 0.238612562417984\n",
      "        vf_loss: 85.01468876520792\n",
      "  num_agent_steps_sampled: 5849312\n",
      "  num_agent_steps_trained: 5849312\n",
      "  num_steps_sampled: 5849340\n",
      "  num_steps_trained: 5849340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 733\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.18421052631579\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.22105263157896\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.0\n",
      "  player_1: 38.333333333333336\n",
      "  player_2: 32.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.833333333333333\n",
      "  player_1: -0.5866666666666664\n",
      "  player_2: 0.7533333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -23.666666666666664\n",
      "  player_1: -27.666666666666664\n",
      "  player_2: -42.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0862297466656725\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2972506850161444\n",
      "  mean_inference_ms: 1.6077815204747379\n",
      "  mean_raw_obs_processing_ms: 0.2104839616279855\n",
      "time_since_restore: 11730.39470410347\n",
      "time_this_iter_s: 15.264387130737305\n",
      "time_total_s: 11730.39470410347\n",
      "timers:\n",
      "  learn_throughput: 541.192\n",
      "  learn_time_ms: 14745.22\n",
      "  load_throughput: 870490.815\n",
      "  load_time_ms: 9.167\n",
      "  sample_throughput: 500.265\n",
      "  sample_time_ms: 15951.551\n",
      "  update_time_ms: 5.593\n",
      "timestamp: 1643547969\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5849340\n",
      "training_iteration: 733\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5865270\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-06-39\n",
      "done: false\n",
      "episode_len_mean: 118.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 65\n",
      "episodes_total: 33933\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4845155609647433\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016606653495380784\n",
      "        policy_loss: -0.09885896353361508\n",
      "        total_loss: 68.04104378859202\n",
      "        vf_explained_var: 0.3369006325801214\n",
      "        vf_loss: 68.1286935710907\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4861233863731225\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017165589375749732\n",
      "        policy_loss: -0.08873680239232878\n",
      "        total_loss: 55.21673235098521\n",
      "        vf_explained_var: 0.28830058336257935\n",
      "        vf_loss: 55.29388226032257\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4854284946620464\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017118025272725768\n",
      "        policy_loss: -0.05326246744642655\n",
      "        total_loss: 59.09524133841197\n",
      "        vf_explained_var: 0.3366165548563004\n",
      "        vf_loss: 59.136948623657226\n",
      "  num_agent_steps_sampled: 5865270\n",
      "  num_agent_steps_trained: 5865270\n",
      "  num_steps_sampled: 5865300\n",
      "  num_steps_trained: 5865300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 735\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.42222222222222\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 27.666666666666664\n",
      "  player_1: 31.333333333333336\n",
      "  player_2: 35.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.3866666666666667\n",
      "  player_1: -1.4533333333333331\n",
      "  player_2: 2.066666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -32.0\n",
      "  player_1: -37.333333333333336\n",
      "  player_2: -42.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08616362253564575\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2966593941664983\n",
      "  mean_inference_ms: 1.6040129200220248\n",
      "  mean_raw_obs_processing_ms: 0.21037826829131867\n",
      "time_since_restore: 11760.568577528\n",
      "time_this_iter_s: 14.358827590942383\n",
      "time_total_s: 11760.568577528\n",
      "timers:\n",
      "  learn_throughput: 549.431\n",
      "  learn_time_ms: 14524.127\n",
      "  load_throughput: 869274.515\n",
      "  load_time_ms: 9.18\n",
      "  sample_throughput: 503.394\n",
      "  sample_time_ms: 15852.387\n",
      "  update_time_ms: 5.705\n",
      "timestamp: 1643547999\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5865300\n",
      "training_iteration: 735\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5881231\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-07-11\n",
      "done: false\n",
      "episode_len_mean: 113.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 34073\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4799887426197529\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017221166394674015\n",
      "        policy_loss: -0.08658838458980123\n",
      "        total_loss: 70.01172702471415\n",
      "        vf_explained_var: 0.24411377420028052\n",
      "        vf_loss: 70.08669056097666\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48754705518484115\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016576711544053067\n",
      "        policy_loss: -0.08702850090960662\n",
      "        total_loss: 48.46266301790873\n",
      "        vf_explained_var: 0.3467894995212555\n",
      "        vf_loss: 48.538502321243286\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48623974914352097\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015338941458702115\n",
      "        policy_loss: -0.0681648157381763\n",
      "        total_loss: 73.02373296896617\n",
      "        vf_explained_var: 0.18447706977526346\n",
      "        vf_loss: 73.0815441608429\n",
      "  num_agent_steps_sampled: 5881231\n",
      "  num_agent_steps_trained: 5881231\n",
      "  num_steps_sampled: 5881260\n",
      "  num_steps_trained: 5881260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 737\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.070000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.20000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 45.33333333333333\n",
      "  player_1: 34.666666666666664\n",
      "  player_2: 37.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 0.6199999999999999\n",
      "  player_1: 1.5499999999999998\n",
      "  player_2: 0.8299999999999996\n",
      "policy_reward_min:\n",
      "  player_0: -36.33333333333333\n",
      "  player_1: -23.0\n",
      "  player_2: -68.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08611490554048296\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2965486876015432\n",
      "  mean_inference_ms: 1.6035594296701832\n",
      "  mean_raw_obs_processing_ms: 0.21042004763551894\n",
      "time_since_restore: 11791.899050712585\n",
      "time_this_iter_s: 16.013071060180664\n",
      "time_total_s: 11791.899050712585\n",
      "timers:\n",
      "  learn_throughput: 548.438\n",
      "  learn_time_ms: 14550.41\n",
      "  load_throughput: 893043.233\n",
      "  load_time_ms: 8.936\n",
      "  sample_throughput: 508.642\n",
      "  sample_time_ms: 15688.819\n",
      "  update_time_ms: 5.729\n",
      "timestamp: 1643548031\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5881260\n",
      "training_iteration: 737\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5897190\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-07-43\n",
      "done: false\n",
      "episode_len_mean: 110.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 34219\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47170409028728805\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01643830854505211\n",
      "        policy_loss: -0.11599662128835916\n",
      "        total_loss: 73.74514324188232\n",
      "        vf_explained_var: 0.36953458925088245\n",
      "        vf_loss: 73.85004417419434\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4885860926906268\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01565261227045691\n",
      "        policy_loss: -0.07301918897156914\n",
      "        total_loss: 64.72713952064514\n",
      "        vf_explained_var: 0.12703627477089563\n",
      "        vf_loss: 64.78959306240081\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4891282110909621\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017470241655098372\n",
      "        policy_loss: -0.0432763025475045\n",
      "        total_loss: 84.80804591496785\n",
      "        vf_explained_var: 0.06412985891103745\n",
      "        vf_loss: 84.83953035036723\n",
      "  num_agent_steps_sampled: 5897190\n",
      "  num_agent_steps_trained: 5897190\n",
      "  num_steps_sampled: 5897220\n",
      "  num_steps_trained: 5897220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 739\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.672222222222224\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 30.0\n",
      "  player_2: 41.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.0066666666666666\n",
      "  player_1: -0.8033333333333335\n",
      "  player_2: 2.7966666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -50.0\n",
      "  player_1: -44.333333333333336\n",
      "  player_2: -46.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08618524292985186\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2967689580262078\n",
      "  mean_inference_ms: 1.6050010718173804\n",
      "  mean_raw_obs_processing_ms: 0.21037510733629838\n",
      "time_since_restore: 11824.216803312302\n",
      "time_this_iter_s: 14.881266355514526\n",
      "time_total_s: 11824.216803312302\n",
      "timers:\n",
      "  learn_throughput: 543.365\n",
      "  learn_time_ms: 14686.248\n",
      "  load_throughput: 877239.477\n",
      "  load_time_ms: 9.097\n",
      "  sample_throughput: 501.388\n",
      "  sample_time_ms: 15915.828\n",
      "  update_time_ms: 5.901\n",
      "timestamp: 1643548063\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5897220\n",
      "training_iteration: 739\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5913150\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-08-13\n",
      "done: false\n",
      "episode_len_mean: 116.28\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 34356\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4922676207621892\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018026438126116772\n",
      "        policy_loss: -0.03613343069795519\n",
      "        total_loss: 61.422370079358416\n",
      "        vf_explained_var: 0.2934563435117404\n",
      "        vf_loss: 61.44633537451426\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49816582202911375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01655254630489727\n",
      "        policy_loss: -0.09774156561431785\n",
      "        total_loss: 60.79019433339437\n",
      "        vf_explained_var: 0.2754483733574549\n",
      "        vf_loss: 60.87676273028056\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4873962744573752\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01625121062024732\n",
      "        policy_loss: -0.10307313976809382\n",
      "        total_loss: 74.97401858011881\n",
      "        vf_explained_var: 0.3090597328543663\n",
      "        vf_loss: 75.066122118632\n",
      "  num_agent_steps_sampled: 5913150\n",
      "  num_agent_steps_trained: 5913150\n",
      "  num_steps_sampled: 5913180\n",
      "  num_steps_trained: 5913180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 741\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.322222222222223\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.66666666666667\n",
      "  player_1: 36.0\n",
      "  player_2: 30.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.75\n",
      "  player_1: 0.10000000000000003\n",
      "  player_2: 1.15\n",
      "policy_reward_min:\n",
      "  player_0: -43.333333333333336\n",
      "  player_1: -49.66666666666667\n",
      "  player_2: -59.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08626879532871945\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2968875171899112\n",
      "  mean_inference_ms: 1.6050262770887906\n",
      "  mean_raw_obs_processing_ms: 0.21049108756967608\n",
      "time_since_restore: 11854.034076690674\n",
      "time_this_iter_s: 14.797755718231201\n",
      "time_total_s: 11854.034076690674\n",
      "timers:\n",
      "  learn_throughput: 554.15\n",
      "  learn_time_ms: 14400.431\n",
      "  load_throughput: 897182.136\n",
      "  load_time_ms: 8.895\n",
      "  sample_throughput: 506.436\n",
      "  sample_time_ms: 15757.174\n",
      "  update_time_ms: 5.801\n",
      "timestamp: 1643548093\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5913180\n",
      "training_iteration: 741\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5929110\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-08-44\n",
      "done: false\n",
      "episode_len_mean: 117.54\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 68\n",
      "episodes_total: 34489\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4639252276221911\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015191764049803283\n",
      "        policy_loss: -0.0854434800442929\n",
      "        total_loss: 72.7256255753835\n",
      "        vf_explained_var: 0.40162592550118764\n",
      "        vf_loss: 72.80081466039022\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48937281042337416\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016306905572406927\n",
      "        policy_loss: -0.10274226314077775\n",
      "        total_loss: 61.03298259894053\n",
      "        vf_explained_var: 0.2418162766098976\n",
      "        vf_loss: 61.124717596371966\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4642451120416323\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016276978630894896\n",
      "        policy_loss: -0.050756965105732285\n",
      "        total_loss: 54.47997062603633\n",
      "        vf_explained_var: 0.3952160025636355\n",
      "        vf_loss: 54.519740889072416\n",
      "  num_agent_steps_sampled: 5929110\n",
      "  num_agent_steps_trained: 5929110\n",
      "  num_steps_sampled: 5929140\n",
      "  num_steps_trained: 5929140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 743\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.13684210526316\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.333333333333336\n",
      "  player_1: 32.333333333333336\n",
      "  player_2: 29.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.883333333333333\n",
      "  player_1: 0.31333333333333313\n",
      "  player_2: 0.8033333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -45.666666666666664\n",
      "  player_1: -33.666666666666664\n",
      "  player_2: -38.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08626745414537675\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2964692856272503\n",
      "  mean_inference_ms: 1.6029908290273085\n",
      "  mean_raw_obs_processing_ms: 0.21037300921190488\n",
      "time_since_restore: 11884.747593164444\n",
      "time_this_iter_s: 15.368280172348022\n",
      "time_total_s: 11884.747593164444\n",
      "timers:\n",
      "  learn_throughput: 561.37\n",
      "  learn_time_ms: 14215.234\n",
      "  load_throughput: 949319.743\n",
      "  load_time_ms: 8.406\n",
      "  sample_throughput: 515.583\n",
      "  sample_time_ms: 15477.638\n",
      "  update_time_ms: 5.762\n",
      "timestamp: 1643548124\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5929140\n",
      "training_iteration: 743\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5945070\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-09-16\n",
      "done: false\n",
      "episode_len_mean: 117.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 65\n",
      "episodes_total: 34623\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47396891832351684\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017010632100951096\n",
      "        policy_loss: -0.0777435583434999\n",
      "        total_loss: 63.89801139831543\n",
      "        vf_explained_var: 0.3489868329962095\n",
      "        vf_loss: 63.96427302837372\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4892920932173729\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01778740787277362\n",
      "        policy_loss: -0.07795854567239682\n",
      "        total_loss: 43.25361107985179\n",
      "        vf_explained_var: 0.3092595840493838\n",
      "        vf_loss: 43.31956316947937\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4803135779996713\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018402995157553618\n",
      "        policy_loss: -0.10168808360894521\n",
      "        total_loss: 79.17053752422333\n",
      "        vf_explained_var: 0.21064425826072694\n",
      "        vf_loss: 79.25980360984802\n",
      "  num_agent_steps_sampled: 5945070\n",
      "  num_agent_steps_trained: 5945070\n",
      "  num_steps_sampled: 5945100\n",
      "  num_steps_trained: 5945100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 745\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.385\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.20000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 27.0\n",
      "  player_1: 24.333333333333332\n",
      "  player_2: 36.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.13333333333333328\n",
      "  player_1: -0.0033333333333335526\n",
      "  player_2: 3.1366666666666663\n",
      "policy_reward_min:\n",
      "  player_0: -53.0\n",
      "  player_1: -23.0\n",
      "  player_2: -37.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08619805050022253\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2965100598641711\n",
      "  mean_inference_ms: 1.6042887292949866\n",
      "  mean_raw_obs_processing_ms: 0.21036148632444118\n",
      "time_since_restore: 11917.222286939621\n",
      "time_this_iter_s: 16.892340183258057\n",
      "time_total_s: 11917.222286939621\n",
      "timers:\n",
      "  learn_throughput: 552.321\n",
      "  learn_time_ms: 14448.117\n",
      "  load_throughput: 956642.97\n",
      "  load_time_ms: 8.342\n",
      "  sample_throughput: 516.58\n",
      "  sample_time_ms: 15447.758\n",
      "  update_time_ms: 5.647\n",
      "timestamp: 1643548156\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5945100\n",
      "training_iteration: 745\n",
      "trial_id: default\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0130 14:09:48.851035606   10363 backup_poller.cc:134]       Run client channel backup poller: {\"created\":\"@1643548188.851011289\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1643548188.851007753\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 5961030\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-09-49\n",
      "done: false\n",
      "episode_len_mean: 122.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 69\n",
      "episodes_total: 34759\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4835018185774485\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017716326884892622\n",
      "        policy_loss: -0.07225073617883027\n",
      "        total_loss: 46.894975295066835\n",
      "        vf_explained_var: 0.4210588486989339\n",
      "        vf_loss: 46.955267523129784\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4854671495159467\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015982580984812672\n",
      "        policy_loss: -0.07472864480068286\n",
      "        total_loss: 47.447853954633075\n",
      "        vf_explained_var: 0.36803756445646285\n",
      "        vf_loss: 47.5117943239212\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45857418606678646\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016487789552550113\n",
      "        policy_loss: -0.10381175804883241\n",
      "        total_loss: 43.607048621177675\n",
      "        vf_explained_var: 0.2685391946633657\n",
      "        vf_loss: 43.69973106543223\n",
      "  num_agent_steps_sampled: 5961030\n",
      "  num_agent_steps_trained: 5961030\n",
      "  num_steps_sampled: 5961060\n",
      "  num_steps_trained: 5961060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 747\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.38888888888889\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 31.0\n",
      "  player_2: 32.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.08\n",
      "  player_1: -0.8199999999999998\n",
      "  player_2: 2.74\n",
      "policy_reward_min:\n",
      "  player_0: -30.0\n",
      "  player_1: -36.333333333333336\n",
      "  player_2: -33.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08603567167244953\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2964431489089971\n",
      "  mean_inference_ms: 1.6038780822376832\n",
      "  mean_raw_obs_processing_ms: 0.21023665177698972\n",
      "time_since_restore: 11949.467523813248\n",
      "time_this_iter_s: 15.073926210403442\n",
      "time_total_s: 11949.467523813248\n",
      "timers:\n",
      "  learn_throughput: 548.939\n",
      "  learn_time_ms: 14537.142\n",
      "  load_throughput: 905348.309\n",
      "  load_time_ms: 8.814\n",
      "  sample_throughput: 502.479\n",
      "  sample_time_ms: 15881.259\n",
      "  update_time_ms: 5.836\n",
      "timestamp: 1643548189\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5961060\n",
      "training_iteration: 747\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5976991\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-10-20\n",
      "done: false\n",
      "episode_len_mean: 121.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 69\n",
      "episodes_total: 34893\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48893867000937463\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017888819970342713\n",
      "        policy_loss: -0.08780451278823118\n",
      "        total_loss: 64.07530249595642\n",
      "        vf_explained_var: 0.3214584428071976\n",
      "        vf_loss: 64.15103195508321\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4688727594912052\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014611751741709705\n",
      "        policy_loss: -0.041022886037050434\n",
      "        total_loss: 94.3464771382014\n",
      "        vf_explained_var: 0.12020370801289876\n",
      "        vf_loss: 94.37763637224833\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4934983163078626\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018028719677328504\n",
      "        policy_loss: -0.11921928896956767\n",
      "        total_loss: 54.97735830307007\n",
      "        vf_explained_var: 0.43537384231885273\n",
      "        vf_loss: 55.08440823872884\n",
      "  num_agent_steps_sampled: 5976991\n",
      "  num_agent_steps_trained: 5976991\n",
      "  num_steps_sampled: 5977020\n",
      "  num_steps_trained: 5977020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 749\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.415789473684208\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.43157894736842\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.666666666666664\n",
      "  player_1: 22.0\n",
      "  player_2: 32.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 2.8633333333333337\n",
      "  player_1: -2.0266666666666664\n",
      "  player_2: 2.1633333333333327\n",
      "policy_reward_min:\n",
      "  player_0: -32.66666666666667\n",
      "  player_1: -58.333333333333336\n",
      "  player_2: -25.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08611343542608697\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2963738871427683\n",
      "  mean_inference_ms: 1.6030440665027248\n",
      "  mean_raw_obs_processing_ms: 0.2103073373160088\n",
      "time_since_restore: 11980.414317369461\n",
      "time_this_iter_s: 15.17904281616211\n",
      "time_total_s: 11980.414317369461\n",
      "timers:\n",
      "  learn_throughput: 554.101\n",
      "  learn_time_ms: 14401.705\n",
      "  load_throughput: 947919.0\n",
      "  load_time_ms: 8.418\n",
      "  sample_throughput: 510.801\n",
      "  sample_time_ms: 15622.528\n",
      "  update_time_ms: 5.693\n",
      "timestamp: 1643548220\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5977020\n",
      "training_iteration: 749\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 5992950\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-10-52\n",
      "done: false\n",
      "episode_len_mean: 109.54\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 75\n",
      "episodes_total: 35037\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47070491562287015\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01855812104268286\n",
      "        policy_loss: -0.10075118158012629\n",
      "        total_loss: 58.96659258842468\n",
      "        vf_explained_var: 0.3115906169017156\n",
      "        vf_loss: 59.054817072550456\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4878108418981234\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016783462314174453\n",
      "        policy_loss: -0.08312539633984367\n",
      "        total_loss: 51.9452792708079\n",
      "        vf_explained_var: 0.3900620034337044\n",
      "        vf_loss: 52.01707574208577\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4916532314817111\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017378458384035865\n",
      "        policy_loss: -0.06364496729026238\n",
      "        total_loss: 62.93936580022176\n",
      "        vf_explained_var: 0.2574556559324265\n",
      "        vf_loss: 62.99128084341685\n",
      "  num_agent_steps_sampled: 5992950\n",
      "  num_agent_steps_trained: 5992950\n",
      "  num_steps_sampled: 5992980\n",
      "  num_steps_trained: 5992980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 751\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.964999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.26000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.666666666666664\n",
      "  player_1: 40.0\n",
      "  player_2: 38.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 0.4533333333333332\n",
      "  player_1: -2.666666666666667\n",
      "  player_2: 5.213333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -33.333333333333336\n",
      "  player_1: -34.666666666666664\n",
      "  player_2: -66.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0860517031612186\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2962671917442965\n",
      "  mean_inference_ms: 1.602410418580946\n",
      "  mean_raw_obs_processing_ms: 0.21037444548433407\n",
      "time_since_restore: 12012.201618671417\n",
      "time_this_iter_s: 16.724022150039673\n",
      "time_total_s: 12012.201618671417\n",
      "timers:\n",
      "  learn_throughput: 546.52\n",
      "  learn_time_ms: 14601.475\n",
      "  load_throughput: 961755.842\n",
      "  load_time_ms: 8.297\n",
      "  sample_throughput: 509.81\n",
      "  sample_time_ms: 15652.9\n",
      "  update_time_ms: 5.72\n",
      "timestamp: 1643548252\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5992980\n",
      "training_iteration: 751\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6008910\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-11-23\n",
      "done: false\n",
      "episode_len_mean: 108.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 70\n",
      "episodes_total: 35180\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4746315468351046\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015694623850329777\n",
      "        policy_loss: -0.10877628864099582\n",
      "        total_loss: 64.93234872500102\n",
      "        vf_explained_var: 0.42183659503857296\n",
      "        vf_loss: 65.03053112983703\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5130315553148588\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017000943645619295\n",
      "        policy_loss: -0.09091196730732917\n",
      "        total_loss: 57.80010736942291\n",
      "        vf_explained_var: 0.23940318564573923\n",
      "        vf_loss: 57.879543862342835\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4767190231879552\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018466218203660675\n",
      "        policy_loss: -0.039638876143532495\n",
      "        total_loss: 46.29454008579254\n",
      "        vf_explained_var: 0.3553730202714602\n",
      "        vf_loss: 46.32171426932017\n",
      "  num_agent_steps_sampled: 6008910\n",
      "  num_agent_steps_trained: 6008910\n",
      "  num_steps_sampled: 6008940\n",
      "  num_steps_trained: 6008940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 753\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.236842105263158\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.333333333333336\n",
      "  player_1: 26.666666666666664\n",
      "  player_2: 28.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.4033333333333333\n",
      "  player_1: -1.0066666666666664\n",
      "  player_2: 2.6033333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -28.666666666666664\n",
      "  player_1: -33.0\n",
      "  player_2: -32.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08608462456452498\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2961325604757396\n",
      "  mean_inference_ms: 1.6012939569263656\n",
      "  mean_raw_obs_processing_ms: 0.2103177928947418\n",
      "time_since_restore: 12042.988042354584\n",
      "time_this_iter_s: 15.634106636047363\n",
      "time_total_s: 12042.988042354584\n",
      "timers:\n",
      "  learn_throughput: 545.418\n",
      "  learn_time_ms: 14630.978\n",
      "  load_throughput: 984940.57\n",
      "  load_time_ms: 8.102\n",
      "  sample_throughput: 504.808\n",
      "  sample_time_ms: 15807.988\n",
      "  update_time_ms: 5.902\n",
      "timestamp: 1643548283\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6008940\n",
      "training_iteration: 753\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6024870\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-11-55\n",
      "done: false\n",
      "episode_len_mean: 106.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 72\n",
      "episodes_total: 35325\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47030464112758635\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017181204330190478\n",
      "        policy_loss: -0.06565048637489478\n",
      "        total_loss: 58.807738043467204\n",
      "        vf_explained_var: 0.4437717085083326\n",
      "        vf_loss: 58.861791149775186\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5156943600376447\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017698063450816523\n",
      "        policy_loss: -0.08081368797769149\n",
      "        total_loss: 66.8979882033666\n",
      "        vf_explained_var: 0.296664084593455\n",
      "        vf_loss: 66.96685578505198\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47895444333553316\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01687715449050984\n",
      "        policy_loss: -0.10362849528590838\n",
      "        total_loss: 57.603873303731284\n",
      "        vf_explained_var: 0.3867060381174088\n",
      "        vf_loss: 57.69610979715983\n",
      "  num_agent_steps_sampled: 6024870\n",
      "  num_agent_steps_trained: 6024870\n",
      "  num_steps_sampled: 6024900\n",
      "  num_steps_trained: 6024900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 755\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.85\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.20000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.333333333333336\n",
      "  player_1: 32.333333333333336\n",
      "  player_2: 29.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 3.4333333333333336\n",
      "  player_1: -3.8266666666666667\n",
      "  player_2: 3.3933333333333326\n",
      "policy_reward_min:\n",
      "  player_0: -31.333333333333336\n",
      "  player_1: -48.33333333333333\n",
      "  player_2: -35.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08604420808015295\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29623335430086756\n",
      "  mean_inference_ms: 1.6028115247712926\n",
      "  mean_raw_obs_processing_ms: 0.21027097521704183\n",
      "time_since_restore: 12075.043724060059\n",
      "time_this_iter_s: 15.904394626617432\n",
      "time_total_s: 12075.043724060059\n",
      "timers:\n",
      "  learn_throughput: 547.042\n",
      "  learn_time_ms: 14587.549\n",
      "  load_throughput: 922106.952\n",
      "  load_time_ms: 8.654\n",
      "  sample_throughput: 501.511\n",
      "  sample_time_ms: 15911.9\n",
      "  update_time_ms: 5.863\n",
      "timestamp: 1643548315\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6024900\n",
      "training_iteration: 755\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6040832\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-12-29\n",
      "done: false\n",
      "episode_len_mean: 117.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 35466\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47971304421623545\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01788579586635971\n",
      "        policy_loss: -0.0932913957691441\n",
      "        total_loss: 51.87146478017171\n",
      "        vf_explained_var: 0.3049773087104162\n",
      "        vf_loss: 51.95268302440643\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5012667821347714\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018039837280433253\n",
      "        policy_loss: -0.07362985898274928\n",
      "        total_loss: 53.92057660738627\n",
      "        vf_explained_var: 0.39790960162878036\n",
      "        vf_loss: 53.98202945709229\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4803666597108046\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017784949576809293\n",
      "        policy_loss: -0.06362750059614579\n",
      "        total_loss: 62.452161712646486\n",
      "        vf_explained_var: 0.4872886817653974\n",
      "        vf_loss: 62.503784510294594\n",
      "  num_agent_steps_sampled: 6040832\n",
      "  num_agent_steps_trained: 6040832\n",
      "  num_steps_sampled: 6040860\n",
      "  num_steps_trained: 6040860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 757\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.985\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.20000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 27.666666666666664\n",
      "  player_1: 32.0\n",
      "  player_2: 26.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.6066666666666674\n",
      "  player_1: -1.3233333333333328\n",
      "  player_2: 1.7166666666666672\n",
      "policy_reward_min:\n",
      "  player_0: -29.666666666666664\n",
      "  player_1: -25.333333333333336\n",
      "  player_2: -38.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08601662568737507\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29612561694054274\n",
      "  mean_inference_ms: 1.6017259052694854\n",
      "  mean_raw_obs_processing_ms: 0.2103465910132541\n",
      "time_since_restore: 12109.637612104416\n",
      "time_this_iter_s: 16.62248134613037\n",
      "time_total_s: 12109.637612104416\n",
      "timers:\n",
      "  learn_throughput: 538.295\n",
      "  learn_time_ms: 14824.591\n",
      "  load_throughput: 956716.8\n",
      "  load_time_ms: 8.341\n",
      "  sample_throughput: 502.294\n",
      "  sample_time_ms: 15887.117\n",
      "  update_time_ms: 5.629\n",
      "timestamp: 1643548349\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6040860\n",
      "training_iteration: 757\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6056795\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-13-00\n",
      "done: false\n",
      "episode_len_mean: 113.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 74\n",
      "episodes_total: 35606\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46910550609230994\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01747021168363214\n",
      "        policy_loss: -0.08519083738327027\n",
      "        total_loss: 69.92143050352732\n",
      "        vf_explained_var: 0.1327936789393425\n",
      "        vf_loss: 69.99482899030049\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48745201220115025\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016700109795416626\n",
      "        policy_loss: -0.07297018333338201\n",
      "        total_loss: 76.85134762128195\n",
      "        vf_explained_var: 0.275164673825105\n",
      "        vf_loss: 76.9130453713735\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47532758007446924\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018129358911775927\n",
      "        policy_loss: -0.08463132127498586\n",
      "        total_loss: 39.713170493443805\n",
      "        vf_explained_var: 0.2885362802942594\n",
      "        vf_loss: 39.785564494133\n",
      "  num_agent_steps_sampled: 6056795\n",
      "  num_agent_steps_trained: 6056795\n",
      "  num_steps_sampled: 6056820\n",
      "  num_steps_trained: 6056820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 759\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.0\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.333333333333336\n",
      "  player_1: 38.0\n",
      "  player_2: 32.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.7000000000000002\n",
      "  player_1: -1.4899999999999993\n",
      "  player_2: 3.79\n",
      "policy_reward_min:\n",
      "  player_0: -34.0\n",
      "  player_1: -37.666666666666664\n",
      "  player_2: -20.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08617025231020406\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29606974734233704\n",
      "  mean_inference_ms: 1.6012075652843143\n",
      "  mean_raw_obs_processing_ms: 0.21035603485137735\n",
      "time_since_restore: 12140.336469888687\n",
      "time_this_iter_s: 15.153982400894165\n",
      "time_total_s: 12140.336469888687\n",
      "timers:\n",
      "  learn_throughput: 539.254\n",
      "  learn_time_ms: 14798.215\n",
      "  load_throughput: 915686.683\n",
      "  load_time_ms: 8.715\n",
      "  sample_throughput: 498.129\n",
      "  sample_time_ms: 16019.957\n",
      "  update_time_ms: 5.736\n",
      "timestamp: 1643548380\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6056820\n",
      "training_iteration: 759\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6072753\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-13-31\n",
      "done: false\n",
      "episode_len_mean: 109.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 72\n",
      "episodes_total: 35748\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4638496532042821\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015803258446094332\n",
      "        policy_loss: -0.06915147359172504\n",
      "        total_loss: 59.43587513446808\n",
      "        vf_explained_var: 0.3879636307557424\n",
      "        vf_loss: 59.49435942490896\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49798123791813853\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01602155140225856\n",
      "        policy_loss: -0.05026758580002934\n",
      "        total_loss: 65.45229361375173\n",
      "        vf_explained_var: 0.3267561345299085\n",
      "        vf_loss: 65.49174644470214\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4840355288485686\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015577335431586334\n",
      "        policy_loss: -0.10143182056645553\n",
      "        total_loss: 91.02968860626221\n",
      "        vf_explained_var: 0.29883449216683705\n",
      "        vf_loss: 91.12060548623403\n",
      "  num_agent_steps_sampled: 6072753\n",
      "  num_agent_steps_trained: 6072753\n",
      "  num_steps_sampled: 6072780\n",
      "  num_steps_trained: 6072780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 761\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.777777777777779\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.333333333333336\n",
      "  player_1: 50.66666666666667\n",
      "  player_2: 36.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 5.326666666666666\n",
      "  player_1: -2.2233333333333336\n",
      "  player_2: -0.10333333333333354\n",
      "policy_reward_min:\n",
      "  player_0: -34.666666666666664\n",
      "  player_1: -35.666666666666664\n",
      "  player_2: -77.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08610232264985437\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29588279697323927\n",
      "  mean_inference_ms: 1.600808427866184\n",
      "  mean_raw_obs_processing_ms: 0.21033481837392684\n",
      "time_since_restore: 12171.020312070847\n",
      "time_this_iter_s: 14.87541913986206\n",
      "time_total_s: 12171.020312070847\n",
      "timers:\n",
      "  learn_throughput: 543.4\n",
      "  learn_time_ms: 14685.311\n",
      "  load_throughput: 766382.877\n",
      "  load_time_ms: 10.413\n",
      "  sample_throughput: 495.907\n",
      "  sample_time_ms: 16091.739\n",
      "  update_time_ms: 5.782\n",
      "timestamp: 1643548411\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6072780\n",
      "training_iteration: 761\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6088710\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-14-01\n",
      "done: false\n",
      "episode_len_mean: 119.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 66\n",
      "episodes_total: 35883\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4918262601892153\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01744081647474862\n",
      "        policy_loss: -0.07985671770758927\n",
      "        total_loss: 46.568497700691225\n",
      "        vf_explained_var: 0.2459520083665848\n",
      "        vf_loss: 46.63658149719238\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4868707330524921\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016573844986536034\n",
      "        policy_loss: -0.07183860794330636\n",
      "        total_loss: 34.71727252960205\n",
      "        vf_explained_var: 0.3713557263215383\n",
      "        vf_loss: 34.77792374610901\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4689477357765039\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016753959939096605\n",
      "        policy_loss: -0.0799176907973985\n",
      "        total_loss: 55.34714848836263\n",
      "        vf_explained_var: 0.05781374941269557\n",
      "        vf_loss: 55.41575737953186\n",
      "  num_agent_steps_sampled: 6088710\n",
      "  num_agent_steps_trained: 6088710\n",
      "  num_steps_sampled: 6088740\n",
      "  num_steps_trained: 6088740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 763\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.494736842105265\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.2421052631579\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 27.0\n",
      "  player_1: 27.0\n",
      "  player_2: 31.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.06999999999999978\n",
      "  player_1: 0.15000000000000008\n",
      "  player_2: 2.779999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -51.0\n",
      "  player_1: -21.0\n",
      "  player_2: -48.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08607734754922547\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29621724687328005\n",
      "  mean_inference_ms: 1.6038405188776972\n",
      "  mean_raw_obs_processing_ms: 0.21021266290554597\n",
      "time_since_restore: 12201.571427106857\n",
      "time_this_iter_s: 15.458157777786255\n",
      "time_total_s: 12201.571427106857\n",
      "timers:\n",
      "  learn_throughput: 544.358\n",
      "  learn_time_ms: 14659.479\n",
      "  load_throughput: 784634.494\n",
      "  load_time_ms: 10.17\n",
      "  sample_throughput: 501.676\n",
      "  sample_time_ms: 15906.681\n",
      "  update_time_ms: 5.653\n",
      "timestamp: 1643548441\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6088740\n",
      "training_iteration: 763\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6104670\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-14-32\n",
      "done: false\n",
      "episode_len_mean: 107.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 72\n",
      "episodes_total: 36030\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46975111146767934\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016208925636637825\n",
      "        policy_loss: -0.09662086650729179\n",
      "        total_loss: 71.980973610878\n",
      "        vf_explained_var: 0.3464552672704061\n",
      "        vf_loss: 72.06665352821351\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5222045732537905\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0181095102361409\n",
      "        policy_loss: -0.07451632621698082\n",
      "        total_loss: 44.94644454161326\n",
      "        vf_explained_var: 0.3509428800145785\n",
      "        vf_loss: 45.008736942609154\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.471842725922664\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016782060722176008\n",
      "        policy_loss: -0.06370974110749861\n",
      "        total_loss: 59.90436250209808\n",
      "        vf_explained_var: 0.19824813236792882\n",
      "        vf_loss: 59.95674430052439\n",
      "  num_agent_steps_sampled: 6104670\n",
      "  num_agent_steps_trained: 6104670\n",
      "  num_steps_sampled: 6104700\n",
      "  num_steps_trained: 6104700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 765\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.226315789473684\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.23157894736843\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 34.0\n",
      "  player_2: 37.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 2.1199999999999997\n",
      "  player_1: -0.9699999999999995\n",
      "  player_2: 1.8499999999999994\n",
      "policy_reward_min:\n",
      "  player_0: -46.666666666666664\n",
      "  player_1: -44.66666666666667\n",
      "  player_2: -38.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0861019168027953\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29617215179797873\n",
      "  mean_inference_ms: 1.6025262731532697\n",
      "  mean_raw_obs_processing_ms: 0.210462114885861\n",
      "time_since_restore: 12232.273150920868\n",
      "time_this_iter_s: 15.40029764175415\n",
      "time_total_s: 12232.273150920868\n",
      "timers:\n",
      "  learn_throughput: 549.426\n",
      "  learn_time_ms: 14524.256\n",
      "  load_throughput: 854398.432\n",
      "  load_time_ms: 9.34\n",
      "  sample_throughput: 504.903\n",
      "  sample_time_ms: 15805.016\n",
      "  update_time_ms: 5.631\n",
      "timestamp: 1643548472\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6104700\n",
      "training_iteration: 765\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6120632\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-15-04\n",
      "done: false\n",
      "episode_len_mean: 109.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 77\n",
      "episodes_total: 36176\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47302334651350975\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016905202368643398\n",
      "        policy_loss: -0.06343272327135006\n",
      "        total_loss: 67.01188501834869\n",
      "        vf_explained_var: 0.4116985849539439\n",
      "        vf_loss: 67.06390693505605\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4738276265064875\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015567412997597216\n",
      "        policy_loss: -0.10823209057872495\n",
      "        total_loss: 64.56160430749257\n",
      "        vf_explained_var: 0.226114275654157\n",
      "        vf_loss: 64.65932845115661\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4843551017343998\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016618936664663502\n",
      "        policy_loss: -0.06784608684480191\n",
      "        total_loss: 72.57080669720968\n",
      "        vf_explained_var: 0.2796635346611341\n",
      "        vf_loss: 72.62743521690369\n",
      "  num_agent_steps_sampled: 6120632\n",
      "  num_agent_steps_trained: 6120632\n",
      "  num_steps_sampled: 6120660\n",
      "  num_steps_trained: 6120660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 767\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.095000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.20000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.333333333333336\n",
      "  player_1: 42.333333333333336\n",
      "  player_2: 32.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 1.3466666666666667\n",
      "  player_1: 0.24666666666666687\n",
      "  player_2: 1.406666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -31.666666666666664\n",
      "  player_1: -38.666666666666664\n",
      "  player_2: -44.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0860607677532208\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29605296823804156\n",
      "  mean_inference_ms: 1.6013064084668127\n",
      "  mean_raw_obs_processing_ms: 0.21031505389765692\n",
      "time_since_restore: 12263.624227523804\n",
      "time_this_iter_s: 15.865031003952026\n",
      "time_total_s: 12263.624227523804\n",
      "timers:\n",
      "  learn_throughput: 562.088\n",
      "  learn_time_ms: 14197.077\n",
      "  load_throughput: 859393.631\n",
      "  load_time_ms: 9.286\n",
      "  sample_throughput: 514.576\n",
      "  sample_time_ms: 15507.923\n",
      "  update_time_ms: 5.718\n",
      "timestamp: 1643548504\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6120660\n",
      "training_iteration: 767\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6136592\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-15-35\n",
      "done: false\n",
      "episode_len_mean: 110.82\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 36323\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48458360955119134\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017112282428541147\n",
      "        policy_loss: -0.07090154035637776\n",
      "        total_loss: 53.928773370583855\n",
      "        vf_explained_var: 0.3095843468109767\n",
      "        vf_loss: 53.98812402566274\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48174139256278675\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01612902881942716\n",
      "        policy_loss: -0.07051507532596588\n",
      "        total_loss: 39.632757245699565\n",
      "        vf_explained_var: 0.28347367852926253\n",
      "        vf_loss: 39.692385257085164\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48858454207579294\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017223229343906514\n",
      "        policy_loss: -0.09579535967049499\n",
      "        total_loss: 60.24935736020406\n",
      "        vf_explained_var: 0.36812023957570394\n",
      "        vf_loss: 60.333527054786686\n",
      "  num_agent_steps_sampled: 6136592\n",
      "  num_agent_steps_trained: 6136592\n",
      "  num_steps_sampled: 6136620\n",
      "  num_steps_trained: 6136620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 769\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.761111111111111\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.666666666666664\n",
      "  player_1: 32.333333333333336\n",
      "  player_2: 28.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 0.5200000000000006\n",
      "  player_1: 1.2300000000000002\n",
      "  player_2: 1.25\n",
      "policy_reward_min:\n",
      "  player_0: -30.333333333333336\n",
      "  player_1: -21.666666666666664\n",
      "  player_2: -57.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08605562916492214\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29585810816576535\n",
      "  mean_inference_ms: 1.6008243776777398\n",
      "  mean_raw_obs_processing_ms: 0.21027936111609377\n",
      "time_since_restore: 12294.830333709717\n",
      "time_this_iter_s: 15.151296615600586\n",
      "time_total_s: 12294.830333709717\n",
      "timers:\n",
      "  learn_throughput: 560.097\n",
      "  learn_time_ms: 14247.542\n",
      "  load_throughput: 905252.813\n",
      "  load_time_ms: 8.815\n",
      "  sample_throughput: 515.322\n",
      "  sample_time_ms: 15485.461\n",
      "  update_time_ms: 7.928\n",
      "timestamp: 1643548535\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6136620\n",
      "training_iteration: 769\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6152550\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-16-07\n",
      "done: false\n",
      "episode_len_mean: 106.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 73\n",
      "episodes_total: 36467\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4645102851589521\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0167111002245656\n",
      "        policy_loss: -0.050957119952266414\n",
      "        total_loss: 71.2946418984731\n",
      "        vf_explained_var: 0.13610868990421296\n",
      "        vf_loss: 71.33431896686554\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4906289608279864\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015447181927132382\n",
      "        policy_loss: -0.08027676007710398\n",
      "        total_loss: 72.63253408273061\n",
      "        vf_explained_var: 0.3727003341913223\n",
      "        vf_loss: 72.70238413333892\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.477024483581384\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016221414678028245\n",
      "        policy_loss: -0.10606750017032027\n",
      "        total_loss: 85.1415845076243\n",
      "        vf_explained_var: 0.14556660294532775\n",
      "        vf_loss: 85.23670248667399\n",
      "  num_agent_steps_sampled: 6152550\n",
      "  num_agent_steps_trained: 6152550\n",
      "  num_steps_sampled: 6152580\n",
      "  num_steps_trained: 6152580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 771\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.889999999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.20000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.0\n",
      "  player_1: 27.66666666666667\n",
      "  player_2: 31.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.746666666666666\n",
      "  player_1: 0.8466666666666667\n",
      "  player_2: -0.5933333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -37.0\n",
      "  player_1: -40.0\n",
      "  player_2: -35.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08598626509708872\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2958163307253337\n",
      "  mean_inference_ms: 1.6005474701331264\n",
      "  mean_raw_obs_processing_ms: 0.21016755876430415\n",
      "time_since_restore: 12326.614076375961\n",
      "time_this_iter_s: 16.76596188545227\n",
      "time_total_s: 12326.614076375961\n",
      "timers:\n",
      "  learn_throughput: 555.778\n",
      "  learn_time_ms: 14358.242\n",
      "  load_throughput: 1117816.167\n",
      "  load_time_ms: 7.139\n",
      "  sample_throughput: 517.934\n",
      "  sample_time_ms: 15407.361\n",
      "  update_time_ms: 8.027\n",
      "timestamp: 1643548567\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6152580\n",
      "training_iteration: 771\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6168510\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-16-39\n",
      "done: false\n",
      "episode_len_mean: 118.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 69\n",
      "episodes_total: 36602\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.472772033760945\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01692703288449138\n",
      "        policy_loss: -0.11458329464929799\n",
      "        total_loss: 53.32069079399109\n",
      "        vf_explained_var: 0.18919064819812775\n",
      "        vf_loss: 53.423848284085594\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.505273910711209\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01656064142879264\n",
      "        policy_loss: -0.07839689623564482\n",
      "        total_loss: 75.91605955441793\n",
      "        vf_explained_var: 0.13257757236560186\n",
      "        vf_loss: 75.98327796459198\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46164755036433536\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015597292598145221\n",
      "        policy_loss: -0.050247244355268776\n",
      "        total_loss: 49.769776717821756\n",
      "        vf_explained_var: 0.10407598425944646\n",
      "        vf_loss: 49.80949588457743\n",
      "  num_agent_steps_sampled: 6168510\n",
      "  num_agent_steps_trained: 6168510\n",
      "  num_steps_sampled: 6168540\n",
      "  num_steps_trained: 6168540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 773\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.661904761904763\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.20000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.0\n",
      "  player_1: 27.666666666666668\n",
      "  player_2: 31.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.4266666666666667\n",
      "  player_1: -1.2033333333333325\n",
      "  player_2: 1.7766666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -30.0\n",
      "  player_1: -54.333333333333336\n",
      "  player_2: -30.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08598101129869544\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29535067036742046\n",
      "  mean_inference_ms: 1.5991377533204882\n",
      "  mean_raw_obs_processing_ms: 0.210112783489695\n",
      "time_since_restore: 12358.570326089859\n",
      "time_this_iter_s: 16.67241358757019\n",
      "time_total_s: 12358.570326089859\n",
      "timers:\n",
      "  learn_throughput: 550.478\n",
      "  learn_time_ms: 14496.502\n",
      "  load_throughput: 979785.368\n",
      "  load_time_ms: 8.145\n",
      "  sample_throughput: 511.175\n",
      "  sample_time_ms: 15611.094\n",
      "  update_time_ms: 7.921\n",
      "timestamp: 1643548599\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6168540\n",
      "training_iteration: 773\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6184470\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-17-11\n",
      "done: false\n",
      "episode_len_mean: 114.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 73\n",
      "episodes_total: 36741\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48872004836797717\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017288318787541355\n",
      "        policy_loss: -0.08702006957959384\n",
      "        total_loss: 60.05843313058217\n",
      "        vf_explained_var: 0.4781248155236244\n",
      "        vf_loss: 60.13378344376882\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4945224769413471\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01687574976932372\n",
      "        policy_loss: -0.06173719530925155\n",
      "        total_loss: 62.02382586479187\n",
      "        vf_explained_var: 0.38008698652187983\n",
      "        vf_loss: 62.07417186101278\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4848681738972664\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017489263232879845\n",
      "        policy_loss: -0.08309754992524783\n",
      "        total_loss: 50.249747362136844\n",
      "        vf_explained_var: 0.402719464302063\n",
      "        vf_loss: 50.32103954633077\n",
      "  num_agent_steps_sampled: 6184470\n",
      "  num_agent_steps_trained: 6184470\n",
      "  num_steps_sampled: 6184500\n",
      "  num_steps_trained: 6184500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 775\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.380952380952381\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.25238095238097\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 32.0\n",
      "  player_2: 44.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -0.06666666666666679\n",
      "  player_1: -1.4766666666666666\n",
      "  player_2: 4.543333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -25.666666666666664\n",
      "  player_1: -47.666666666666664\n",
      "  player_2: -32.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.085996527868275\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2958008173470411\n",
      "  mean_inference_ms: 1.6006294690051175\n",
      "  mean_raw_obs_processing_ms: 0.21020323635030727\n",
      "time_since_restore: 12390.645625591278\n",
      "time_this_iter_s: 16.965853691101074\n",
      "time_total_s: 12390.645625591278\n",
      "timers:\n",
      "  learn_throughput: 545.408\n",
      "  learn_time_ms: 14631.262\n",
      "  load_throughput: 986746.676\n",
      "  load_time_ms: 8.087\n",
      "  sample_throughput: 507.87\n",
      "  sample_time_ms: 15712.668\n",
      "  update_time_ms: 7.944\n",
      "timestamp: 1643548631\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6184500\n",
      "training_iteration: 775\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6200434\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-17-41\n",
      "done: false\n",
      "episode_len_mean: 114.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 36882\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48130671297510463\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016908154282658263\n",
      "        policy_loss: -0.07822735938554008\n",
      "        total_loss: 55.64539903322856\n",
      "        vf_explained_var: 0.3852484339475632\n",
      "        vf_loss: 55.712213253974916\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49267203922073044\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016521309263757757\n",
      "        policy_loss: -0.08722872816336652\n",
      "        total_loss: 48.82767769336701\n",
      "        vf_explained_var: 0.11020893802245459\n",
      "        vf_loss: 48.90375444412231\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4818298435707887\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016657464985856525\n",
      "        policy_loss: -0.06424849277362227\n",
      "        total_loss: 43.93161212603251\n",
      "        vf_explained_var: 0.3738029272357623\n",
      "        vf_loss: 43.98461681207021\n",
      "  num_agent_steps_sampled: 6200434\n",
      "  num_agent_steps_trained: 6200434\n",
      "  num_steps_sampled: 6200460\n",
      "  num_steps_trained: 6200460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 777\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.078947368421053\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 31.0\n",
      "  player_2: 34.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -0.5599999999999998\n",
      "  player_1: -0.46999999999999986\n",
      "  player_2: 4.03\n",
      "policy_reward_min:\n",
      "  player_0: -36.666666666666664\n",
      "  player_1: -37.0\n",
      "  player_2: -32.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08599760416703385\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29570589220650023\n",
      "  mean_inference_ms: 1.5994179404978461\n",
      "  mean_raw_obs_processing_ms: 0.2101838171880445\n",
      "time_since_restore: 12420.700307369232\n",
      "time_this_iter_s: 15.272623538970947\n",
      "time_total_s: 12420.700307369232\n",
      "timers:\n",
      "  learn_throughput: 550.275\n",
      "  learn_time_ms: 14501.851\n",
      "  load_throughput: 960742.688\n",
      "  load_time_ms: 8.306\n",
      "  sample_throughput: 505.15\n",
      "  sample_time_ms: 15797.273\n",
      "  update_time_ms: 7.926\n",
      "timestamp: 1643548661\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6200460\n",
      "training_iteration: 777\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6216390\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-18-15\n",
      "done: false\n",
      "episode_len_mean: 116.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 69\n",
      "episodes_total: 37022\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47428700601061186\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01679363669652711\n",
      "        policy_loss: -0.1134811851584042\n",
      "        total_loss: 90.16538041114808\n",
      "        vf_explained_var: 0.2739666447043419\n",
      "        vf_loss: 90.267525917689\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4934318155546983\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01720561178925588\n",
      "        policy_loss: -0.06620780681570371\n",
      "        total_loss: 52.00310611565908\n",
      "        vf_explained_var: 0.19113938798507055\n",
      "        vf_loss: 52.05770016988119\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4662536059319973\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016007295036906347\n",
      "        policy_loss: -0.05654431518167257\n",
      "        total_loss: 64.88088330904642\n",
      "        vf_explained_var: 0.34931705266237256\n",
      "        vf_loss: 64.92662282625834\n",
      "  num_agent_steps_sampled: 6216390\n",
      "  num_agent_steps_trained: 6216390\n",
      "  num_steps_sampled: 6216420\n",
      "  num_steps_trained: 6216420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 779\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.700000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.22727272727275\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 47.333333333333336\n",
      "  player_1: 30.333333333333336\n",
      "  player_2: 33.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 0.9833333333333332\n",
      "  player_1: -1.606666666666667\n",
      "  player_2: 3.6233333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -43.333333333333336\n",
      "  player_1: -39.333333333333336\n",
      "  player_2: -50.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08598607863673989\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2958172296409021\n",
      "  mean_inference_ms: 1.6013161503855726\n",
      "  mean_raw_obs_processing_ms: 0.21024882347700166\n",
      "time_since_restore: 12454.37635231018\n",
      "time_this_iter_s: 17.802234649658203\n",
      "time_total_s: 12454.37635231018\n",
      "timers:\n",
      "  learn_throughput: 540.987\n",
      "  learn_time_ms: 14750.828\n",
      "  load_throughput: 911628.892\n",
      "  load_time_ms: 8.754\n",
      "  sample_throughput: 507.857\n",
      "  sample_time_ms: 15713.069\n",
      "  update_time_ms: 5.778\n",
      "timestamp: 1643548695\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6216420\n",
      "training_iteration: 779\n",
      "trial_id: default\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0130 14:18:23.850987829   14905 backup_poller.cc:134]       Run client channel backup poller: {\"created\":\"@1643548703.850963502\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1643548703.850960937\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 6232351\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-18-47\n",
      "done: false\n",
      "episode_len_mean: 114.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 69\n",
      "episodes_total: 37163\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4642292709151904\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015913048648345314\n",
      "        policy_loss: -0.06275642617295185\n",
      "        total_loss: 100.35060678005219\n",
      "        vf_explained_var: 0.22782085021336873\n",
      "        vf_loss: 100.40262151877086\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4978249167899291\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014900731008480458\n",
      "        policy_loss: -0.09161695594433696\n",
      "        total_loss: 101.75568370342255\n",
      "        vf_explained_var: 0.34094310333331423\n",
      "        vf_loss: 101.83724300702413\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47731632346908254\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016112820850994466\n",
      "        policy_loss: -0.07093115440569818\n",
      "        total_loss: 64.37136952559153\n",
      "        vf_explained_var: 0.2064081605275472\n",
      "        vf_loss: 64.43142452081044\n",
      "  num_agent_steps_sampled: 6232351\n",
      "  num_agent_steps_trained: 6232351\n",
      "  num_steps_sampled: 6232380\n",
      "  num_steps_trained: 6232380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 781\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.504545454545452\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.41363636363634\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 45.66666666666667\n",
      "  player_1: 32.666666666666664\n",
      "  player_2: 28.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.32\n",
      "  player_1: 0.8499999999999999\n",
      "  player_2: -0.17\n",
      "policy_reward_min:\n",
      "  player_0: -51.333333333333336\n",
      "  player_1: -59.33333333333333\n",
      "  player_2: -47.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08602977074290959\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29541252888914643\n",
      "  mean_inference_ms: 1.5985596323880418\n",
      "  mean_raw_obs_processing_ms: 0.21023614445672378\n",
      "time_since_restore: 12487.070171356201\n",
      "time_this_iter_s: 17.63689374923706\n",
      "time_total_s: 12487.070171356201\n",
      "timers:\n",
      "  learn_throughput: 538.309\n",
      "  learn_time_ms: 14824.204\n",
      "  load_throughput: 913320.452\n",
      "  load_time_ms: 8.737\n",
      "  sample_throughput: 498.677\n",
      "  sample_time_ms: 16002.327\n",
      "  update_time_ms: 5.555\n",
      "timestamp: 1643548727\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6232380\n",
      "training_iteration: 781\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6248312\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-19-20\n",
      "done: false\n",
      "episode_len_mean: 116.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 72\n",
      "episodes_total: 37301\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46340220545729\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017628445771888816\n",
      "        policy_loss: -0.08632978828003009\n",
      "        total_loss: 58.98718129793803\n",
      "        vf_explained_var: 0.0948269201318423\n",
      "        vf_loss: 59.06161184151967\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48702617213129995\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014920991559735815\n",
      "        policy_loss: -0.0949036674791326\n",
      "        total_loss: 38.77424075921377\n",
      "        vf_explained_var: 0.3416455975174904\n",
      "        vf_loss: 38.85907271067301\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46481747965017955\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01579124680073979\n",
      "        policy_loss: -0.047687930832616986\n",
      "        total_loss: 61.91120046138764\n",
      "        vf_explained_var: 0.2943198664983114\n",
      "        vf_loss: 61.94822915712992\n",
      "  num_agent_steps_sampled: 6248312\n",
      "  num_agent_steps_trained: 6248312\n",
      "  num_steps_sampled: 6248340\n",
      "  num_steps_trained: 6248340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 783\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.078947368421053\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.20526315789475\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.0\n",
      "  player_1: 22.333333333333332\n",
      "  player_2: 39.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -1.0233333333333337\n",
      "  player_1: -0.5033333333333337\n",
      "  player_2: 4.526666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -45.0\n",
      "  player_1: -32.666666666666664\n",
      "  player_2: -43.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08593704715012723\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29516218626335833\n",
      "  mean_inference_ms: 1.5979089688800008\n",
      "  mean_raw_obs_processing_ms: 0.2100771018756737\n",
      "time_since_restore: 12520.025751590729\n",
      "time_this_iter_s: 15.66304087638855\n",
      "time_total_s: 12520.025751590729\n",
      "timers:\n",
      "  learn_throughput: 534.648\n",
      "  learn_time_ms: 14925.7\n",
      "  load_throughput: 985508.983\n",
      "  load_time_ms: 8.097\n",
      "  sample_throughput: 490.453\n",
      "  sample_time_ms: 16270.66\n",
      "  update_time_ms: 5.536\n",
      "timestamp: 1643548760\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6248340\n",
      "training_iteration: 783\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6264270\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-19-51\n",
      "done: false\n",
      "episode_len_mean: 106.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 72\n",
      "episodes_total: 37445\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47580671111742656\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017297145567499683\n",
      "        policy_loss: -0.08528111971293886\n",
      "        total_loss: 66.64952812671662\n",
      "        vf_explained_var: 0.44198059956232705\n",
      "        vf_loss: 66.72313339392345\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48740747779607774\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014961220129819897\n",
      "        policy_loss: -0.06382802019516627\n",
      "        total_loss: 61.98199842611949\n",
      "        vf_explained_var: 0.4001897413531939\n",
      "        vf_loss: 62.03572762489319\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4782913139959176\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01610283117917182\n",
      "        policy_loss: -0.09720394627191126\n",
      "        total_loss: 79.66285915851593\n",
      "        vf_explained_var: 0.41125537067651746\n",
      "        vf_loss: 79.74919389883677\n",
      "  num_agent_steps_sampled: 6264270\n",
      "  num_agent_steps_trained: 6264270\n",
      "  num_steps_sampled: 6264300\n",
      "  num_steps_trained: 6264300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 785\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.015789473684212\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.33333333333333\n",
      "  player_1: 37.66666666666667\n",
      "  player_2: 40.0\n",
      "policy_reward_mean:\n",
      "  player_0: -1.2266666666666663\n",
      "  player_1: 1.2233333333333332\n",
      "  player_2: 3.003333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -45.0\n",
      "  player_1: -40.666666666666664\n",
      "  player_2: -49.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08590240409298021\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29515258229006186\n",
      "  mean_inference_ms: 1.5981594139969193\n",
      "  mean_raw_obs_processing_ms: 0.21012954170707815\n",
      "time_since_restore: 12550.533161401749\n",
      "time_this_iter_s: 15.202531099319458\n",
      "time_total_s: 12550.533161401749\n",
      "timers:\n",
      "  learn_throughput: 540.177\n",
      "  learn_time_ms: 14772.943\n",
      "  load_throughput: 986330.86\n",
      "  load_time_ms: 8.091\n",
      "  sample_throughput: 492.992\n",
      "  sample_time_ms: 16186.871\n",
      "  update_time_ms: 5.611\n",
      "timestamp: 1643548791\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6264300\n",
      "training_iteration: 785\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6280230\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-20-21\n",
      "done: false\n",
      "episode_len_mean: 115.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 69\n",
      "episodes_total: 37581\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4739695294698079\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016882291057878925\n",
      "        policy_loss: -0.0709301025296251\n",
      "        total_loss: 56.87541589419047\n",
      "        vf_explained_var: 0.26116618533929187\n",
      "        vf_loss: 56.934950466156\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47608304768800735\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016640247177750023\n",
      "        policy_loss: -0.04830960809874038\n",
      "        total_loss: 58.12965133031209\n",
      "        vf_explained_var: 0.2989578355352084\n",
      "        vf_loss: 58.1667285490036\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4860105422635873\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01752020995095336\n",
      "        policy_loss: -0.10305037374297778\n",
      "        total_loss: 39.2647607310613\n",
      "        vf_explained_var: 0.3243234916528066\n",
      "        vf_loss: 39.35598478158315\n",
      "  num_agent_steps_sampled: 6280230\n",
      "  num_agent_steps_trained: 6280230\n",
      "  num_steps_sampled: 6280260\n",
      "  num_steps_trained: 6280260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 787\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.63888888888889\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.25\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.0\n",
      "  player_1: 28.333333333333336\n",
      "  player_2: 27.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -0.1066666666666666\n",
      "  player_1: 0.6333333333333334\n",
      "  player_2: 2.473333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -29.0\n",
      "  player_1: -35.666666666666664\n",
      "  player_2: -29.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08591262720685801\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2953350620116895\n",
      "  mean_inference_ms: 1.598691744762673\n",
      "  mean_raw_obs_processing_ms: 0.21008461316854166\n",
      "time_since_restore: 12580.547731876373\n",
      "time_this_iter_s: 14.869431257247925\n",
      "time_total_s: 12580.547731876373\n",
      "timers:\n",
      "  learn_throughput: 540.266\n",
      "  learn_time_ms: 14770.497\n",
      "  load_throughput: 965451.029\n",
      "  load_time_ms: 8.266\n",
      "  sample_throughput: 497.079\n",
      "  sample_time_ms: 16053.773\n",
      "  update_time_ms: 7.672\n",
      "timestamp: 1643548821\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6280260\n",
      "training_iteration: 787\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6296191\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-20-52\n",
      "done: false\n",
      "episode_len_mean: 120.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 63\n",
      "episodes_total: 37718\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4679456687470277\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01599467562656855\n",
      "        policy_loss: -0.07659288497020801\n",
      "        total_loss: 61.99612083594004\n",
      "        vf_explained_var: 0.08493503093719483\n",
      "        vf_loss: 62.06191707452138\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46755423838893573\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017667935810700707\n",
      "        policy_loss: -0.056866840968529384\n",
      "        total_loss: 55.86916187604268\n",
      "        vf_explained_var: 0.2934600720802943\n",
      "        vf_loss: 55.91410282770793\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4716901633143425\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016201291002441092\n",
      "        policy_loss: -0.08262479056526596\n",
      "        total_loss: 35.981537892023724\n",
      "        vf_explained_var: 0.22378377497196197\n",
      "        vf_loss: 36.053226879437766\n",
      "  num_agent_steps_sampled: 6296191\n",
      "  num_agent_steps_trained: 6296191\n",
      "  num_steps_sampled: 6296220\n",
      "  num_steps_trained: 6296220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 789\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.633333333333333\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.2\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.0\n",
      "  player_1: 30.666666666666664\n",
      "  player_2: 39.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.5433333333333333\n",
      "  player_1: 0.3133333333333335\n",
      "  player_2: 2.143333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -55.0\n",
      "  player_1: -44.333333333333336\n",
      "  player_2: -26.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08579654402285199\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29476094478189857\n",
      "  mean_inference_ms: 1.5968874539225255\n",
      "  mean_raw_obs_processing_ms: 0.20992625326316605\n",
      "time_since_restore: 12611.72744011879\n",
      "time_this_iter_s: 14.906697750091553\n",
      "time_total_s: 12611.72744011879\n",
      "timers:\n",
      "  learn_throughput: 549.494\n",
      "  learn_time_ms: 14522.444\n",
      "  load_throughput: 1017113.048\n",
      "  load_time_ms: 7.846\n",
      "  sample_throughput: 497.03\n",
      "  sample_time_ms: 16055.364\n",
      "  update_time_ms: 7.517\n",
      "timestamp: 1643548852\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6296220\n",
      "training_iteration: 789\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6312150\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-21-24\n",
      "done: false\n",
      "episode_len_mean: 112.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 37858\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45547845870256426\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016753866357186665\n",
      "        policy_loss: -0.06754690887406468\n",
      "        total_loss: 71.32870102564493\n",
      "        vf_explained_var: 0.3742144434650739\n",
      "        vf_loss: 71.3849390220642\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4981089664498965\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017683693619834837\n",
      "        policy_loss: -0.07403798774039994\n",
      "        total_loss: 69.75175533930461\n",
      "        vf_explained_var: 0.28108285387357074\n",
      "        vf_loss: 69.81385657310486\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48809565757711726\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01751575971611999\n",
      "        policy_loss: -0.10609682854575415\n",
      "        total_loss: 55.66378086725871\n",
      "        vf_explained_var: 0.30985449502865475\n",
      "        vf_loss: 55.758054280281065\n",
      "  num_agent_steps_sampled: 6312150\n",
      "  num_agent_steps_trained: 6312150\n",
      "  num_steps_sampled: 6312180\n",
      "  num_steps_trained: 6312180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 791\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.704999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.20000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.333333333333336\n",
      "  player_1: 23.333333333333336\n",
      "  player_2: 33.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.6333333333333324\n",
      "  player_1: -3.736666666666667\n",
      "  player_2: 4.1033333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -38.333333333333336\n",
      "  player_1: -47.0\n",
      "  player_2: -26.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08602292459080538\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29562831405365697\n",
      "  mean_inference_ms: 1.5997129945930126\n",
      "  mean_raw_obs_processing_ms: 0.21024327987868946\n",
      "time_since_restore: 12643.684795618057\n",
      "time_this_iter_s: 16.925958395004272\n",
      "time_total_s: 12643.684795618057\n",
      "timers:\n",
      "  learn_throughput: 551.607\n",
      "  learn_time_ms: 14466.829\n",
      "  load_throughput: 981681.999\n",
      "  load_time_ms: 8.129\n",
      "  sample_throughput: 506.783\n",
      "  sample_time_ms: 15746.373\n",
      "  update_time_ms: 7.564\n",
      "timestamp: 1643548884\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6312180\n",
      "training_iteration: 791\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6328110\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-21-57\n",
      "done: false\n",
      "episode_len_mean: 109.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 73\n",
      "episodes_total: 38002\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45921872978409134\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017254401348384362\n",
      "        policy_loss: -0.08115578006021679\n",
      "        total_loss: 51.255467891693115\n",
      "        vf_explained_var: 0.20959107756614684\n",
      "        vf_loss: 51.324976711273194\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48375933249791464\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016477313574223443\n",
      "        policy_loss: -0.05417269165317218\n",
      "        total_loss: 68.46600224177043\n",
      "        vf_explained_var: 0.2308944829305013\n",
      "        vf_loss: 68.50905265172322\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46853419244289396\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016557327443172957\n",
      "        policy_loss: -0.10747975476707021\n",
      "        total_loss: 74.712382868131\n",
      "        vf_explained_var: 0.28653103053569795\n",
      "        vf_loss: 74.80868656953176\n",
      "  num_agent_steps_sampled: 6328110\n",
      "  num_agent_steps_trained: 6328110\n",
      "  num_steps_sampled: 6328140\n",
      "  num_steps_trained: 6328140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 793\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.552380952380954\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.20000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 32.0\n",
      "  player_2: 33.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.2966666666666664\n",
      "  player_1: -0.2433333333333333\n",
      "  player_2: -0.05333333333333297\n",
      "policy_reward_min:\n",
      "  player_0: -45.333333333333336\n",
      "  player_1: -37.666666666666664\n",
      "  player_2: -54.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08587780696283356\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29536570855742766\n",
      "  mean_inference_ms: 1.5987114246222485\n",
      "  mean_raw_obs_processing_ms: 0.21008794758684446\n",
      "time_since_restore: 12676.095923900604\n",
      "time_this_iter_s: 16.874484300613403\n",
      "time_total_s: 12676.095923900604\n",
      "timers:\n",
      "  learn_throughput: 553.736\n",
      "  learn_time_ms: 14411.207\n",
      "  load_throughput: 964385.619\n",
      "  load_time_ms: 8.275\n",
      "  sample_throughput: 514.221\n",
      "  sample_time_ms: 15518.629\n",
      "  update_time_ms: 7.582\n",
      "timestamp: 1643548917\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6328140\n",
      "training_iteration: 793\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6344070\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-22-29\n",
      "done: false\n",
      "episode_len_mean: 114.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 68\n",
      "episodes_total: 38134\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4778981075187524\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017457973919859645\n",
      "        policy_loss: -0.07542878068983555\n",
      "        total_loss: 76.59645709673563\n",
      "        vf_explained_var: 0.39280484269062677\n",
      "        vf_loss: 76.66010215282441\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47426404704650243\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01729761643109498\n",
      "        policy_loss: -0.08742206168826669\n",
      "        total_loss: 48.050351244608564\n",
      "        vf_explained_var: 0.3573955579598745\n",
      "        vf_loss: 48.126097585360206\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4756777914861838\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015463763972589391\n",
      "        policy_loss: -0.08063214374085267\n",
      "        total_loss: 56.3234650627772\n",
      "        vf_explained_var: 0.3546203507979711\n",
      "        vf_loss: 56.393659092585246\n",
      "  num_agent_steps_sampled: 6344070\n",
      "  num_agent_steps_trained: 6344070\n",
      "  num_steps_sampled: 6344100\n",
      "  num_steps_trained: 6344100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 795\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.390909090909092\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.28181818181817\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.0\n",
      "  player_1: 30.666666666666664\n",
      "  player_2: 35.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.7766666666666663\n",
      "  player_1: -1.1766666666666665\n",
      "  player_2: 4.953333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -53.333333333333336\n",
      "  player_1: -30.0\n",
      "  player_2: -25.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08593377275942898\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2955808899051549\n",
      "  mean_inference_ms: 1.6004427142631057\n",
      "  mean_raw_obs_processing_ms: 0.2102099531573425\n",
      "time_since_restore: 12708.415048360825\n",
      "time_this_iter_s: 17.460487842559814\n",
      "time_total_s: 12708.415048360825\n",
      "timers:\n",
      "  learn_throughput: 546.95\n",
      "  learn_time_ms: 14589.993\n",
      "  load_throughput: 954446.958\n",
      "  load_time_ms: 8.361\n",
      "  sample_throughput: 511.68\n",
      "  sample_time_ms: 15595.672\n",
      "  update_time_ms: 7.74\n",
      "timestamp: 1643548949\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6344100\n",
      "training_iteration: 795\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6360030\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-23-00\n",
      "done: false\n",
      "episode_len_mean: 114.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 38276\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4740567057331403\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018127849201227945\n",
      "        policy_loss: -0.07521413440195222\n",
      "        total_loss: 50.30717113176981\n",
      "        vf_explained_var: 0.35286362359921136\n",
      "        vf_loss: 50.370148866971334\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4882763943572839\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01872976362736722\n",
      "        policy_loss: -0.08567465022361527\n",
      "        total_loss: 53.2995505062739\n",
      "        vf_explained_var: 0.3179695199926694\n",
      "        vf_loss: 53.372582581837975\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48139885226885476\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017271919901847167\n",
      "        policy_loss: -0.10230928334252289\n",
      "        total_loss: 55.20237006187439\n",
      "        vf_explained_var: 0.2795840970675151\n",
      "        vf_loss: 55.29302073319753\n",
      "  num_agent_steps_sampled: 6360030\n",
      "  num_agent_steps_trained: 6360030\n",
      "  num_steps_sampled: 6360060\n",
      "  num_steps_trained: 6360060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 797\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.305263157894737\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.3\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.0\n",
      "  player_1: 34.666666666666664\n",
      "  player_2: 37.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.0100000000000002\n",
      "  player_1: -0.6399999999999999\n",
      "  player_2: 2.630000000000001\n",
      "policy_reward_min:\n",
      "  player_0: -34.0\n",
      "  player_1: -39.0\n",
      "  player_2: -33.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08605009816288513\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29556380020229045\n",
      "  mean_inference_ms: 1.599101831737541\n",
      "  mean_raw_obs_processing_ms: 0.21039328321837775\n",
      "time_since_restore: 12738.78914475441\n",
      "time_this_iter_s: 15.46286678314209\n",
      "time_total_s: 12738.78914475441\n",
      "timers:\n",
      "  learn_throughput: 545.68\n",
      "  learn_time_ms: 14623.968\n",
      "  load_throughput: 992022.63\n",
      "  load_time_ms: 8.044\n",
      "  sample_throughput: 505.24\n",
      "  sample_time_ms: 15794.46\n",
      "  update_time_ms: 5.644\n",
      "timestamp: 1643548980\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6360060\n",
      "training_iteration: 797\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6375990\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-23-33\n",
      "done: false\n",
      "episode_len_mean: 127.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 60\n",
      "episodes_total: 38403\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46876961703101794\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016052057400132373\n",
      "        policy_loss: -0.09912056697842976\n",
      "        total_loss: 47.026893184979755\n",
      "        vf_explained_var: 0.37212279309829077\n",
      "        vf_loss: 47.11517854531606\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4796625023583571\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016084274432857152\n",
      "        policy_loss: -0.08387714449626704\n",
      "        total_loss: 36.8244616484642\n",
      "        vf_explained_var: 0.3386653726299604\n",
      "        vf_loss: 36.89748209794362\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4767995596925418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01628497975099435\n",
      "        policy_loss: -0.05872471697473278\n",
      "        total_loss: 57.77734940687815\n",
      "        vf_explained_var: 0.21095502863327661\n",
      "        vf_loss: 57.82508179823557\n",
      "  num_agent_steps_sampled: 6375990\n",
      "  num_agent_steps_trained: 6375990\n",
      "  num_steps_sampled: 6376020\n",
      "  num_steps_trained: 6376020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 799\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.375\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.31499999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.0\n",
      "  player_1: 34.666666666666664\n",
      "  player_2: 26.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 0.26666666666666666\n",
      "  player_1: -0.723333333333333\n",
      "  player_2: 3.456666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -38.666666666666664\n",
      "  player_1: -37.666666666666664\n",
      "  player_2: -46.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08589363419392416\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2949191969999233\n",
      "  mean_inference_ms: 1.5969956970817654\n",
      "  mean_raw_obs_processing_ms: 0.21003779397177394\n",
      "time_since_restore: 12771.811561346054\n",
      "time_this_iter_s: 16.298351287841797\n",
      "time_total_s: 12771.811561346054\n",
      "timers:\n",
      "  learn_throughput: 538.98\n",
      "  learn_time_ms: 14805.747\n",
      "  load_throughput: 915624.059\n",
      "  load_time_ms: 8.715\n",
      "  sample_throughput: 502.067\n",
      "  sample_time_ms: 15894.294\n",
      "  update_time_ms: 5.645\n",
      "timestamp: 1643549013\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6376020\n",
      "training_iteration: 799\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6391950\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-24-03\n",
      "done: false\n",
      "episode_len_mean: 116.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 68\n",
      "episodes_total: 38540\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4751007682085037\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01684324402308827\n",
      "        policy_loss: -0.09892395045297842\n",
      "        total_loss: 59.9972328488032\n",
      "        vf_explained_var: 0.20686144024133682\n",
      "        vf_loss: 60.084787381490074\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4965501490732034\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01607037395071482\n",
      "        policy_loss: -0.061397542742391426\n",
      "        total_loss: 45.20220656394959\n",
      "        vf_explained_var: 0.21194348206122715\n",
      "        vf_loss: 45.252756555875145\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46647791713476183\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016639014050523808\n",
      "        policy_loss: -0.04546010561597844\n",
      "        total_loss: 47.205015312830604\n",
      "        vf_explained_var: 0.40315723021825156\n",
      "        vf_loss: 47.23924409866333\n",
      "  num_agent_steps_sampled: 6391950\n",
      "  num_agent_steps_trained: 6391950\n",
      "  num_steps_sampled: 6391980\n",
      "  num_steps_trained: 6391980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 801\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.66111111111111\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.3\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.333333333333336\n",
      "  player_1: 36.0\n",
      "  player_2: 43.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.86\n",
      "  player_1: -1.72\n",
      "  player_2: 5.58\n",
      "policy_reward_min:\n",
      "  player_0: -39.0\n",
      "  player_1: -46.666666666666664\n",
      "  player_2: -34.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08589597999282819\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2950469481443394\n",
      "  mean_inference_ms: 1.5978483048823806\n",
      "  mean_raw_obs_processing_ms: 0.21010060931510025\n",
      "time_since_restore: 12801.745975255966\n",
      "time_this_iter_s: 14.858101606369019\n",
      "time_total_s: 12801.745975255966\n",
      "timers:\n",
      "  learn_throughput: 546.378\n",
      "  learn_time_ms: 14605.27\n",
      "  load_throughput: 965256.131\n",
      "  load_time_ms: 8.267\n",
      "  sample_throughput: 497.623\n",
      "  sample_time_ms: 16036.238\n",
      "  update_time_ms: 5.704\n",
      "timestamp: 1643549043\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6391980\n",
      "training_iteration: 801\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6407910\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-24-36\n",
      "done: false\n",
      "episode_len_mean: 106.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 77\n",
      "episodes_total: 38688\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46696496586004893\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017591658388834808\n",
      "        policy_loss: -0.09093123281374574\n",
      "        total_loss: 68.86671660423279\n",
      "        vf_explained_var: 0.3830908880631129\n",
      "        vf_loss: 68.94577323595682\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5053372180461884\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017348850445705465\n",
      "        policy_loss: -0.06712910891200105\n",
      "        total_loss: 75.24711877187093\n",
      "        vf_explained_var: 0.23925000697374343\n",
      "        vf_loss: 75.30253716786703\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47086183791359265\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015360348088365186\n",
      "        policy_loss: -0.09066357402907063\n",
      "        total_loss: 62.91384754657745\n",
      "        vf_explained_var: 0.33951318701108296\n",
      "        vf_loss: 62.99414297739665\n",
      "  num_agent_steps_sampled: 6407910\n",
      "  num_agent_steps_trained: 6407910\n",
      "  num_steps_sampled: 6407940\n",
      "  num_steps_trained: 6407940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 803\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.633333333333335\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.22857142857144\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 38.0\n",
      "  player_2: 29.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.4666666666666668\n",
      "  player_1: 0.7566666666666668\n",
      "  player_2: 0.7766666666666672\n",
      "policy_reward_min:\n",
      "  player_0: -28.666666666666664\n",
      "  player_1: -41.0\n",
      "  player_2: -38.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08594274051841527\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29516103056442233\n",
      "  mean_inference_ms: 1.5977303386827655\n",
      "  mean_raw_obs_processing_ms: 0.210158252503023\n",
      "time_since_restore: 12834.917126893997\n",
      "time_this_iter_s: 17.380274057388306\n",
      "time_total_s: 12834.917126893997\n",
      "timers:\n",
      "  learn_throughput: 543.552\n",
      "  learn_time_ms: 14681.215\n",
      "  load_throughput: 968454.027\n",
      "  load_time_ms: 8.24\n",
      "  sample_throughput: 503.098\n",
      "  sample_time_ms: 15861.716\n",
      "  update_time_ms: 7.947\n",
      "timestamp: 1643549076\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6407940\n",
      "training_iteration: 803\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6423874\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-25-06\n",
      "done: false\n",
      "episode_len_mean: 115.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 38830\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47088514124353725\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015728883637095428\n",
      "        policy_loss: -0.08101706544558207\n",
      "        total_loss: 64.65862483342488\n",
      "        vf_explained_var: 0.38788121183713276\n",
      "        vf_loss: 64.72902485211691\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4906495869159698\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01576771329528242\n",
      "        policy_loss: -0.07375312613633772\n",
      "        total_loss: 61.74569548447927\n",
      "        vf_explained_var: 0.23928534587224323\n",
      "        vf_loss: 61.80880544662475\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4670702800651391\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016947865884138765\n",
      "        policy_loss: -0.07723171376312772\n",
      "        total_loss: 52.980236854553226\n",
      "        vf_explained_var: 0.39491944124301276\n",
      "        vf_loss: 53.046028834184014\n",
      "  num_agent_steps_sampled: 6423874\n",
      "  num_agent_steps_trained: 6423874\n",
      "  num_steps_sampled: 6423900\n",
      "  num_steps_trained: 6423900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 805\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.163157894736841\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.26315789473685\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.0\n",
      "  player_1: 41.333333333333336\n",
      "  player_2: 29.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.0766666666666664\n",
      "  player_1: -1.9333333333333338\n",
      "  player_2: 3.8566666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -51.666666666666664\n",
      "  player_1: -33.666666666666664\n",
      "  player_2: -25.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08580377370058873\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29464215192298815\n",
      "  mean_inference_ms: 1.5954627912212203\n",
      "  mean_raw_obs_processing_ms: 0.21001594735894294\n",
      "time_since_restore: 12865.25230717659\n",
      "time_this_iter_s: 14.970835447311401\n",
      "time_total_s: 12865.25230717659\n",
      "timers:\n",
      "  learn_throughput: 550.991\n",
      "  learn_time_ms: 14483.001\n",
      "  load_throughput: 972785.863\n",
      "  load_time_ms: 8.203\n",
      "  sample_throughput: 499.94\n",
      "  sample_time_ms: 15961.902\n",
      "  update_time_ms: 7.74\n",
      "timestamp: 1643549106\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6423900\n",
      "training_iteration: 805\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6439832\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-25-38\n",
      "done: false\n",
      "episode_len_mean: 109.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 72\n",
      "episodes_total: 38968\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4750562205910683\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017468887524358555\n",
      "        policy_loss: -0.09349542255202929\n",
      "        total_loss: 47.36900815169017\n",
      "        vf_explained_var: 0.29541388869285584\n",
      "        vf_loss: 47.45071217219035\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49399090617895125\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017926905365805414\n",
      "        policy_loss: -0.06950823530865212\n",
      "        total_loss: 42.377379857699076\n",
      "        vf_explained_var: 0.3022633326053619\n",
      "        vf_loss: 42.43478740374247\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4648415390153726\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018035699650833598\n",
      "        policy_loss: -0.0861832041417559\n",
      "        total_loss: 61.32641230583191\n",
      "        vf_explained_var: 0.36181374758481977\n",
      "        vf_loss: 61.40042144139608\n",
      "  num_agent_steps_sampled: 6439832\n",
      "  num_agent_steps_trained: 6439832\n",
      "  num_steps_sampled: 6439860\n",
      "  num_steps_trained: 6439860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 807\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.955000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.20000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 34.0\n",
      "  player_2: 35.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.1666666666666665\n",
      "  player_1: -0.21333333333333368\n",
      "  player_2: 2.0466666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -39.0\n",
      "  player_1: -29.333333333333336\n",
      "  player_2: -35.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0858563860631732\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2950364907143098\n",
      "  mean_inference_ms: 1.5981553881872717\n",
      "  mean_raw_obs_processing_ms: 0.21014808292780543\n",
      "time_since_restore: 12897.049746751785\n",
      "time_this_iter_s: 16.144620180130005\n",
      "time_total_s: 12897.049746751785\n",
      "timers:\n",
      "  learn_throughput: 545.403\n",
      "  learn_time_ms: 14631.381\n",
      "  load_throughput: 963788.572\n",
      "  load_time_ms: 8.28\n",
      "  sample_throughput: 505.595\n",
      "  sample_time_ms: 15783.392\n",
      "  update_time_ms: 7.677\n",
      "timestamp: 1643549138\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6439860\n",
      "training_iteration: 807\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6455790\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-26-11\n",
      "done: false\n",
      "episode_len_mean: 107.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 39113\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.464981649518013\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015605252241896702\n",
      "        policy_loss: -0.1080446336666743\n",
      "        total_loss: 63.19504371960958\n",
      "        vf_explained_var: 0.33993392050266263\n",
      "        vf_loss: 63.29255476474762\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4973753148317337\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01569424314876111\n",
      "        policy_loss: -0.09586943206377327\n",
      "        total_loss: 66.84961841265361\n",
      "        vf_explained_var: 0.4229718560973803\n",
      "        vf_loss: 66.93489412148793\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46038373226920765\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01632799414588741\n",
      "        policy_loss: -0.032001875375087065\n",
      "        total_loss: 51.10548174699147\n",
      "        vf_explained_var: 0.17960379868745804\n",
      "        vf_loss: 51.12646206378937\n",
      "  num_agent_steps_sampled: 6455790\n",
      "  num_agent_steps_trained: 6455790\n",
      "  num_steps_sampled: 6455820\n",
      "  num_steps_trained: 6455820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 809\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.41\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.215\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.333333333333336\n",
      "  player_1: 43.666666666666664\n",
      "  player_2: 44.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 2.4933333333333336\n",
      "  player_1: -1.4466666666666665\n",
      "  player_2: 1.953333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -46.33333333333333\n",
      "  player_1: -28.0\n",
      "  player_2: -57.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08583495497311862\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.295161599031829\n",
      "  mean_inference_ms: 1.5987096805032293\n",
      "  mean_raw_obs_processing_ms: 0.21010157877385402\n",
      "time_since_restore: 12929.995255231857\n",
      "time_this_iter_s: 16.07579755783081\n",
      "time_total_s: 12929.995255231857\n",
      "timers:\n",
      "  learn_throughput: 545.587\n",
      "  learn_time_ms: 14626.442\n",
      "  load_throughput: 993568.653\n",
      "  load_time_ms: 8.032\n",
      "  sample_throughput: 502.77\n",
      "  sample_time_ms: 15872.071\n",
      "  update_time_ms: 7.628\n",
      "timestamp: 1643549171\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6455820\n",
      "training_iteration: 809\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6471750\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-26-41\n",
      "done: false\n",
      "episode_len_mean: 116.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 39253\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4528258071343104\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015984806475781852\n",
      "        policy_loss: -0.07797312910978993\n",
      "        total_loss: 69.42486940542857\n",
      "        vf_explained_var: 0.36912811289230985\n",
      "        vf_loss: 69.49205260912578\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4772848437726498\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01509171754244259\n",
      "        policy_loss: -0.0866148288672169\n",
      "        total_loss: 80.9987299187978\n",
      "        vf_explained_var: 0.34707620650529863\n",
      "        vf_loss: 81.07515782992046\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4607684617737929\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017287870070324327\n",
      "        policy_loss: -0.0754089410447826\n",
      "        total_loss: 83.57464387257893\n",
      "        vf_explained_var: 0.33635730763276417\n",
      "        vf_loss: 83.63838398297628\n",
      "  num_agent_steps_sampled: 6471750\n",
      "  num_agent_steps_trained: 6471750\n",
      "  num_steps_sampled: 6471780\n",
      "  num_steps_trained: 6471780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 811\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.67222222222222\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.27222222222223\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.333333333333336\n",
      "  player_1: 33.0\n",
      "  player_2: 43.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 1.8299999999999994\n",
      "  player_1: -2.0600000000000005\n",
      "  player_2: 3.23\n",
      "policy_reward_min:\n",
      "  player_0: -63.33333333333334\n",
      "  player_1: -58.666666666666664\n",
      "  player_2: -42.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08583971315089607\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2950002290871134\n",
      "  mean_inference_ms: 1.5973472232886037\n",
      "  mean_raw_obs_processing_ms: 0.2100430611889952\n",
      "time_since_restore: 12959.86375451088\n",
      "time_this_iter_s: 14.718866109848022\n",
      "time_total_s: 12959.86375451088\n",
      "timers:\n",
      "  learn_throughput: 545.932\n",
      "  learn_time_ms: 14617.218\n",
      "  load_throughput: 940823.423\n",
      "  load_time_ms: 8.482\n",
      "  sample_throughput: 503.231\n",
      "  sample_time_ms: 15857.539\n",
      "  update_time_ms: 7.627\n",
      "timestamp: 1643549201\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6471780\n",
      "training_iteration: 811\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6487711\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-27-12\n",
      "done: false\n",
      "episode_len_mean: 106.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 72\n",
      "episodes_total: 39400\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4664847063521544\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017930950890484874\n",
      "        policy_loss: -0.06756678668161233\n",
      "        total_loss: 80.68241411050161\n",
      "        vf_explained_var: 0.47171443621317544\n",
      "        vf_loss: 80.73787726402283\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4955018494526545\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016162490538634604\n",
      "        policy_loss: -0.08182035677600652\n",
      "        total_loss: 71.85043746312459\n",
      "        vf_explained_var: 0.26761605600516003\n",
      "        vf_loss: 71.92134806950887\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48011775175730387\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017169184879449895\n",
      "        policy_loss: -0.08777218490218122\n",
      "        total_loss: 76.58914797623952\n",
      "        vf_explained_var: 0.28591835568348567\n",
      "        vf_loss: 76.66533072948455\n",
      "  num_agent_steps_sampled: 6487711\n",
      "  num_agent_steps_trained: 6487711\n",
      "  num_steps_sampled: 6487740\n",
      "  num_steps_trained: 6487740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 813\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.309999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.20000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 40.333333333333336\n",
      "  player_1: 35.0\n",
      "  player_2: 32.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 0.2266666666666665\n",
      "  player_1: 1.916666666666667\n",
      "  player_2: 0.8566666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -52.33333333333333\n",
      "  player_1: -37.0\n",
      "  player_2: -58.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08586489057480833\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2950002439478657\n",
      "  mean_inference_ms: 1.5974307231737463\n",
      "  mean_raw_obs_processing_ms: 0.21014866282846703\n",
      "time_since_restore: 12991.224221229553\n",
      "time_this_iter_s: 16.44766402244568\n",
      "time_total_s: 12991.224221229553\n",
      "timers:\n",
      "  learn_throughput: 552.733\n",
      "  learn_time_ms: 14437.34\n",
      "  load_throughput: 938704.623\n",
      "  load_time_ms: 8.501\n",
      "  sample_throughput: 506.724\n",
      "  sample_time_ms: 15748.21\n",
      "  update_time_ms: 5.369\n",
      "timestamp: 1643549232\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6487740\n",
      "training_iteration: 813\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6503670\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-27-43\n",
      "done: false\n",
      "episode_len_mean: 106.62\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 73\n",
      "episodes_total: 39550\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47385416467984515\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016641167811586116\n",
      "        policy_loss: -0.05142515370311836\n",
      "        total_loss: 72.47441442171733\n",
      "        vf_explained_var: 0.3744599049290021\n",
      "        vf_loss: 72.5146066125234\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4960195190211137\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01619789237277549\n",
      "        policy_loss: -0.081113623160248\n",
      "        total_loss: 60.60885389486948\n",
      "        vf_explained_var: 0.09491566042105357\n",
      "        vf_loss: 60.67903412183126\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4641669904688994\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017165227092785926\n",
      "        policy_loss: -0.09375068396019438\n",
      "        total_loss: 63.36630978425344\n",
      "        vf_explained_var: 0.2455983214577039\n",
      "        vf_loss: 63.44847381750743\n",
      "  num_agent_steps_sampled: 6503670\n",
      "  num_agent_steps_trained: 6503670\n",
      "  num_steps_sampled: 6503700\n",
      "  num_steps_trained: 6503700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 815\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.966666666666665\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.25555555555555\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 48.66666666666667\n",
      "  player_1: 29.66666666666667\n",
      "  player_2: 28.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.3033333333333332\n",
      "  player_1: -0.30666666666666687\n",
      "  player_2: 0.0033333333333335703\n",
      "policy_reward_min:\n",
      "  player_0: -39.0\n",
      "  player_1: -30.666666666666664\n",
      "  player_2: -75.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08585417950032781\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29470431036459344\n",
      "  mean_inference_ms: 1.5961897591877747\n",
      "  mean_raw_obs_processing_ms: 0.20994726338249478\n",
      "time_since_restore: 13021.485306024551\n",
      "time_this_iter_s: 14.967737674713135\n",
      "time_total_s: 13021.485306024551\n",
      "timers:\n",
      "  learn_throughput: 552.945\n",
      "  learn_time_ms: 14431.802\n",
      "  load_throughput: 913303.006\n",
      "  load_time_ms: 8.738\n",
      "  sample_throughput: 509.969\n",
      "  sample_time_ms: 15648.012\n",
      "  update_time_ms: 5.393\n",
      "timestamp: 1643549263\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6503700\n",
      "training_iteration: 815\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6519631\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-28-15\n",
      "done: false\n",
      "episode_len_mean: 108.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 75\n",
      "episodes_total: 39697\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4768255026141803\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017291061038771052\n",
      "        policy_loss: -0.09538093751917283\n",
      "        total_loss: 70.045154226621\n",
      "        vf_explained_var: 0.31061795314153035\n",
      "        vf_loss: 70.12886322339376\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47953127627571424\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015440491156584054\n",
      "        policy_loss: -0.06605701599890987\n",
      "        total_loss: 78.64185198783875\n",
      "        vf_explained_var: 0.4244555623332659\n",
      "        vf_loss: 78.69748666763306\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4754841051995754\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01504174735267289\n",
      "        policy_loss: -0.08954393962398172\n",
      "        total_loss: 90.51874192873638\n",
      "        vf_explained_var: 0.32543205479780835\n",
      "        vf_loss: 90.59813256899515\n",
      "  num_agent_steps_sampled: 6519631\n",
      "  num_agent_steps_trained: 6519631\n",
      "  num_steps_sampled: 6519660\n",
      "  num_steps_trained: 6519660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 817\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.909523809523808\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.28571428571428\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.66666666666667\n",
      "  player_1: 42.0\n",
      "  player_2: 32.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 0.9499999999999994\n",
      "  player_1: 0.31999999999999973\n",
      "  player_2: 1.73\n",
      "policy_reward_min:\n",
      "  player_0: -47.0\n",
      "  player_1: -53.33333333333333\n",
      "  player_2: -47.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08586634430648481\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2945085236218036\n",
      "  mean_inference_ms: 1.5950674300356469\n",
      "  mean_raw_obs_processing_ms: 0.21008738405368085\n",
      "time_since_restore: 13053.922201871872\n",
      "time_this_iter_s: 16.978774785995483\n",
      "time_total_s: 13053.922201871872\n",
      "timers:\n",
      "  learn_throughput: 550.715\n",
      "  learn_time_ms: 14490.245\n",
      "  load_throughput: 914560.759\n",
      "  load_time_ms: 8.726\n",
      "  sample_throughput: 510.52\n",
      "  sample_time_ms: 15631.114\n",
      "  update_time_ms: 5.416\n",
      "timestamp: 1643549295\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6519660\n",
      "training_iteration: 817\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6535592\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-28-47\n",
      "done: false\n",
      "episode_len_mean: 116.28\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 72\n",
      "episodes_total: 39836\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46194651037454604\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016598753771183396\n",
      "        policy_loss: -0.11013781768735498\n",
      "        total_loss: 56.04342971642812\n",
      "        vf_explained_var: 0.17824785421291986\n",
      "        vf_loss: 56.14236350854238\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.480537677804629\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016546737222136773\n",
      "        policy_loss: -0.09198179606348277\n",
      "        total_loss: 55.5985666958491\n",
      "        vf_explained_var: 0.43113618582487107\n",
      "        vf_loss: 55.67937926133474\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4798851454257965\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016296137647619464\n",
      "        policy_loss: -0.04447235345840454\n",
      "        total_loss: 62.08119329770406\n",
      "        vf_explained_var: 0.397153456111749\n",
      "        vf_loss: 62.11466556708018\n",
      "  num_agent_steps_sampled: 6535592\n",
      "  num_agent_steps_trained: 6535592\n",
      "  num_steps_sampled: 6535620\n",
      "  num_steps_trained: 6535620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 819\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.035\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.21000000000001\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 41.66666666666667\n",
      "  player_1: 27.666666666666664\n",
      "  player_2: 30.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.0900000000000007\n",
      "  player_1: -1.5099999999999998\n",
      "  player_2: 1.4199999999999997\n",
      "policy_reward_min:\n",
      "  player_0: -26.0\n",
      "  player_1: -31.0\n",
      "  player_2: -34.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08585401877677627\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2949361670278918\n",
      "  mean_inference_ms: 1.597461107201565\n",
      "  mean_raw_obs_processing_ms: 0.2101144043114371\n",
      "time_since_restore: 13085.309934139252\n",
      "time_this_iter_s: 16.33220410346985\n",
      "time_total_s: 13085.309934139252\n",
      "timers:\n",
      "  learn_throughput: 556.776\n",
      "  learn_time_ms: 14332.503\n",
      "  load_throughput: 958876.581\n",
      "  load_time_ms: 8.322\n",
      "  sample_throughput: 513.836\n",
      "  sample_time_ms: 15530.257\n",
      "  update_time_ms: 5.515\n",
      "timestamp: 1643549327\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6535620\n",
      "training_iteration: 819\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6551550\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-29-21\n",
      "done: false\n",
      "episode_len_mean: 109.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 72\n",
      "episodes_total: 39983\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45006125018000603\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01697646105792046\n",
      "        policy_loss: -0.06556441695429385\n",
      "        total_loss: 67.83457591533661\n",
      "        vf_explained_var: 0.11167542209227879\n",
      "        vf_loss: 67.88868122736613\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4751497183740139\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016072324333924068\n",
      "        policy_loss: -0.08897759143883983\n",
      "        total_loss: 70.01919590473175\n",
      "        vf_explained_var: 0.18748758614063263\n",
      "        vf_loss: 70.09732466697693\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.453342167288065\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017408321527705842\n",
      "        policy_loss: -0.08433683661743999\n",
      "        total_loss: 56.491161619822186\n",
      "        vf_explained_var: 0.29099152753750485\n",
      "        vf_loss: 56.56374798138936\n",
      "  num_agent_steps_sampled: 6551550\n",
      "  num_agent_steps_trained: 6551550\n",
      "  num_steps_sampled: 6551580\n",
      "  num_steps_trained: 6551580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 821\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.395238095238094\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.29047619047617\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.33333333333333\n",
      "  player_1: 32.666666666666664\n",
      "  player_2: 37.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 1.0966666666666662\n",
      "  player_1: 0.3166666666666665\n",
      "  player_2: 1.586666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -32.666666666666664\n",
      "  player_1: -41.333333333333336\n",
      "  player_2: -45.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08584255424384608\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2946974069088764\n",
      "  mean_inference_ms: 1.5964282769802538\n",
      "  mean_raw_obs_processing_ms: 0.21031643026850902\n",
      "time_since_restore: 13120.024759531021\n",
      "time_this_iter_s: 17.790941953659058\n",
      "time_total_s: 13120.024759531021\n",
      "timers:\n",
      "  learn_throughput: 539.993\n",
      "  learn_time_ms: 14777.965\n",
      "  load_throughput: 968204.697\n",
      "  load_time_ms: 8.242\n",
      "  sample_throughput: 505.88\n",
      "  sample_time_ms: 15774.496\n",
      "  update_time_ms: 5.494\n",
      "timestamp: 1643549361\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6551580\n",
      "training_iteration: 821\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6567510\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-29-53\n",
      "done: false\n",
      "episode_len_mean: 114.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 70\n",
      "episodes_total: 40121\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4476425473888715\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01597613074524664\n",
      "        policy_loss: -0.09466204966418446\n",
      "        total_loss: 57.03738868713379\n",
      "        vf_explained_var: 0.30991009265184405\n",
      "        vf_loss: 57.12126688639323\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4890928154687087\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017251996473078558\n",
      "        policy_loss: -0.05657379607359568\n",
      "        total_loss: 64.78510386308034\n",
      "        vf_explained_var: 0.41073650648196536\n",
      "        vf_loss: 64.8300328652064\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4590952368080616\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016749590121801248\n",
      "        policy_loss: -0.07648513280165692\n",
      "        total_loss: 69.94438936074575\n",
      "        vf_explained_var: 0.4333664720257123\n",
      "        vf_loss: 70.00956870396932\n",
      "  num_agent_steps_sampled: 6567510\n",
      "  num_agent_steps_trained: 6567510\n",
      "  num_steps_sampled: 6567540\n",
      "  num_steps_trained: 6567540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 823\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.310526315789476\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.27894736842104\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.666666666666668\n",
      "  player_1: 48.0\n",
      "  player_2: 30.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: -0.996666666666667\n",
      "  player_1: -0.05666666666666675\n",
      "  player_2: 4.053333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -38.0\n",
      "  player_1: -39.666666666666664\n",
      "  player_2: -57.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08584081737604865\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2945952057518925\n",
      "  mean_inference_ms: 1.5971165309366364\n",
      "  mean_raw_obs_processing_ms: 0.2103070072475076\n",
      "time_since_restore: 13151.32083439827\n",
      "time_this_iter_s: 15.591696977615356\n",
      "time_total_s: 13151.32083439827\n",
      "timers:\n",
      "  learn_throughput: 540.169\n",
      "  learn_time_ms: 14773.158\n",
      "  load_throughput: 1008145.937\n",
      "  load_time_ms: 7.916\n",
      "  sample_throughput: 494.992\n",
      "  sample_time_ms: 16121.484\n",
      "  update_time_ms: 5.601\n",
      "timestamp: 1643549393\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6567540\n",
      "training_iteration: 823\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6583470\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-30-24\n",
      "done: false\n",
      "episode_len_mean: 106.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 80\n",
      "episodes_total: 40269\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4708794440329075\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017198408924839537\n",
      "        policy_loss: -0.0680379059429591\n",
      "        total_loss: 74.35887406826019\n",
      "        vf_explained_var: 0.2403132607539495\n",
      "        vf_loss: 74.4153029425939\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5090738470852375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016886613192073126\n",
      "        policy_loss: -0.12267262693339338\n",
      "        total_loss: 77.24551202615102\n",
      "        vf_explained_var: 0.2970768369237582\n",
      "        vf_loss: 77.35678604443868\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47475650583704315\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017526157025215146\n",
      "        policy_loss: -0.06920092360116542\n",
      "        total_loss: 54.15841498374939\n",
      "        vf_explained_var: 0.3864333771665891\n",
      "        vf_loss: 54.21578603744507\n",
      "  num_agent_steps_sampled: 6583470\n",
      "  num_agent_steps_trained: 6583470\n",
      "  num_steps_sampled: 6583500\n",
      "  num_steps_trained: 6583500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 825\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.021052631578948\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.28947368421052\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 34.0\n",
      "  player_2: 33.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 0.5100000000000002\n",
      "  player_1: 0.49000000000000005\n",
      "  player_2: 2.0000000000000004\n",
      "policy_reward_min:\n",
      "  player_0: -31.666666666666664\n",
      "  player_1: -27.0\n",
      "  player_2: -29.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08591321958900779\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29490869497553496\n",
      "  mean_inference_ms: 1.5970281560150397\n",
      "  mean_raw_obs_processing_ms: 0.21035148186155672\n",
      "time_since_restore: 13182.055497646332\n",
      "time_this_iter_s: 15.202836275100708\n",
      "time_total_s: 13182.055497646332\n",
      "timers:\n",
      "  learn_throughput: 538.386\n",
      "  learn_time_ms: 14822.088\n",
      "  load_throughput: 1050662.843\n",
      "  load_time_ms: 7.595\n",
      "  sample_throughput: 496.858\n",
      "  sample_time_ms: 16060.935\n",
      "  update_time_ms: 5.674\n",
      "timestamp: 1643549424\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6583500\n",
      "training_iteration: 825\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6599432\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-30-54\n",
      "done: false\n",
      "episode_len_mean: 107.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 74\n",
      "episodes_total: 40412\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46494218687216443\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016688618855453873\n",
      "        policy_loss: -0.0656567223680516\n",
      "        total_loss: 81.09427656173706\n",
      "        vf_explained_var: 0.3411548287669818\n",
      "        vf_loss: 81.1486684735616\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48989907642205555\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015271758899082973\n",
      "        policy_loss: -0.12193098295480014\n",
      "        total_loss: 73.96099866549174\n",
      "        vf_explained_var: 0.3945551942785581\n",
      "        vf_loss: 74.07262132167816\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47777030150095623\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017139035188747397\n",
      "        policy_loss: -0.042912141412186126\n",
      "        total_loss: 67.0481661828359\n",
      "        vf_explained_var: 0.2687472272912661\n",
      "        vf_loss: 67.07950939973195\n",
      "  num_agent_steps_sampled: 6599432\n",
      "  num_agent_steps_trained: 6599432\n",
      "  num_steps_sampled: 6599460\n",
      "  num_steps_trained: 6599460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 827\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.772222222222224\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.26666666666667\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.33333333333333\n",
      "  player_1: 35.333333333333336\n",
      "  player_2: 32.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.04\n",
      "  player_1: -1.69\n",
      "  player_2: 3.6500000000000004\n",
      "policy_reward_min:\n",
      "  player_0: -44.666666666666664\n",
      "  player_1: -40.0\n",
      "  player_2: -52.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08567263645079617\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29456808275229657\n",
      "  mean_inference_ms: 1.5970528645505158\n",
      "  mean_raw_obs_processing_ms: 0.21013501625914813\n",
      "time_since_restore: 13212.04724907875\n",
      "time_this_iter_s: 14.702689409255981\n",
      "time_total_s: 13212.04724907875\n",
      "timers:\n",
      "  learn_throughput: 547.359\n",
      "  learn_time_ms: 14579.099\n",
      "  load_throughput: 1052658.68\n",
      "  load_time_ms: 7.581\n",
      "  sample_throughput: 496.669\n",
      "  sample_time_ms: 16067.027\n",
      "  update_time_ms: 5.784\n",
      "timestamp: 1643549454\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6599460\n",
      "training_iteration: 827\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6615390\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-31-28\n",
      "done: false\n",
      "episode_len_mean: 105.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 74\n",
      "episodes_total: 40556\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45748613362511\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017419963736004622\n",
      "        policy_loss: -0.061646646422644454\n",
      "        total_loss: 63.5707143497467\n",
      "        vf_explained_var: 0.3266816259423892\n",
      "        vf_loss: 63.620602844556174\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.492059713502725\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01480432664178314\n",
      "        policy_loss: -0.09237908733387788\n",
      "        total_loss: 102.56082478523254\n",
      "        vf_explained_var: 0.3497474659482638\n",
      "        vf_loss: 102.6432109816869\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48407731890678407\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016558360263850318\n",
      "        policy_loss: -0.08334658895929654\n",
      "        total_loss: 88.17283311684926\n",
      "        vf_explained_var: 0.2688231456279755\n",
      "        vf_loss: 88.24500271320343\n",
      "  num_agent_steps_sampled: 6615390\n",
      "  num_agent_steps_trained: 6615390\n",
      "  num_steps_sampled: 6615420\n",
      "  num_steps_trained: 6615420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 829\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.930000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.25\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.33333333333333\n",
      "  player_1: 31.333333333333336\n",
      "  player_2: 50.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 1.083333333333333\n",
      "  player_1: 0.7933333333333331\n",
      "  player_2: 1.1233333333333337\n",
      "policy_reward_min:\n",
      "  player_0: -33.333333333333336\n",
      "  player_1: -85.66666666666667\n",
      "  player_2: -35.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08575666975148705\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2946088684029369\n",
      "  mean_inference_ms: 1.596115490223562\n",
      "  mean_raw_obs_processing_ms: 0.21029557872221694\n",
      "time_since_restore: 13246.165699005127\n",
      "time_this_iter_s: 16.471585512161255\n",
      "time_total_s: 13246.165699005127\n",
      "timers:\n",
      "  learn_throughput: 537.268\n",
      "  learn_time_ms: 14852.911\n",
      "  load_throughput: 1059345.347\n",
      "  load_time_ms: 7.533\n",
      "  sample_throughput: 495.663\n",
      "  sample_time_ms: 16099.664\n",
      "  update_time_ms: 5.781\n",
      "timestamp: 1643549488\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6615420\n",
      "training_iteration: 829\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6631350\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-32-01\n",
      "done: false\n",
      "episode_len_mean: 117.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 70\n",
      "episodes_total: 40695\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4531424372891585\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016200444171457395\n",
      "        policy_loss: -0.06991038563971719\n",
      "        total_loss: 60.643337114652\n",
      "        vf_explained_var: 0.38254614869753517\n",
      "        vf_loss: 60.70231207529704\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4765427220861117\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014456409018554647\n",
      "        policy_loss: -0.0526847118511796\n",
      "        total_loss: 63.0709868144989\n",
      "        vf_explained_var: 0.2404636518160502\n",
      "        vf_loss: 63.1139139175415\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4678151079515616\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017435980666781082\n",
      "        policy_loss: -0.11321671546592067\n",
      "        total_loss: 49.84523176987966\n",
      "        vf_explained_var: 0.39632239441076916\n",
      "        vf_loss: 49.94667898972829\n",
      "  num_agent_steps_sampled: 6631350\n",
      "  num_agent_steps_trained: 6631350\n",
      "  num_steps_sampled: 6631380\n",
      "  num_steps_trained: 6631380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 831\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.789473684210524\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.3\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 43.666666666666664\n",
      "  player_1: 32.0\n",
      "  player_2: 39.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.5333333333333337\n",
      "  player_1: 0.5933333333333333\n",
      "  player_2: -0.12666666666666593\n",
      "policy_reward_min:\n",
      "  player_0: -36.0\n",
      "  player_1: -37.333333333333336\n",
      "  player_2: -34.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08574478412512743\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29475048707457163\n",
      "  mean_inference_ms: 1.598396978453336\n",
      "  mean_raw_obs_processing_ms: 0.21022956353415367\n",
      "time_since_restore: 13278.778700590134\n",
      "time_this_iter_s: 15.789105892181396\n",
      "time_total_s: 13278.778700590134\n",
      "timers:\n",
      "  learn_throughput: 543.545\n",
      "  learn_time_ms: 14681.407\n",
      "  load_throughput: 1108832.281\n",
      "  load_time_ms: 7.197\n",
      "  sample_throughput: 496.794\n",
      "  sample_time_ms: 16062.988\n",
      "  update_time_ms: 5.94\n",
      "timestamp: 1643549521\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6631380\n",
      "training_iteration: 831\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6647310\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-32-32\n",
      "done: false\n",
      "episode_len_mean: 115.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 74\n",
      "episodes_total: 40835\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4685705694059531\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01675542140090992\n",
      "        policy_loss: -0.06362044071778655\n",
      "        total_loss: 65.09095737775166\n",
      "        vf_explained_var: 0.36493227670590084\n",
      "        vf_loss: 65.14326790173848\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4672104711830616\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014483160947035003\n",
      "        policy_loss: -0.1061754852750649\n",
      "        total_loss: 51.146718378067014\n",
      "        vf_explained_var: 0.2703657108545303\n",
      "        vf_loss: 51.243117915789284\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46703342502315837\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015414402016895111\n",
      "        policy_loss: -0.07456282656950255\n",
      "        total_loss: 65.61350024382273\n",
      "        vf_explained_var: 0.22582647820313773\n",
      "        vf_loss: 65.67765804449718\n",
      "  num_agent_steps_sampled: 6647310\n",
      "  num_agent_steps_trained: 6647310\n",
      "  num_steps_sampled: 6647340\n",
      "  num_steps_trained: 6647340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 833\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.285\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.31\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.666666666666664\n",
      "  player_1: 32.333333333333336\n",
      "  player_2: 32.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.2166666666666666\n",
      "  player_1: 0.6266666666666666\n",
      "  player_2: 1.1566666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -35.666666666666664\n",
      "  player_1: -42.0\n",
      "  player_2: -37.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08581365068308269\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2944228967292345\n",
      "  mean_inference_ms: 1.5954621304951353\n",
      "  mean_raw_obs_processing_ms: 0.21018825596859478\n",
      "time_since_restore: 13310.442412137985\n",
      "time_this_iter_s: 16.33184266090393\n",
      "time_total_s: 13310.442412137985\n",
      "timers:\n",
      "  learn_throughput: 542.144\n",
      "  learn_time_ms: 14719.341\n",
      "  load_throughput: 1128375.325\n",
      "  load_time_ms: 7.072\n",
      "  sample_throughput: 503.075\n",
      "  sample_time_ms: 15862.457\n",
      "  update_time_ms: 5.922\n",
      "timestamp: 1643549552\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6647340\n",
      "training_iteration: 833\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6663271\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-33-03\n",
      "done: false\n",
      "episode_len_mean: 118.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 65\n",
      "episodes_total: 40970\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.466359776755174\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01549615005769321\n",
      "        policy_loss: -0.09081568314693868\n",
      "        total_loss: 46.951859893798826\n",
      "        vf_explained_var: 0.44482247869173686\n",
      "        vf_loss: 47.03221555074056\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48675541778405507\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018297325213807197\n",
      "        policy_loss: -0.11006555409481128\n",
      "        total_loss: 69.20513715267181\n",
      "        vf_explained_var: 0.3574157500267029\n",
      "        vf_loss: 69.30285212993621\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45842363094290095\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014674322235735247\n",
      "        policy_loss: -0.022796748011993866\n",
      "        total_loss: 46.869437342484794\n",
      "        vf_explained_var: 0.21587919185558954\n",
      "        vf_loss: 46.88232901732127\n",
      "  num_agent_steps_sampled: 6663271\n",
      "  num_agent_steps_trained: 6663271\n",
      "  num_steps_sampled: 6663300\n",
      "  num_steps_trained: 6663300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 835\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.722222222222221\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.31666666666666\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.666666666666664\n",
      "  player_1: 49.0\n",
      "  player_2: 29.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.653333333333334\n",
      "  player_1: -1.1866666666666663\n",
      "  player_2: 2.533333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -66.0\n",
      "  player_1: -53.333333333333336\n",
      "  player_2: -29.666666666666668\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08567776825346343\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2942620337624047\n",
      "  mean_inference_ms: 1.5959682951176917\n",
      "  mean_raw_obs_processing_ms: 0.21007737897786094\n",
      "time_since_restore: 13341.50594997406\n",
      "time_this_iter_s: 14.994463920593262\n",
      "time_total_s: 13341.50594997406\n",
      "timers:\n",
      "  learn_throughput: 540.976\n",
      "  learn_time_ms: 14751.112\n",
      "  load_throughput: 1123470.515\n",
      "  load_time_ms: 7.103\n",
      "  sample_throughput: 499.034\n",
      "  sample_time_ms: 15990.896\n",
      "  update_time_ms: 5.759\n",
      "timestamp: 1643549583\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6663300\n",
      "training_iteration: 835\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6679231\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-33-37\n",
      "done: false\n",
      "episode_len_mean: 116.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 67\n",
      "episodes_total: 41108\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4473211620748043\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016147562249558784\n",
      "        policy_loss: -0.06550024591386318\n",
      "        total_loss: 55.988452444076536\n",
      "        vf_explained_var: 0.35543274929126106\n",
      "        vf_loss: 56.04305292526881\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46570671454072\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017305078710522063\n",
      "        policy_loss: -0.08538335570755104\n",
      "        total_loss: 53.47597887357076\n",
      "        vf_explained_var: 0.400469359656175\n",
      "        vf_loss: 53.54968134403229\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4814535454909007\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018241276060064895\n",
      "        policy_loss: -0.09749031426695486\n",
      "        total_loss: 44.401865549087525\n",
      "        vf_explained_var: 0.45961452335119246\n",
      "        vf_loss: 44.487043226559955\n",
      "  num_agent_steps_sampled: 6679231\n",
      "  num_agent_steps_trained: 6679231\n",
      "  num_steps_sampled: 6679260\n",
      "  num_steps_trained: 6679260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 837\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.614285714285716\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.27619047619048\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.0\n",
      "  player_1: 41.333333333333336\n",
      "  player_2: 29.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.1166666666666665\n",
      "  player_1: -1.2333333333333336\n",
      "  player_2: 3.1166666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -32.333333333333336\n",
      "  player_1: -46.0\n",
      "  player_2: -47.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08577593612298452\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2942294464334394\n",
      "  mean_inference_ms: 1.5951459088911306\n",
      "  mean_raw_obs_processing_ms: 0.2102015426095477\n",
      "time_since_restore: 13374.754396915436\n",
      "time_this_iter_s: 17.153003215789795\n",
      "time_total_s: 13374.754396915436\n",
      "timers:\n",
      "  learn_throughput: 529.359\n",
      "  learn_time_ms: 15074.834\n",
      "  load_throughput: 1078192.516\n",
      "  load_time_ms: 7.401\n",
      "  sample_throughput: 497.226\n",
      "  sample_time_ms: 16049.038\n",
      "  update_time_ms: 5.786\n",
      "timestamp: 1643549617\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6679260\n",
      "training_iteration: 837\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6695190\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-34-09\n",
      "done: false\n",
      "episode_len_mean: 114.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.9999999999999893\n",
      "episodes_this_iter: 68\n",
      "episodes_total: 41249\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4617781674365203\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01741260605206302\n",
      "        policy_loss: -0.10450879353719453\n",
      "        total_loss: 39.02265013535818\n",
      "        vf_explained_var: 0.3088079245885213\n",
      "        vf_loss: 39.11540513356527\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4795815163354079\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01578541400052965\n",
      "        policy_loss: -0.05905423884280026\n",
      "        total_loss: 50.518532835642496\n",
      "        vf_explained_var: 0.3925793744126956\n",
      "        vf_loss: 50.56693211396535\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4884156050781409\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01778726394136413\n",
      "        policy_loss: -0.08693217121995986\n",
      "        total_loss: 37.3203794280688\n",
      "        vf_explained_var: 0.3834228788812955\n",
      "        vf_loss: 37.39530511538187\n",
      "  num_agent_steps_sampled: 6695190\n",
      "  num_agent_steps_trained: 6695190\n",
      "  num_steps_sampled: 6695220\n",
      "  num_steps_trained: 6695220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 839\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.057142857142857\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.29999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.666666666666664\n",
      "  player_1: 31.333333333333336\n",
      "  player_2: 32.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 4.449999999999999\n",
      "  player_1: -1.770000000000001\n",
      "  player_2: 0.31999999999999934\n",
      "policy_reward_min:\n",
      "  player_0: -27.666666666666664\n",
      "  player_1: -31.66666666666667\n",
      "  player_2: -40.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08574590124934517\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.294222892117911\n",
      "  mean_inference_ms: 1.5952701336898616\n",
      "  mean_raw_obs_processing_ms: 0.2102704352280002\n",
      "time_since_restore: 13406.969596862793\n",
      "time_this_iter_s: 17.037323236465454\n",
      "time_total_s: 13406.969596862793\n",
      "timers:\n",
      "  learn_throughput: 536.155\n",
      "  learn_time_ms: 14883.757\n",
      "  load_throughput: 1076912.427\n",
      "  load_time_ms: 7.41\n",
      "  sample_throughput: 497.262\n",
      "  sample_time_ms: 16047.879\n",
      "  update_time_ms: 5.727\n",
      "timestamp: 1643549649\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6695220\n",
      "training_iteration: 839\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6711150\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-34-39\n",
      "done: false\n",
      "episode_len_mean: 112.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 69\n",
      "episodes_total: 41389\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4632949852446715\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016072077987812083\n",
      "        policy_loss: -0.08936765260683993\n",
      "        total_loss: 68.18973634243011\n",
      "        vf_explained_var: 0.3698557326197624\n",
      "        vf_loss: 68.26825536727905\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.480084458142519\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017044994339471865\n",
      "        policy_loss: -0.08749496114129822\n",
      "        total_loss: 62.09564343770345\n",
      "        vf_explained_var: 0.3384728708863258\n",
      "        vf_loss: 62.1716329908371\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4693474937478701\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016585299812496904\n",
      "        policy_loss: -0.06704192362725735\n",
      "        total_loss: 56.89175957043965\n",
      "        vf_explained_var: 0.11934085786342621\n",
      "        vf_loss: 56.94760644753774\n",
      "  num_agent_steps_sampled: 6711150\n",
      "  num_agent_steps_trained: 6711150\n",
      "  num_steps_sampled: 6711180\n",
      "  num_steps_trained: 6711180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 841\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.363157894736844\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.27368421052631\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.333333333333336\n",
      "  player_1: 33.0\n",
      "  player_2: 33.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 0.71\n",
      "  player_1: 1.5700000000000003\n",
      "  player_2: 0.72\n",
      "policy_reward_min:\n",
      "  player_0: -62.0\n",
      "  player_1: -42.333333333333336\n",
      "  player_2: -32.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08571713764836116\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29402227369031597\n",
      "  mean_inference_ms: 1.5940973666944398\n",
      "  mean_raw_obs_processing_ms: 0.21013747385323817\n",
      "time_since_restore: 13437.19459605217\n",
      "time_this_iter_s: 15.157041549682617\n",
      "time_total_s: 13437.19459605217\n",
      "timers:\n",
      "  learn_throughput: 544.937\n",
      "  learn_time_ms: 14643.902\n",
      "  load_throughput: 1048963.775\n",
      "  load_time_ms: 7.608\n",
      "  sample_throughput: 500.88\n",
      "  sample_time_ms: 15931.971\n",
      "  update_time_ms: 5.641\n",
      "timestamp: 1643549679\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6711180\n",
      "training_iteration: 841\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6727111\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-35-09\n",
      "done: false\n",
      "episode_len_mean: 117.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 69\n",
      "episodes_total: 41526\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4529115487635136\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01584165762643276\n",
      "        policy_loss: -0.09569406327791512\n",
      "        total_loss: 67.15528191884358\n",
      "        vf_explained_var: 0.3600930162270864\n",
      "        vf_loss: 67.24028275966644\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48443566232919694\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017233182113576315\n",
      "        policy_loss: -0.10791518229991198\n",
      "        total_loss: 53.39614860057831\n",
      "        vf_explained_var: 0.304845089217027\n",
      "        vf_loss: 53.492431584994\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46827066441377\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016931089005829562\n",
      "        policy_loss: -0.057836678891132275\n",
      "        total_loss: 55.24032301743825\n",
      "        vf_explained_var: 0.3404726880788803\n",
      "        vf_loss: 55.28673130989075\n",
      "  num_agent_steps_sampled: 6727111\n",
      "  num_agent_steps_trained: 6727111\n",
      "  num_steps_sampled: 6727140\n",
      "  num_steps_trained: 6727140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 843\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.533333333333335\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.28888888888888\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 33.0\n",
      "  player_2: 28.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.12666666666666654\n",
      "  player_1: -2.293333333333333\n",
      "  player_2: 5.166666666666668\n",
      "policy_reward_min:\n",
      "  player_0: -42.666666666666664\n",
      "  player_1: -38.33333333333333\n",
      "  player_2: -33.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08578214048029828\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29419842410773567\n",
      "  mean_inference_ms: 1.5953429117154718\n",
      "  mean_raw_obs_processing_ms: 0.21022110217798073\n",
      "time_since_restore: 13466.876536130905\n",
      "time_this_iter_s: 14.802431344985962\n",
      "time_total_s: 13466.876536130905\n",
      "timers:\n",
      "  learn_throughput: 552.489\n",
      "  learn_time_ms: 14443.734\n",
      "  load_throughput: 1022854.041\n",
      "  load_time_ms: 7.802\n",
      "  sample_throughput: 504.275\n",
      "  sample_time_ms: 15824.703\n",
      "  update_time_ms: 5.476\n",
      "timestamp: 1643549709\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6727140\n",
      "training_iteration: 843\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6743070\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-35-39\n",
      "done: false\n",
      "episode_len_mean: 117.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 69\n",
      "episodes_total: 41666\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4617774005730947\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01743000865499198\n",
      "        policy_loss: -0.09463285963827123\n",
      "        total_loss: 69.31459339777629\n",
      "        vf_explained_var: 0.2057697617014249\n",
      "        vf_loss: 69.39746112187703\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5016734564801058\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01765719932880529\n",
      "        policy_loss: -0.07816926528699696\n",
      "        total_loss: 60.1047979259491\n",
      "        vf_explained_var: 0.36864033639431\n",
      "        vf_loss: 60.171048444112145\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4926657707989216\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017547371804091463\n",
      "        policy_loss: -0.07387137797971567\n",
      "        total_loss: 51.140751390457154\n",
      "        vf_explained_var: 0.47778334508339565\n",
      "        vf_loss: 51.20277836004893\n",
      "  num_agent_steps_sampled: 6743070\n",
      "  num_agent_steps_trained: 6743070\n",
      "  num_steps_sampled: 6743100\n",
      "  num_steps_trained: 6743100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 845\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.29473684210526\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.34210526315788\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.666666666666664\n",
      "  player_1: 31.666666666666664\n",
      "  player_2: 26.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.36\n",
      "  player_1: 0.8999999999999999\n",
      "  player_2: -0.26\n",
      "policy_reward_min:\n",
      "  player_0: -27.333333333333336\n",
      "  player_1: -47.666666666666664\n",
      "  player_2: -45.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08567289755407864\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29374864825857255\n",
      "  mean_inference_ms: 1.5932315155140995\n",
      "  mean_raw_obs_processing_ms: 0.20995707523442575\n",
      "time_since_restore: 13497.198053121567\n",
      "time_this_iter_s: 14.943558692932129\n",
      "time_total_s: 13497.198053121567\n",
      "timers:\n",
      "  learn_throughput: 555.427\n",
      "  learn_time_ms: 14367.315\n",
      "  load_throughput: 1013494.24\n",
      "  load_time_ms: 7.874\n",
      "  sample_throughput: 511.379\n",
      "  sample_time_ms: 15604.86\n",
      "  update_time_ms: 5.672\n",
      "timestamp: 1643549739\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6743100\n",
      "training_iteration: 845\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6759032\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-36-11\n",
      "done: false\n",
      "episode_len_mean: 110.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 68\n",
      "episodes_total: 41804\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44924983809391655\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016010647995871825\n",
      "        policy_loss: -0.08190580456207196\n",
      "        total_loss: 82.19007974465688\n",
      "        vf_explained_var: 0.17147118786970775\n",
      "        vf_loss: 82.26117844581604\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48350453436374663\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01596356875518893\n",
      "        policy_loss: -0.05813959222286939\n",
      "        total_loss: 66.47989157835643\n",
      "        vf_explained_var: 0.2678970444202423\n",
      "        vf_loss: 66.52725554625194\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4751086087524891\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01663777404914678\n",
      "        policy_loss: -0.09025656450539828\n",
      "        total_loss: 66.5362982527415\n",
      "        vf_explained_var: 0.33602743039528526\n",
      "        vf_loss: 66.6153244638443\n",
      "  num_agent_steps_sampled: 6759032\n",
      "  num_agent_steps_trained: 6759032\n",
      "  num_steps_sampled: 6759060\n",
      "  num_steps_trained: 6759060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 847\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.985\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.33999999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.0\n",
      "  player_1: 39.666666666666664\n",
      "  player_2: 45.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.5566666666666666\n",
      "  player_1: 0.9966666666666668\n",
      "  player_2: -0.5533333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -59.0\n",
      "  player_1: -44.0\n",
      "  player_2: -43.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08575828145879144\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29415223811042546\n",
      "  mean_inference_ms: 1.5947391651488898\n",
      "  mean_raw_obs_processing_ms: 0.21021342665506892\n",
      "time_since_restore: 13528.785566091537\n",
      "time_this_iter_s: 16.330531358718872\n",
      "time_total_s: 13528.785566091537\n",
      "timers:\n",
      "  learn_throughput: 561.85\n",
      "  learn_time_ms: 14203.086\n",
      "  load_throughput: 1019181.925\n",
      "  load_time_ms: 7.83\n",
      "  sample_throughput: 514.336\n",
      "  sample_time_ms: 15515.15\n",
      "  update_time_ms: 6.02\n",
      "timestamp: 1643549771\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6759060\n",
      "training_iteration: 847\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6774990\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-36-41\n",
      "done: false\n",
      "episode_len_mean: 115.42\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 41944\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4660700983305772\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016639237501707007\n",
      "        policy_loss: -0.069441356312794\n",
      "        total_loss: 63.270766281286875\n",
      "        vf_explained_var: 0.2876057638724645\n",
      "        vf_loss: 63.32897589524587\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48177116627494493\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015855755801918954\n",
      "        policy_loss: -0.08980582928285002\n",
      "        total_loss: 59.33834478378296\n",
      "        vf_explained_var: 0.32229473878939946\n",
      "        vf_loss: 59.41744806607564\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46880817905068395\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01640354801952981\n",
      "        policy_loss: -0.08201600718622407\n",
      "        total_loss: 73.43321625391643\n",
      "        vf_explained_var: 0.24164751221736272\n",
      "        vf_loss: 73.50416003227234\n",
      "  num_agent_steps_sampled: 6774990\n",
      "  num_agent_steps_trained: 6774990\n",
      "  num_steps_sampled: 6775020\n",
      "  num_steps_trained: 6775020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 849\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.526315789473681\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.3\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 29.333333333333336\n",
      "  player_2: 33.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.213333333333333\n",
      "  player_1: -1.6766666666666665\n",
      "  player_2: 2.463333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -47.333333333333336\n",
      "  player_1: -40.333333333333336\n",
      "  player_2: -30.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08572206449070226\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2940556871391689\n",
      "  mean_inference_ms: 1.5940043187105375\n",
      "  mean_raw_obs_processing_ms: 0.21023933770607173\n",
      "time_since_restore: 13559.080182313919\n",
      "time_this_iter_s: 15.299408912658691\n",
      "time_total_s: 13559.080182313919\n",
      "timers:\n",
      "  learn_throughput: 569.573\n",
      "  learn_time_ms: 14010.485\n",
      "  load_throughput: 925452.03\n",
      "  load_time_ms: 8.623\n",
      "  sample_throughput: 517.718\n",
      "  sample_time_ms: 15413.809\n",
      "  update_time_ms: 6.035\n",
      "timestamp: 1643549801\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6775020\n",
      "training_iteration: 849\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6790951\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-37-14\n",
      "done: false\n",
      "episode_len_mean: 106.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 73\n",
      "episodes_total: 42085\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46412911449869476\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017863646146982243\n",
      "        policy_loss: -0.06446818659702937\n",
      "        total_loss: 61.52809937318166\n",
      "        vf_explained_var: 0.3432363876700401\n",
      "        vf_loss: 61.580509703954064\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.5083139122029146\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01885111017028976\n",
      "        policy_loss: -0.10193274731437366\n",
      "        total_loss: 56.49951723575592\n",
      "        vf_explained_var: 0.20637036701043446\n",
      "        vf_loss: 56.5887254301707\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47317889059583346\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01689072622299998\n",
      "        policy_loss: -0.09385621740327527\n",
      "        total_loss: 42.25636841456095\n",
      "        vf_explained_var: 0.23969898790121077\n",
      "        vf_loss: 42.338823657035825\n",
      "  num_agent_steps_sampled: 6790951\n",
      "  num_agent_steps_trained: 6790951\n",
      "  num_steps_sampled: 6790980\n",
      "  num_steps_trained: 6790980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 851\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.795238095238098\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.29999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.0\n",
      "  player_1: 30.333333333333336\n",
      "  player_2: 28.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.6000000000000003\n",
      "  player_1: 0.8899999999999999\n",
      "  player_2: 0.5099999999999997\n",
      "policy_reward_min:\n",
      "  player_0: -29.333333333333336\n",
      "  player_1: -31.0\n",
      "  player_2: -36.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08566111461496252\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29450246230103194\n",
      "  mean_inference_ms: 1.597167457444961\n",
      "  mean_raw_obs_processing_ms: 0.21013187759052357\n",
      "time_since_restore: 13592.189834356308\n",
      "time_this_iter_s: 17.717854976654053\n",
      "time_total_s: 13592.189834356308\n",
      "timers:\n",
      "  learn_throughput: 557.912\n",
      "  learn_time_ms: 14303.32\n",
      "  load_throughput: 889814.381\n",
      "  load_time_ms: 8.968\n",
      "  sample_throughput: 522.68\n",
      "  sample_time_ms: 15267.468\n",
      "  update_time_ms: 5.964\n",
      "timestamp: 1643549834\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6790980\n",
      "training_iteration: 851\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6806912\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-37-45\n",
      "done: false\n",
      "episode_len_mean: 113.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 78\n",
      "episodes_total: 42229\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4709423882762591\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017412356556439666\n",
      "        policy_loss: -0.08512269660520057\n",
      "        total_loss: 62.4954047425588\n",
      "        vf_explained_var: 0.16573192993799846\n",
      "        vf_loss: 62.568773937225345\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4980800181130568\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017766868798741293\n",
      "        policy_loss: -0.1053837700560689\n",
      "        total_loss: 55.87921512444814\n",
      "        vf_explained_var: 0.29182455241680144\n",
      "        vf_loss: 55.97260619481405\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4632596577703953\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01700161000942804\n",
      "        policy_loss: -0.059275263988723355\n",
      "        total_loss: 73.18676951726277\n",
      "        vf_explained_var: 0.3340872572859128\n",
      "        vf_loss: 73.2345685005188\n",
      "  num_agent_steps_sampled: 6806912\n",
      "  num_agent_steps_trained: 6806912\n",
      "  num_steps_sampled: 6806940\n",
      "  num_steps_trained: 6806940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 853\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.594736842105263\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.3\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.33333333333333\n",
      "  player_1: 33.666666666666664\n",
      "  player_2: 45.0\n",
      "policy_reward_mean:\n",
      "  player_0: 4.613333333333333\n",
      "  player_1: -3.5266666666666664\n",
      "  player_2: 1.913333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -38.0\n",
      "  player_1: -53.66666666666667\n",
      "  player_2: -30.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08583425081244839\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29440708166064616\n",
      "  mean_inference_ms: 1.594834640139241\n",
      "  mean_raw_obs_processing_ms: 0.2103007226778274\n",
      "time_since_restore: 13622.64281654358\n",
      "time_this_iter_s: 15.315940380096436\n",
      "time_total_s: 13622.64281654358\n",
      "timers:\n",
      "  learn_throughput: 554.909\n",
      "  learn_time_ms: 14380.746\n",
      "  load_throughput: 836644.426\n",
      "  load_time_ms: 9.538\n",
      "  sample_throughput: 513.161\n",
      "  sample_time_ms: 15550.669\n",
      "  update_time_ms: 6.131\n",
      "timestamp: 1643549865\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6806940\n",
      "training_iteration: 853\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6822870\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-38-17\n",
      "done: false\n",
      "episode_len_mean: 109.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 73\n",
      "episodes_total: 42374\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4575246050953865\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015849202186999114\n",
      "        policy_loss: -0.10342633448541165\n",
      "        total_loss: 76.5448378944397\n",
      "        vf_explained_var: 0.3363797615965207\n",
      "        vf_loss: 76.63756584326426\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49069684093197186\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016918825750435643\n",
      "        policy_loss: -0.0846517023219106\n",
      "        total_loss: 64.42438978513081\n",
      "        vf_explained_var: 0.4245786946018537\n",
      "        vf_loss: 64.49762126604716\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46915367260575297\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016847734542950358\n",
      "        policy_loss: -0.049124629826595385\n",
      "        total_loss: 49.58105060100556\n",
      "        vf_explained_var: 0.3738290184736252\n",
      "        vf_loss: 49.618803141911826\n",
      "  num_agent_steps_sampled: 6822870\n",
      "  num_agent_steps_trained: 6822870\n",
      "  num_steps_sampled: 6822900\n",
      "  num_steps_trained: 6822900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 855\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.004999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.33\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.333333333333336\n",
      "  player_1: 34.333333333333336\n",
      "  player_2: 30.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.0699999999999994\n",
      "  player_1: -0.2899999999999998\n",
      "  player_2: 0.21999999999999964\n",
      "policy_reward_min:\n",
      "  player_0: -45.333333333333336\n",
      "  player_1: -37.666666666666664\n",
      "  player_2: -29.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08572349817109239\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29396668658691516\n",
      "  mean_inference_ms: 1.5934639455619581\n",
      "  mean_raw_obs_processing_ms: 0.2101918542429212\n",
      "time_since_restore: 13654.28772187233\n",
      "time_this_iter_s: 16.04826259613037\n",
      "time_total_s: 13654.28772187233\n",
      "timers:\n",
      "  learn_throughput: 549.888\n",
      "  learn_time_ms: 14512.048\n",
      "  load_throughput: 830683.048\n",
      "  load_time_ms: 9.607\n",
      "  sample_throughput: 510.753\n",
      "  sample_time_ms: 15623.979\n",
      "  update_time_ms: 5.977\n",
      "timestamp: 1643549897\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6822900\n",
      "training_iteration: 855\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6838830\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-38-47\n",
      "done: false\n",
      "episode_len_mean: 113.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 65\n",
      "episodes_total: 42508\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4520949945350488\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016979534020368496\n",
      "        policy_loss: -0.09046359438449145\n",
      "        total_loss: 52.07515543619792\n",
      "        vf_explained_var: 0.3623142828543981\n",
      "        vf_loss: 52.15415785630544\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47400377213954925\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015644390390540743\n",
      "        policy_loss: -0.09872784715533878\n",
      "        total_loss: 52.75219671885173\n",
      "        vf_explained_var: 0.40971743484338125\n",
      "        vf_loss: 52.840364524523416\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4728322667876879\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017596088882887672\n",
      "        policy_loss: -0.055219953649987774\n",
      "        total_loss: 44.67979568799337\n",
      "        vf_explained_var: 0.4373265153169632\n",
      "        vf_loss: 44.72313815434774\n",
      "  num_agent_steps_sampled: 6838830\n",
      "  num_agent_steps_trained: 6838830\n",
      "  num_steps_sampled: 6838860\n",
      "  num_steps_trained: 6838860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 857\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.026315789473685\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.34210526315788\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 24.666666666666668\n",
      "  player_1: 30.0\n",
      "  player_2: 32.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 0.5633333333333332\n",
      "  player_1: -0.826666666666667\n",
      "  player_2: 3.263333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -33.333333333333336\n",
      "  player_1: -33.0\n",
      "  player_2: -24.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08563758953313196\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29406445876617615\n",
      "  mean_inference_ms: 1.5950989368842812\n",
      "  mean_raw_obs_processing_ms: 0.21011558086415463\n",
      "time_since_restore: 13684.809220790863\n",
      "time_this_iter_s: 14.711485862731934\n",
      "time_total_s: 13684.809220790863\n",
      "timers:\n",
      "  learn_throughput: 553.95\n",
      "  learn_time_ms: 14405.627\n",
      "  load_throughput: 826159.886\n",
      "  load_time_ms: 9.659\n",
      "  sample_throughput: 505.298\n",
      "  sample_time_ms: 15792.674\n",
      "  update_time_ms: 6.012\n",
      "timestamp: 1643549927\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6838860\n",
      "training_iteration: 857\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6854792\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-39-18\n",
      "done: false\n",
      "episode_len_mean: 112.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 74\n",
      "episodes_total: 42647\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48015558049082757\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0174675921205926\n",
      "        policy_loss: -0.07393604866539438\n",
      "        total_loss: 92.35315627733867\n",
      "        vf_explained_var: 0.27819972813129423\n",
      "        vf_loss: 92.41530174255371\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4926255126297474\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01744048594725124\n",
      "        policy_loss: -0.08808671042323113\n",
      "        total_loss: 66.25083364486694\n",
      "        vf_explained_var: 0.40262406369050346\n",
      "        vf_loss: 66.32714776674906\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4442602284749349\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01597116120098993\n",
      "        policy_loss: -0.09254381375697752\n",
      "        total_loss: 66.5816621764501\n",
      "        vf_explained_var: 0.2281769256790479\n",
      "        vf_loss: 66.66342560450236\n",
      "  num_agent_steps_sampled: 6854792\n",
      "  num_agent_steps_trained: 6854792\n",
      "  num_steps_sampled: 6854820\n",
      "  num_steps_trained: 6854820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 859\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.689473684210528\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.3\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 27.666666666666664\n",
      "  player_1: 32.333333333333336\n",
      "  player_2: 46.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.6166666666666666\n",
      "  player_1: -1.8833333333333335\n",
      "  player_2: 4.266666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -64.0\n",
      "  player_1: -38.333333333333336\n",
      "  player_2: -35.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0856889939137708\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2936264574534589\n",
      "  mean_inference_ms: 1.592651602560473\n",
      "  mean_raw_obs_processing_ms: 0.21018949528978992\n",
      "time_since_restore: 13715.972149133682\n",
      "time_this_iter_s: 15.3308687210083\n",
      "time_total_s: 13715.972149133682\n",
      "timers:\n",
      "  learn_throughput: 552.252\n",
      "  learn_time_ms: 14449.914\n",
      "  load_throughput: 857225.624\n",
      "  load_time_ms: 9.309\n",
      "  sample_throughput: 506.434\n",
      "  sample_time_ms: 15757.225\n",
      "  update_time_ms: 6.202\n",
      "timestamp: 1643549958\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6854820\n",
      "training_iteration: 859\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6870754\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-39-49\n",
      "done: false\n",
      "episode_len_mean: 116.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 67\n",
      "episodes_total: 42785\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4546585321923097\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017110898286969226\n",
      "        policy_loss: -0.08707207998260856\n",
      "        total_loss: 48.97561346371969\n",
      "        vf_explained_var: 0.29425925473372144\n",
      "        vf_loss: 49.05113574028015\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4964391613006592\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016153048395639757\n",
      "        policy_loss: -0.07600516155051688\n",
      "        total_loss: 52.83490709622701\n",
      "        vf_explained_var: 0.33159207940101626\n",
      "        vf_loss: 52.90000864823659\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4719945476949215\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017567300646854467\n",
      "        policy_loss: -0.08961703225970269\n",
      "        total_loss: 40.04463660558065\n",
      "        vf_explained_var: 0.26318218996127446\n",
      "        vf_loss: 40.122395633856456\n",
      "  num_agent_steps_sampled: 6870754\n",
      "  num_agent_steps_trained: 6870754\n",
      "  num_steps_sampled: 6870780\n",
      "  num_steps_trained: 6870780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 861\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.933333333333334\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.33333333333333\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 28.333333333333336\n",
      "  player_2: 27.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.20666666666666678\n",
      "  player_1: 1.1366666666666667\n",
      "  player_2: 1.656666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -31.0\n",
      "  player_1: -24.0\n",
      "  player_2: -36.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08570298484785692\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29409598543026033\n",
      "  mean_inference_ms: 1.5948008120877302\n",
      "  mean_raw_obs_processing_ms: 0.210456868569688\n",
      "time_since_restore: 13746.300931692123\n",
      "time_this_iter_s: 14.969571352005005\n",
      "time_total_s: 13746.300931692123\n",
      "timers:\n",
      "  learn_throughput: 563.101\n",
      "  learn_time_ms: 14171.532\n",
      "  load_throughput: 909674.021\n",
      "  load_time_ms: 8.772\n",
      "  sample_throughput: 507.841\n",
      "  sample_time_ms: 15713.587\n",
      "  update_time_ms: 6.206\n",
      "timestamp: 1643549989\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6870780\n",
      "training_iteration: 861\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6886712\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-40-21\n",
      "done: false\n",
      "episode_len_mean: 118.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 42917\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4633269195755323\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01663350024726602\n",
      "        policy_loss: -0.10146704043416928\n",
      "        total_loss: 70.53110382080078\n",
      "        vf_explained_var: 0.2751222907503446\n",
      "        vf_loss: 70.62134310245514\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4830310209095478\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016611221525464924\n",
      "        policy_loss: -0.04143658421933651\n",
      "        total_loss: 75.06056985855102\n",
      "        vf_explained_var: 0.23971630066633223\n",
      "        vf_loss: 75.09079394340515\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.465348607301712\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016408453561220806\n",
      "        policy_loss: -0.08989219701228042\n",
      "        total_loss: 80.03079598585765\n",
      "        vf_explained_var: 0.3583820614218712\n",
      "        vf_loss: 80.10961258570353\n",
      "  num_agent_steps_sampled: 6886712\n",
      "  num_agent_steps_trained: 6886712\n",
      "  num_steps_sampled: 6886740\n",
      "  num_steps_trained: 6886740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 863\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.14\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.33999999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 38.666666666666664\n",
      "  player_2: 32.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.8000000000000003\n",
      "  player_1: 1.06\n",
      "  player_2: 0.13999999999999993\n",
      "policy_reward_min:\n",
      "  player_0: -44.666666666666664\n",
      "  player_1: -34.0\n",
      "  player_2: -44.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08575332251441303\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2939902100954378\n",
      "  mean_inference_ms: 1.59503212615463\n",
      "  mean_raw_obs_processing_ms: 0.21041098711797304\n",
      "time_since_restore: 13778.116256713867\n",
      "time_this_iter_s: 16.21574854850769\n",
      "time_total_s: 13778.116256713867\n",
      "timers:\n",
      "  learn_throughput: 557.855\n",
      "  learn_time_ms: 14304.796\n",
      "  load_throughput: 926687.042\n",
      "  load_time_ms: 8.611\n",
      "  sample_throughput: 515.269\n",
      "  sample_time_ms: 15487.07\n",
      "  update_time_ms: 6.156\n",
      "timestamp: 1643550021\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6886740\n",
      "training_iteration: 863\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6902670\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-40-51\n",
      "done: false\n",
      "episode_len_mean: 110.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 67\n",
      "episodes_total: 43056\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.477976190050443\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017250201932210038\n",
      "        policy_loss: -0.08001150132312129\n",
      "        total_loss: 57.9936443010966\n",
      "        vf_explained_var: 0.4185772925615311\n",
      "        vf_loss: 58.062011594772336\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4852685721218586\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01762740695109945\n",
      "        policy_loss: -0.07170713573073348\n",
      "        total_loss: 46.92939396381378\n",
      "        vf_explained_var: 0.34162551989157997\n",
      "        vf_loss: 46.989202642440794\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46816797092556955\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017548863171390015\n",
      "        policy_loss: -0.09533991371591886\n",
      "        total_loss: 59.609353901545205\n",
      "        vf_explained_var: 0.4600350155433019\n",
      "        vf_loss: 59.69284879366557\n",
      "  num_agent_steps_sampled: 6902670\n",
      "  num_agent_steps_trained: 6902670\n",
      "  num_steps_sampled: 6902700\n",
      "  num_steps_trained: 6902700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 865\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.383333333333333\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.34444444444443\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.333333333333336\n",
      "  player_1: 28.0\n",
      "  player_2: 34.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 1.1600000000000006\n",
      "  player_1: -0.8399999999999995\n",
      "  player_2: 2.6800000000000006\n",
      "policy_reward_min:\n",
      "  player_0: -32.33333333333333\n",
      "  player_1: -32.0\n",
      "  player_2: -42.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08575137504320683\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2942197411715695\n",
      "  mean_inference_ms: 1.5954960007182135\n",
      "  mean_raw_obs_processing_ms: 0.2104305178318251\n",
      "time_since_restore: 13807.957802772522\n",
      "time_this_iter_s: 14.413483142852783\n",
      "time_total_s: 13807.957802772522\n",
      "timers:\n",
      "  learn_throughput: 564.947\n",
      "  learn_time_ms: 14125.226\n",
      "  load_throughput: 928870.441\n",
      "  load_time_ms: 8.591\n",
      "  sample_throughput: 512.973\n",
      "  sample_time_ms: 15556.376\n",
      "  update_time_ms: 6.15\n",
      "timestamp: 1643550051\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6902700\n",
      "training_iteration: 865\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6918630\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-41-25\n",
      "done: false\n",
      "episode_len_mean: 120.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 66\n",
      "episodes_total: 43194\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4578071008622646\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017158258320068853\n",
      "        policy_loss: -0.06854264322978755\n",
      "        total_loss: 59.74353696346283\n",
      "        vf_explained_var: 0.2687076594432195\n",
      "        vf_loss: 59.800497948328655\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49109202533960344\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017662723558937426\n",
      "        policy_loss: -0.07829330768746634\n",
      "        total_loss: 59.58094130039215\n",
      "        vf_explained_var: 0.40302838385105133\n",
      "        vf_loss: 59.64731227397919\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4623380294442177\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01680548500301029\n",
      "        policy_loss: -0.1167045786899204\n",
      "        total_loss: 42.24811374823253\n",
      "        vf_explained_var: 0.30293233921130497\n",
      "        vf_loss: 42.35347472190857\n",
      "  num_agent_steps_sampled: 6918630\n",
      "  num_agent_steps_trained: 6918630\n",
      "  num_steps_sampled: 6918660\n",
      "  num_steps_trained: 6918660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 867\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.281818181818181\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.32272727272726\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.33333333333333\n",
      "  player_1: 36.0\n",
      "  player_2: 28.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 5.686666666666667\n",
      "  player_1: -4.3133333333333335\n",
      "  player_2: 1.6266666666666663\n",
      "policy_reward_min:\n",
      "  player_0: -31.0\n",
      "  player_1: -39.0\n",
      "  player_2: -60.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0857234527426297\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2937824438686888\n",
      "  mean_inference_ms: 1.5941091023356655\n",
      "  mean_raw_obs_processing_ms: 0.2103773401351947\n",
      "time_since_restore: 13842.514265537262\n",
      "time_this_iter_s: 17.335343599319458\n",
      "time_total_s: 13842.514265537262\n",
      "timers:\n",
      "  learn_throughput: 549.277\n",
      "  learn_time_ms: 14528.185\n",
      "  load_throughput: 927290.37\n",
      "  load_time_ms: 8.606\n",
      "  sample_throughput: 513.774\n",
      "  sample_time_ms: 15532.12\n",
      "  update_time_ms: 5.722\n",
      "timestamp: 1643550085\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6918660\n",
      "training_iteration: 867\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6934591\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-41-56\n",
      "done: false\n",
      "episode_len_mean: 108.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 43344\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4593423096338908\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016438239118094677\n",
      "        policy_loss: -0.07672989738484223\n",
      "        total_loss: 47.80224703629812\n",
      "        vf_explained_var: 0.21574428935845694\n",
      "        vf_loss: 47.86788106441498\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47476396019260086\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017376510688177027\n",
      "        policy_loss: -0.095112292895404\n",
      "        total_loss: 62.188633346557616\n",
      "        vf_explained_var: 0.36642666190862655\n",
      "        vf_loss: 62.27201645215352\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4659122631450494\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01716671328159767\n",
      "        policy_loss: -0.06069335151153306\n",
      "        total_loss: 45.370483045578005\n",
      "        vf_explained_var: 0.3990620748202006\n",
      "        vf_loss: 45.41958903948466\n",
      "  num_agent_steps_sampled: 6934591\n",
      "  num_agent_steps_trained: 6934591\n",
      "  num_steps_sampled: 6934620\n",
      "  num_steps_trained: 6934620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 869\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.14\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.33999999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 42.333333333333336\n",
      "  player_1: 33.333333333333336\n",
      "  player_2: 35.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.8033333333333331\n",
      "  player_1: -0.29666666666666686\n",
      "  player_2: 2.493333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -36.333333333333336\n",
      "  player_1: -38.666666666666664\n",
      "  player_2: -28.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08563687160183321\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29320120186080706\n",
      "  mean_inference_ms: 1.5907916884712674\n",
      "  mean_raw_obs_processing_ms: 0.21021505109845143\n",
      "time_since_restore: 13873.411218881607\n",
      "time_this_iter_s: 15.838193655014038\n",
      "time_total_s: 13873.411218881607\n",
      "timers:\n",
      "  learn_throughput: 548.624\n",
      "  learn_time_ms: 14545.485\n",
      "  load_throughput: 928350.019\n",
      "  load_time_ms: 8.596\n",
      "  sample_throughput: 509.054\n",
      "  sample_time_ms: 15676.132\n",
      "  update_time_ms: 5.68\n",
      "timestamp: 1643550116\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6934620\n",
      "training_iteration: 869\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6950552\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-42-27\n",
      "done: false\n",
      "episode_len_mean: 117.54\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 67\n",
      "episodes_total: 43480\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46317830473184585\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016201696748504447\n",
      "        policy_loss: -0.08426363093157609\n",
      "        total_loss: 61.05238868713379\n",
      "        vf_explained_var: 0.4303009209036827\n",
      "        vf_loss: 61.125716325442\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4793833601971467\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01589392604188712\n",
      "        policy_loss: -0.017628869398807485\n",
      "        total_loss: 47.79040562788646\n",
      "        vf_explained_var: 0.30938460846741994\n",
      "        vf_loss: 47.79730601469676\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44515588134527206\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015510456666719014\n",
      "        policy_loss: -0.1164552318273733\n",
      "        total_loss: 45.500159362157184\n",
      "        vf_explained_var: 0.42001819183429084\n",
      "        vf_loss: 45.606144903500876\n",
      "  num_agent_steps_sampled: 6950552\n",
      "  num_agent_steps_trained: 6950552\n",
      "  num_steps_sampled: 6950580\n",
      "  num_steps_trained: 6950580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 871\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.28947368421053\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.3\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.0\n",
      "  player_1: 30.0\n",
      "  player_2: 35.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.8633333333333324\n",
      "  player_1: -2.436666666666667\n",
      "  player_2: 1.5733333333333328\n",
      "policy_reward_min:\n",
      "  player_0: -34.0\n",
      "  player_1: -31.333333333333336\n",
      "  player_2: -31.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08569244142319832\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29378392097074146\n",
      "  mean_inference_ms: 1.5941076493681168\n",
      "  mean_raw_obs_processing_ms: 0.210356068996052\n",
      "time_since_restore: 13903.921571016312\n",
      "time_this_iter_s: 15.14476466178894\n",
      "time_total_s: 13903.921571016312\n",
      "timers:\n",
      "  learn_throughput: 547.986\n",
      "  learn_time_ms: 14562.426\n",
      "  load_throughput: 886824.914\n",
      "  load_time_ms: 8.998\n",
      "  sample_throughput: 505.983\n",
      "  sample_time_ms: 15771.285\n",
      "  update_time_ms: 5.782\n",
      "timestamp: 1643550147\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6950580\n",
      "training_iteration: 871\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6966510\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-42-56\n",
      "done: false\n",
      "episode_len_mean: 112.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 69\n",
      "episodes_total: 43620\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4641241798301538\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017433743296155817\n",
      "        policy_loss: -0.06739376825590929\n",
      "        total_loss: 61.73464124361674\n",
      "        vf_explained_var: 0.28307435830434163\n",
      "        vf_loss: 61.79026705265045\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4852951803803444\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01632085130379636\n",
      "        policy_loss: -0.0884541107636566\n",
      "        total_loss: 59.79424535910289\n",
      "        vf_explained_var: 0.2587912708520889\n",
      "        vf_loss: 59.871682912508646\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46727503622571626\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016651936786873496\n",
      "        policy_loss: -0.0940362682317694\n",
      "        total_loss: 43.10615142504374\n",
      "        vf_explained_var: 0.3606452386577924\n",
      "        vf_loss: 43.18894774278005\n",
      "  num_agent_steps_sampled: 6966510\n",
      "  num_agent_steps_trained: 6966510\n",
      "  num_steps_sampled: 6966540\n",
      "  num_steps_trained: 6966540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 873\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.555555555555555\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.3\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.666666666666664\n",
      "  player_1: 34.0\n",
      "  player_2: 28.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 1.586666666666667\n",
      "  player_1: 1.0866666666666671\n",
      "  player_2: 0.32666666666666694\n",
      "policy_reward_min:\n",
      "  player_0: -50.33333333333333\n",
      "  player_1: -28.666666666666668\n",
      "  player_2: -36.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0856426353479224\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29365910082050645\n",
      "  mean_inference_ms: 1.5940124246817988\n",
      "  mean_raw_obs_processing_ms: 0.21021799865169072\n",
      "time_since_restore: 13933.554063796997\n",
      "time_this_iter_s: 14.837990522384644\n",
      "time_total_s: 13933.554063796997\n",
      "timers:\n",
      "  learn_throughput: 556.091\n",
      "  learn_time_ms: 14350.176\n",
      "  load_throughput: 951289.376\n",
      "  load_time_ms: 8.389\n",
      "  sample_throughput: 508.025\n",
      "  sample_time_ms: 15707.88\n",
      "  update_time_ms: 5.687\n",
      "timestamp: 1643550176\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6966540\n",
      "training_iteration: 873\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6982472\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-43-29\n",
      "done: false\n",
      "episode_len_mean: 115.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 72\n",
      "episodes_total: 43761\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4603045859436194\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016336949490286277\n",
      "        policy_loss: -0.07342323595813165\n",
      "        total_loss: 64.06186164061228\n",
      "        vf_explained_var: 0.28948667695124947\n",
      "        vf_loss: 64.12425744215648\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49266441504160563\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016400672626131912\n",
      "        policy_loss: -0.07032083277901013\n",
      "        total_loss: 53.079549504915875\n",
      "        vf_explained_var: 0.3378705458839734\n",
      "        vf_loss: 53.138799732526145\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44900026559829714\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016843893139975988\n",
      "        policy_loss: -0.09421219188409546\n",
      "        total_loss: 65.23621829032898\n",
      "        vf_explained_var: 0.3894343650341034\n",
      "        vf_loss: 65.31906048933665\n",
      "  num_agent_steps_sampled: 6982472\n",
      "  num_agent_steps_trained: 6982472\n",
      "  num_steps_sampled: 6982500\n",
      "  num_steps_trained: 6982500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 875\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.024999999999997\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.29999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 42.666666666666664\n",
      "  player_1: 33.66666666666667\n",
      "  player_2: 28.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.5733333333333324\n",
      "  player_1: -1.3666666666666663\n",
      "  player_2: 0.7933333333333336\n",
      "policy_reward_min:\n",
      "  player_0: -45.666666666666664\n",
      "  player_1: -27.666666666666664\n",
      "  player_2: -41.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08566275466279888\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2936452099220378\n",
      "  mean_inference_ms: 1.5928259717138349\n",
      "  mean_raw_obs_processing_ms: 0.21029762343975725\n",
      "time_since_restore: 13966.35250544548\n",
      "time_this_iter_s: 16.034990787506104\n",
      "time_total_s: 13966.35250544548\n",
      "timers:\n",
      "  learn_throughput: 544.797\n",
      "  learn_time_ms: 14647.666\n",
      "  load_throughput: 966954.387\n",
      "  load_time_ms: 8.253\n",
      "  sample_throughput: 508.114\n",
      "  sample_time_ms: 15705.147\n",
      "  update_time_ms: 5.664\n",
      "timestamp: 1643550209\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6982500\n",
      "training_iteration: 875\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 6998434\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-44-00\n",
      "done: false\n",
      "episode_len_mean: 112.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 43908\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46053260897596676\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01599765669368859\n",
      "        policy_loss: -0.0770023203579088\n",
      "        total_loss: 52.95960591634115\n",
      "        vf_explained_var: 0.31500450084606807\n",
      "        vf_loss: 53.02580998738607\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49916095366080604\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016881948577720324\n",
      "        policy_loss: -0.08830911004915834\n",
      "        total_loss: 51.34823967456818\n",
      "        vf_explained_var: 0.17931235323349634\n",
      "        vf_loss: 51.425153104464215\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46253603319327036\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016867060560101284\n",
      "        policy_loss: -0.06993032323506972\n",
      "        total_loss: 66.41687376022338\n",
      "        vf_explained_var: 0.11378579278786977\n",
      "        vf_loss: 66.47541898091634\n",
      "  num_agent_steps_sampled: 6998434\n",
      "  num_agent_steps_trained: 6998434\n",
      "  num_steps_sampled: 6998460\n",
      "  num_steps_trained: 6998460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 877\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.811111111111112\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.3\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.666666666666664\n",
      "  player_1: 34.0\n",
      "  player_2: 35.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.78\n",
      "  player_1: -0.15000000000000013\n",
      "  player_2: 0.3699999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -51.333333333333336\n",
      "  player_1: -30.666666666666664\n",
      "  player_2: -40.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08570248309098255\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29370119823283336\n",
      "  mean_inference_ms: 1.59354560710278\n",
      "  mean_raw_obs_processing_ms: 0.2103175249110106\n",
      "time_since_restore: 13996.670018434525\n",
      "time_this_iter_s: 15.037522077560425\n",
      "time_total_s: 13996.670018434525\n",
      "timers:\n",
      "  learn_throughput: 560.906\n",
      "  learn_time_ms: 14226.981\n",
      "  load_throughput: 1033003.281\n",
      "  load_time_ms: 7.725\n",
      "  sample_throughput: 509.174\n",
      "  sample_time_ms: 15672.439\n",
      "  update_time_ms: 5.727\n",
      "timestamp: 1643550240\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6998460\n",
      "training_iteration: 877\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7014390\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-44-32\n",
      "done: false\n",
      "episode_len_mean: 107.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 73\n",
      "episodes_total: 44053\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45805391391118366\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017258828058426542\n",
      "        policy_loss: -0.08105172201370199\n",
      "        total_loss: 43.072945823669436\n",
      "        vf_explained_var: 0.5174346268177032\n",
      "        vf_loss: 43.14234770933787\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49683860595027607\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01758317310490573\n",
      "        policy_loss: -0.11077855232171714\n",
      "        total_loss: 61.22839106400808\n",
      "        vf_explained_var: 0.2593289832274119\n",
      "        vf_loss: 61.327301003138224\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4597867950797081\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018234704215072572\n",
      "        policy_loss: -0.0599597600226601\n",
      "        total_loss: 50.16812029361725\n",
      "        vf_explained_var: 0.3416389926274617\n",
      "        vf_loss: 50.21577179908753\n",
      "  num_agent_steps_sampled: 7014390\n",
      "  num_agent_steps_trained: 7014390\n",
      "  num_steps_sampled: 7014420\n",
      "  num_steps_trained: 7014420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 879\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.409090909090907\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.35454545454544\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 39.0\n",
      "  player_1: 23.0\n",
      "  player_2: 28.0\n",
      "policy_reward_mean:\n",
      "  player_0: 4.39\n",
      "  player_1: -3.2399999999999993\n",
      "  player_2: 1.8499999999999996\n",
      "policy_reward_min:\n",
      "  player_0: -46.0\n",
      "  player_1: -49.0\n",
      "  player_2: -31.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08562379937562406\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29340853144173384\n",
      "  mean_inference_ms: 1.591619342688226\n",
      "  mean_raw_obs_processing_ms: 0.21031423493319018\n",
      "time_since_restore: 14029.472111940384\n",
      "time_this_iter_s: 17.665500164031982\n",
      "time_total_s: 14029.472111940384\n",
      "timers:\n",
      "  learn_throughput: 553.58\n",
      "  learn_time_ms: 14415.249\n",
      "  load_throughput: 1072306.492\n",
      "  load_time_ms: 7.442\n",
      "  sample_throughput: 516.496\n",
      "  sample_time_ms: 15450.266\n",
      "  update_time_ms: 5.581\n",
      "timestamp: 1643550272\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7014420\n",
      "training_iteration: 879\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7030350\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-45-05\n",
      "done: false\n",
      "episode_len_mean: 115.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 69\n",
      "episodes_total: 44190\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4669899080693722\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016235342700892944\n",
      "        policy_loss: -0.06950542623798052\n",
      "        total_loss: 75.47364666779836\n",
      "        vf_explained_var: 0.3577338362733523\n",
      "        vf_loss: 75.53219293276469\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49338552668690683\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01713831009132453\n",
      "        policy_loss: -0.06689812510274351\n",
      "        total_loss: 46.51354015350342\n",
      "        vf_explained_var: 0.37342727204163867\n",
      "        vf_loss: 46.568869861761726\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4575683399538199\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01732136216679161\n",
      "        policy_loss: -0.10215467545514305\n",
      "        total_loss: 52.95250119050344\n",
      "        vf_explained_var: 0.31780062228441236\n",
      "        vf_loss: 53.04296406428019\n",
      "  num_agent_steps_sampled: 7030350\n",
      "  num_agent_steps_trained: 7030350\n",
      "  num_steps_sampled: 7030380\n",
      "  num_steps_trained: 7030380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 881\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.677777777777777\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.3\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.0\n",
      "  player_1: 33.0\n",
      "  player_2: 25.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 1.7666666666666664\n",
      "  player_1: -1.603333333333334\n",
      "  player_2: 2.8366666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -43.666666666666664\n",
      "  player_1: -43.0\n",
      "  player_2: -28.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08573827932777184\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29375730719185195\n",
      "  mean_inference_ms: 1.594379257455991\n",
      "  mean_raw_obs_processing_ms: 0.2102285991513403\n",
      "time_since_restore: 14062.381787538528\n",
      "time_this_iter_s: 14.905242919921875\n",
      "time_total_s: 14062.381787538528\n",
      "timers:\n",
      "  learn_throughput: 544.543\n",
      "  learn_time_ms: 14654.484\n",
      "  load_throughput: 1081694.42\n",
      "  load_time_ms: 7.377\n",
      "  sample_throughput: 501.979\n",
      "  sample_time_ms: 15897.091\n",
      "  update_time_ms: 5.547\n",
      "timestamp: 1643550305\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7030380\n",
      "training_iteration: 881\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7046310\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-45-36\n",
      "done: false\n",
      "episode_len_mean: 115.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 67\n",
      "episodes_total: 44330\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46106205696860947\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016479007229589417\n",
      "        policy_loss: -0.051086319863485795\n",
      "        total_loss: 61.47324204126994\n",
      "        vf_explained_var: 0.2193048295378685\n",
      "        vf_loss: 61.513204927444455\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4784102129439513\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016752484798873108\n",
      "        policy_loss: -0.07611296822006504\n",
      "        total_loss: 52.69913856347402\n",
      "        vf_explained_var: 0.12211862554152807\n",
      "        vf_loss: 52.76394345124562\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4554387521247069\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01638367335407679\n",
      "        policy_loss: -0.1123210756185775\n",
      "        total_loss: 45.59948907693227\n",
      "        vf_explained_var: 0.09974603960911432\n",
      "        vf_loss: 45.7007511138916\n",
      "  num_agent_steps_sampled: 7046310\n",
      "  num_agent_steps_trained: 7046310\n",
      "  num_steps_sampled: 7046340\n",
      "  num_steps_trained: 7046340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 883\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.136842105263156\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.34736842105262\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.666666666666664\n",
      "  player_1: 27.333333333333332\n",
      "  player_2: 41.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 1.8766666666666663\n",
      "  player_1: -1.0133333333333336\n",
      "  player_2: 2.1366666666666663\n",
      "policy_reward_min:\n",
      "  player_0: -32.666666666666664\n",
      "  player_1: -40.66666666666667\n",
      "  player_2: -34.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08569155949651056\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2933834664681173\n",
      "  mean_inference_ms: 1.5919520425936622\n",
      "  mean_raw_obs_processing_ms: 0.21031779619235288\n",
      "time_since_restore: 14092.528541088104\n",
      "time_this_iter_s: 15.05079197883606\n",
      "time_total_s: 14092.528541088104\n",
      "timers:\n",
      "  learn_throughput: 542.731\n",
      "  learn_time_ms: 14703.407\n",
      "  load_throughput: 1084308.588\n",
      "  load_time_ms: 7.36\n",
      "  sample_throughput: 501.78\n",
      "  sample_time_ms: 15903.374\n",
      "  update_time_ms: 5.712\n",
      "timestamp: 1643550336\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7046340\n",
      "training_iteration: 883\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7062271\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-46-09\n",
      "done: false\n",
      "episode_len_mean: 111.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 73\n",
      "episodes_total: 44478\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47134139612317083\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016889588861001056\n",
      "        policy_loss: -0.07886338774114847\n",
      "        total_loss: 55.802263350486754\n",
      "        vf_explained_var: 0.36351429065068563\n",
      "        vf_loss: 55.86972616672516\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4772088402012984\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01723150988571206\n",
      "        policy_loss: -0.10953457252743344\n",
      "        total_loss: 62.12642954508463\n",
      "        vf_explained_var: 0.36160216212272644\n",
      "        vf_loss: 62.22433300018311\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4704944939911366\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01731836408985089\n",
      "        policy_loss: -0.04784299307502806\n",
      "        total_loss: 59.19203837076823\n",
      "        vf_explained_var: 0.24024626562992732\n",
      "        vf_loss: 59.22819145361582\n",
      "  num_agent_steps_sampled: 7062271\n",
      "  num_agent_steps_trained: 7062271\n",
      "  num_steps_sampled: 7062300\n",
      "  num_steps_trained: 7062300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 885\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.025\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.33\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.666666666666664\n",
      "  player_1: 35.333333333333336\n",
      "  player_2: 39.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.396666666666667\n",
      "  player_1: -0.3233333333333333\n",
      "  player_2: 0.9266666666666669\n",
      "policy_reward_min:\n",
      "  player_0: -27.666666666666668\n",
      "  player_1: -34.333333333333336\n",
      "  player_2: -43.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08555095141971902\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2935359128189761\n",
      "  mean_inference_ms: 1.5926252071340787\n",
      "  mean_raw_obs_processing_ms: 0.21023443287941654\n",
      "time_since_restore: 14125.495242357254\n",
      "time_this_iter_s: 16.59933066368103\n",
      "time_total_s: 14125.495242357254\n",
      "timers:\n",
      "  learn_throughput: 542.082\n",
      "  learn_time_ms: 14721.017\n",
      "  load_throughput: 1078494.769\n",
      "  load_time_ms: 7.399\n",
      "  sample_throughput: 502.413\n",
      "  sample_time_ms: 15883.357\n",
      "  update_time_ms: 5.733\n",
      "timestamp: 1643550369\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7062300\n",
      "training_iteration: 885\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7078231\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-46-40\n",
      "done: false\n",
      "episode_len_mean: 112.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 72\n",
      "episodes_total: 44618\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46695169548193616\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01636941144049634\n",
      "        policy_loss: -0.0797768430178985\n",
      "        total_loss: 72.22491216659546\n",
      "        vf_explained_var: 0.2626693042119344\n",
      "        vf_loss: 72.29363948345184\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47399776404102645\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018756759077656397\n",
      "        policy_loss: -0.08513824622146786\n",
      "        total_loss: 57.05395345687866\n",
      "        vf_explained_var: 0.39612061679363253\n",
      "        vf_loss: 57.12643057982127\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46436025351285937\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018744898821583054\n",
      "        policy_loss: -0.0788829126333197\n",
      "        total_loss: 53.81147562185923\n",
      "        vf_explained_var: 0.46383054872353874\n",
      "        vf_loss: 53.8777058839798\n",
      "  num_agent_steps_sampled: 7078231\n",
      "  num_agent_steps_trained: 7078231\n",
      "  num_steps_sampled: 7078260\n",
      "  num_steps_trained: 7078260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 887\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.428571428571429\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.37142857142855\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.0\n",
      "  player_1: 34.0\n",
      "  player_2: 27.666666666666668\n",
      "policy_reward_mean:\n",
      "  player_0: 3.81\n",
      "  player_1: 0.05000000000000014\n",
      "  player_2: -0.8599999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -38.333333333333336\n",
      "  player_1: -43.333333333333336\n",
      "  player_2: -41.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08565381308093997\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29324903275420294\n",
      "  mean_inference_ms: 1.5911171953922423\n",
      "  mean_raw_obs_processing_ms: 0.21019605712860515\n",
      "time_since_restore: 14157.091126441956\n",
      "time_this_iter_s: 16.68863868713379\n",
      "time_total_s: 14157.091126441956\n",
      "timers:\n",
      "  learn_throughput: 537.476\n",
      "  learn_time_ms: 14847.162\n",
      "  load_throughput: 1066175.24\n",
      "  load_time_ms: 7.485\n",
      "  sample_throughput: 501.799\n",
      "  sample_time_ms: 15902.786\n",
      "  update_time_ms: 5.597\n",
      "timestamp: 1643550400\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7078260\n",
      "training_iteration: 887\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7094190\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-47-11\n",
      "done: false\n",
      "episode_len_mean: 108.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 76\n",
      "episodes_total: 44769\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.472099928210179\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0167684479055589\n",
      "        policy_loss: -0.10398319787501047\n",
      "        total_loss: 54.0421512667338\n",
      "        vf_explained_var: 0.44043872624635694\n",
      "        vf_loss: 54.13481577396393\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48222749690214795\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016383282369559236\n",
      "        policy_loss: -0.07086706616605322\n",
      "        total_loss: 73.31063565889994\n",
      "        vf_explained_var: 0.21080329030752182\n",
      "        vf_loss: 73.37044390837352\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46975083847840626\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018000248163501736\n",
      "        policy_loss: -0.06338214117412766\n",
      "        total_loss: 68.43225597381591\n",
      "        vf_explained_var: 0.48291234562794366\n",
      "        vf_loss: 68.48348785082499\n",
      "  num_agent_steps_sampled: 7094190\n",
      "  num_agent_steps_trained: 7094190\n",
      "  num_steps_sampled: 7094220\n",
      "  num_steps_trained: 7094220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 889\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.215\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.35999999999999\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.666666666666664\n",
      "  player_1: 38.333333333333336\n",
      "  player_2: 37.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 3.2000000000000006\n",
      "  player_1: 2.03\n",
      "  player_2: -2.23\n",
      "policy_reward_min:\n",
      "  player_0: -40.666666666666664\n",
      "  player_1: -44.333333333333336\n",
      "  player_2: -55.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08565912347644612\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29385204710315915\n",
      "  mean_inference_ms: 1.5938238657560937\n",
      "  mean_raw_obs_processing_ms: 0.21042650004542757\n",
      "time_since_restore: 14188.141667604446\n",
      "time_this_iter_s: 16.226908922195435\n",
      "time_total_s: 14188.141667604446\n",
      "timers:\n",
      "  learn_throughput: 543.757\n",
      "  learn_time_ms: 14675.675\n",
      "  load_throughput: 1089131.83\n",
      "  load_time_ms: 7.327\n",
      "  sample_throughput: 497.633\n",
      "  sample_time_ms: 16035.923\n",
      "  update_time_ms: 5.696\n",
      "timestamp: 1643550431\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7094220\n",
      "training_iteration: 889\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7110151\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-47-43\n",
      "done: false\n",
      "episode_len_mean: 109.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 69\n",
      "episodes_total: 44910\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4607760556042194\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01665880725836208\n",
      "        policy_loss: -0.07612944278866053\n",
      "        total_loss: 48.62221570650736\n",
      "        vf_explained_var: 0.3115109747648239\n",
      "        vf_loss: 48.68710060278575\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47208029637734095\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01620767754086008\n",
      "        policy_loss: -0.09459569384033481\n",
      "        total_loss: 66.86060171763103\n",
      "        vf_explained_var: 0.259954441289107\n",
      "        vf_loss: 66.94425731817881\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45831299071510634\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016624305287271757\n",
      "        policy_loss: -0.06259223573996375\n",
      "        total_loss: 56.19443906784058\n",
      "        vf_explained_var: 0.38309248934189477\n",
      "        vf_loss: 56.245810084342956\n",
      "  num_agent_steps_sampled: 7110151\n",
      "  num_agent_steps_trained: 7110151\n",
      "  num_steps_sampled: 7110180\n",
      "  num_steps_trained: 7110180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 891\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.13684210526316\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.33157894736841\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 53.0\n",
      "  player_1: 34.0\n",
      "  player_2: 33.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.0466666666666673\n",
      "  player_1: -0.9733333333333333\n",
      "  player_2: 1.9266666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -47.333333333333336\n",
      "  player_1: -43.0\n",
      "  player_2: -51.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08558097321819773\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2931152100213535\n",
      "  mean_inference_ms: 1.5912061139661762\n",
      "  mean_raw_obs_processing_ms: 0.21023784766830034\n",
      "time_since_restore: 14220.139166593552\n",
      "time_this_iter_s: 15.981176853179932\n",
      "time_total_s: 14220.139166593552\n",
      "timers:\n",
      "  learn_throughput: 547.067\n",
      "  learn_time_ms: 14586.872\n",
      "  load_throughput: 1089521.814\n",
      "  load_time_ms: 7.324\n",
      "  sample_throughput: 508.451\n",
      "  sample_time_ms: 15694.722\n",
      "  update_time_ms: 5.701\n",
      "timestamp: 1643550463\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7110180\n",
      "training_iteration: 891\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7126110\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-48-14\n",
      "done: false\n",
      "episode_len_mean: 103.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 45054\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4673858430981636\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015398557615529853\n",
      "        policy_loss: -0.08467758151392142\n",
      "        total_loss: 73.13510529041291\n",
      "        vf_explained_var: 0.4296882850925128\n",
      "        vf_loss: 73.20938883781433\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4876463813583056\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015488683510187874\n",
      "        policy_loss: -0.08091395451997717\n",
      "        total_loss: 86.98086228052775\n",
      "        vf_explained_var: 0.3018305899699529\n",
      "        vf_loss: 87.05132137616475\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45892968490719793\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016818905271120458\n",
      "        policy_loss: -0.06302943132817745\n",
      "        total_loss: 47.378842198053995\n",
      "        vf_explained_var: 0.38320110420385994\n",
      "        vf_loss: 47.430518889427184\n",
      "  num_agent_steps_sampled: 7126110\n",
      "  num_agent_steps_trained: 7126110\n",
      "  num_steps_sampled: 7126140\n",
      "  num_steps_trained: 7126140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 893\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.13684210526316\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.34736842105262\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 44.0\n",
      "  player_1: 32.666666666666664\n",
      "  player_2: 34.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 1.586666666666667\n",
      "  player_1: -0.7533333333333337\n",
      "  player_2: 2.1666666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -39.0\n",
      "  player_1: -65.0\n",
      "  player_2: -25.666666666666668\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08561919207825955\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2931754517726025\n",
      "  mean_inference_ms: 1.5912809185692205\n",
      "  mean_raw_obs_processing_ms: 0.21025480740361707\n",
      "time_since_restore: 14251.086691379547\n",
      "time_this_iter_s: 15.13109803199768\n",
      "time_total_s: 14251.086691379547\n",
      "timers:\n",
      "  learn_throughput: 544.123\n",
      "  learn_time_ms: 14665.811\n",
      "  load_throughput: 1020950.836\n",
      "  load_time_ms: 7.816\n",
      "  sample_throughput: 502.661\n",
      "  sample_time_ms: 15875.501\n",
      "  update_time_ms: 5.635\n",
      "timestamp: 1643550494\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7126140\n",
      "training_iteration: 893\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7142070\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-48-45\n",
      "done: false\n",
      "episode_len_mean: 107.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 73\n",
      "episodes_total: 45199\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.471164530167977\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016424559494336865\n",
      "        policy_loss: -0.07795482530569037\n",
      "        total_loss: 77.6481774409612\n",
      "        vf_explained_var: 0.11812944829463959\n",
      "        vf_loss: 77.71504619280498\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46632246047258374\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01668760154852464\n",
      "        policy_loss: -0.08970054640745123\n",
      "        total_loss: 85.47307768980662\n",
      "        vf_explained_var: 0.2678321040670077\n",
      "        vf_loss: 85.55151452541351\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46728319853544237\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01658444052791045\n",
      "        policy_loss: -0.08066540659715732\n",
      "        total_loss: 71.79619369029999\n",
      "        vf_explained_var: 0.25824956595897675\n",
      "        vf_loss: 71.86566475391388\n",
      "  num_agent_steps_sampled: 7142070\n",
      "  num_agent_steps_trained: 7142070\n",
      "  num_steps_sampled: 7142100\n",
      "  num_steps_trained: 7142100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 895\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.29473684210526\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.35789473684208\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.0\n",
      "  player_1: 56.66666666666667\n",
      "  player_2: 42.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 0.19666666666666655\n",
      "  player_1: 0.7666666666666664\n",
      "  player_2: 2.0366666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -70.33333333333333\n",
      "  player_1: -46.666666666666664\n",
      "  player_2: -48.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08560996865137561\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2935350134712849\n",
      "  mean_inference_ms: 1.5937884991013425\n",
      "  mean_raw_obs_processing_ms: 0.21024708081607701\n",
      "time_since_restore: 14281.672731637955\n",
      "time_this_iter_s: 15.469589948654175\n",
      "time_total_s: 14281.672731637955\n",
      "timers:\n",
      "  learn_throughput: 553.197\n",
      "  learn_time_ms: 14425.237\n",
      "  load_throughput: 971718.317\n",
      "  load_time_ms: 8.212\n",
      "  sample_throughput: 506.354\n",
      "  sample_time_ms: 15759.731\n",
      "  update_time_ms: 5.627\n",
      "timestamp: 1643550525\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7142100\n",
      "training_iteration: 895\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7158030\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-49-17\n",
      "done: false\n",
      "episode_len_mean: 109.42\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 70\n",
      "episodes_total: 45343\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4597869474689166\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014920079221859244\n",
      "        policy_loss: -0.07599786911470195\n",
      "        total_loss: 62.97214408238729\n",
      "        vf_explained_var: 0.3129051179687182\n",
      "        vf_loss: 63.038070713679\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47792454168200493\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016798978945273196\n",
      "        policy_loss: -0.08212343554322918\n",
      "        total_loss: 78.85547854264577\n",
      "        vf_explained_var: 0.20552097171545028\n",
      "        vf_loss: 78.9262630367279\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45721110100547474\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016664543486808535\n",
      "        policy_loss: -0.05967037238801519\n",
      "        total_loss: 60.7847282187144\n",
      "        vf_explained_var: 0.4237438373764356\n",
      "        vf_loss: 60.83315031369527\n",
      "  num_agent_steps_sampled: 7158030\n",
      "  num_agent_steps_trained: 7158030\n",
      "  num_steps_sampled: 7158060\n",
      "  num_steps_trained: 7158060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 897\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.689473684210522\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.34736842105262\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.0\n",
      "  player_1: 30.333333333333336\n",
      "  player_2: 30.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.10666666666666692\n",
      "  player_1: 0.6866666666666669\n",
      "  player_2: 2.206666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -42.0\n",
      "  player_1: -47.33333333333333\n",
      "  player_2: -34.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08562243732965111\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2933472318755315\n",
      "  mean_inference_ms: 1.5921194802272611\n",
      "  mean_raw_obs_processing_ms: 0.2102137055441434\n",
      "time_since_restore: 14313.255890130997\n",
      "time_this_iter_s: 15.509934186935425\n",
      "time_total_s: 14313.255890130997\n",
      "timers:\n",
      "  learn_throughput: 553.397\n",
      "  learn_time_ms: 14420.024\n",
      "  load_throughput: 783980.22\n",
      "  load_time_ms: 10.179\n",
      "  sample_throughput: 506.303\n",
      "  sample_time_ms: 15761.297\n",
      "  update_time_ms: 5.69\n",
      "timestamp: 1643550557\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7158060\n",
      "training_iteration: 897\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7173995\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-49-48\n",
      "done: false\n",
      "episode_len_mean: 108.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 75\n",
      "episodes_total: 45493\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4701804149647554\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017324743245383162\n",
      "        policy_loss: -0.06308472097851336\n",
      "        total_loss: 54.8910808022817\n",
      "        vf_explained_var: 0.32563676049311957\n",
      "        vf_loss: 54.94247125784556\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4889424130817254\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01738135869180856\n",
      "        policy_loss: -0.06116916848967473\n",
      "        total_loss: 65.49015459060669\n",
      "        vf_explained_var: 0.12951991339524588\n",
      "        vf_loss: 65.53959178288778\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4562972641487916\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016630385812783668\n",
      "        policy_loss: -0.10958908301467697\n",
      "        total_loss: 63.26926779270172\n",
      "        vf_explained_var: 0.18664498805999755\n",
      "        vf_loss: 63.367631258964536\n",
      "  num_agent_steps_sampled: 7173995\n",
      "  num_agent_steps_trained: 7173995\n",
      "  num_steps_sampled: 7174020\n",
      "  num_steps_trained: 7174020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 899\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.533333333333331\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.34999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 41.0\n",
      "  player_2: 28.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.606666666666667\n",
      "  player_1: -1.0933333333333333\n",
      "  player_2: 1.4866666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -43.0\n",
      "  player_1: -47.666666666666664\n",
      "  player_2: -50.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08564931073738827\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29314822164753773\n",
      "  mean_inference_ms: 1.5905558512620408\n",
      "  mean_raw_obs_processing_ms: 0.2102226526425894\n",
      "time_since_restore: 14344.437084197998\n",
      "time_this_iter_s: 15.180804252624512\n",
      "time_total_s: 14344.437084197998\n",
      "timers:\n",
      "  learn_throughput: 552.983\n",
      "  learn_time_ms: 14430.819\n",
      "  load_throughput: 776018.834\n",
      "  load_time_ms: 10.283\n",
      "  sample_throughput: 506.34\n",
      "  sample_time_ms: 15760.15\n",
      "  update_time_ms: 5.771\n",
      "timestamp: 1643550588\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7174020\n",
      "training_iteration: 899\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7189952\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-50-22\n",
      "done: false\n",
      "episode_len_mean: 112.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 69\n",
      "episodes_total: 45634\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46289690693219504\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015431269529320085\n",
      "        policy_loss: -0.07492057934403419\n",
      "        total_loss: 72.73965034802755\n",
      "        vf_explained_var: 0.3150267239411672\n",
      "        vf_loss: 72.80415479660034\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47223978633681934\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01758046526161621\n",
      "        policy_loss: -0.08749310579771796\n",
      "        total_loss: 63.3495359992981\n",
      "        vf_explained_var: 0.25285542955001195\n",
      "        vf_loss: 63.425162359873454\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46016842444737754\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016002719316575546\n",
      "        policy_loss: -0.0683274490882953\n",
      "        total_loss: 51.64450157483419\n",
      "        vf_explained_var: 0.21765257308880487\n",
      "        vf_loss: 51.70202741305033\n",
      "  num_agent_steps_sampled: 7189952\n",
      "  num_agent_steps_trained: 7189952\n",
      "  num_steps_sampled: 7189980\n",
      "  num_steps_trained: 7189980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 901\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.05\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.34999999999998\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.666666666666664\n",
      "  player_1: 38.66666666666667\n",
      "  player_2: 40.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 0.15000000000000002\n",
      "  player_1: 0.23999999999999974\n",
      "  player_2: 2.61\n",
      "policy_reward_min:\n",
      "  player_0: -60.33333333333333\n",
      "  player_1: -52.666666666666664\n",
      "  player_2: -35.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08564059881256769\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2930493158876379\n",
      "  mean_inference_ms: 1.590421866396252\n",
      "  mean_raw_obs_processing_ms: 0.2102952126458299\n",
      "time_since_restore: 14378.309490919113\n",
      "time_this_iter_s: 16.290629386901855\n",
      "time_total_s: 14378.309490919113\n",
      "timers:\n",
      "  learn_throughput: 546.034\n",
      "  learn_time_ms: 14614.465\n",
      "  load_throughput: 767192.696\n",
      "  load_time_ms: 10.402\n",
      "  sample_throughput: 504.666\n",
      "  sample_time_ms: 15812.423\n",
      "  update_time_ms: 5.946\n",
      "timestamp: 1643550622\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7189980\n",
      "training_iteration: 901\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7205910\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-50-56\n",
      "done: false\n",
      "episode_len_mean: 104.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 76\n",
      "episodes_total: 45783\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47141767139236135\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01638102329305184\n",
      "        policy_loss: -0.08268249880212049\n",
      "        total_loss: 77.33432437578837\n",
      "        vf_explained_var: 0.44662745287021\n",
      "        vf_loss: 77.4059492746989\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4930574674407641\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016083768123970117\n",
      "        policy_loss: -0.0677109219490861\n",
      "        total_loss: 65.35221903642018\n",
      "        vf_explained_var: 0.4401091742515564\n",
      "        vf_loss: 65.40907332579295\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4683518500626087\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01599484801100554\n",
      "        policy_loss: -0.08292591190586487\n",
      "        total_loss: 80.36916982650757\n",
      "        vf_explained_var: 0.27994103958209354\n",
      "        vf_loss: 80.44129876613617\n",
      "  num_agent_steps_sampled: 7205910\n",
      "  num_agent_steps_trained: 7205910\n",
      "  num_steps_sampled: 7205940\n",
      "  num_steps_trained: 7205940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 903\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.285\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.37000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.33333333333333\n",
      "  player_1: 39.33333333333333\n",
      "  player_2: 32.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.036666666666667\n",
      "  player_1: 1.0366666666666666\n",
      "  player_2: -1.0733333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -58.666666666666664\n",
      "  player_1: -40.0\n",
      "  player_2: -65.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08567895301161216\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2935230656193181\n",
      "  mean_inference_ms: 1.5924124869009382\n",
      "  mean_raw_obs_processing_ms: 0.2102578322096634\n",
      "time_since_restore: 14412.393513202667\n",
      "time_this_iter_s: 16.856218338012695\n",
      "time_total_s: 14412.393513202667\n",
      "timers:\n",
      "  learn_throughput: 535.651\n",
      "  learn_time_ms: 14897.769\n",
      "  load_throughput: 731735.782\n",
      "  load_time_ms: 10.906\n",
      "  sample_throughput: 499.056\n",
      "  sample_time_ms: 15990.177\n",
      "  update_time_ms: 6.08\n",
      "timestamp: 1643550656\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7205940\n",
      "training_iteration: 903\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7221870\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-51-31\n",
      "done: false\n",
      "episode_len_mean: 107.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 75\n",
      "episodes_total: 45932\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44051650539040565\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017023879302482024\n",
      "        policy_loss: -0.0771289819975694\n",
      "        total_loss: 54.387632576624554\n",
      "        vf_explained_var: 0.28644283473491666\n",
      "        vf_loss: 54.4532706006368\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47888475030660627\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016785410972266466\n",
      "        policy_loss: -0.09121513897242646\n",
      "        total_loss: 34.41560605208079\n",
      "        vf_explained_var: 0.3553120116392771\n",
      "        vf_loss: 34.4954909769694\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4794473018248876\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01736777902842884\n",
      "        policy_loss: -0.07049832039823135\n",
      "        total_loss: 59.690955006281534\n",
      "        vf_explained_var: 0.363380900323391\n",
      "        vf_loss: 59.749730275472004\n",
      "  num_agent_steps_sampled: 7221870\n",
      "  num_agent_steps_trained: 7221870\n",
      "  num_steps_sampled: 7221900\n",
      "  num_steps_trained: 7221900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 905\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.228571428571431\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.3142857142857\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.0\n",
      "  player_1: 26.0\n",
      "  player_2: 27.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 1.0766666666666669\n",
      "  player_1: 0.6566666666666668\n",
      "  player_2: 1.266666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -27.666666666666664\n",
      "  player_1: -30.0\n",
      "  player_2: -38.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08567952839677918\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29338339237353606\n",
      "  mean_inference_ms: 1.592181004411952\n",
      "  mean_raw_obs_processing_ms: 0.21031496735554026\n",
      "time_since_restore: 14447.800218820572\n",
      "time_this_iter_s: 16.90001893043518\n",
      "time_total_s: 14447.800218820572\n",
      "timers:\n",
      "  learn_throughput: 519.468\n",
      "  learn_time_ms: 15361.873\n",
      "  load_throughput: 701601.813\n",
      "  load_time_ms: 11.374\n",
      "  sample_throughput: 483.602\n",
      "  sample_time_ms: 16501.166\n",
      "  update_time_ms: 6.11\n",
      "timestamp: 1643550691\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7221900\n",
      "training_iteration: 905\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7237830\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-52-06\n",
      "done: false\n",
      "episode_len_mean: 119.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 66\n",
      "episodes_total: 46065\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46086014042298\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017018659269560126\n",
      "        policy_loss: -0.056024287833521765\n",
      "        total_loss: 56.657659760316214\n",
      "        vf_explained_var: 0.3880121986071269\n",
      "        vf_loss: 56.702196884950006\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4793515371779601\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016629708793859097\n",
      "        policy_loss: -0.10281573647012313\n",
      "        total_loss: 51.971182123819986\n",
      "        vf_explained_var: 0.26394098709026975\n",
      "        vf_loss: 52.06277249177297\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4553391728798548\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01585380159401666\n",
      "        policy_loss: -0.0825093681551516\n",
      "        total_loss: 45.45290275891622\n",
      "        vf_explained_var: 0.33471466283003487\n",
      "        vf_loss: 45.52471089363098\n",
      "  num_agent_steps_sampled: 7237830\n",
      "  num_agent_steps_trained: 7237830\n",
      "  num_steps_sampled: 7237860\n",
      "  num_steps_trained: 7237860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 907\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.436363636363636\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.33636363636361\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.333333333333336\n",
      "  player_1: 33.0\n",
      "  player_2: 30.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.443333333333334\n",
      "  player_1: -1.3166666666666667\n",
      "  player_2: 0.8733333333333337\n",
      "policy_reward_min:\n",
      "  player_0: -24.666666666666664\n",
      "  player_1: -34.666666666666664\n",
      "  player_2: -50.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08560974175262928\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2931080902630304\n",
      "  mean_inference_ms: 1.5922068653919002\n",
      "  mean_raw_obs_processing_ms: 0.21045566321225517\n",
      "time_since_restore: 14482.588184833527\n",
      "time_this_iter_s: 17.72223997116089\n",
      "time_total_s: 14482.588184833527\n",
      "timers:\n",
      "  learn_throughput: 509.465\n",
      "  learn_time_ms: 15663.501\n",
      "  load_throughput: 776542.757\n",
      "  load_time_ms: 10.276\n",
      "  sample_throughput: 476.25\n",
      "  sample_time_ms: 16755.903\n",
      "  update_time_ms: 6.295\n",
      "timestamp: 1643550726\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7237860\n",
      "training_iteration: 907\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7253790\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-52-41\n",
      "done: false\n",
      "episode_len_mean: 112.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 70\n",
      "episodes_total: 46207\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4804753973086675\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01717922102242786\n",
      "        policy_loss: -0.06905568241452177\n",
      "        total_loss: 50.57234426816304\n",
      "        vf_explained_var: 0.3040294575691223\n",
      "        vf_loss: 50.629803977012635\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48030953829487166\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016647063272694898\n",
      "        policy_loss: -0.10209981104514251\n",
      "        total_loss: 55.48083193937937\n",
      "        vf_explained_var: 0.20801583011945088\n",
      "        vf_loss: 55.57169521172841\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4618083302179972\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015867786805425037\n",
      "        policy_loss: -0.07017472131488224\n",
      "        total_loss: 49.4569525718689\n",
      "        vf_explained_var: 0.4624548128247261\n",
      "        vf_loss: 49.516416614850364\n",
      "  num_agent_steps_sampled: 7253790\n",
      "  num_agent_steps_trained: 7253790\n",
      "  num_steps_sampled: 7253820\n",
      "  num_steps_trained: 7253820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 909\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.980952380952377\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.33333333333333\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 23.333333333333336\n",
      "  player_1: 27.333333333333332\n",
      "  player_2: 28.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.28\n",
      "  player_1: 0.6399999999999999\n",
      "  player_2: 0.07999999999999993\n",
      "policy_reward_min:\n",
      "  player_0: -33.666666666666664\n",
      "  player_1: -35.333333333333336\n",
      "  player_2: -29.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08559090993778319\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29297126559469866\n",
      "  mean_inference_ms: 1.590839327060269\n",
      "  mean_raw_obs_processing_ms: 0.21029936660126666\n",
      "time_since_restore: 14517.282353878021\n",
      "time_this_iter_s: 16.869084358215332\n",
      "time_total_s: 14517.282353878021\n",
      "timers:\n",
      "  learn_throughput: 498.582\n",
      "  learn_time_ms: 16005.385\n",
      "  load_throughput: 784535.18\n",
      "  load_time_ms: 10.172\n",
      "  sample_throughput: 465.257\n",
      "  sample_time_ms: 17151.826\n",
      "  update_time_ms: 6.104\n",
      "timestamp: 1643550761\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7253820\n",
      "training_iteration: 909\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7269750\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-53-15\n",
      "done: false\n",
      "episode_len_mean: 109.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 74\n",
      "episodes_total: 46353\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46426205212871235\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016559349107013947\n",
      "        policy_loss: -0.0713131458995243\n",
      "        total_loss: 48.06309018611908\n",
      "        vf_explained_var: 0.2798486758271853\n",
      "        vf_loss: 48.12322570721308\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4731683521469434\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01622059854011847\n",
      "        policy_loss: -0.09883737465987603\n",
      "        total_loss: 65.08537307739257\n",
      "        vf_explained_var: 0.16252896934747696\n",
      "        vf_loss: 65.17326162656148\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4620037333170573\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016133246387330095\n",
      "        policy_loss: -0.0663030786610519\n",
      "        total_loss: 60.1549277305603\n",
      "        vf_explained_var: 0.32850321183602016\n",
      "        vf_loss: 60.21034095923106\n",
      "  num_agent_steps_sampled: 7269750\n",
      "  num_agent_steps_trained: 7269750\n",
      "  num_steps_sampled: 7269780\n",
      "  num_steps_trained: 7269780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 911\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.184999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.39500000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.666666666666664\n",
      "  player_1: 31.0\n",
      "  player_2: 23.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 4.6466666666666665\n",
      "  player_1: 0.33666666666666634\n",
      "  player_2: -1.9833333333333336\n",
      "policy_reward_min:\n",
      "  player_0: -31.0\n",
      "  player_1: -47.333333333333336\n",
      "  player_2: -49.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0856857491320266\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2933330349568148\n",
      "  mean_inference_ms: 1.5919836840980628\n",
      "  mean_raw_obs_processing_ms: 0.21043669129548156\n",
      "time_since_restore: 14551.524636983871\n",
      "time_this_iter_s: 16.30982780456543\n",
      "time_total_s: 14551.524636983871\n",
      "timers:\n",
      "  learn_throughput: 498.008\n",
      "  learn_time_ms: 16023.841\n",
      "  load_throughput: 783224.387\n",
      "  load_time_ms: 10.189\n",
      "  sample_throughput: 459.475\n",
      "  sample_time_ms: 17367.649\n",
      "  update_time_ms: 5.913\n",
      "timestamp: 1643550795\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7269780\n",
      "training_iteration: 911\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7285710\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-53-46\n",
      "done: false\n",
      "episode_len_mean: 109.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 76\n",
      "episodes_total: 46499\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4622534449895223\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016028061764378664\n",
      "        policy_loss: -0.0719237512908876\n",
      "        total_loss: 69.40699443340301\n",
      "        vf_explained_var: 0.415147091448307\n",
      "        vf_loss: 69.46809949080149\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47322644288341204\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01670931457546999\n",
      "        policy_loss: -0.08343155054065088\n",
      "        total_loss: 75.26695425033569\n",
      "        vf_explained_var: 0.3699426760276159\n",
      "        vf_loss: 75.33910689353942\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47275023311376574\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016240360098696935\n",
      "        policy_loss: -0.07000852497604985\n",
      "        total_loss: 78.78418163299561\n",
      "        vf_explained_var: 0.28332123935222625\n",
      "        vf_loss: 78.84322813510894\n",
      "  num_agent_steps_sampled: 7285710\n",
      "  num_agent_steps_trained: 7285710\n",
      "  num_steps_sampled: 7285740\n",
      "  num_steps_trained: 7285740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 913\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.14736842105263\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.36315789473683\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.0\n",
      "  player_1: 38.666666666666664\n",
      "  player_2: 26.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.4033333333333333\n",
      "  player_1: 2.0733333333333333\n",
      "  player_2: -2.476666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -33.333333333333336\n",
      "  player_1: -29.666666666666664\n",
      "  player_2: -52.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08563532088059617\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2931860997725234\n",
      "  mean_inference_ms: 1.591478261520902\n",
      "  mean_raw_obs_processing_ms: 0.2103547630467092\n",
      "time_since_restore: 14582.39945268631\n",
      "time_this_iter_s: 15.944085121154785\n",
      "time_total_s: 14582.39945268631\n",
      "timers:\n",
      "  learn_throughput: 507.159\n",
      "  learn_time_ms: 15734.698\n",
      "  load_throughput: 864568.16\n",
      "  load_time_ms: 9.23\n",
      "  sample_throughput: 466.128\n",
      "  sample_time_ms: 17119.77\n",
      "  update_time_ms: 5.755\n",
      "timestamp: 1643550826\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7285740\n",
      "training_iteration: 913\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7301671\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-54-20\n",
      "done: false\n",
      "episode_len_mean: 112.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 46642\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46721106265981993\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016583215441694013\n",
      "        policy_loss: -0.0831354080947737\n",
      "        total_loss: 50.54131495157878\n",
      "        vf_explained_var: 0.3025587472319603\n",
      "        vf_loss: 50.613256794611615\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4813871336479982\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016653156355713843\n",
      "        policy_loss: -0.1044317932954679\n",
      "        total_loss: 75.70258391857148\n",
      "        vf_explained_var: 0.26318636576334636\n",
      "        vf_loss: 75.79577481587728\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4686677167812983\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015705734520965395\n",
      "        policy_loss: -0.0561382052457581\n",
      "        total_loss: 64.20883258342742\n",
      "        vf_explained_var: 0.34719027797381086\n",
      "        vf_loss: 64.25436956246693\n",
      "  num_agent_steps_sampled: 7301671\n",
      "  num_agent_steps_trained: 7301671\n",
      "  num_steps_sampled: 7301700\n",
      "  num_steps_trained: 7301700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 915\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.570000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.40500000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 56.333333333333336\n",
      "  player_1: 33.333333333333336\n",
      "  player_2: 33.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.3933333333333335\n",
      "  player_1: 1.5533333333333337\n",
      "  player_2: -0.9466666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -25.666666666666664\n",
      "  player_1: -43.0\n",
      "  player_2: -65.66666666666666\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08558701442741018\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2929472080592684\n",
      "  mean_inference_ms: 1.5904850963411787\n",
      "  mean_raw_obs_processing_ms: 0.21030847931748553\n",
      "time_since_restore: 14615.67788028717\n",
      "time_this_iter_s: 15.929631233215332\n",
      "time_total_s: 14615.67788028717\n",
      "timers:\n",
      "  learn_throughput: 513.567\n",
      "  learn_time_ms: 15538.381\n",
      "  load_throughput: 919679.668\n",
      "  load_time_ms: 8.677\n",
      "  sample_throughput: 471.783\n",
      "  sample_time_ms: 16914.551\n",
      "  update_time_ms: 5.893\n",
      "timestamp: 1643550860\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7301700\n",
      "training_iteration: 915\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7317634\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-54-50\n",
      "done: false\n",
      "episode_len_mean: 112.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 72\n",
      "episodes_total: 46785\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4642299883564313\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015691394937027023\n",
      "        policy_loss: -0.03599328065911929\n",
      "        total_loss: 73.49703616937002\n",
      "        vf_explained_var: 0.2706424574057261\n",
      "        vf_loss: 73.52243764400482\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4739432525634766\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01707238307443428\n",
      "        policy_loss: -0.11239186627479891\n",
      "        total_loss: 54.686398862202964\n",
      "        vf_explained_var: 0.3027199797828992\n",
      "        vf_loss: 54.78726687749227\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4869039054214954\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017392699048683652\n",
      "        policy_loss: -0.07662598546594382\n",
      "        total_loss: 49.08122070630392\n",
      "        vf_explained_var: 0.38181529452403384\n",
      "        vf_loss: 49.14610695521037\n",
      "  num_agent_steps_sampled: 7317634\n",
      "  num_agent_steps_trained: 7317634\n",
      "  num_steps_sampled: 7317660\n",
      "  num_steps_trained: 7317660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 917\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.06842105263158\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.43684210526317\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.0\n",
      "  player_1: 36.666666666666664\n",
      "  player_2: 31.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.2800000000000002\n",
      "  player_1: 1.24\n",
      "  player_2: 1.48\n",
      "policy_reward_min:\n",
      "  player_0: -42.0\n",
      "  player_1: -26.666666666666664\n",
      "  player_2: -42.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08564125497273153\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2933573041342265\n",
      "  mean_inference_ms: 1.592262546425907\n",
      "  mean_raw_obs_processing_ms: 0.21041490974177754\n",
      "time_since_restore: 14646.285178422928\n",
      "time_this_iter_s: 15.306215047836304\n",
      "time_total_s: 14646.285178422928\n",
      "timers:\n",
      "  learn_throughput: 527.084\n",
      "  learn_time_ms: 15139.893\n",
      "  load_throughput: 1024525.959\n",
      "  load_time_ms: 7.789\n",
      "  sample_throughput: 479.779\n",
      "  sample_time_ms: 16632.646\n",
      "  update_time_ms: 5.658\n",
      "timestamp: 1643550890\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7317660\n",
      "training_iteration: 917\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7333591\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-55-22\n",
      "done: false\n",
      "episode_len_mean: 109.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 76\n",
      "episodes_total: 46931\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4508083738883336\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015480673143674722\n",
      "        policy_loss: -0.09090123135441293\n",
      "        total_loss: 72.07130773067475\n",
      "        vf_explained_var: 0.42848704010248184\n",
      "        vf_loss: 72.15175954341889\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.474215869307518\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017537972498451116\n",
      "        policy_loss: -0.06694125128599504\n",
      "        total_loss: 63.584019632339476\n",
      "        vf_explained_var: 0.30126291076342265\n",
      "        vf_loss: 63.63912284056346\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48368101398150126\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01699740474346072\n",
      "        policy_loss: -0.0897695261798799\n",
      "        total_loss: 72.37503215471904\n",
      "        vf_explained_var: 0.43540351808071137\n",
      "        vf_loss: 72.45332848230997\n",
      "  num_agent_steps_sampled: 7333591\n",
      "  num_agent_steps_trained: 7333591\n",
      "  num_steps_sampled: 7333620\n",
      "  num_steps_trained: 7333620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 919\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.772222222222224\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.41111111111111\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.0\n",
      "  player_1: 30.333333333333336\n",
      "  player_2: 26.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.26\n",
      "  player_1: 0.75\n",
      "  player_2: -0.009999999999999893\n",
      "policy_reward_min:\n",
      "  player_0: -40.333333333333336\n",
      "  player_1: -27.333333333333336\n",
      "  player_2: -41.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08561789566813306\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2932914814773518\n",
      "  mean_inference_ms: 1.5917968704307361\n",
      "  mean_raw_obs_processing_ms: 0.2103437259650542\n",
      "time_since_restore: 14677.905175924301\n",
      "time_this_iter_s: 14.950058937072754\n",
      "time_total_s: 14677.905175924301\n",
      "timers:\n",
      "  learn_throughput: 537.7\n",
      "  learn_time_ms: 14840.989\n",
      "  load_throughput: 1021165.761\n",
      "  load_time_ms: 7.815\n",
      "  sample_throughput: 490.064\n",
      "  sample_time_ms: 16283.598\n",
      "  update_time_ms: 5.554\n",
      "timestamp: 1643550922\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7333620\n",
      "training_iteration: 919\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7349550\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-55-55\n",
      "done: false\n",
      "episode_len_mean: 111.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 72\n",
      "episodes_total: 47072\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4489875248571237\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016358064361667934\n",
      "        policy_loss: -0.10457082631687323\n",
      "        total_loss: 67.10877517064412\n",
      "        vf_explained_var: 0.289179238875707\n",
      "        vf_loss: 67.20230414708455\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48705844809611637\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018236169565645544\n",
      "        policy_loss: -0.0853484215401113\n",
      "        total_loss: 67.43724043210348\n",
      "        vf_explained_var: 0.4320961645245552\n",
      "        vf_loss: 67.51027937889098\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4695685634513696\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016253821108842506\n",
      "        policy_loss: -0.06271416400559246\n",
      "        total_loss: 76.15177492141724\n",
      "        vf_explained_var: 0.023224106729030608\n",
      "        vf_loss: 76.20351789951324\n",
      "  num_agent_steps_sampled: 7349550\n",
      "  num_agent_steps_trained: 7349550\n",
      "  num_steps_sampled: 7349580\n",
      "  num_steps_trained: 7349580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 921\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.934999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.42000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 40.333333333333336\n",
      "  player_1: 31.333333333333332\n",
      "  player_2: 37.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 1.2366666666666666\n",
      "  player_1: 1.5966666666666662\n",
      "  player_2: 0.1666666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -26.333333333333336\n",
      "  player_1: -34.333333333333336\n",
      "  player_2: -54.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08556770003079876\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29276291956682615\n",
      "  mean_inference_ms: 1.5896167756031911\n",
      "  mean_raw_obs_processing_ms: 0.21030074932419748\n",
      "time_since_restore: 14710.486942529678\n",
      "time_this_iter_s: 16.696258544921875\n",
      "time_total_s: 14710.486942529678\n",
      "timers:\n",
      "  learn_throughput: 543.075\n",
      "  learn_time_ms: 14694.107\n",
      "  load_throughput: 1085803.55\n",
      "  load_time_ms: 7.349\n",
      "  sample_throughput: 502.808\n",
      "  sample_time_ms: 15870.875\n",
      "  update_time_ms: 5.541\n",
      "timestamp: 1643550955\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7349580\n",
      "training_iteration: 921\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7365510\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-56-28\n",
      "done: false\n",
      "episode_len_mean: 109.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 68\n",
      "episodes_total: 47211\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4634827337662379\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01650081328233474\n",
      "        policy_loss: -0.09167313703956703\n",
      "        total_loss: 55.736463618278506\n",
      "        vf_explained_var: 0.3261295607686043\n",
      "        vf_loss: 55.81699862162272\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4993604809045792\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017752644198377916\n",
      "        policy_loss: -0.051860299787173665\n",
      "        total_loss: 49.20854523976644\n",
      "        vf_explained_var: 0.16386484305063884\n",
      "        vf_loss: 49.24842245260874\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44766551872094473\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01628209801469135\n",
      "        policy_loss: -0.07942288725326459\n",
      "        total_loss: 60.67215739568075\n",
      "        vf_explained_var: 0.4221780296166738\n",
      "        vf_loss: 60.74058994452159\n",
      "  num_agent_steps_sampled: 7365510\n",
      "  num_agent_steps_trained: 7365510\n",
      "  num_steps_sampled: 7365540\n",
      "  num_steps_trained: 7365540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 923\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.633333333333335\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.42857142857144\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.333333333333336\n",
      "  player_1: 24.666666666666664\n",
      "  player_2: 27.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 5.2333333333333325\n",
      "  player_1: -3.436666666666667\n",
      "  player_2: 1.2033333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -34.33333333333333\n",
      "  player_1: -39.666666666666664\n",
      "  player_2: -34.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08557130155919418\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29288369481856963\n",
      "  mean_inference_ms: 1.5900660672607285\n",
      "  mean_raw_obs_processing_ms: 0.2103347286275229\n",
      "time_since_restore: 14743.25735616684\n",
      "time_this_iter_s: 17.042317867279053\n",
      "time_total_s: 14743.25735616684\n",
      "timers:\n",
      "  learn_throughput: 536.22\n",
      "  learn_time_ms: 14881.95\n",
      "  load_throughput: 999810.195\n",
      "  load_time_ms: 7.982\n",
      "  sample_throughput: 498.739\n",
      "  sample_time_ms: 16000.346\n",
      "  update_time_ms: 5.674\n",
      "timestamp: 1643550988\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7365540\n",
      "training_iteration: 923\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7381472\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-56-59\n",
      "done: false\n",
      "episode_len_mean: 109.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 75\n",
      "episodes_total: 47356\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4556246633330981\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016219354043245933\n",
      "        policy_loss: -0.08643425806115071\n",
      "        total_loss: 63.915454970200855\n",
      "        vf_explained_var: 0.3590803822875023\n",
      "        vf_loss: 63.990941480000814\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47234940752387045\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01723366139481186\n",
      "        policy_loss: -0.08393316038729003\n",
      "        total_loss: 65.60575480302175\n",
      "        vf_explained_var: 0.3221392385164897\n",
      "        vf_loss: 65.67805514494579\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4802902507285277\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016607021893315543\n",
      "        policy_loss: -0.07729509529812882\n",
      "        total_loss: 66.62501094500224\n",
      "        vf_explained_var: 0.418457348048687\n",
      "        vf_loss: 66.69109639167786\n",
      "  num_agent_steps_sampled: 7381472\n",
      "  num_agent_steps_trained: 7381472\n",
      "  num_steps_sampled: 7381500\n",
      "  num_steps_trained: 7381500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 925\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.050000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.43333333333334\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.0\n",
      "  player_1: 39.0\n",
      "  player_2: 31.0\n",
      "policy_reward_mean:\n",
      "  player_0: 4.043333333333334\n",
      "  player_1: -1.0766666666666662\n",
      "  player_2: 0.033333333333333715\n",
      "policy_reward_min:\n",
      "  player_0: -36.33333333333333\n",
      "  player_1: -35.0\n",
      "  player_2: -34.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08562587892427466\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2929667716795561\n",
      "  mean_inference_ms: 1.5905006495624512\n",
      "  mean_raw_obs_processing_ms: 0.2104352899667181\n",
      "time_since_restore: 14774.470243215561\n",
      "time_this_iter_s: 14.859413862228394\n",
      "time_total_s: 14774.470243215561\n",
      "timers:\n",
      "  learn_throughput: 543.619\n",
      "  learn_time_ms: 14679.401\n",
      "  load_throughput: 1002520.365\n",
      "  load_time_ms: 7.96\n",
      "  sample_throughput: 498.469\n",
      "  sample_time_ms: 16009.033\n",
      "  update_time_ms: 5.653\n",
      "timestamp: 1643551019\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7381500\n",
      "training_iteration: 925\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7397431\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-57-29\n",
      "done: false\n",
      "episode_len_mean: 105.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 77\n",
      "episodes_total: 47504\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46587270200252534\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016522794713962032\n",
      "        policy_loss: -0.06207016177475452\n",
      "        total_loss: 57.12228106975555\n",
      "        vf_explained_var: 0.26787726521492006\n",
      "        vf_loss: 57.17319862206777\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48541859661539394\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01699799485439693\n",
      "        policy_loss: -0.08535655412822962\n",
      "        total_loss: 59.03008259296417\n",
      "        vf_explained_var: 0.18695001681645712\n",
      "        vf_loss: 59.10396540323893\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4730002823472023\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016877813164230513\n",
      "        policy_loss: -0.0729963108835121\n",
      "        total_loss: 51.48101210276286\n",
      "        vf_explained_var: 0.36103973388671873\n",
      "        vf_loss: 51.542615672747296\n",
      "  num_agent_steps_sampled: 7397431\n",
      "  num_agent_steps_trained: 7397431\n",
      "  num_steps_sampled: 7397460\n",
      "  num_steps_trained: 7397460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 927\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.83888888888889\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.44444444444446\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.666666666666664\n",
      "  player_1: 27.666666666666664\n",
      "  player_2: 27.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.2066666666666674\n",
      "  player_1: -1.6333333333333337\n",
      "  player_2: 1.4266666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -29.0\n",
      "  player_1: -45.333333333333336\n",
      "  player_2: -29.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08568500391137905\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2934423342762977\n",
      "  mean_inference_ms: 1.5930568244346404\n",
      "  mean_raw_obs_processing_ms: 0.2103937870631071\n",
      "time_since_restore: 14804.883683919907\n",
      "time_this_iter_s: 15.148986339569092\n",
      "time_total_s: 14804.883683919907\n",
      "timers:\n",
      "  learn_throughput: 544.288\n",
      "  learn_time_ms: 14661.354\n",
      "  load_throughput: 997379.082\n",
      "  load_time_ms: 8.001\n",
      "  sample_throughput: 501.955\n",
      "  sample_time_ms: 15897.841\n",
      "  update_time_ms: 5.669\n",
      "timestamp: 1643551049\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7397460\n",
      "training_iteration: 927\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7413390\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-58-01\n",
      "done: false\n",
      "episode_len_mean: 108.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 77\n",
      "episodes_total: 47654\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47404900004466377\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01678221524520855\n",
      "        policy_loss: -0.08129383677616715\n",
      "        total_loss: 43.72989923477173\n",
      "        vf_explained_var: 0.3193456425269445\n",
      "        vf_loss: 43.79986495018005\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46989070817828177\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017458081081103957\n",
      "        policy_loss: -0.06853907860815525\n",
      "        total_loss: 63.278230282465614\n",
      "        vf_explained_var: 0.09276604940493902\n",
      "        vf_loss: 63.33498523394267\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4504675773779551\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016947462680491868\n",
      "        policy_loss: -0.09382807809238633\n",
      "        total_loss: 68.65423876603444\n",
      "        vf_explained_var: 0.30247524360815686\n",
      "        vf_loss: 68.73662711779276\n",
      "  num_agent_steps_sampled: 7413390\n",
      "  num_agent_steps_trained: 7413390\n",
      "  num_steps_sampled: 7413420\n",
      "  num_steps_trained: 7413420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 929\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.56111111111111\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.40000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.666666666666664\n",
      "  player_1: 23.333333333333336\n",
      "  player_2: 32.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 4.6899999999999995\n",
      "  player_1: -4.609999999999999\n",
      "  player_2: 2.92\n",
      "policy_reward_min:\n",
      "  player_0: -32.666666666666664\n",
      "  player_1: -44.0\n",
      "  player_2: -35.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08565355528015076\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29324170612191386\n",
      "  mean_inference_ms: 1.592314526873237\n",
      "  mean_raw_obs_processing_ms: 0.21035122535014567\n",
      "time_since_restore: 14837.03421497345\n",
      "time_this_iter_s: 15.448705434799194\n",
      "time_total_s: 14837.03421497345\n",
      "timers:\n",
      "  learn_throughput: 542.336\n",
      "  learn_time_ms: 14714.129\n",
      "  load_throughput: 972503.216\n",
      "  load_time_ms: 8.206\n",
      "  sample_throughput: 502.219\n",
      "  sample_time_ms: 15889.48\n",
      "  update_time_ms: 5.797\n",
      "timestamp: 1643551081\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7413420\n",
      "training_iteration: 929\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7429350\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-58-32\n",
      "done: false\n",
      "episode_len_mean: 104.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 74\n",
      "episodes_total: 47807\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4743137650191784\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017677783904070262\n",
      "        policy_loss: -0.0757840766509374\n",
      "        total_loss: 50.14270885785421\n",
      "        vf_explained_var: 0.3755521043141683\n",
      "        vf_loss: 50.20656028429667\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.49899808983008065\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01688655616584583\n",
      "        policy_loss: -0.09237781579295794\n",
      "        total_loss: 64.16709257602692\n",
      "        vf_explained_var: 0.3551610348622004\n",
      "        vf_loss: 64.24807206789653\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.470082503259182\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016616218338106895\n",
      "        policy_loss: -0.07176088447527339\n",
      "        total_loss: 59.740989813804624\n",
      "        vf_explained_var: 0.2975034406781197\n",
      "        vf_loss: 59.801534439722694\n",
      "  num_agent_steps_sampled: 7429350\n",
      "  num_agent_steps_trained: 7429350\n",
      "  num_steps_sampled: 7429380\n",
      "  num_steps_trained: 7429380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 931\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.152631578947368\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.36315789473683\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.0\n",
      "  player_1: 34.666666666666664\n",
      "  player_2: 34.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.9099999999999993\n",
      "  player_1: 0.01999999999999954\n",
      "  player_2: 0.0699999999999996\n",
      "policy_reward_min:\n",
      "  player_0: -24.666666666666668\n",
      "  player_1: -41.0\n",
      "  player_2: -44.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08568481975959225\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2932384541627956\n",
      "  mean_inference_ms: 1.5915862929755247\n",
      "  mean_raw_obs_processing_ms: 0.21045893006209354\n",
      "time_since_restore: 14867.582362174988\n",
      "time_this_iter_s: 15.627355813980103\n",
      "time_total_s: 14867.582362174988\n",
      "timers:\n",
      "  learn_throughput: 549.99\n",
      "  learn_time_ms: 14509.357\n",
      "  load_throughput: 928126.057\n",
      "  load_time_ms: 8.598\n",
      "  sample_throughput: 503.634\n",
      "  sample_time_ms: 15844.832\n",
      "  update_time_ms: 5.813\n",
      "timestamp: 1643551112\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7429380\n",
      "training_iteration: 931\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7445310\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-59-07\n",
      "done: false\n",
      "episode_len_mean: 103.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 78\n",
      "episodes_total: 47957\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44844183286031086\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015868126556655017\n",
      "        policy_loss: -0.11510467805123578\n",
      "        total_loss: 48.47779772281647\n",
      "        vf_explained_var: 0.3416760388016701\n",
      "        vf_loss: 48.58219135125478\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4910235295196374\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016974984256648704\n",
      "        policy_loss: -0.09910939070706566\n",
      "        total_loss: 58.47498848438263\n",
      "        vf_explained_var: 0.30382727493842443\n",
      "        vf_loss: 58.562639886538186\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47980412383874255\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016747879041267878\n",
      "        policy_loss: -0.04351900211845835\n",
      "        total_loss: 66.9301925722758\n",
      "        vf_explained_var: 0.3415428025523822\n",
      "        vf_loss: 66.96240692138672\n",
      "  num_agent_steps_sampled: 7445310\n",
      "  num_agent_steps_trained: 7445310\n",
      "  num_steps_sampled: 7445340\n",
      "  num_steps_trained: 7445340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 933\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.954545454545451\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.37727272727274\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.333333333333336\n",
      "  player_1: 39.0\n",
      "  player_2: 35.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.2933333333333334\n",
      "  player_1: 0.003333333333333197\n",
      "  player_2: 0.7033333333333335\n",
      "policy_reward_min:\n",
      "  player_0: -31.666666666666664\n",
      "  player_1: -22.333333333333336\n",
      "  player_2: -50.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08565091997869295\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29284551092234795\n",
      "  mean_inference_ms: 1.5897301134998292\n",
      "  mean_raw_obs_processing_ms: 0.21021654636529494\n",
      "time_since_restore: 14902.879568099976\n",
      "time_this_iter_s: 17.93570899963379\n",
      "time_total_s: 14902.879568099976\n",
      "timers:\n",
      "  learn_throughput: 541.288\n",
      "  learn_time_ms: 14742.601\n",
      "  load_throughput: 940884.251\n",
      "  load_time_ms: 8.481\n",
      "  sample_throughput: 501.232\n",
      "  sample_time_ms: 15920.759\n",
      "  update_time_ms: 5.811\n",
      "timestamp: 1643551147\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7445340\n",
      "training_iteration: 933\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7461271\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_14-59-39\n",
      "done: false\n",
      "episode_len_mean: 118.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 70\n",
      "episodes_total: 48095\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4584493366877238\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016405799761463127\n",
      "        policy_loss: -0.04505639737161497\n",
      "        total_loss: 40.92398626963298\n",
      "        vf_explained_var: 0.3580727712313334\n",
      "        vf_loss: 40.95796892642975\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4891815323134263\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01664540006239482\n",
      "        policy_loss: -0.09875326881728445\n",
      "        total_loss: 51.15575652917226\n",
      "        vf_explained_var: 0.38651623169581095\n",
      "        vf_loss: 51.24327411810557\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4665894573926926\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017404787906715076\n",
      "        policy_loss: -0.10325618909671902\n",
      "        total_loss: 40.61846328894297\n",
      "        vf_explained_var: 0.32916047781705854\n",
      "        vf_loss: 40.70997127374013\n",
      "  num_agent_steps_sampled: 7461271\n",
      "  num_agent_steps_trained: 7461271\n",
      "  num_steps_sampled: 7461300\n",
      "  num_steps_trained: 7461300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 935\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.919999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.42000000000003\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.0\n",
      "  player_1: 46.33333333333333\n",
      "  player_2: 33.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 5.773333333333333\n",
      "  player_1: -1.9766666666666666\n",
      "  player_2: -0.796666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -27.666666666666664\n",
      "  player_1: -33.333333333333336\n",
      "  player_2: -72.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08558699141728464\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29274413553622675\n",
      "  mean_inference_ms: 1.5899649651256191\n",
      "  mean_raw_obs_processing_ms: 0.21032943195261097\n",
      "time_since_restore: 14934.804827690125\n",
      "time_this_iter_s: 16.021925687789917\n",
      "time_total_s: 14934.804827690125\n",
      "timers:\n",
      "  learn_throughput: 538.816\n",
      "  learn_time_ms: 14810.252\n",
      "  load_throughput: 883519.5\n",
      "  load_time_ms: 9.032\n",
      "  sample_throughput: 500.435\n",
      "  sample_time_ms: 15946.142\n",
      "  update_time_ms: 7.766\n",
      "timestamp: 1643551179\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7461300\n",
      "training_iteration: 935\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7477230\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-00-10\n",
      "done: false\n",
      "episode_len_mean: 117.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 73\n",
      "episodes_total: 48232\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4423653769989808\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015896308857075686\n",
      "        policy_loss: -0.0670752128213644\n",
      "        total_loss: 55.628970832824706\n",
      "        vf_explained_var: 0.4082151890794436\n",
      "        vf_loss: 55.685315833091735\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4934358369310697\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01796457236936476\n",
      "        policy_loss: -0.0911115183463941\n",
      "        total_loss: 52.73238948345184\n",
      "        vf_explained_var: 0.39789910048246385\n",
      "        vf_loss: 52.81137461344401\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4640311524271965\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016610391436062837\n",
      "        policy_loss: -0.09811796219088137\n",
      "        total_loss: 42.43405430952708\n",
      "        vf_explained_var: 0.4307071833809217\n",
      "        vf_loss: 42.52096028804779\n",
      "  num_agent_steps_sampled: 7477230\n",
      "  num_agent_steps_trained: 7477230\n",
      "  num_steps_sampled: 7477260\n",
      "  num_steps_trained: 7477260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 937\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.827777777777776\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.43333333333334\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.0\n",
      "  player_1: 27.0\n",
      "  player_2: 35.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.816666666666667\n",
      "  player_1: 0.4366666666666667\n",
      "  player_2: -1.253333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -35.333333333333336\n",
      "  player_1: -32.0\n",
      "  player_2: -30.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08558163268419182\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2927013657364871\n",
      "  mean_inference_ms: 1.5892165931803661\n",
      "  mean_raw_obs_processing_ms: 0.21036189983226358\n",
      "time_since_restore: 14964.815752029419\n",
      "time_this_iter_s: 14.89504361152649\n",
      "time_total_s: 14964.815752029419\n",
      "timers:\n",
      "  learn_throughput: 540.219\n",
      "  learn_time_ms: 14771.794\n",
      "  load_throughput: 893772.956\n",
      "  load_time_ms: 8.928\n",
      "  sample_throughput: 497.132\n",
      "  sample_time_ms: 16052.091\n",
      "  update_time_ms: 7.795\n",
      "timestamp: 1643551210\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7477260\n",
      "training_iteration: 937\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7493190\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-00-42\n",
      "done: false\n",
      "episode_len_mean: 109.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 78\n",
      "episodes_total: 48378\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4588706502318382\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016333447050405424\n",
      "        policy_loss: -0.11717988243326545\n",
      "        total_loss: 62.82018027305603\n",
      "        vf_explained_var: 0.3544782468676567\n",
      "        vf_loss: 62.92633508682251\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.481592626174291\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01631015652123703\n",
      "        policy_loss: -0.036689978766565524\n",
      "        total_loss: 65.56605067253113\n",
      "        vf_explained_var: 0.18956651975711186\n",
      "        vf_loss: 65.5917313639323\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4597331037620703\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015485103232919831\n",
      "        policy_loss: -0.07649977911884585\n",
      "        total_loss: 76.27066120465597\n",
      "        vf_explained_var: 0.2706128070751826\n",
      "        vf_loss: 76.33670870780945\n",
      "  num_agent_steps_sampled: 7493190\n",
      "  num_agent_steps_trained: 7493190\n",
      "  num_steps_sampled: 7493220\n",
      "  num_steps_trained: 7493220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 939\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.209999999999999\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.405\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.33333333333333\n",
      "  player_1: 43.0\n",
      "  player_2: 38.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.71\n",
      "  player_1: -0.03000000000000014\n",
      "  player_2: 0.3199999999999996\n",
      "policy_reward_min:\n",
      "  player_0: -33.333333333333336\n",
      "  player_1: -51.333333333333336\n",
      "  player_2: -37.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08561904582604807\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2931052760990777\n",
      "  mean_inference_ms: 1.5904965049929851\n",
      "  mean_raw_obs_processing_ms: 0.21031926072797127\n",
      "time_since_restore: 14997.102065086365\n",
      "time_this_iter_s: 16.495150804519653\n",
      "time_total_s: 14997.102065086365\n",
      "timers:\n",
      "  learn_throughput: 539.754\n",
      "  learn_time_ms: 14784.524\n",
      "  load_throughput: 888897.486\n",
      "  load_time_ms: 8.977\n",
      "  sample_throughput: 500.868\n",
      "  sample_time_ms: 15932.33\n",
      "  update_time_ms: 7.77\n",
      "timestamp: 1643551242\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7493220\n",
      "training_iteration: 939\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7509150\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-01-12\n",
      "done: false\n",
      "episode_len_mean: 113.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 67\n",
      "episodes_total: 48519\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4491814556221167\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016811703409621258\n",
      "        policy_loss: -0.10240019815042616\n",
      "        total_loss: 36.94461140791575\n",
      "        vf_explained_var: 0.3658491544922193\n",
      "        vf_loss: 37.03566379388173\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4745351606110732\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017003267271282995\n",
      "        policy_loss: -0.05635111058751742\n",
      "        total_loss: 57.60456298987071\n",
      "        vf_explained_var: 0.38287643710772196\n",
      "        vf_loss: 57.649436589876814\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46737944955627125\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01705101611751767\n",
      "        policy_loss: -0.07289926934987306\n",
      "        total_loss: 44.520289770762126\n",
      "        vf_explained_var: 0.2795679653684298\n",
      "        vf_loss: 44.581679519017534\n",
      "  num_agent_steps_sampled: 7509150\n",
      "  num_agent_steps_trained: 7509150\n",
      "  num_steps_sampled: 7509180\n",
      "  num_steps_trained: 7509180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 941\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.527777777777779\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.40555555555555\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.333333333333332\n",
      "  player_1: 32.0\n",
      "  player_2: 38.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 2.2966666666666664\n",
      "  player_1: -0.08333333333333368\n",
      "  player_2: 0.7866666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -31.0\n",
      "  player_1: -44.33333333333333\n",
      "  player_2: -39.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08547101852028756\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2921851775832402\n",
      "  mean_inference_ms: 1.5880401329586937\n",
      "  mean_raw_obs_processing_ms: 0.21023033937180194\n",
      "time_since_restore: 15027.114085674286\n",
      "time_this_iter_s: 14.942641019821167\n",
      "time_total_s: 15027.114085674286\n",
      "timers:\n",
      "  learn_throughput: 541.709\n",
      "  learn_time_ms: 14731.161\n",
      "  load_throughput: 914645.732\n",
      "  load_time_ms: 8.725\n",
      "  sample_throughput: 497.056\n",
      "  sample_time_ms: 16054.519\n",
      "  update_time_ms: 7.699\n",
      "timestamp: 1643551272\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7509180\n",
      "training_iteration: 941\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7525112\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-01-44\n",
      "done: false\n",
      "episode_len_mean: 107.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 73\n",
      "episodes_total: 48666\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4607731987039248\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016669610673178757\n",
      "        policy_loss: -0.04863486373797059\n",
      "        total_loss: 61.30290799299876\n",
      "        vf_explained_var: 0.2910677520434062\n",
      "        vf_loss: 61.340290700594586\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48138492037852604\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01732155033832896\n",
      "        policy_loss: -0.11398709163224946\n",
      "        total_loss: 50.52498233318329\n",
      "        vf_explained_var: 0.30122208803892137\n",
      "        vf_loss: 50.627277398109435\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.461290957480669\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016964039482094222\n",
      "        policy_loss: -0.08903251356134811\n",
      "        total_loss: 56.77065234025319\n",
      "        vf_explained_var: 0.12452245871225993\n",
      "        vf_loss: 56.84823426405589\n",
      "  num_agent_steps_sampled: 7525112\n",
      "  num_agent_steps_trained: 7525112\n",
      "  num_steps_sampled: 7525140\n",
      "  num_steps_trained: 7525140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 943\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.242105263157892\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.41052631578948\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 27.666666666666664\n",
      "  player_1: 32.333333333333336\n",
      "  player_2: 29.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.2966666666666669\n",
      "  player_1: 2.2933333333333334\n",
      "  player_2: 1.0033333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -40.0\n",
      "  player_1: -37.333333333333336\n",
      "  player_2: -35.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08563423268093505\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2924358104221142\n",
      "  mean_inference_ms: 1.5883771773000686\n",
      "  mean_raw_obs_processing_ms: 0.2103435415811334\n",
      "time_since_restore: 15058.666624069214\n",
      "time_this_iter_s: 15.84797477722168\n",
      "time_total_s: 15058.666624069214\n",
      "timers:\n",
      "  learn_throughput: 555.083\n",
      "  learn_time_ms: 14376.227\n",
      "  load_throughput: 937878.695\n",
      "  load_time_ms: 8.509\n",
      "  sample_throughput: 504.989\n",
      "  sample_time_ms: 15802.31\n",
      "  update_time_ms: 7.65\n",
      "timestamp: 1643551304\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7525140\n",
      "training_iteration: 943\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7541071\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-02-15\n",
      "done: false\n",
      "episode_len_mean: 111.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 72\n",
      "episodes_total: 48808\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44824321165680886\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016939508506495712\n",
      "        policy_loss: -0.0563380982602636\n",
      "        total_loss: 46.15350690046946\n",
      "        vf_explained_var: 0.41574456622203193\n",
      "        vf_loss: 46.19841082890829\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4654086098074913\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017214805943561562\n",
      "        policy_loss: -0.07066663037985563\n",
      "        total_loss: 48.73993434270223\n",
      "        vf_explained_var: 0.3764733050266902\n",
      "        vf_loss: 48.798980889320376\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46409925987323125\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015189939282920856\n",
      "        policy_loss: -0.08622308388197174\n",
      "        total_loss: 63.02965799967448\n",
      "        vf_explained_var: 0.386385222474734\n",
      "        vf_loss: 63.10562788168589\n",
      "  num_agent_steps_sampled: 7541071\n",
      "  num_agent_steps_trained: 7541071\n",
      "  num_steps_sampled: 7541100\n",
      "  num_steps_trained: 7541100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 945\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.550000000000002\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.38333333333334\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.66666666666667\n",
      "  player_1: 30.66666666666667\n",
      "  player_2: 29.0\n",
      "policy_reward_mean:\n",
      "  player_0: 4.716666666666667\n",
      "  player_1: 0.5566666666666668\n",
      "  player_2: -2.273333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -35.333333333333336\n",
      "  player_1: -31.0\n",
      "  player_2: -66.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08552225605109946\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2924361739516026\n",
      "  mean_inference_ms: 1.587080105085457\n",
      "  mean_raw_obs_processing_ms: 0.21044709881034854\n",
      "time_since_restore: 15089.702852487564\n",
      "time_this_iter_s: 15.116269826889038\n",
      "time_total_s: 15089.702852487564\n",
      "timers:\n",
      "  learn_throughput: 558.445\n",
      "  learn_time_ms: 14289.679\n",
      "  load_throughput: 989620.418\n",
      "  load_time_ms: 8.064\n",
      "  sample_throughput: 511.063\n",
      "  sample_time_ms: 15614.502\n",
      "  update_time_ms: 5.807\n",
      "timestamp: 1643551335\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7541100\n",
      "training_iteration: 945\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7557032\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-02-46\n",
      "done: false\n",
      "episode_len_mean: 111.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 73\n",
      "episodes_total: 48954\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45494773849844933\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01661026731512\n",
      "        policy_loss: -0.10464846930311372\n",
      "        total_loss: 50.33044610659282\n",
      "        vf_explained_var: 0.4581031965216001\n",
      "        vf_loss: 50.42388255437215\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45850790669520697\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017417454009298203\n",
      "        policy_loss: -0.09261591865681112\n",
      "        total_loss: 46.79295746644338\n",
      "        vf_explained_var: 0.44119267344474794\n",
      "        vf_loss: 46.87381694952647\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.462449712852637\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017913241275722004\n",
      "        policy_loss: -0.0431919542249913\n",
      "        total_loss: 40.651183292071025\n",
      "        vf_explained_var: 0.2861624066034953\n",
      "        vf_loss: 40.68228378454844\n",
      "  num_agent_steps_sampled: 7557032\n",
      "  num_agent_steps_trained: 7557032\n",
      "  num_steps_sampled: 7557060\n",
      "  num_steps_trained: 7557060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 947\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.599999999999998\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.44761904761907\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 27.666666666666664\n",
      "  player_1: 31.666666666666664\n",
      "  player_2: 22.666666666666668\n",
      "policy_reward_mean:\n",
      "  player_0: 0.42000000000000015\n",
      "  player_1: 3.0100000000000007\n",
      "  player_2: -0.43000000000000016\n",
      "policy_reward_min:\n",
      "  player_0: -34.333333333333336\n",
      "  player_1: -21.333333333333332\n",
      "  player_2: -31.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08549506575489722\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2925773075657766\n",
      "  mean_inference_ms: 1.5898328052764577\n",
      "  mean_raw_obs_processing_ms: 0.2102558518695137\n",
      "time_since_restore: 15121.303756713867\n",
      "time_this_iter_s: 16.752292156219482\n",
      "time_total_s: 15121.303756713867\n",
      "timers:\n",
      "  learn_throughput: 552.371\n",
      "  learn_time_ms: 14446.81\n",
      "  load_throughput: 991467.239\n",
      "  load_time_ms: 8.049\n",
      "  sample_throughput: 515.092\n",
      "  sample_time_ms: 15492.375\n",
      "  update_time_ms: 5.811\n",
      "timestamp: 1643551366\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7557060\n",
      "training_iteration: 947\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7572990\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-03-19\n",
      "done: false\n",
      "episode_len_mean: 109.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 73\n",
      "episodes_total: 49096\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47294587512811026\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017110375612858206\n",
      "        policy_loss: -0.050181899275630715\n",
      "        total_loss: 39.73644022226333\n",
      "        vf_explained_var: 0.19604024708271026\n",
      "        vf_loss: 39.77507260958354\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4862851603329182\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0168076384601837\n",
      "        policy_loss: -0.08930612443014979\n",
      "        total_loss: 66.48391700426737\n",
      "        vf_explained_var: 0.20560442159573236\n",
      "        vf_loss: 66.56187785466513\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4701643832027912\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016578550468964674\n",
      "        policy_loss: -0.08821222707008322\n",
      "        total_loss: 37.92034181674322\n",
      "        vf_explained_var: 0.2875717707475026\n",
      "        vf_loss: 37.99736370404561\n",
      "  num_agent_steps_sampled: 7572990\n",
      "  num_agent_steps_trained: 7572990\n",
      "  num_steps_sampled: 7573020\n",
      "  num_steps_trained: 7573020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 949\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.715\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.42000000000003\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 31.333333333333336\n",
      "  player_1: 28.666666666666664\n",
      "  player_2: 34.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.356666666666667\n",
      "  player_1: 0.39666666666666656\n",
      "  player_2: 0.2466666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -34.333333333333336\n",
      "  player_1: -42.666666666666664\n",
      "  player_2: -33.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08551685987720095\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2927840931592756\n",
      "  mean_inference_ms: 1.5892830651564804\n",
      "  mean_raw_obs_processing_ms: 0.21027398313985843\n",
      "time_since_restore: 15153.636209726334\n",
      "time_this_iter_s: 15.951493501663208\n",
      "time_total_s: 15153.636209726334\n",
      "timers:\n",
      "  learn_throughput: 552.124\n",
      "  learn_time_ms: 14453.263\n",
      "  load_throughput: 973102.624\n",
      "  load_time_ms: 8.201\n",
      "  sample_throughput: 507.072\n",
      "  sample_time_ms: 15737.416\n",
      "  update_time_ms: 5.847\n",
      "timestamp: 1643551399\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7573020\n",
      "training_iteration: 949\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7588950\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-03-49\n",
      "done: false\n",
      "episode_len_mean: 112.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 70\n",
      "episodes_total: 49238\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45415018156170844\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016571705358001055\n",
      "        policy_loss: -0.07013679614290595\n",
      "        total_loss: 48.50483026583989\n",
      "        vf_explained_var: 0.5295753109455109\n",
      "        vf_loss: 48.56378123283386\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47444548035661377\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01706281166196362\n",
      "        policy_loss: -0.08249761882548531\n",
      "        total_loss: 49.05610383351644\n",
      "        vf_explained_var: 0.28038777053356173\n",
      "        vf_loss: 49.12708391984304\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4533456969261169\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016936112878550867\n",
      "        policy_loss: -0.08843645828465621\n",
      "        total_loss: 67.69718114535014\n",
      "        vf_explained_var: 0.4029680126905441\n",
      "        vf_loss: 67.77418596585592\n",
      "  num_agent_steps_sampled: 7588950\n",
      "  num_agent_steps_trained: 7588950\n",
      "  num_steps_sampled: 7588980\n",
      "  num_steps_trained: 7588980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 951\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.588888888888887\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.43333333333334\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.0\n",
      "  player_1: 34.0\n",
      "  player_2: 42.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 1.3966666666666663\n",
      "  player_1: 0.5766666666666664\n",
      "  player_2: 1.0266666666666668\n",
      "policy_reward_min:\n",
      "  player_0: -40.666666666666664\n",
      "  player_1: -27.666666666666664\n",
      "  player_2: -48.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08550083586400589\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29257403304206236\n",
      "  mean_inference_ms: 1.58895613432622\n",
      "  mean_raw_obs_processing_ms: 0.21019454499075166\n",
      "time_since_restore: 15184.346327781677\n",
      "time_this_iter_s: 15.143234729766846\n",
      "time_total_s: 15184.346327781677\n",
      "timers:\n",
      "  learn_throughput: 549.354\n",
      "  learn_time_ms: 14526.153\n",
      "  load_throughput: 994362.674\n",
      "  load_time_ms: 8.025\n",
      "  sample_throughput: 507.292\n",
      "  sample_time_ms: 15730.582\n",
      "  update_time_ms: 5.89\n",
      "timestamp: 1643551429\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7588980\n",
      "training_iteration: 951\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7604912\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-04-20\n",
      "done: false\n",
      "episode_len_mean: 110.62\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 75\n",
      "episodes_total: 49385\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4519970471660296\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016774322465723232\n",
      "        policy_loss: -0.09068245388877888\n",
      "        total_loss: 53.68700538317363\n",
      "        vf_explained_var: 0.4421682034929593\n",
      "        vf_loss: 53.76636520544688\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48352214445670444\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017197654010620244\n",
      "        policy_loss: -0.06827844788009922\n",
      "        total_loss: 63.00769547462463\n",
      "        vf_explained_var: 0.3220512960354487\n",
      "        vf_loss: 63.064365542729696\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4698137969772021\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017506235413897193\n",
      "        policy_loss: -0.07162714244176945\n",
      "        total_loss: 47.60078191280365\n",
      "        vf_explained_var: 0.2759179844458898\n",
      "        vf_loss: 47.660592318375905\n",
      "  num_agent_steps_sampled: 7604912\n",
      "  num_agent_steps_trained: 7604912\n",
      "  num_steps_sampled: 7604940\n",
      "  num_steps_trained: 7604940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 953\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.672222222222224\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.43333333333334\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.666666666666664\n",
      "  player_1: 29.666666666666664\n",
      "  player_2: 24.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 4.8566666666666665\n",
      "  player_1: -0.6933333333333336\n",
      "  player_2: -1.1633333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -36.333333333333336\n",
      "  player_1: -56.333333333333336\n",
      "  player_2: -27.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08558138531344149\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2931219147943359\n",
      "  mean_inference_ms: 1.5914371662056908\n",
      "  mean_raw_obs_processing_ms: 0.2103807659902551\n",
      "time_since_restore: 15215.010781049728\n",
      "time_this_iter_s: 15.143814325332642\n",
      "time_total_s: 15215.010781049728\n",
      "timers:\n",
      "  learn_throughput: 552.64\n",
      "  learn_time_ms: 14439.773\n",
      "  load_throughput: 1028596.986\n",
      "  load_time_ms: 7.758\n",
      "  sample_throughput: 507.276\n",
      "  sample_time_ms: 15731.087\n",
      "  update_time_ms: 6.005\n",
      "timestamp: 1643551460\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7604940\n",
      "training_iteration: 953\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7620871\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-04-51\n",
      "done: false\n",
      "episode_len_mean: 105.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 49530\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44830659846464793\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016430279933212926\n",
      "        policy_loss: -0.076968071864297\n",
      "        total_loss: 59.84960666100184\n",
      "        vf_explained_var: 0.46286905884742735\n",
      "        vf_loss: 59.91548393408458\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4735573308666547\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016133961667837107\n",
      "        policy_loss: -0.07464966303048035\n",
      "        total_loss: 49.327015186945594\n",
      "        vf_explained_var: 0.39036414990822477\n",
      "        vf_loss: 49.390774167378744\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4720252504448096\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017475810249392223\n",
      "        policy_loss: -0.08398532963978747\n",
      "        total_loss: 41.613442697525024\n",
      "        vf_explained_var: 0.4498904989163081\n",
      "        vf_loss: 41.68563164313634\n",
      "  num_agent_steps_sampled: 7620871\n",
      "  num_agent_steps_trained: 7620871\n",
      "  num_steps_sampled: 7620900\n",
      "  num_steps_trained: 7620900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 955\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.744444444444447\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.43333333333334\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.333333333333336\n",
      "  player_1: 38.0\n",
      "  player_2: 25.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 1.776666666666666\n",
      "  player_1: 1.6166666666666663\n",
      "  player_2: -0.3933333333333336\n",
      "policy_reward_min:\n",
      "  player_0: -38.0\n",
      "  player_1: -36.333333333333336\n",
      "  player_2: -27.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08553238094565441\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29240426235333106\n",
      "  mean_inference_ms: 1.5890470744005372\n",
      "  mean_raw_obs_processing_ms: 0.21011309603952785\n",
      "time_since_restore: 15246.006203651428\n",
      "time_this_iter_s: 15.260509014129639\n",
      "time_total_s: 15246.006203651428\n",
      "timers:\n",
      "  learn_throughput: 552.688\n",
      "  learn_time_ms: 14438.517\n",
      "  load_throughput: 1083424.106\n",
      "  load_time_ms: 7.366\n",
      "  sample_throughput: 510.194\n",
      "  sample_time_ms: 15641.103\n",
      "  update_time_ms: 5.772\n",
      "timestamp: 1643551491\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7620900\n",
      "training_iteration: 955\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7636830\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-05-23\n",
      "done: false\n",
      "episode_len_mean: 113.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 67\n",
      "episodes_total: 49672\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44904020552833873\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014962050573275765\n",
      "        policy_loss: -0.08088070972512165\n",
      "        total_loss: 55.18125223000845\n",
      "        vf_explained_var: 0.2034595388174057\n",
      "        vf_loss: 55.25203363895416\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46574030766884483\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01641045384452203\n",
      "        policy_loss: -0.027447903168698152\n",
      "        total_loss: 55.45182984670003\n",
      "        vf_explained_var: 0.3825949875513713\n",
      "        vf_loss: 55.4682005294164\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4486551617085934\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014248057564506478\n",
      "        policy_loss: -0.09348874633510908\n",
      "        total_loss: 35.794054821332296\n",
      "        vf_explained_var: 0.41976774285236995\n",
      "        vf_loss: 35.87792614062627\n",
      "  num_agent_steps_sampled: 7636830\n",
      "  num_agent_steps_trained: 7636830\n",
      "  num_steps_sampled: 7636860\n",
      "  num_steps_trained: 7636860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 957\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.99\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.43000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.666666666666664\n",
      "  player_1: 36.333333333333336\n",
      "  player_2: 31.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 0.38999999999999957\n",
      "  player_1: 1.5599999999999996\n",
      "  player_2: 1.0499999999999998\n",
      "policy_reward_min:\n",
      "  player_0: -42.33333333333333\n",
      "  player_1: -30.0\n",
      "  player_2: -52.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08547191825947024\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29265981532284596\n",
      "  mean_inference_ms: 1.5893820102012015\n",
      "  mean_raw_obs_processing_ms: 0.2102987454850794\n",
      "time_since_restore: 15277.520440340042\n",
      "time_this_iter_s: 15.906567811965942\n",
      "time_total_s: 15277.520440340042\n",
      "timers:\n",
      "  learn_throughput: 553.086\n",
      "  learn_time_ms: 14428.125\n",
      "  load_throughput: 1081145.858\n",
      "  load_time_ms: 7.381\n",
      "  sample_throughput: 507.199\n",
      "  sample_time_ms: 15733.473\n",
      "  update_time_ms: 5.671\n",
      "timestamp: 1643551523\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7636860\n",
      "training_iteration: 957\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7652790\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-05-54\n",
      "done: false\n",
      "episode_len_mean: 106.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 73\n",
      "episodes_total: 49825\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4481741320590178\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017352606131299905\n",
      "        policy_loss: -0.10366275631822645\n",
      "        total_loss: 62.22700895786286\n",
      "        vf_explained_var: 0.2862795615196228\n",
      "        vf_loss: 62.31895862738291\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46626496195793155\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017376326896585775\n",
      "        policy_loss: -0.07353060804617902\n",
      "        total_loss: 59.030973664919536\n",
      "        vf_explained_var: 0.3861050733923912\n",
      "        vf_loss: 59.09277528444926\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4628670198718707\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016706494685886353\n",
      "        policy_loss: -0.06676738235478599\n",
      "        total_loss: 55.019706536928815\n",
      "        vf_explained_var: 0.3360473454991976\n",
      "        vf_loss: 55.075196827252704\n",
      "  num_agent_steps_sampled: 7652790\n",
      "  num_agent_steps_trained: 7652790\n",
      "  num_steps_sampled: 7652820\n",
      "  num_steps_trained: 7652820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 959\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.494736842105265\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.43157894736844\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 32.333333333333336\n",
      "  player_2: 21.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.326666666666666\n",
      "  player_1: 0.336666666666667\n",
      "  player_2: -0.6633333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -34.666666666666664\n",
      "  player_1: -26.0\n",
      "  player_2: -39.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08545675572642959\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2925490689243584\n",
      "  mean_inference_ms: 1.589578875408849\n",
      "  mean_raw_obs_processing_ms: 0.21025311074833777\n",
      "time_since_restore: 15308.711920976639\n",
      "time_this_iter_s: 15.766409397125244\n",
      "time_total_s: 15308.711920976639\n",
      "timers:\n",
      "  learn_throughput: 557.511\n",
      "  learn_time_ms: 14313.628\n",
      "  load_throughput: 1145490.528\n",
      "  load_time_ms: 6.966\n",
      "  sample_throughput: 513.184\n",
      "  sample_time_ms: 15549.99\n",
      "  update_time_ms: 5.699\n",
      "timestamp: 1643551554\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7652820\n",
      "training_iteration: 959\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7668750\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-06-28\n",
      "done: false\n",
      "episode_len_mean: 105.82\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 72\n",
      "episodes_total: 49978\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45052838921546934\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014964919859013813\n",
      "        policy_loss: -0.12869088705629111\n",
      "        total_loss: 52.49162714163462\n",
      "        vf_explained_var: 0.3106432984272639\n",
      "        vf_loss: 52.61021675745646\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.48325619762142497\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017424208592601645\n",
      "        policy_loss: -0.07010846788684527\n",
      "        total_loss: 47.1663650894165\n",
      "        vf_explained_var: 0.37350371599197385\n",
      "        vf_loss: 47.22471231778463\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46467188318570457\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0159278629399887\n",
      "        policy_loss: -0.01808517750352621\n",
      "        total_loss: 45.494330789248146\n",
      "        vf_explained_var: 0.38626432567834856\n",
      "        vf_loss: 45.50166448434194\n",
      "  num_agent_steps_sampled: 7668750\n",
      "  num_agent_steps_trained: 7668750\n",
      "  num_steps_sampled: 7668780\n",
      "  num_steps_trained: 7668780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 961\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.747619047619045\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.40952380952383\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.666666666666664\n",
      "  player_1: 27.333333333333336\n",
      "  player_2: 32.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 0.7900000000000003\n",
      "  player_1: 0.5700000000000002\n",
      "  player_2: 1.6400000000000003\n",
      "policy_reward_min:\n",
      "  player_0: -54.33333333333333\n",
      "  player_1: -34.66666666666667\n",
      "  player_2: -37.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08551484630262436\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29293660337534183\n",
      "  mean_inference_ms: 1.590899991293091\n",
      "  mean_raw_obs_processing_ms: 0.21032183550246358\n",
      "time_since_restore: 15343.050640106201\n",
      "time_this_iter_s: 17.483478546142578\n",
      "time_total_s: 15343.050640106201\n",
      "timers:\n",
      "  learn_throughput: 543.757\n",
      "  learn_time_ms: 14675.677\n",
      "  load_throughput: 1145608.149\n",
      "  load_time_ms: 6.966\n",
      "  sample_throughput: 509.571\n",
      "  sample_time_ms: 15660.225\n",
      "  update_time_ms: 5.763\n",
      "timestamp: 1643551588\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7668780\n",
      "training_iteration: 961\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7684710\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-06-59\n",
      "done: false\n",
      "episode_len_mean: 111.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 72\n",
      "episodes_total: 50124\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4432663908104102\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017418016354758568\n",
      "        policy_loss: -0.09543711371098955\n",
      "        total_loss: 49.41709083398183\n",
      "        vf_explained_var: 0.3832455681761106\n",
      "        vf_loss: 49.500770659446715\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4717554626862208\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01692626276634788\n",
      "        policy_loss: -0.09116254383698105\n",
      "        total_loss: 69.60555167992909\n",
      "        vf_explained_var: 0.3712607482075691\n",
      "        vf_loss: 69.68528877099355\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4584676992893219\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017019356042178514\n",
      "        policy_loss: -0.05803139701640854\n",
      "        total_loss: 41.16795434315999\n",
      "        vf_explained_var: 0.3402400242288907\n",
      "        vf_loss: 41.21449772834778\n",
      "  num_agent_steps_sampled: 7684710\n",
      "  num_agent_steps_trained: 7684710\n",
      "  num_steps_sampled: 7684740\n",
      "  num_steps_trained: 7684740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 963\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.872222222222225\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.43333333333334\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 30.333333333333336\n",
      "  player_1: 26.333333333333336\n",
      "  player_2: 34.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.283333333333333\n",
      "  player_1: -1.1566666666666665\n",
      "  player_2: 0.8733333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -22.333333333333332\n",
      "  player_1: -61.666666666666664\n",
      "  player_2: -35.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08541510560420372\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29261585924868827\n",
      "  mean_inference_ms: 1.5898787880579994\n",
      "  mean_raw_obs_processing_ms: 0.21026898301401786\n",
      "time_since_restore: 15373.589347839355\n",
      "time_this_iter_s: 14.927229881286621\n",
      "time_total_s: 15373.589347839355\n",
      "timers:\n",
      "  learn_throughput: 544.351\n",
      "  learn_time_ms: 14659.66\n",
      "  load_throughput: 1085588.726\n",
      "  load_time_ms: 7.351\n",
      "  sample_throughput: 501.733\n",
      "  sample_time_ms: 15904.877\n",
      "  update_time_ms: 5.509\n",
      "timestamp: 1643551619\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7684740\n",
      "training_iteration: 963\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7700672\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-07-31\n",
      "done: false\n",
      "episode_len_mean: 101.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 81\n",
      "episodes_total: 50282\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.460751442660888\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017427958074358686\n",
      "        policy_loss: -0.07143984033415715\n",
      "        total_loss: 58.090398438771565\n",
      "        vf_explained_var: 0.34975409189860024\n",
      "        vf_loss: 58.15007443110148\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46496384993195533\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01758830569987405\n",
      "        policy_loss: -0.08062210130194823\n",
      "        total_loss: 76.74506010373433\n",
      "        vf_explained_var: 0.42655699998140334\n",
      "        vf_loss: 76.81381003379822\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4869529686868191\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017895824419343665\n",
      "        policy_loss: -0.09739530918498834\n",
      "        total_loss: 66.40716635068257\n",
      "        vf_explained_var: 0.4391853446761767\n",
      "        vf_loss: 66.49248229503631\n",
      "  num_agent_steps_sampled: 7700672\n",
      "  num_agent_steps_trained: 7700672\n",
      "  num_steps_sampled: 7700700\n",
      "  num_steps_trained: 7700700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 965\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.284210526315789\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.42105263157897\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.333333333333336\n",
      "  player_1: 29.333333333333336\n",
      "  player_2: 35.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 0.9033333333333333\n",
      "  player_1: -1.2066666666666666\n",
      "  player_2: 3.3033333333333337\n",
      "policy_reward_min:\n",
      "  player_0: -56.66666666666667\n",
      "  player_1: -39.333333333333336\n",
      "  player_2: -40.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08560546898324725\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29233859051873595\n",
      "  mean_inference_ms: 1.5879102508692853\n",
      "  mean_raw_obs_processing_ms: 0.2101883084873569\n",
      "time_since_restore: 15405.518003702164\n",
      "time_this_iter_s: 15.232619762420654\n",
      "time_total_s: 15405.518003702164\n",
      "timers:\n",
      "  learn_throughput: 541.09\n",
      "  learn_time_ms: 14748.001\n",
      "  load_throughput: 1093701.465\n",
      "  load_time_ms: 7.296\n",
      "  sample_throughput: 499.384\n",
      "  sample_time_ms: 15979.697\n",
      "  update_time_ms: 5.523\n",
      "timestamp: 1643551651\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7700700\n",
      "training_iteration: 965\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7716630\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-08-03\n",
      "done: false\n",
      "episode_len_mean: 109.42\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 70\n",
      "episodes_total: 50428\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45316004917025565\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015963595868247655\n",
      "        policy_loss: -0.06337169408798218\n",
      "        total_loss: 70.78139203389486\n",
      "        vf_explained_var: 0.29079916963974634\n",
      "        vf_loss: 70.8339885632197\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4593509241938591\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016726891816800465\n",
      "        policy_loss: -0.09090396350560088\n",
      "        total_loss: 54.452990713119505\n",
      "        vf_explained_var: 0.04765305350224177\n",
      "        vf_loss: 54.5326037534078\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45608167906602226\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01615602614775478\n",
      "        policy_loss: -0.07235289821556459\n",
      "        total_loss: 69.903553759257\n",
      "        vf_explained_var: 0.3708739533027013\n",
      "        vf_loss: 69.96500119845072\n",
      "  num_agent_steps_sampled: 7716630\n",
      "  num_agent_steps_trained: 7716630\n",
      "  num_steps_sampled: 7716660\n",
      "  num_steps_trained: 7716660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 967\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.777777777777779\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.40000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 28.666666666666664\n",
      "  player_1: 29.0\n",
      "  player_2: 47.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 1.1133333333333328\n",
      "  player_1: -1.5166666666666666\n",
      "  player_2: 3.4033333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -43.333333333333336\n",
      "  player_1: -35.333333333333336\n",
      "  player_2: -39.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08545956711710019\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29205222839918066\n",
      "  mean_inference_ms: 1.5870449426123263\n",
      "  mean_raw_obs_processing_ms: 0.21025680704804514\n",
      "time_since_restore: 15437.206368207932\n",
      "time_this_iter_s: 15.12877082824707\n",
      "time_total_s: 15437.206368207932\n",
      "timers:\n",
      "  learn_throughput: 540.473\n",
      "  learn_time_ms: 14764.843\n",
      "  load_throughput: 971013.555\n",
      "  load_time_ms: 8.218\n",
      "  sample_throughput: 496.516\n",
      "  sample_time_ms: 16071.993\n",
      "  update_time_ms: 5.631\n",
      "timestamp: 1643551683\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7716660\n",
      "training_iteration: 967\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7732590\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-08-36\n",
      "done: false\n",
      "episode_len_mean: 109.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 74\n",
      "episodes_total: 50569\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45949907317757605\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017131078622159446\n",
      "        policy_loss: -0.06019081805367023\n",
      "        total_loss: 72.10986733436584\n",
      "        vf_explained_var: 0.2282271083196004\n",
      "        vf_loss: 72.15849443594615\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46658863296111425\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01662902611343346\n",
      "        policy_loss: -0.06300793283618987\n",
      "        total_loss: 62.91346816221873\n",
      "        vf_explained_var: 0.4599782913923264\n",
      "        vf_loss: 62.96525163650513\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47000320345163343\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016343649198077705\n",
      "        policy_loss: -0.11696971586905419\n",
      "        total_loss: 63.44764382998149\n",
      "        vf_explained_var: 0.36077048430840175\n",
      "        vf_loss: 63.55358147303264\n",
      "  num_agent_steps_sampled: 7732590\n",
      "  num_agent_steps_trained: 7732590\n",
      "  num_steps_sampled: 7732620\n",
      "  num_steps_trained: 7732620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 969\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.576190476190478\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.41904761904765\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.666666666666664\n",
      "  player_1: 30.66666666666667\n",
      "  player_2: 24.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.2300000000000004\n",
      "  player_1: 0.2799999999999999\n",
      "  player_2: 0.4899999999999997\n",
      "policy_reward_min:\n",
      "  player_0: -43.0\n",
      "  player_1: -36.0\n",
      "  player_2: -38.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08545462453023063\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.291994878066969\n",
      "  mean_inference_ms: 1.5856070080339217\n",
      "  mean_raw_obs_processing_ms: 0.2102320373251706\n",
      "time_since_restore: 15470.889297485352\n",
      "time_this_iter_s: 17.020108461380005\n",
      "time_total_s: 15470.889297485352\n",
      "timers:\n",
      "  learn_throughput: 531.503\n",
      "  learn_time_ms: 15014.028\n",
      "  load_throughput: 978499.267\n",
      "  load_time_ms: 8.155\n",
      "  sample_throughput: 494.97\n",
      "  sample_time_ms: 16122.202\n",
      "  update_time_ms: 5.61\n",
      "timestamp: 1643551716\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7732620\n",
      "training_iteration: 969\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7748551\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-09-10\n",
      "done: false\n",
      "episode_len_mean: 103.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 77\n",
      "episodes_total: 50724\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4574238419036071\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016001394274007527\n",
      "        policy_loss: -0.0606278116752704\n",
      "        total_loss: 87.70623746871948\n",
      "        vf_explained_var: 0.28661779542764027\n",
      "        vf_loss: 87.75606462796529\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47149857744574547\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01561282061234427\n",
      "        policy_loss: -0.09709168087070187\n",
      "        total_loss: 64.47021838347118\n",
      "        vf_explained_var: 0.3772718883554141\n",
      "        vf_loss: 64.55677149772644\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46726641431450844\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01750446613088305\n",
      "        policy_loss: -0.07469629920398195\n",
      "        total_loss: 66.77471899827322\n",
      "        vf_explained_var: 0.2934366185466448\n",
      "        vf_loss: 66.837599457105\n",
      "  num_agent_steps_sampled: 7748551\n",
      "  num_agent_steps_trained: 7748551\n",
      "  num_steps_sampled: 7748580\n",
      "  num_steps_trained: 7748580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 971\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.671428571428573\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.42857142857144\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.666666666666664\n",
      "  player_1: 28.333333333333336\n",
      "  player_2: 34.0\n",
      "policy_reward_mean:\n",
      "  player_0: 2.6033333333333335\n",
      "  player_1: -2.1566666666666667\n",
      "  player_2: 2.5533333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -37.333333333333336\n",
      "  player_1: -50.333333333333336\n",
      "  player_2: -45.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08556184528760996\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2925474214612815\n",
      "  mean_inference_ms: 1.589488724446659\n",
      "  mean_raw_obs_processing_ms: 0.21017188465522152\n",
      "time_since_restore: 15503.871946573257\n",
      "time_this_iter_s: 17.113105535507202\n",
      "time_total_s: 15503.871946573257\n",
      "timers:\n",
      "  learn_throughput: 536.399\n",
      "  learn_time_ms: 14876.975\n",
      "  load_throughput: 919141.725\n",
      "  load_time_ms: 8.682\n",
      "  sample_throughput: 494.128\n",
      "  sample_time_ms: 16149.672\n",
      "  update_time_ms: 5.482\n",
      "timestamp: 1643551750\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7748580\n",
      "training_iteration: 971\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7764511\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-09-40\n",
      "done: false\n",
      "episode_len_mean: 106.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 79\n",
      "episodes_total: 50874\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4777228726943334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017016019408358337\n",
      "        policy_loss: -0.049569738892217476\n",
      "        total_loss: 81.16021657148997\n",
      "        vf_explained_var: 0.4360429977377256\n",
      "        vf_loss: 81.19830087502797\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47113740146160127\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017652374937217132\n",
      "        policy_loss: -0.07187351865693926\n",
      "        total_loss: 56.69064936161041\n",
      "        vf_explained_var: 0.4153985266884168\n",
      "        vf_loss: 56.750607498486836\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4622506090005239\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01655649383884641\n",
      "        policy_loss: -0.10133434215405335\n",
      "        total_loss: 67.74179885546367\n",
      "        vf_explained_var: 0.31241762657960254\n",
      "        vf_loss: 67.83195780277252\n",
      "  num_agent_steps_sampled: 7764511\n",
      "  num_agent_steps_trained: 7764511\n",
      "  num_steps_sampled: 7764540\n",
      "  num_steps_trained: 7764540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 973\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.089473684210528\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.44736842105266\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 33.333333333333336\n",
      "  player_1: 28.333333333333336\n",
      "  player_2: 39.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 1.026666666666667\n",
      "  player_1: -0.20333333333333378\n",
      "  player_2: 2.176666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -42.0\n",
      "  player_1: -28.333333333333336\n",
      "  player_2: -29.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08548135573934726\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2923700091600682\n",
      "  mean_inference_ms: 1.5877584831950255\n",
      "  mean_raw_obs_processing_ms: 0.21021360881464646\n",
      "time_since_restore: 15534.262117862701\n",
      "time_this_iter_s: 15.579030275344849\n",
      "time_total_s: 15534.262117862701\n",
      "timers:\n",
      "  learn_throughput: 537.003\n",
      "  learn_time_ms: 14860.245\n",
      "  load_throughput: 882289.802\n",
      "  load_time_ms: 9.045\n",
      "  sample_throughput: 497.765\n",
      "  sample_time_ms: 16031.664\n",
      "  update_time_ms: 5.58\n",
      "timestamp: 1643551780\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7764540\n",
      "training_iteration: 973\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7780472\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-10-12\n",
      "done: false\n",
      "episode_len_mean: 109.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 76\n",
      "episodes_total: 51020\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4678086794912815\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01712544323149757\n",
      "        policy_loss: -0.08056729572204252\n",
      "        total_loss: 70.1486047077179\n",
      "        vf_explained_var: 0.32710290402173997\n",
      "        vf_loss: 70.21761273860932\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47165205533305804\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016495965908091725\n",
      "        policy_loss: -0.09713761147111655\n",
      "        total_loss: 57.293658615748086\n",
      "        vf_explained_var: 0.23428268899520238\n",
      "        vf_loss: 57.379661337534586\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4732862216234207\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01603602702941269\n",
      "        policy_loss: -0.07172425319130223\n",
      "        total_loss: 61.452979780832926\n",
      "        vf_explained_var: 0.2921151218811671\n",
      "        vf_loss: 61.513879731496175\n",
      "  num_agent_steps_sampled: 7780472\n",
      "  num_agent_steps_trained: 7780472\n",
      "  num_steps_sampled: 7780500\n",
      "  num_steps_trained: 7780500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 975\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.652380952380952\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.45714285714288\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.0\n",
      "  player_1: 28.333333333333336\n",
      "  player_2: 41.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.8766666666666674\n",
      "  player_1: -1.3233333333333328\n",
      "  player_2: 1.4466666666666674\n",
      "policy_reward_min:\n",
      "  player_0: -41.666666666666664\n",
      "  player_1: -38.0\n",
      "  player_2: -30.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08538905580721573\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2923334565700832\n",
      "  mean_inference_ms: 1.5890373941921854\n",
      "  mean_raw_obs_processing_ms: 0.2101419635577909\n",
      "time_since_restore: 15566.52797627449\n",
      "time_this_iter_s: 16.946555376052856\n",
      "time_total_s: 15566.52797627449\n",
      "timers:\n",
      "  learn_throughput: 535.816\n",
      "  learn_time_ms: 14893.159\n",
      "  load_throughput: 837827.689\n",
      "  load_time_ms: 9.525\n",
      "  sample_throughput: 500.056\n",
      "  sample_time_ms: 15958.224\n",
      "  update_time_ms: 5.579\n",
      "timestamp: 1643551812\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7780500\n",
      "training_iteration: 975\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7796431\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-10-43\n",
      "done: false\n",
      "episode_len_mean: 108.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 75\n",
      "episodes_total: 51166\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4640646928548813\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015269302668022722\n",
      "        policy_loss: -0.08755638871341943\n",
      "        total_loss: 61.1186842362086\n",
      "        vf_explained_var: 0.4670593892534574\n",
      "        vf_loss: 61.19593404054642\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46867029363910356\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016878143710223035\n",
      "        policy_loss: -0.05301997059956193\n",
      "        total_loss: 65.32458643754323\n",
      "        vf_explained_var: 0.23628054489692052\n",
      "        vf_loss: 65.36621332883834\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4461940228442351\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017055493951071792\n",
      "        policy_loss: -0.09236241299969454\n",
      "        total_loss: 47.6238534116745\n",
      "        vf_explained_var: 0.31381712714831034\n",
      "        vf_loss: 47.70470325390498\n",
      "  num_agent_steps_sampled: 7796431\n",
      "  num_agent_steps_trained: 7796431\n",
      "  num_steps_sampled: 7796460\n",
      "  num_steps_trained: 7796460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 977\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.247368421052634\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.44736842105266\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 46.33333333333333\n",
      "  player_1: 45.0\n",
      "  player_2: 30.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 0.42000000000000026\n",
      "  player_1: 0.2599999999999997\n",
      "  player_2: 2.32\n",
      "policy_reward_min:\n",
      "  player_0: -42.0\n",
      "  player_1: -45.66666666666667\n",
      "  player_2: -28.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08552472916921185\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2922235712773444\n",
      "  mean_inference_ms: 1.586477925235426\n",
      "  mean_raw_obs_processing_ms: 0.21029819213286483\n",
      "time_since_restore: 15597.27424120903\n",
      "time_this_iter_s: 15.129836559295654\n",
      "time_total_s: 15597.27424120903\n",
      "timers:\n",
      "  learn_throughput: 539.169\n",
      "  learn_time_ms: 14800.562\n",
      "  load_throughput: 886601.749\n",
      "  load_time_ms: 9.001\n",
      "  sample_throughput: 497.747\n",
      "  sample_time_ms: 16032.226\n",
      "  update_time_ms: 5.62\n",
      "timestamp: 1643551843\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7796460\n",
      "training_iteration: 977\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7812390\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-11-16\n",
      "done: false\n",
      "episode_len_mean: 109.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 73\n",
      "episodes_total: 51310\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4747497170170148\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016093641779540726\n",
      "        policy_loss: -0.05829488427688678\n",
      "        total_loss: 60.26696397304535\n",
      "        vf_explained_var: 0.38441390117009483\n",
      "        vf_loss: 60.31439555168152\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4712648752331734\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016392167361669485\n",
      "        policy_loss: -0.10969449542152385\n",
      "        total_loss: 73.01267840067545\n",
      "        vf_explained_var: 0.21715739260117212\n",
      "        vf_loss: 73.11130814552307\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4713101428250472\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016412379305468978\n",
      "        policy_loss: -0.062192127325882515\n",
      "        total_loss: 55.15862432638804\n",
      "        vf_explained_var: 0.319751013815403\n",
      "        vf_loss: 55.209738030433655\n",
      "  num_agent_steps_sampled: 7812390\n",
      "  num_agent_steps_trained: 7812390\n",
      "  num_steps_sampled: 7812420\n",
      "  num_steps_trained: 7812420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 979\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.375\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.43000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 34.333333333333336\n",
      "  player_1: 39.0\n",
      "  player_2: 31.666666666666664\n",
      "policy_reward_mean:\n",
      "  player_0: 2.586666666666667\n",
      "  player_1: -2.1533333333333333\n",
      "  player_2: 2.566666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -41.0\n",
      "  player_1: -56.0\n",
      "  player_2: -42.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08544965152401586\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2922175686786635\n",
      "  mean_inference_ms: 1.5880187220599191\n",
      "  mean_raw_obs_processing_ms: 0.21016443162458387\n",
      "time_since_restore: 15630.290674686432\n",
      "time_this_iter_s: 16.43446707725525\n",
      "time_total_s: 15630.290674686432\n",
      "timers:\n",
      "  learn_throughput: 541.589\n",
      "  learn_time_ms: 14734.412\n",
      "  load_throughput: 874057.496\n",
      "  load_time_ms: 9.13\n",
      "  sample_throughput: 497.797\n",
      "  sample_time_ms: 16030.637\n",
      "  update_time_ms: 5.561\n",
      "timestamp: 1643551876\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7812420\n",
      "training_iteration: 979\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7828351\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-11-47\n",
      "done: false\n",
      "episode_len_mean: 100.54\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 79\n",
      "episodes_total: 51471\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4902727113167445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01863049320056149\n",
      "        policy_loss: -0.08208282912770908\n",
      "        total_loss: 68.34442644437154\n",
      "        vf_explained_var: 0.2820477384328842\n",
      "        vf_loss: 68.41393366177877\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4716403619448344\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016621141184650506\n",
      "        policy_loss: -0.11344132240861654\n",
      "        total_loss: 53.975040799776714\n",
      "        vf_explained_var: 0.38825045069058733\n",
      "        vf_loss: 54.07726266860962\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46103431065877276\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016202482903427153\n",
      "        policy_loss: -0.061936511996512615\n",
      "        total_loss: 55.76475426991781\n",
      "        vf_explained_var: 0.3979837794105212\n",
      "        vf_loss: 55.81575428167979\n",
      "  num_agent_steps_sampled: 7828351\n",
      "  num_agent_steps_trained: 7828351\n",
      "  num_steps_sampled: 7828380\n",
      "  num_steps_trained: 7828380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 981\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.244444444444444\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.40000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 37.666666666666664\n",
      "  player_1: 31.0\n",
      "  player_2: 37.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.9566666666666668\n",
      "  player_1: -0.243333333333333\n",
      "  player_2: 1.2866666666666668\n",
      "policy_reward_min:\n",
      "  player_0: -31.0\n",
      "  player_1: -38.333333333333336\n",
      "  player_2: -32.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08560484393038878\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29210374675133005\n",
      "  mean_inference_ms: 1.5873889861364738\n",
      "  mean_raw_obs_processing_ms: 0.21023020497080502\n",
      "time_since_restore: 15660.60065484047\n",
      "time_this_iter_s: 14.580507755279541\n",
      "time_total_s: 15660.60065484047\n",
      "timers:\n",
      "  learn_throughput: 551.693\n",
      "  learn_time_ms: 14464.567\n",
      "  load_throughput: 906081.119\n",
      "  load_time_ms: 8.807\n",
      "  sample_throughput: 500.088\n",
      "  sample_time_ms: 15957.197\n",
      "  update_time_ms: 5.642\n",
      "timestamp: 1643551907\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7828380\n",
      "training_iteration: 981\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7844310\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-12-17\n",
      "done: false\n",
      "episode_len_mean: 103.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 71\n",
      "episodes_total: 51616\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46421236301461855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016503542132384343\n",
      "        policy_loss: -0.07155072846760352\n",
      "        total_loss: 75.60930064519246\n",
      "        vf_explained_var: 0.2721375996867816\n",
      "        vf_loss: 75.66971139272054\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46939130847652755\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01687261433695312\n",
      "        policy_loss: -0.10036470965792735\n",
      "        total_loss: 66.88186220486959\n",
      "        vf_explained_var: 0.33315438906351724\n",
      "        vf_loss: 66.97083794911703\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4773299498856068\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01661625209966966\n",
      "        policy_loss: -0.04934743953713527\n",
      "        total_loss: 52.11739461263021\n",
      "        vf_explained_var: 0.3336987394094467\n",
      "        vf_loss: 52.155526264508566\n",
      "  num_agent_steps_sampled: 7844310\n",
      "  num_agent_steps_trained: 7844310\n",
      "  num_steps_sampled: 7844340\n",
      "  num_steps_trained: 7844340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 983\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.200000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.47894736842107\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 41.666666666666664\n",
      "  player_1: 47.33333333333333\n",
      "  player_2: 29.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.963333333333333\n",
      "  player_1: -1.9366666666666665\n",
      "  player_2: 1.9733333333333325\n",
      "policy_reward_min:\n",
      "  player_0: -34.333333333333336\n",
      "  player_1: -33.666666666666664\n",
      "  player_2: -39.333333333333336\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08543349053310952\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29216782678067393\n",
      "  mean_inference_ms: 1.5876854547907187\n",
      "  mean_raw_obs_processing_ms: 0.21016989978906056\n",
      "time_since_restore: 15691.075501680374\n",
      "time_this_iter_s: 15.42460322380066\n",
      "time_total_s: 15691.075501680374\n",
      "timers:\n",
      "  learn_throughput: 551.368\n",
      "  learn_time_ms: 14473.085\n",
      "  load_throughput: 978665.21\n",
      "  load_time_ms: 8.154\n",
      "  sample_throughput: 507.325\n",
      "  sample_time_ms: 15729.561\n",
      "  update_time_ms: 5.68\n",
      "timestamp: 1643551937\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7844340\n",
      "training_iteration: 983\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7860270\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-12-48\n",
      "done: false\n",
      "episode_len_mean: 105.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 79\n",
      "episodes_total: 51764\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4863019723196824\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016071105863913242\n",
      "        policy_loss: -0.08085837146888177\n",
      "        total_loss: 61.00585335095723\n",
      "        vf_explained_var: 0.32555255919694903\n",
      "        vf_loss: 61.075863852500916\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4822809206446012\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018241956563734524\n",
      "        policy_loss: -0.09612994676455855\n",
      "        total_loss: 55.97906260490417\n",
      "        vf_explained_var: 0.3140846330920855\n",
      "        vf_loss: 56.062879152297974\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4681421376268069\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016454832222792294\n",
      "        policy_loss: -0.07017721004784107\n",
      "        total_loss: 54.014150301615395\n",
      "        vf_explained_var: 0.45638569255669914\n",
      "        vf_loss: 54.07322046438853\n",
      "  num_agent_steps_sampled: 7860270\n",
      "  num_agent_steps_trained: 7860270\n",
      "  num_steps_sampled: 7860300\n",
      "  num_steps_trained: 7860300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 985\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.905263157894737\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.44736842105266\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 29.666666666666664\n",
      "  player_1: 29.666666666666664\n",
      "  player_2: 35.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 2.0533333333333332\n",
      "  player_1: 0.13333333333333341\n",
      "  player_2: 0.8133333333333336\n",
      "policy_reward_min:\n",
      "  player_0: -43.33333333333333\n",
      "  player_1: -31.666666666666664\n",
      "  player_2: -38.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08547350466864391\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2923844544996979\n",
      "  mean_inference_ms: 1.5879289759344914\n",
      "  mean_raw_obs_processing_ms: 0.21020071205190086\n",
      "time_since_restore: 15721.960792779922\n",
      "time_this_iter_s: 15.352905035018921\n",
      "time_total_s: 15721.960792779922\n",
      "timers:\n",
      "  learn_throughput: 556.576\n",
      "  learn_time_ms: 14337.66\n",
      "  load_throughput: 984526.274\n",
      "  load_time_ms: 8.105\n",
      "  sample_throughput: 507.173\n",
      "  sample_time_ms: 15734.273\n",
      "  update_time_ms: 5.711\n",
      "timestamp: 1643551968\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7860300\n",
      "training_iteration: 985\n",
      "trial_id: default\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0130 15:12:49.009127654   11551 backup_poller.cc:134]       Run client channel backup poller: {\"created\":\"@1643551969.009091716\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1643551969.009085233\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 7876230\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-13-20\n",
      "done: false\n",
      "episode_len_mean: 104.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 79\n",
      "episodes_total: 51913\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4677635258436203\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0165134158967993\n",
      "        policy_loss: -0.10777415073476732\n",
      "        total_loss: 57.29183735370636\n",
      "        vf_explained_var: 0.44846080015103024\n",
      "        vf_loss: 57.38846488952637\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47894075165192285\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01765352508732273\n",
      "        policy_loss: -0.07581834174071748\n",
      "        total_loss: 62.467944377263386\n",
      "        vf_explained_var: 0.33289183805386224\n",
      "        vf_loss: 62.53184647242228\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46627314950029053\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01710215611887482\n",
      "        policy_loss: -0.07472223123845954\n",
      "        total_loss: 49.68894761721293\n",
      "        vf_explained_var: 0.33064780781666436\n",
      "        vf_loss: 49.75212600390116\n",
      "  num_agent_steps_sampled: 7876230\n",
      "  num_agent_steps_trained: 7876230\n",
      "  num_steps_sampled: 7876260\n",
      "  num_steps_trained: 7876260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 987\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.770000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.47500000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 32.666666666666664\n",
      "  player_1: 37.0\n",
      "  player_2: 36.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: -0.033333333333333784\n",
      "  player_1: 2.7066666666666657\n",
      "  player_2: 0.32666666666666655\n",
      "policy_reward_min:\n",
      "  player_0: -42.0\n",
      "  player_1: -40.0\n",
      "  player_2: -28.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08559495573332818\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29223733256944695\n",
      "  mean_inference_ms: 1.5868664591366917\n",
      "  mean_raw_obs_processing_ms: 0.21030569755869127\n",
      "time_since_restore: 15753.99773311615\n",
      "time_this_iter_s: 16.282540321350098\n",
      "time_total_s: 15753.99773311615\n",
      "timers:\n",
      "  learn_throughput: 551.71\n",
      "  learn_time_ms: 14464.129\n",
      "  load_throughput: 966954.387\n",
      "  load_time_ms: 8.253\n",
      "  sample_throughput: 511.819\n",
      "  sample_time_ms: 15591.446\n",
      "  update_time_ms: 5.651\n",
      "timestamp: 1643552000\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7876260\n",
      "training_iteration: 987\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7892190\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-13-50\n",
      "done: false\n",
      "episode_len_mean: 110.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 75\n",
      "episodes_total: 52061\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4731398853162924\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01794183353507227\n",
      "        policy_loss: -0.07465133015376826\n",
      "        total_loss: 58.207392848332724\n",
      "        vf_explained_var: 0.31201035767793656\n",
      "        vf_loss: 58.26993359883626\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45833351274331413\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016519804708342842\n",
      "        policy_loss: -0.07246373851162692\n",
      "        total_loss: 49.951093274752296\n",
      "        vf_explained_var: 0.2401835326353709\n",
      "        vf_loss: 50.01240618546804\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4527964337170124\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01621402837619712\n",
      "        policy_loss: -0.08146303218634178\n",
      "        total_loss: 58.44648175398509\n",
      "        vf_explained_var: 0.17402760962645214\n",
      "        vf_loss: 58.517000285784405\n",
      "  num_agent_steps_sampled: 7892190\n",
      "  num_agent_steps_trained: 7892190\n",
      "  num_steps_sampled: 7892220\n",
      "  num_steps_trained: 7892220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 989\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.777777777777779\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.47222222222224\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.0\n",
      "  player_1: 33.666666666666664\n",
      "  player_2: 27.0\n",
      "policy_reward_mean:\n",
      "  player_0: 4.6866666666666665\n",
      "  player_1: -1.5433333333333334\n",
      "  player_2: -0.14333333333333345\n",
      "policy_reward_min:\n",
      "  player_0: -25.666666666666664\n",
      "  player_1: -32.666666666666664\n",
      "  player_2: -43.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08550855739267918\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29217724941621415\n",
      "  mean_inference_ms: 1.5868881477182597\n",
      "  mean_raw_obs_processing_ms: 0.21035264755822852\n",
      "time_since_restore: 15783.762510538101\n",
      "time_this_iter_s: 14.698934555053711\n",
      "time_total_s: 15783.762510538101\n",
      "timers:\n",
      "  learn_throughput: 564.56\n",
      "  learn_time_ms: 14134.905\n",
      "  load_throughput: 921276.993\n",
      "  load_time_ms: 8.662\n",
      "  sample_throughput: 513.28\n",
      "  sample_time_ms: 15547.057\n",
      "  update_time_ms: 5.666\n",
      "timestamp: 1643552030\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7892220\n",
      "training_iteration: 989\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7908151\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-14-21\n",
      "done: false\n",
      "episode_len_mean: 103.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 73\n",
      "episodes_total: 52217\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4570831928153833\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015841461961287752\n",
      "        policy_loss: -0.1119470176504304\n",
      "        total_loss: 61.62535919984182\n",
      "        vf_explained_var: 0.4226125686367353\n",
      "        vf_loss: 61.72661308288574\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4692250239352385\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017836673517558667\n",
      "        policy_loss: -0.06774618805230906\n",
      "        total_loss: 65.57596863428752\n",
      "        vf_explained_var: 0.2680321858326594\n",
      "        vf_loss: 65.63167508602142\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4794916971027851\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017358454290224472\n",
      "        policy_loss: -0.05768720379720132\n",
      "        total_loss: 67.64296944936116\n",
      "        vf_explained_var: 0.4658853793144226\n",
      "        vf_loss: 67.68893954118093\n",
      "  num_agent_steps_sampled: 7908151\n",
      "  num_agent_steps_trained: 7908151\n",
      "  num_steps_sampled: 7908180\n",
      "  num_steps_trained: 7908180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 991\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.0\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.46000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 38.333333333333336\n",
      "  player_1: 30.666666666666664\n",
      "  player_2: 31.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.5\n",
      "  player_1: -1.0700000000000003\n",
      "  player_2: 1.57\n",
      "policy_reward_min:\n",
      "  player_0: -40.333333333333336\n",
      "  player_1: -34.666666666666664\n",
      "  player_2: -36.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0854768857169474\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29210696714674844\n",
      "  mean_inference_ms: 1.5873084219987614\n",
      "  mean_raw_obs_processing_ms: 0.2101139139908127\n",
      "time_since_restore: 15814.833458185196\n",
      "time_this_iter_s: 16.231322288513184\n",
      "time_total_s: 15814.833458185196\n",
      "timers:\n",
      "  learn_throughput: 561.331\n",
      "  learn_time_ms: 14216.21\n",
      "  load_throughput: 943600.45\n",
      "  load_time_ms: 8.457\n",
      "  sample_throughput: 522.145\n",
      "  sample_time_ms: 15283.106\n",
      "  update_time_ms: 5.591\n",
      "timestamp: 1643552061\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7908180\n",
      "training_iteration: 991\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7924111\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-14-52\n",
      "done: false\n",
      "episode_len_mean: 98.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 80\n",
      "episodes_total: 52372\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4590494977434476\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015288559903500526\n",
      "        policy_loss: -0.0777983973051111\n",
      "        total_loss: 88.43382336298625\n",
      "        vf_explained_var: 0.4273149327437083\n",
      "        vf_loss: 88.5013021628062\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47602177957693736\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016501078432544697\n",
      "        policy_loss: -0.09523263032237689\n",
      "        total_loss: 62.74534096399943\n",
      "        vf_explained_var: 0.4603054482738177\n",
      "        vf_loss: 62.82943531990051\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4730391389628251\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01674870760792146\n",
      "        policy_loss: -0.06787511514499783\n",
      "        total_loss: 95.18589001337688\n",
      "        vf_explained_var: 0.3923629540205002\n",
      "        vf_loss: 95.24245981216431\n",
      "  num_agent_steps_sampled: 7924111\n",
      "  num_agent_steps_trained: 7924111\n",
      "  num_steps_sampled: 7924140\n",
      "  num_steps_trained: 7924140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 993\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.047368421052633\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.44736842105266\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 36.0\n",
      "  player_1: 37.0\n",
      "  player_2: 40.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 2.1966666666666668\n",
      "  player_1: 0.06666666666666679\n",
      "  player_2: 0.7366666666666662\n",
      "policy_reward_min:\n",
      "  player_0: -47.333333333333336\n",
      "  player_1: -40.666666666666664\n",
      "  player_2: -46.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08565923232370429\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29207994696842177\n",
      "  mean_inference_ms: 1.58655100344481\n",
      "  mean_raw_obs_processing_ms: 0.2102111917129866\n",
      "time_since_restore: 15845.456738948822\n",
      "time_this_iter_s: 14.896277904510498\n",
      "time_total_s: 15845.456738948822\n",
      "timers:\n",
      "  learn_throughput: 560.734\n",
      "  learn_time_ms: 14231.351\n",
      "  load_throughput: 902064.066\n",
      "  load_time_ms: 8.846\n",
      "  sample_throughput: 514.296\n",
      "  sample_time_ms: 15516.368\n",
      "  update_time_ms: 5.564\n",
      "timestamp: 1643552092\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7924140\n",
      "training_iteration: 993\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7940070\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-15-23\n",
      "done: false\n",
      "episode_len_mean: 109.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000007\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 73\n",
      "episodes_total: 52516\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4702123059829076\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0175626451610151\n",
      "        policy_loss: -0.06860425805207342\n",
      "        total_loss: 59.14416690508524\n",
      "        vf_explained_var: 0.19263732741276424\n",
      "        vf_loss: 59.200916334788005\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.46370300342639287\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017118488573340567\n",
      "        policy_loss: -0.10576633679059645\n",
      "        total_loss: 44.28825641314189\n",
      "        vf_explained_var: 0.47360840181509656\n",
      "        vf_loss: 44.3824678405126\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4612457499901454\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018197409629382263\n",
      "        policy_loss: -0.081679442464374\n",
      "        total_loss: 57.27884064356486\n",
      "        vf_explained_var: 0.2738460145394007\n",
      "        vf_loss: 57.34823667367299\n",
      "  num_agent_steps_sampled: 7940070\n",
      "  num_agent_steps_trained: 7940070\n",
      "  num_steps_sampled: 7940100\n",
      "  num_steps_trained: 7940100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 995\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.61111111111111\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.45000000000002\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 40.666666666666664\n",
      "  player_1: 32.0\n",
      "  player_2: 34.0\n",
      "policy_reward_mean:\n",
      "  player_0: 0.4033333333333337\n",
      "  player_1: 0.823333333333333\n",
      "  player_2: 1.7733333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -45.0\n",
      "  player_1: -35.333333333333336\n",
      "  player_2: -31.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08537911769148845\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.2917850717738716\n",
      "  mean_inference_ms: 1.585895284163184\n",
      "  mean_raw_obs_processing_ms: 0.2102044889827991\n",
      "time_since_restore: 15876.220363140106\n",
      "time_this_iter_s: 14.68276309967041\n",
      "time_total_s: 15876.220363140106\n",
      "timers:\n",
      "  learn_throughput: 561.097\n",
      "  learn_time_ms: 14222.15\n",
      "  load_throughput: 947403.837\n",
      "  load_time_ms: 8.423\n",
      "  sample_throughput: 514.004\n",
      "  sample_time_ms: 15525.165\n",
      "  update_time_ms: 5.592\n",
      "timestamp: 1643552123\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7940100\n",
      "training_iteration: 995\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7956032\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-15-53\n",
      "done: false\n",
      "episode_len_mean: 104.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 77\n",
      "episodes_total: 52670\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.47256463368733725\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016401019474833446\n",
      "        policy_loss: -0.06809553178648153\n",
      "        total_loss: 53.45140145460765\n",
      "        vf_explained_var: 0.2556872659921646\n",
      "        vf_loss: 53.50842664241791\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4770627616345882\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01632161253637624\n",
      "        policy_loss: -0.09440711962990463\n",
      "        total_loss: 71.74964320818583\n",
      "        vf_explained_var: 0.4115595187743505\n",
      "        vf_loss: 71.83303335030874\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.4601076238354047\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016216185236644757\n",
      "        policy_loss: -0.07368784741188089\n",
      "        total_loss: 81.02116543134053\n",
      "        vf_explained_var: 0.2531102158625921\n",
      "        vf_loss: 81.08390753746033\n",
      "  num_agent_steps_sampled: 7956032\n",
      "  num_agent_steps_trained: 7956032\n",
      "  num_steps_sampled: 7956060\n",
      "  num_steps_trained: 7956060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 997\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.777777777777779\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.47222222222224\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 48.0\n",
      "  player_1: 37.0\n",
      "  player_2: 49.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.94\n",
      "  player_1: -0.24000000000000005\n",
      "  player_2: 0.30000000000000027\n",
      "policy_reward_min:\n",
      "  player_0: -29.0\n",
      "  player_1: -66.66666666666666\n",
      "  player_2: -38.666666666666664\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08552156884199844\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29192889825128576\n",
      "  mean_inference_ms: 1.58618952124555\n",
      "  mean_raw_obs_processing_ms: 0.21021417575519707\n",
      "time_since_restore: 15906.571560144424\n",
      "time_this_iter_s: 14.80927324295044\n",
      "time_total_s: 15906.571560144424\n",
      "timers:\n",
      "  learn_throughput: 567.589\n",
      "  learn_time_ms: 14059.459\n",
      "  load_throughput: 982927.411\n",
      "  load_time_ms: 8.119\n",
      "  sample_throughput: 517.006\n",
      "  sample_time_ms: 15435.035\n",
      "  update_time_ms: 5.654\n",
      "timestamp: 1643552153\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7956060\n",
      "training_iteration: 997\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 7971991\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_15-16-24\n",
      "done: false\n",
      "episode_len_mean: 98.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000018\n",
      "episode_reward_min: 2.999999999999993\n",
      "episodes_this_iter: 80\n",
      "episodes_total: 52829\n",
      "experiment_id: 3e2b5b7f5b6d4c4695fedd0410301df6\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.44194677730401355\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016223439916366638\n",
      "        policy_loss: -0.07390123826141158\n",
      "        total_loss: 54.5713889503479\n",
      "        vf_explained_var: 0.2527655423680941\n",
      "        vf_loss: 54.63433926741282\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.45897004087766013\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017943730649069872\n",
      "        policy_loss: -0.09708885961134607\n",
      "        total_loss: 49.19683332284291\n",
      "        vf_explained_var: 0.48608780235052107\n",
      "        vf_loss: 49.281810348828635\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.467799551486969\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017897645420697092\n",
      "        policy_loss: -0.0723096497884641\n",
      "        total_loss: 62.225877628326415\n",
      "        vf_explained_var: 0.3450049551328023\n",
      "        vf_loss: 62.28610655466716\n",
      "  num_agent_steps_sampled: 7971991\n",
      "  num_agent_steps_trained: 7971991\n",
      "  num_steps_sampled: 7972020\n",
      "  num_steps_trained: 7972020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 999\n",
      "node_ip: 172.20.107.113\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.942105263157893\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 65.49473684210528\n",
      "  vram_util_percent0: 0.37418619791666674\n",
      "pid: 299\n",
      "policy_reward_max:\n",
      "  player_0: 35.333333333333336\n",
      "  player_1: 28.0\n",
      "  player_2: 37.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 4.133333333333335\n",
      "  player_1: -0.7466666666666665\n",
      "  player_2: -0.386666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -54.33333333333333\n",
      "  player_1: -46.666666666666664\n",
      "  player_2: -40.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08559463717579732\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.29174059344061437\n",
      "  mean_inference_ms: 1.5847427116964434\n",
      "  mean_raw_obs_processing_ms: 0.2102196641730363\n",
      "time_since_restore: 15937.437987327576\n",
      "time_this_iter_s: 15.410277843475342\n",
      "time_total_s: 15937.437987327576\n",
      "timers:\n",
      "  learn_throughput: 563.149\n",
      "  learn_time_ms: 14170.313\n",
      "  load_throughput: 987427.859\n",
      "  load_time_ms: 8.082\n",
      "  sample_throughput: 520.595\n",
      "  sample_time_ms: 15328.601\n",
      "  update_time_ms: 5.613\n",
      "timestamp: 1643552184\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7972020\n",
      "training_iteration: 999\n",
      "trial_id: default\n",
      "\n",
      "training done, because max_iters 1000 reached\n",
      "Finished training. Running manual test/inference loop.\n"
     ]
    }
   ],
   "source": [
    "# run manual training loop and print results after each iteration\n",
    "max_steps = 1e8\n",
    "max_iters = 1000\n",
    "for iters in range(max_iters):\n",
    "    result = trainer.train()\n",
    "    if iters % 2 ==0:\n",
    "        print(pretty_print(result))\n",
    "    # stop training if the target train steps or reward are reached\n",
    "    if result[\"timesteps_total\"] >= max_steps:\n",
    "        print(f\"training done, because max_steps {max_steps} {result['timesteps_total']} reached\")\n",
    "        break\n",
    "else:\n",
    "    print(f\"training done, because max_iters {max_iters} reached\")\n",
    "# manual test loop\n",
    "print(\"Finished training. Running manual test/inference loop.\")\n",
    "# prepare environment with max 10 steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "  warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "/home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "  warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n"
     ]
    }
   ],
   "source": [
    "from rlskyjo.game.skyjo import SkyjoGame\n",
    "\n",
    "env = PettingZooEnv(env_creator())\n",
    "obs = env.reset()\n",
    "done = {'__all__': False}\n",
    "# run one iteration until done\n",
    "\n",
    "def rec_deep(env, attiribute = \"table\"):\n",
    "    if hasattr(env, attiribute):\n",
    "        return getattr(env, attiribute)\n",
    "    else:\n",
    "        return rec_deep(env.env, attiribute)\n",
    "\n",
    "stop = False\n",
    "for i in range(2000):\n",
    "    for i in range(1000):\n",
    "        if done[\"__all__\"]:\n",
    "            # print(\"game done\")\n",
    "            break\n",
    "        # get agent from current observation\n",
    "        agent = list(obs.keys())[0]\n",
    "\n",
    "        # format observation dict\n",
    "        # print(obs)\n",
    "        obs = obs[agent]\n",
    "        # env.render()\n",
    "        \n",
    "        # get deterministic action\n",
    "        # trainer.compute_single_action(obs, policy_id=agent)\n",
    "        policy = trainer.get_policy(policy_id=agent)\n",
    "        action_exploration_policy, _, action_info = policy.compute_single_action(obs)\n",
    "        # logits = action_info['action_dist_inputs']\n",
    "        # action = logits.argmax()\n",
    "        # \n",
    "        action = action_exploration_policy\n",
    "        # print(\"agent \", agent, \" action \", SkyjoGame.render_action_explainer(action))\n",
    "        obs, reward, done, _ = env.step({agent: action})\n",
    "        # observations contain original observations and the action mask\n",
    "        # print(f\"Obs: {obs}, Action: {action}, done: {done}\")\n",
    "\n",
    "    table = rec_deep(env, \"table\")\n",
    "\n",
    "    if any(table.game_metrics[\"num_refunded\"]):\n",
    "        env.render()\n",
    "        print(table.game_metrics)\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([76.0, 41.0, 21.0],\n",
       " array([[-1,  9,  7, -2,  4,  2,  0,  7,  4,  0,  3,  5],\n",
       "        [ 0,  7,  1, 10,  7,  2,  0,  6,  1, -1, -1,  9],\n",
       "        [-1,  6,  5, -2,  4,  2,  1,  4, -2,  3, -2,  3]], dtype=int8))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.game_metrics[\"num_refunded\"]\n",
    "table._evaluate_game(table.players_cards, 0, score_penalty=2), table.players_cards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent  player_0  action  place card (1) - col:0 row:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 'player_0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = \"player_0\"\n",
    "obs = { agent: {'observations': np.array([27,  0,  3,  \n",
    "        3,  6,  6,  3,  6,  4,  4,  6,  2,  2,  6,  4,  5,  2, 4, \n",
    "        12, # holding card\n",
    "        # player 0\n",
    "        1, 15, 15, \n",
    "        -1, 4, -2, \n",
    "        12, 15, 12,\n",
    "        1, 0, 0,\n",
    "        # player 1\n",
    "         9, 0, 2,\n",
    "         1, -1, 3,\n",
    "         1,-2, 5,\n",
    "         -2,15,11, \n",
    "        # player 2\n",
    "        -1, 9, 6,\n",
    "        2, 3, 0,\n",
    "        1, 0, 1,\n",
    "        7, 7,15\n",
    "        ], dtype=np.int8), \n",
    " 'action_mask': np.array(\n",
    "     [ # put\n",
    "      1, 1, 1, \n",
    "      1, 1, 1, \n",
    "      1, 1, 1, \n",
    "      1, 1, 1,\n",
    "      # reveal\n",
    "      0, 1, 1, \n",
    "      0, 0, 0, \n",
    "      0, 1, 0, \n",
    "      0, 0, 0,\n",
    "      # draw\n",
    "      1, 1], dtype=np.int8)}}\n",
    "\n",
    "action = trainer.compute_single_action(obs[agent], policy_id=agent)\n",
    "print(\"agent \", agent, \" action \", SkyjoGame.render_action_explainer(action))\n",
    "action, agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 16:59:07 (running for 00:00:00.13)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 16:59:12 (running for 00:00:05.13)<br>Memory usage on this node: 16.7/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[36m(scheduler +6h10m10s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h10m10s)\u001b[0m Error: No available node types can fulfill resource request {'GPU': 1.0, 'CPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 16:59:18 (running for 00:00:10.13)<br>Memory usage on this node: 16.7/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 16:59:23 (running for 00:00:15.13)<br>Memory usage on this node: 16.7/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 16:59:28 (running for 00:00:20.14)<br>Memory usage on this node: 16.7/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 16:59:33 (running for 00:00:25.14)<br>Memory usage on this node: 16.7/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 16:59:38 (running for 00:00:30.14)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 16:59:43 (running for 00:00:35.14)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 16:59:48 (running for 00:00:40.15)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h10m45s)\u001b[0m Error: No available node types can fulfill resource request {'CPU': 1.0, 'GPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 16:59:53 (running for 00:00:45.15)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 16:59:58 (running for 00:00:50.15)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:00:03 (running for 00:00:55.15)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 17:00:08,002\tWARNING trial_executor.py:312 -- Ignore this message if the cluster is autoscaling. You asked for 16.0 cpu and 1.0 gpu per trial, but the cluster only has 16.0 cpu and 0 gpu. Stop the tuning job and adjust the resources requested per trial (possibly via `resources_per_trial` or via `num_workers` for rllib) and/or add more resources to your Ray runtime.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:00:08 (running for 00:01:00.15)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:00:13 (running for 00:01:05.16)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:00:18 (running for 00:01:10.16)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:00:23 (running for 00:01:15.16)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h11m20s)\u001b[0m Error: No available node types can fulfill resource request {'GPU': 1.0, 'CPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:00:28 (running for 00:01:20.16)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:00:33 (running for 00:01:25.16)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:00:38 (running for 00:01:30.17)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:00:43 (running for 00:01:35.17)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:00:48 (running for 00:01:40.17)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:00:53 (running for 00:01:45.17)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:00:58 (running for 00:01:50.17)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h11m55s)\u001b[0m Error: No available node types can fulfill resource request {'CPU': 1.0, 'GPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:01:03 (running for 00:01:55.18)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 17:01:08,004\tWARNING trial_executor.py:312 -- Ignore this message if the cluster is autoscaling. You asked for 16.0 cpu and 1.0 gpu per trial, but the cluster only has 16.0 cpu and 0 gpu. Stop the tuning job and adjust the resources requested per trial (possibly via `resources_per_trial` or via `num_workers` for rllib) and/or add more resources to your Ray runtime.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:01:08 (running for 00:02:00.18)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:01:13 (running for 00:02:05.18)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:01:18 (running for 00:02:10.18)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:01:23 (running for 00:02:15.18)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:01:28 (running for 00:02:20.19)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:01:33 (running for 00:02:25.19)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h12m31s)\u001b[0m Error: No available node types can fulfill resource request {'CPU': 1.0, 'GPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:01:38 (running for 00:02:30.19)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:01:43 (running for 00:02:35.19)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:01:48 (running for 00:02:40.20)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:01:53 (running for 00:02:45.20)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:01:58 (running for 00:02:50.20)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:02:03 (running for 00:02:55.20)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 17:02:08,005\tWARNING trial_executor.py:312 -- Ignore this message if the cluster is autoscaling. You asked for 16.0 cpu and 1.0 gpu per trial, but the cluster only has 16.0 cpu and 0 gpu. Stop the tuning job and adjust the resources requested per trial (possibly via `resources_per_trial` or via `num_workers` for rllib) and/or add more resources to your Ray runtime.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:02:08 (running for 00:03:00.21)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h13m6s)\u001b[0m Error: No available node types can fulfill resource request {'GPU': 1.0, 'CPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:02:13 (running for 00:03:05.21)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:02:18 (running for 00:03:10.21)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:02:23 (running for 00:03:15.21)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:02:28 (running for 00:03:20.21)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:02:33 (running for 00:03:25.21)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:02:38 (running for 00:03:30.22)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:02:43 (running for 00:03:35.22)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h13m41s)\u001b[0m Error: No available node types can fulfill resource request {'GPU': 1.0, 'CPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:02:48 (running for 00:03:40.22)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:02:53 (running for 00:03:45.22)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:02:58 (running for 00:03:50.23)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:03:03 (running for 00:03:55.23)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 17:03:08,006\tWARNING trial_executor.py:312 -- Ignore this message if the cluster is autoscaling. You asked for 16.0 cpu and 1.0 gpu per trial, but the cluster only has 16.0 cpu and 0 gpu. Stop the tuning job and adjust the resources requested per trial (possibly via `resources_per_trial` or via `num_workers` for rllib) and/or add more resources to your Ray runtime.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:03:08 (running for 00:04:00.23)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:03:13 (running for 00:04:05.23)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:03:18 (running for 00:04:10.24)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h14m16s)\u001b[0m Error: No available node types can fulfill resource request {'CPU': 1.0, 'GPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:03:23 (running for 00:04:15.24)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:03:28 (running for 00:04:20.24)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:03:33 (running for 00:04:25.25)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:03:38 (running for 00:04:30.25)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:03:43 (running for 00:04:35.25)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:03:48 (running for 00:04:40.25)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:03:53 (running for 00:04:45.25)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h14m51s)\u001b[0m Error: No available node types can fulfill resource request {'CPU': 1.0, 'GPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:03:58 (running for 00:04:50.26)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:04:03 (running for 00:04:55.26)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 17:04:08,007\tWARNING trial_executor.py:312 -- Ignore this message if the cluster is autoscaling. You asked for 16.0 cpu and 1.0 gpu per trial, but the cluster only has 16.0 cpu and 0 gpu. Stop the tuning job and adjust the resources requested per trial (possibly via `resources_per_trial` or via `num_workers` for rllib) and/or add more resources to your Ray runtime.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:04:08 (running for 00:05:00.26)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:04:13 (running for 00:05:05.26)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:04:18 (running for 00:05:10.26)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:04:23 (running for 00:05:15.27)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:04:28 (running for 00:05:20.27)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h15m26s)\u001b[0m Error: No available node types can fulfill resource request {'CPU': 1.0, 'GPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:04:33 (running for 00:05:25.28)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:04:38 (running for 00:05:30.28)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:04:43 (running for 00:05:35.28)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:04:48 (running for 00:05:40.28)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:04:53 (running for 00:05:45.28)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:04:58 (running for 00:05:50.28)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:05:03 (running for 00:05:55.28)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h16m1s)\u001b[0m Error: No available node types can fulfill resource request {'CPU': 1.0, 'GPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 17:05:08,008\tWARNING trial_executor.py:312 -- Ignore this message if the cluster is autoscaling. You asked for 16.0 cpu and 1.0 gpu per trial, but the cluster only has 16.0 cpu and 0 gpu. Stop the tuning job and adjust the resources requested per trial (possibly via `resources_per_trial` or via `num_workers` for rllib) and/or add more resources to your Ray runtime.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:05:08 (running for 00:06:00.28)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:05:13 (running for 00:06:05.28)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:05:18 (running for 00:06:10.29)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:05:23 (running for 00:06:15.29)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:05:28 (running for 00:06:20.29)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:05:33 (running for 00:06:25.30)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:05:38 (running for 00:06:30.30)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h16m36s)\u001b[0m Error: No available node types can fulfill resource request {'CPU': 1.0, 'GPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:05:43 (running for 00:06:35.30)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:05:48 (running for 00:06:40.30)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:05:53 (running for 00:06:45.30)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:05:58 (running for 00:06:50.32)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:06:03 (running for 00:06:55.32)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 17:06:08,010\tWARNING trial_executor.py:312 -- Ignore this message if the cluster is autoscaling. You asked for 16.0 cpu and 1.0 gpu per trial, but the cluster only has 16.0 cpu and 0 gpu. Stop the tuning job and adjust the resources requested per trial (possibly via `resources_per_trial` or via `num_workers` for rllib) and/or add more resources to your Ray runtime.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:06:08 (running for 00:07:00.33)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:06:13 (running for 00:07:05.33)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h17m11s)\u001b[0m Error: No available node types can fulfill resource request {'CPU': 1.0, 'GPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:06:18 (running for 00:07:10.33)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:06:23 (running for 00:07:15.33)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:06:28 (running for 00:07:20.34)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:06:33 (running for 00:07:25.34)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:06:38 (running for 00:07:30.34)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:06:43 (running for 00:07:35.34)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:06:48 (running for 00:07:40.34)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h17m46s)\u001b[0m Error: No available node types can fulfill resource request {'GPU': 1.0, 'CPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:06:53 (running for 00:07:45.34)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:06:58 (running for 00:07:50.35)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:07:03 (running for 00:07:55.35)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 17:07:08,012\tWARNING trial_executor.py:312 -- Ignore this message if the cluster is autoscaling. You asked for 16.0 cpu and 1.0 gpu per trial, but the cluster only has 16.0 cpu and 0 gpu. Stop the tuning job and adjust the resources requested per trial (possibly via `resources_per_trial` or via `num_workers` for rllib) and/or add more resources to your Ray runtime.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:07:08 (running for 00:08:00.36)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:07:13 (running for 00:08:05.36)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:07:18 (running for 00:08:10.36)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:07:23 (running for 00:08:15.36)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h18m21s)\u001b[0m Error: No available node types can fulfill resource request {'CPU': 1.0, 'GPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:07:28 (running for 00:08:20.37)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:07:33 (running for 00:08:25.37)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:07:38 (running for 00:08:30.39)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:07:43 (running for 00:08:35.40)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:07:48 (running for 00:08:40.40)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:07:53 (running for 00:08:45.40)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:07:58 (running for 00:08:50.40)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h18m56s)\u001b[0m Error: No available node types can fulfill resource request {'CPU': 1.0, 'GPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:08:03 (running for 00:08:55.40)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 17:08:08,014\tWARNING trial_executor.py:312 -- Ignore this message if the cluster is autoscaling. You asked for 16.0 cpu and 1.0 gpu per trial, but the cluster only has 16.0 cpu and 0 gpu. Stop the tuning job and adjust the resources requested per trial (possibly via `resources_per_trial` or via `num_workers` for rllib) and/or add more resources to your Ray runtime.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:08:08 (running for 00:09:00.41)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:08:13 (running for 00:09:05.41)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:08:18 (running for 00:09:10.41)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:08:23 (running for 00:09:15.41)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:08:28 (running for 00:09:20.42)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:08:33 (running for 00:09:25.42)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h19m31s)\u001b[0m Error: No available node types can fulfill resource request {'GPU': 1.0, 'CPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:08:38 (running for 00:09:30.43)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:08:43 (running for 00:09:35.43)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:08:48 (running for 00:09:40.43)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:08:53 (running for 00:09:45.43)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:08:58 (running for 00:09:50.44)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:09:03 (running for 00:09:55.44)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 17:09:05,975\tWARNING util.py:165 -- The `start_trial` operation took 0.537 s, which may be a performance bottleneck.\n",
      "2022-01-30 17:09:08,015\tWARNING trial_executor.py:312 -- Ignore this message if the cluster is autoscaling. You asked for 16.0 cpu and 1.0 gpu per trial, but the cluster only has 16.0 cpu and 0 gpu. Stop the tuning job and adjust the resources requested per trial (possibly via `resources_per_trial` or via `num_workers` for rllib) and/or add more resources to your Ray runtime.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:09:08 (running for 00:10:00.44)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h20m6s)\u001b[0m Error: No available node types can fulfill resource request {'GPU': 1.0, 'CPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:09:13 (running for 00:10:05.44)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:09:18 (running for 00:10:10.45)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:09:23 (running for 00:10:15.46)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:09:28 (running for 00:10:20.46)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:09:33 (running for 00:10:25.46)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:09:38 (running for 00:10:30.46)<br>Memory usage on this node: 16.6/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:09:43 (running for 00:10:35.46)<br>Memory usage on this node: 16.6/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h20m41s)\u001b[0m Error: No available node types can fulfill resource request {'GPU': 1.0, 'CPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:09:48 (running for 00:10:40.46)<br>Memory usage on this node: 16.6/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:09:53 (running for 00:10:45.46)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:09:58 (running for 00:10:50.47)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:10:03 (running for 00:10:55.47)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 17:10:08,017\tWARNING trial_executor.py:312 -- Ignore this message if the cluster is autoscaling. You asked for 16.0 cpu and 1.0 gpu per trial, but the cluster only has 16.0 cpu and 0 gpu. Stop the tuning job and adjust the resources requested per trial (possibly via `resources_per_trial` or via `num_workers` for rllib) and/or add more resources to your Ray runtime.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:10:08 (running for 00:11:00.48)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:10:13 (running for 00:11:05.48)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:10:18 (running for 00:11:10.48)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h21m16s)\u001b[0m Error: No available node types can fulfill resource request {'GPU': 1.0, 'CPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:10:23 (running for 00:11:15.48)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:10:28 (running for 00:11:20.48)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:10:33 (running for 00:11:25.49)<br>Memory usage on this node: 16.5/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:10:38 (running for 00:11:30.49)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:10:43 (running for 00:11:35.49)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:10:48 (running for 00:11:40.50)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:10:53 (running for 00:11:45.50)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +6h21m51s)\u001b[0m Error: No available node types can fulfill resource request {'GPU': 1.0, 'CPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 17:10:54,025\tWARNING tune.py:582 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 17:10:54 (running for 00:11:46.16)<br>Memory usage on this node: 16.4/19.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/10.64 GiB heap, 0.0/5.32 GiB objects<br>Result logdir: /home/michi/skybo_rl/models/PPO_2022-01-30_16-59-07<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_pettingzoo_skyjo_8e6bd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 17:10:54,248\tERROR tune.py:622 -- Trials did not complete: [PPO_pettingzoo_skyjo_8e6bd_00000]\n",
      "2022-01-30 17:10:54,249\tINFO tune.py:626 -- Total run time: 706.40 seconds (706.16 seconds for the tuning loop).\n",
      "2022-01-30 17:10:54,249\tWARNING tune.py:630 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n",
      "E0130 17:55:28.589606166    5678 backup_poller.cc:134]       Run client channel backup poller: {\"created\":\"@1643561728.588626256\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1643561728.588618531\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import os \n",
    "from rlskyjo.utils import get_project_root\n",
    "analysis = ray.tune.run(ppo.PPOTrainer, config=ppo_config, local_dir=os.path.join(get_project_root(), \"models\"), stop={\"training_iteration\": 10},\n",
    "                            checkpoint_at_end=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
