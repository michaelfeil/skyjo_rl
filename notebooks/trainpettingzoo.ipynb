{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We create an instance of a skyjo_env environment, call reset() to initialize the game and list the available agents (players):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rlskyjo.environment.skyjo_env' from '/home/michi/skybo_rl/rlskyjo/environment/skyjo_env.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from rlskyjo.environment import skyjo_env\n",
    "from importlib import reload\n",
    "\n",
    "reload(skyjo_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "skyjo_env_cfg = {\"num_players\": 3}\n",
    "env_pettingzoo = skyjo_env.env(**skyjo_env_cfg)\n",
    "env_pettingzoo.reset()\n",
    "\n",
    "env_pettingzoo.agents, env_pettingzoo.agent_selection\n",
    "\n",
    "\n",
    "def sample_place():\n",
    "    return np.random.randint(0, 23)\n",
    "\n",
    "\n",
    "def sample_draw():\n",
    "    return np.random.randint(24, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_0 draw\n",
      "player_0 place\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n"
     ]
    }
   ],
   "source": [
    "print(env_pettingzoo.agent_selection, \"draw\")\n",
    "env_pettingzoo.step(sample_draw())\n",
    "print(env_pettingzoo.agent_selection, \"place\")\n",
    "env_pettingzoo.step(sample_place())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_admissible_policy(observation, action_mask):\n",
    "    \"\"\"picks randomly an admissible action from the action mask\"\"\"\n",
    "    return np.random.choice(\n",
    "        np.arange(len(action_mask)), p=action_mask / np.sum(action_mask)\n",
    "    )\n",
    "\n",
    "\n",
    "assert 1 not in [random_admissible_policy(None, [1, 0, 1]) for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training fct: {'observations': array([ 5, 10,  0,  0,  1,  0,  0,  1,  0,  1,  0,  0,  3,  0,  0,  0,  1,\n",
      "        8, 15, 15, 12, 15, 15, 15, 15, 15, 15,  8, 15, 15, 15,  8, 15, 15,\n",
      "       15, 15, 15,  3, 15, 15, 15, 15, 15, 15, 15,  0, 15,  5, 15, 15, 15,\n",
      "       15, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [12\t u\t u\t u]\n",
      " [u\t u\t 8\t u]]\n",
      "======= Player 1 ========== \n",
      "[[8\t u\t 3\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t u\t u\t u]]\n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([ 5, 10,  0,  0,  1,  0,  0,  1,  0,  1,  0,  0,  2,  0,  0,  0,  1,\n",
      "       -3,  8, 15, 12, 15, 15, 15, 15, 15, 15,  8, 15, 15, 15,  8, 15, 15,\n",
      "       15, 15, 15,  3, 15, 15, 15, 15, 15, 15, 15,  0, 15,  5, 15, 15, 15,\n",
      "       15, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 8 \n",
      "discard pile top: empty \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [12\t u\t u\t u]\n",
      " [u\t u\t 8\t u]]\n",
      "======= Player 1 ========== \n",
      "[[8\t u\t 3\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t u\t u\t u]]\n",
      "\n",
      "sampled action player_0: 10\n",
      "training fct: {'observations': array([ 5,  9,  0,  0,  1,  0,  0,  1,  0,  1,  1,  0,  3,  0,  0,  0,  1,\n",
      "        6, 15, 15, 12, 15, 15, 15, 15, 15, 15,  8, 15,  8, 15,  8, 15, 15,\n",
      "       15, 15, 15,  3, 15, 15, 15, 15, 15, 15, 15,  0, 15,  5, 15, 15, 15,\n",
      "       15, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t u]]\n",
      "======= Player 1 ========== \n",
      "[[8\t u\t 3\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t u\t u\t u]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([ 5,  9,  0,  0,  1,  0,  0,  1,  0,  1,  1,  0,  3,  0,  0,  0,  1,\n",
      "        6,  7, 15, 12, 15, 15, 15, 15, 15, 15,  8, 15,  8, 15,  8, 15, 15,\n",
      "       15, 15, 15,  3, 15, 15, 15, 15, 15, 15, 15,  0, 15,  5, 15, 15, 15,\n",
      "       15, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 7 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t u]]\n",
      "======= Player 1 ========== \n",
      "[[8\t u\t 3\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t u\t u\t u]]\n",
      "\n",
      "sampled action player_1: 17\n",
      "training fct: {'observations': array([ 5,  9,  0,  0,  2,  0,  0,  1,  0,  1,  1,  1,  3,  0,  0,  0,  1,\n",
      "        7, 15, 15, 12, 15, 15, 15, 15, 15, 15,  8, 15,  8, 15,  8, 15, 15,\n",
      "       15, 15,  0,  3, 15, 15, 15, 15, 15, 15, 15,  0, 15,  5, 15, 15, 15,\n",
      "       15, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 7 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t u]]\n",
      "======= Player 1 ========== \n",
      "[[8\t u\t 3\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t u\t u\t u]]\n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([ 5,  9,  0,  0,  2,  0,  0,  1,  0,  1,  1,  0,  3,  0,  0,  0,  1,\n",
      "        6,  7, 15, 12, 15, 15, 15, 15, 15, 15,  8, 15,  8, 15,  8, 15, 15,\n",
      "       15, 15,  0,  3, 15, 15, 15, 15, 15, 15, 15,  0, 15,  5, 15, 15, 15,\n",
      "       15, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 7 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t u]]\n",
      "======= Player 1 ========== \n",
      "[[8\t u\t 3\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t u\t u\t u]]\n",
      "\n",
      "sampled action player_2: 5\n",
      "training fct: {'observations': array([11,  9,  0,  0,  2,  0,  0,  1,  0,  1,  1,  1,  4,  0,  0,  0,  1,\n",
      "        8, 15, 15, 12, 15, 15, 15, 15, 15, 15,  8, 15,  8, 15,  8, 15, 15,\n",
      "       15, 15,  0,  3, 15, 15, 15, 15, 15, 15, 15,  0, 15,  5,  7, 15, 15,\n",
      "       15, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t u]]\n",
      "======= Player 1 ========== \n",
      "[[8\t u\t 3\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t u\t u]]\n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([11,  9,  0,  0,  2,  0,  0,  1,  0,  1,  1,  1,  4,  0,  0,  0,  1,\n",
      "        8,  2, 15, 12, 15, 15, 15, 15, 15, 15,  8, 15,  8, 15,  8, 15, 15,\n",
      "       15, 15,  0,  3, 15, 15, 15, 15, 15, 15, 15,  0, 15,  5,  7, 15, 15,\n",
      "       15, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 2 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t u]]\n",
      "======= Player 1 ========== \n",
      "[[8\t u\t 3\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t u\t u]]\n",
      "\n",
      "sampled action player_0: 6\n",
      "training fct: {'observations': array([11,  8,  0,  0,  2,  0,  1,  1,  0,  2,  1,  1,  4,  0,  0,  0,  1,\n",
      "        5, 15, 15, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8, 15,  8, 15, 15,\n",
      "       15, 15,  0,  3, 15, 15, 15, 15, 15, 15, 15,  0, 15,  5,  7, 15, 15,\n",
      "       15, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t u]]\n",
      "======= Player 1 ========== \n",
      "[[8\t u\t 3\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t u\t u]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([11,  8,  0,  0,  2,  0,  1,  1,  0,  2,  1,  1,  4,  0,  0,  0,  1,\n",
      "        5,  2, 15, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8, 15,  8, 15, 15,\n",
      "       15, 15,  0,  3, 15, 15, 15, 15, 15, 15, 15,  0, 15,  5,  7, 15, 15,\n",
      "       15, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 2 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t u]]\n",
      "======= Player 1 ========== \n",
      "[[8\t u\t 3\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t u\t u]]\n",
      "\n",
      "sampled action player_1: 7\n",
      "training fct: {'observations': array([12,  8,  0,  0,  2,  0,  2,  1,  0,  2,  1,  2,  4,  0,  0,  0,  1,\n",
      "        7, 15, 15, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8, 15,  8, 15, 15,\n",
      "       15, 15,  0,  3,  2, 15, 15, 15, 15, 15, 15,  0, 15,  5,  7, 15, 15,\n",
      "       15, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 7 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t u]]\n",
      "======= Player 1 ========== \n",
      "[[8\t u\t 3\t u]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t u\t u]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([12,  8,  0,  0,  2,  0,  2,  1,  0,  2,  1,  2,  4,  0,  0,  0,  1,\n",
      "        7,  1, 15, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8, 15,  8, 15, 15,\n",
      "       15, 15,  0,  3,  2, 15, 15, 15, 15, 15, 15,  0, 15,  5,  7, 15, 15,\n",
      "       15, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 1 \n",
      "discard pile top: 7 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t u]]\n",
      "======= Player 1 ========== \n",
      "[[8\t u\t 3\t u]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t u\t u]]\n",
      "\n",
      "sampled action player_2: 8\n",
      "training fct: {'observations': array([13,  8,  0,  0,  2,  1,  2,  1,  0,  3,  1,  2,  4,  0,  0,  0,  1,\n",
      "        5, 15, 15, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8, 15,  8, 15, 15,\n",
      "       15, 15,  0,  3,  2, 15, 15, 15, 15, 15, 15,  0, 15,  5,  7, 15, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t u]]\n",
      "======= Player 1 ========== \n",
      "[[8\t u\t 3\t u]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([13,  8,  0,  0,  2,  1,  2,  1,  0,  2,  1,  2,  4,  0,  0,  0,  1,\n",
      "        7,  5, 15, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8, 15,  8, 15, 15,\n",
      "       15, 15,  0,  3,  2, 15, 15, 15, 15, 15, 15,  0, 15,  5,  7, 15, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 5 \n",
      "discard pile top: 7 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t u]]\n",
      "======= Player 1 ========== \n",
      "[[8\t u\t 3\t u]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_0: 11\n",
      "training fct: {'observations': array([13,  7,  0,  0,  2,  1,  2,  1,  0,  3,  2,  2,  4,  0,  0,  0,  1,\n",
      "        6, 15, 15, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8,  5,  8, 15, 15,\n",
      "       15, 15,  0,  3,  2, 15, 15, 15, 15, 15, 15,  0, 15,  5,  7, 15, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t 5]]\n",
      "======= Player 1 ========== \n",
      "[[8\t u\t 3\t u]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([13,  7,  0,  0,  2,  1,  2,  1,  0,  3,  2,  2,  4,  0,  0,  0,  1,\n",
      "        6, 11, 15, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8,  5,  8, 15, 15,\n",
      "       15, 15,  0,  3,  2, 15, 15, 15, 15, 15, 15,  0, 15,  5,  7, 15, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 11 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t 5]]\n",
      "======= Player 1 ========== \n",
      "[[8\t u\t 3\t u]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_1: 15\n",
      "training fct: {'observations': array([13,  7,  0,  0,  2,  1,  2,  1,  0,  4,  2,  2,  4,  0,  0,  1,  1,\n",
      "       11, 15, 15, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8,  5,  8, 15, 15,\n",
      "        5, 15,  0,  3,  2, 15, 15, 15, 15, 15, 15,  0, 15,  5,  7, 15, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t 5]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t 3\t u]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([13,  7,  0,  0,  2,  1,  2,  1,  0,  4,  2,  2,  4,  0,  0,  1,  1,\n",
      "       11,  6, 15, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8,  5,  8, 15, 15,\n",
      "        5, 15,  0,  3,  2, 15, 15, 15, 15, 15, 15,  0, 15,  5,  7, 15, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 6 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t 5]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t 3\t u]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_2: 3\n",
      "training fct: {'observations': array([18,  7,  0,  0,  2,  1,  2,  1,  0,  4,  3,  2,  5,  0,  0,  1,  1,\n",
      "        8, 15, 15, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8,  5,  8, 15, 15,\n",
      "        5, 15,  0,  3,  2, 15, 15, 15, 15, 15, 15,  0,  6,  5,  7, 15, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t 5]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t 3\t u]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t 6\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([18,  7,  0,  0,  2,  1,  2,  1,  0,  4,  3,  2,  5,  0,  0,  1,  1,\n",
      "        8,  2, 15, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8,  5,  8, 15, 15,\n",
      "        5, 15,  0,  3,  2, 15, 15, 15, 15, 15, 15,  0,  6,  5,  7, 15, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 2 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[u\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t 5]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t 3\t u]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t 6\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_0: 12\n",
      "training fct: {'observations': array([18,  6,  0,  0,  2,  1,  3,  1,  0,  4,  3,  2,  5,  0,  1,  1,  1,\n",
      "        2, 15, 10, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8,  5,  8, 15, 15,\n",
      "        5, 15,  0,  3,  2, 15, 15, 15, 15, 15, 15,  0,  6,  5,  7, 15, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t 5]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t 3\t u]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t 6\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([18,  6,  0,  0,  2,  1,  3,  1,  0,  4,  3,  2,  5,  0,  1,  1,  1,\n",
      "        2,  7, 10, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8,  5,  8, 15, 15,\n",
      "        5, 15,  0,  3,  2, 15, 15, 15, 15, 15, 15,  0,  6,  5,  7, 15, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 7 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t 5]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t 3\t u]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t 6\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_1: 21\n",
      "training fct: {'observations': array([19,  6,  0,  0,  2,  2,  3,  1,  0,  4,  3,  3,  5,  0,  1,  1,  1,\n",
      "        7, 15, 10, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8,  5,  8, 15, 15,\n",
      "        5, 15,  0,  3,  2, 15,  1, 15, 15, 15, 15,  0,  6,  5,  7, 15, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 7 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t 5]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t 3\t 1]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t 6\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([19,  6,  0,  0,  2,  2,  3,  1,  0,  4,  3,  3,  5,  0,  1,  1,  1,\n",
      "        7,  6, 10, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8,  5,  8, 15, 15,\n",
      "        5, 15,  0,  3,  2, 15,  1, 15, 15, 15, 15,  0,  6,  5,  7, 15, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 6 \n",
      "discard pile top: 7 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t 5]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t 3\t 1]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t 6\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_2: 12\n",
      "training fct: {'observations': array([19,  6,  0,  0,  2,  3,  3,  1,  0,  4,  4,  3,  5,  0,  1,  1,  1,\n",
      "        6, 15, 10, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8,  5,  8, 15, 15,\n",
      "        5, 15,  0,  3,  2, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7, 15, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t 5]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t 3\t 1]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([19,  6,  0,  0,  2,  3,  3,  1,  0,  4,  4,  3,  5,  0,  1,  1,  1,\n",
      "        6,  2, 10, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8,  5,  8, 15, 15,\n",
      "        5, 15,  0,  3,  2, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7, 15, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 2 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t 5]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t 3\t 1]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_0: 11\n",
      "training fct: {'observations': array([19,  6,  0,  0,  2,  3,  4,  1,  0,  4,  4,  3,  5,  0,  1,  1,  1,\n",
      "        5, 15, 10, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8,  2,  8, 15, 15,\n",
      "        5, 15,  0,  3,  2, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7, 15, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t 3\t 1]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([19,  6,  0,  0,  2,  3,  4,  1,  0,  4,  4,  3,  5,  0,  1,  1,  1,\n",
      "        5, -2, 10, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8,  2,  8, 15, 15,\n",
      "        5, 15,  0,  3,  2, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7, 15, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: -2 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t 3\t 1]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_1: 6\n",
      "training fct: {'observations': array([14,  6,  1,  0,  2,  3,  4,  1,  0,  4,  4,  3,  5,  0,  1,  1,  1,\n",
      "        3, 15, 10, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8,  2,  8, 15, 15,\n",
      "        5, 15,  0, -2,  2, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7, 15, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 3 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([14,  6,  1,  0,  2,  3,  4,  0,  0,  4,  4,  3,  5,  0,  1,  1,  1,\n",
      "        5,  3, 10, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8,  2,  8, 15, 15,\n",
      "        5, 15,  0, -2,  2, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7, 15, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 3 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t u\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_2: 6\n",
      "training fct: {'observations': array([14,  5,  1,  0,  2,  3,  4,  1,  0,  4,  4,  3,  5,  0,  1,  1,  2,\n",
      "       12, 15, 10, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8,  2,  8, 15, 15,\n",
      "        5, 15,  0, -2,  2, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7,  3, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([14,  5,  1,  0,  2,  3,  4,  1,  0,  4,  4,  3,  5,  0,  1,  1,  2,\n",
      "       12,  1, 10, 12, 15, 15, 15, 15,  2, 15,  8, 15,  8,  2,  8, 15, 15,\n",
      "        5, 15,  0, -2,  2, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7,  3, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 1 \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t u\t 8]\n",
      " [u\t u\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_0: 19\n",
      "training fct: {'observations': array([14,  5,  1,  0,  2,  4,  4,  1,  0,  4,  5,  3,  5,  0,  1,  1,  2,\n",
      "        1, 15, 10, 12, 15, 15, 15, 15,  2,  6,  8, 15,  8,  2,  8, 15, 15,\n",
      "        5, 15,  0, -2,  2, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7,  3, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t 6\t 8]\n",
      " [u\t u\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([14,  5,  1,  0,  2,  4,  4,  1,  0,  4,  5,  3,  5,  0,  1,  1,  2,\n",
      "        1, 10, 10, 12, 15, 15, 15, 15,  2,  6,  8, 15,  8,  2,  8, 15, 15,\n",
      "        5, 15,  0, -2,  2, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7,  3, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 10 \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t 6\t 8]\n",
      " [u\t u\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [u\t u\t 2\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_1: 7\n",
      "training fct: {'observations': array([22,  5,  1,  0,  2,  4,  4,  1,  0,  4,  5,  3,  5,  0,  2,  1,  2,\n",
      "        2, 15, 10, 12, 15, 15, 15, 15,  2,  6,  8, 15,  8,  2,  8, 15, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7,  3, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t 6\t 8]\n",
      " [u\t u\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [u\t u\t 10\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([22,  5,  1,  0,  2,  4,  4,  1,  0,  4,  5,  3,  5,  0,  2,  1,  2,\n",
      "        2, 10, 10, 12, 15, 15, 15, 15,  2,  6,  8, 15,  8,  2,  8, 15, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7,  3, 15,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 10 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t 6\t 8]\n",
      " [u\t u\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [u\t u\t 10\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t u\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_2: 19\n",
      "training fct: {'observations': array([21,  4,  2,  0,  2,  4,  4,  1,  0,  4,  5,  3,  5,  0,  3,  1,  2,\n",
      "       10, 15, 10, 12, 15, 15, 15, 15,  2,  6,  8, 15,  8,  2,  8, 15, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7,  3, -2,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t 6\t 8]\n",
      " [u\t u\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [u\t u\t 10\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([21,  4,  2,  0,  2,  4,  4,  1,  0,  4,  5,  3,  5,  0,  3,  1,  2,\n",
      "       10, 11, 10, 12, 15, 15, 15, 15,  2,  6,  8, 15,  8,  2,  8, 15, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7,  3, -2,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 11 \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t 6\t 8]\n",
      " [u\t u\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [u\t u\t 10\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_0: 5\n",
      "training fct: {'observations': array([21,  4,  2,  0,  2,  4,  4,  1,  1,  4,  5,  3,  5,  0,  3,  2,  2,\n",
      "        4, 15, 10, 12, 15, 15, 15, 11,  2,  6,  8, 15,  8,  2,  8, 15, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7,  3, -2,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t 6\t 8]\n",
      " [u\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [u\t u\t 10\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([21,  4,  2,  0,  2,  4,  4,  1,  0,  4,  5,  3,  5,  0,  3,  2,  2,\n",
      "       10,  4, 10, 12, 15, 15, 15, 11,  2,  6,  8, 15,  8,  2,  8, 15, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7,  3, -2,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 4 \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t 6\t 8]\n",
      " [u\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [u\t u\t 10\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_1: 1\n",
      "training fct: {'observations': array([21,  4,  2,  0,  3,  4,  4,  1,  1,  4,  5,  3,  5,  0,  3,  2,  2,\n",
      "        0, 15, 10, 12, 15, 15, 15, 11,  2,  6,  8, 15,  8,  2,  8,  4, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7,  3, -2,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t 6\t 8]\n",
      " [u\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([21,  4,  2,  0,  3,  4,  4,  1,  1,  4,  5,  3,  5,  0,  3,  2,  2,\n",
      "        0, 12, 10, 12, 15, 15, 15, 11,  2,  6,  8, 15,  8,  2,  8,  4, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7,  3, -2,\n",
      "        1, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 12 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t 6\t 8]\n",
      " [u\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 7\t 1\t u]]\n",
      "\n",
      "sampled action player_2: 23\n",
      "training fct: {'observations': array([24,  3,  2,  0,  3,  4,  4,  2,  1,  4,  5,  3,  5,  0,  3,  2,  3,\n",
      "       12, 15, 10, 12, 15, 15, 15, 11,  2,  6,  8, 15,  8,  2,  8,  4, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7,  3, -2,\n",
      "        1, 15, 15,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t 6\t 8]\n",
      " [u\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 7\t 1\t 3]]\n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([24,  3,  2,  0,  3,  4,  4,  2,  1,  4,  5,  3,  5,  0,  3,  2,  3,\n",
      "       12, 11, 10, 12, 15, 15, 15, 11,  2,  6,  8, 15,  8,  2,  8,  4, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7,  3, -2,\n",
      "        1, 15, 15,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 11 \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t 6\t 8]\n",
      " [u\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 7\t 1\t 3]]\n",
      "\n",
      "sampled action player_0: 2\n",
      "training fct: {'observations': array([24,  3,  2,  0,  3,  4,  4,  2,  2,  4,  5,  3,  5,  0,  3,  3,  3,\n",
      "        4, 15, 10, 12, 11, 15, 15, 11,  2,  6,  8, 15,  8,  2,  8,  4, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7,  3, -2,\n",
      "        1, 15, 15,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t 6\t 8]\n",
      " [11\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 7\t 1\t 3]]\n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([24,  3,  2,  0,  3,  4,  4,  2,  1,  4,  5,  3,  5,  0,  3,  3,  3,\n",
      "       12,  4, 10, 12, 11, 15, 15, 11,  2,  6,  8, 15,  8,  2,  8,  4, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1, 15, 15,  1, 15,  0,  6,  5,  7,  3, -2,\n",
      "        1, 15, 15,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "       1, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 4 \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t 6\t 8]\n",
      " [11\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t u]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 7\t 1\t 3]]\n",
      "\n",
      "sampled action player_1: 22\n",
      "training fct: {'observations': array([24,  3,  2,  0,  3,  4,  4,  2,  2,  5,  5,  3,  5,  0,  3,  3,  3,\n",
      "        4, 15, 10, 12, 11, 15, 15, 11,  2,  6,  8, 15,  8,  2,  8,  4, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1,  5, 15,  1, 15,  0,  6,  5,  7,  3, -2,\n",
      "        1, 15, 15,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t 6\t 8]\n",
      " [11\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t 5]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 7\t 1\t 3]]\n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([24,  3,  2,  0,  3,  4,  4,  2,  1,  5,  5,  3,  5,  0,  3,  3,  3,\n",
      "       12,  4, 10, 12, 11, 15, 15, 11,  2,  6,  8, 15,  8,  2,  8,  4, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1,  5, 15,  1, 15,  0,  6,  5,  7,  3, -2,\n",
      "        1, 15, 15,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "       1, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 4 \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t 6\t 8]\n",
      " [11\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t 5]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 7\t 1\t 3]]\n",
      "\n",
      "sampled action player_2: 5\n",
      "training fct: {'observations': array([21,  3,  2,  0,  3,  4,  4,  2,  2,  5,  5,  3,  5,  0,  3,  3,  3,\n",
      "        7, 15, 10, 12, 11, 15, 15, 11,  2,  6,  8, 15,  8,  2,  8,  4, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1,  5, 15,  1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15, 15,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 7 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t 6\t 8]\n",
      " [11\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t 5]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([21,  3,  2,  0,  3,  4,  4,  2,  2,  5,  5,  2,  5,  0,  3,  3,  3,\n",
      "       12,  7, 10, 12, 11, 15, 15, 11,  2,  6,  8, 15,  8,  2,  8,  4, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1,  5, 15,  1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15, 15,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 7 \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t u\t 6\t 8]\n",
      " [11\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t 5]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_0: 16\n",
      "training fct: {'observations': array([21,  2,  2,  0,  3,  4,  4,  2,  2,  5,  5,  3,  5,  1,  3,  3,  3,\n",
      "        7, 15, 10, 12, 11, 15,  9, 11,  2,  6,  8, 15,  8,  2,  8,  4, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1,  5, 15,  1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15, 15,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 7 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 9\t 6\t 8]\n",
      " [11\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t 5]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([21,  2,  2,  0,  3,  4,  4,  2,  2,  5,  5,  3,  5,  1,  3,  3,  3,\n",
      "        7, 11, 10, 12, 11, 15,  9, 11,  2,  6,  8, 15,  8,  2,  8,  4, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1,  5, 15,  1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15, 15,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 11 \n",
      "discard pile top: 7 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 9\t 6\t 8]\n",
      " [11\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t 5]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_1: 10\n",
      "training fct: {'observations': array([21,  2,  2,  0,  3,  4,  4,  2,  2,  5,  5,  3,  5,  1,  3,  4,  3,\n",
      "        5, 15, 10, 12, 11, 15,  9, 11,  2,  6,  8, 15,  8,  2,  8,  4, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1, 11, 15,  1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15, 15,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 9\t 6\t 8]\n",
      " [11\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t 11]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([21,  2,  2,  0,  3,  4,  4,  2,  2,  5,  5,  3,  5,  1,  3,  4,  3,\n",
      "        5, -1, 10, 12, 11, 15,  9, 11,  2,  6,  8, 15,  8,  2,  8,  4, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1, 11, 15,  1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15, 15,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "       1, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: -1 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 9\t 6\t 8]\n",
      " [11\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t 11]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_2: 0\n",
      "training fct: {'observations': array([19,  2,  2,  1,  3,  4,  4,  2,  2,  5,  5,  3,  5,  1,  3,  4,  3,\n",
      "        1, 15, 10, 12, 11, 15,  9, 11,  2,  6,  8, 15,  8,  2,  8,  4, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15, 15,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 9\t 6\t 8]\n",
      " [11\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t 11]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([19,  2,  2,  1,  3,  3,  4,  2,  2,  5,  5,  3,  5,  1,  3,  4,  3,\n",
      "        5,  1, 10, 12, 11, 15,  9, 11,  2,  6,  8, 15,  8,  2,  8,  4, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15, 15,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 1 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 9\t 6\t 8]\n",
      " [11\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t 11]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_0: 4\n",
      "training fct: {'observations': array([19,  2,  2,  1,  3,  4,  4,  2,  2,  5,  5,  3,  5,  1,  3,  4,  3,\n",
      "        9, 15, 10, 12, 11, 15,  1, 11,  2,  6,  8, 15,  8,  2,  8,  4, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15, 15,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 8]\n",
      " [11\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t 11]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([19,  2,  2,  1,  3,  4,  4,  2,  2,  5,  5,  3,  5,  0,  3,  4,  3,\n",
      "        5,  9, 10, 12, 11, 15,  1, 11,  2,  6,  8, 15,  8,  2,  8,  4, 15,\n",
      "        5, 15,  0, -2, 10, 15,  1, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15, 15,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 9 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 8]\n",
      " [11\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t 11]\n",
      " [u\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_1: 14\n",
      "training fct: {'observations': array([19,  2,  2,  1,  3,  4,  4,  2,  2,  5,  5,  3,  5,  1,  3,  4,  4,\n",
      "        9, 15, 10, 12, 11, 15,  1, 11,  2,  6,  8, 15,  8,  2,  8,  4, 12,\n",
      "        5, 15,  0, -2, 10, 15,  1, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15, 15,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 8]\n",
      " [11\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([19,  2,  2,  1,  3,  4,  4,  2,  2,  5,  5,  3,  5,  1,  3,  4,  4,\n",
      "        9,  1, 10, 12, 11, 15,  1, 11,  2,  6,  8, 15,  8,  2,  8,  4, 12,\n",
      "        5, 15,  0, -2, 10, 15,  1, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15, 15,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "       1, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 1 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 8]\n",
      " [11\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t u]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_2: 10\n",
      "training fct: {'observations': array([20,  2,  2,  1,  3,  5,  4,  2,  2,  6,  5,  3,  5,  1,  3,  4,  4,\n",
      "        5, 15, 10, 12, 11, 15,  1, 11,  2,  6,  8, 15,  8,  2,  8,  4, 12,\n",
      "        5, 15,  0, -2, 10, 15,  1, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 8]\n",
      " [11\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([20,  2,  2,  1,  3,  5,  4,  2,  2,  5,  5,  3,  5,  1,  3,  4,  4,\n",
      "        9,  5, 10, 12, 11, 15,  1, 11,  2,  6,  8, 15,  8,  2,  8,  4, 12,\n",
      "        5, 15,  0, -2, 10, 15,  1, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 5 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 8]\n",
      " [11\t 11\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_0: 5\n",
      "training fct: {'observations': array([20,  2,  2,  1,  3,  5,  4,  2,  2,  6,  5,  3,  5,  1,  3,  4,  4,\n",
      "       11, 15, 10, 12, 11, 15,  1,  5,  2,  6,  8, 15,  8,  2,  8,  4, 12,\n",
      "        5, 15,  0, -2, 10, 15,  1, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 8]\n",
      " [11\t 5\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([20,  2,  2,  1,  3,  5,  4,  2,  2,  6,  5,  3,  5,  1,  3,  4,  4,\n",
      "       11, 10, 10, 12, 11, 15,  1,  5,  2,  6,  8, 15,  8,  2,  8,  4, 12,\n",
      "        5, 15,  0, -2, 10, 15,  1, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 10 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 8]\n",
      " [11\t 5\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [4\t u\t 10\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_1: 1\n",
      "training fct: {'observations': array([20,  2,  2,  1,  3,  5,  4,  2,  2,  6,  5,  3,  5,  1,  4,  4,  4,\n",
      "        4, 15, 10, 12, 11, 15,  1,  5,  2,  6,  8, 15,  8,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 10, 15,  1, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 8]\n",
      " [11\t 5\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [10\t u\t 10\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([20,  2,  2,  1,  3,  5,  4,  2,  1,  6,  5,  3,  5,  1,  4,  4,  4,\n",
      "       11,  4, 10, 12, 11, 15,  1,  5,  2,  6,  8, 15,  8,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 10, 15,  1, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 4 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 8]\n",
      " [11\t 5\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [10\t u\t 10\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_2: 5\n",
      "training fct: {'observations': array([20,  2,  2,  1,  3,  5,  4,  2,  2,  6,  5,  3,  5,  1,  4,  4,  4,\n",
      "        4, 15, 10, 12, 11, 15,  1,  5,  2,  6,  8, 15,  8,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 10, 15,  1, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 8]\n",
      " [11\t 5\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [10\t u\t 10\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([20,  2,  2,  1,  3,  5,  4,  2,  2,  6,  5,  3,  5,  1,  4,  4,  4,\n",
      "        4,  2, 10, 12, 11, 15,  1,  5,  2,  6,  8, 15,  8,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 10, 15,  1, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 2 \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 8]\n",
      " [11\t 5\t 8\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [10\t u\t 10\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_0: 8\n",
      "training fct: {'observations': array([20,  2,  2,  1,  3,  5,  5,  2,  2,  6,  5,  3,  5,  1,  4,  4,  4,\n",
      "        8, 15, 10, 12, 11, 15,  1,  5,  2,  6,  2, 15,  8,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 10, 15,  1, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 8]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [10\t u\t 10\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([20,  2,  2,  1,  3,  5,  5,  2,  2,  6,  5,  3,  5,  1,  4,  4,  4,\n",
      "        8, -2, 10, 12, 11, 15,  1,  5,  2,  6,  2, 15,  8,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 10, 15,  1, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: -2 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 8]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t 1]\n",
      " [10\t u\t 10\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_1: 9\n",
      "training fct: {'observations': array([20,  2,  3,  1,  3,  5,  5,  2,  2,  6,  5,  3,  5,  1,  4,  4,  4,\n",
      "        1, 15, 10, 12, 11, 15,  1,  5,  2,  6,  2, 15,  8,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 10, 15, -2, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 8]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t -2]\n",
      " [10\t u\t 10\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([20,  2,  3,  1,  3,  4,  5,  2,  2,  6,  5,  3,  5,  1,  4,  4,  4,\n",
      "        8,  1, 10, 12, 11, 15,  1,  5,  2,  6,  2, 15,  8,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 10, 15, -2, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 1 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 8]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t -2]\n",
      " [10\t u\t 10\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_2: 8\n",
      "training fct: {'observations': array([20,  2,  3,  1,  3,  5,  5,  2,  2,  6,  5,  3,  5,  1,  4,  4,  4,\n",
      "        1, 15, 10, 12, 11, 15,  1,  5,  2,  6,  2, 15,  8,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 10, 15, -2, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 8]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t -2]\n",
      " [10\t u\t 10\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([20,  2,  3,  1,  3,  5,  5,  2,  2,  6,  5,  3,  5,  1,  4,  4,  4,\n",
      "        1, 12, 10, 12, 11, 15,  1,  5,  2,  6,  2, 15,  8,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 10, 15, -2, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 12 \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 8]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t -2]\n",
      " [10\t u\t 10\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_0: 10\n",
      "training fct: {'observations': array([20,  2,  3,  1,  3,  5,  5,  2,  2,  6,  5,  3,  5,  1,  4,  4,  5,\n",
      "        8, 15, 10, 12, 11, 15,  1,  5,  2,  6,  2, 15, 12,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 10, 15, -2, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 12]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t -2]\n",
      " [10\t u\t 10\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([20,  2,  3,  1,  3,  5,  5,  2,  2,  6,  5,  3,  5,  1,  4,  4,  5,\n",
      "        8, 12, 10, 12, 11, 15,  1,  5,  2,  6,  2, 15, 12,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 10, 15, -2, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 12 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 12]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t -2]\n",
      " [10\t u\t 10\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_1: 7\n",
      "training fct: {'observations': array([20,  2,  3,  1,  3,  5,  5,  2,  2,  6,  5,  3,  5,  1,  4,  4,  6,\n",
      "       10, 15, 10, 12, 11, 15,  1,  5,  2,  6,  2, 15, 12,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 12, 15, -2, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 12]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t -2]\n",
      " [10\t u\t 12\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([20,  2,  3,  1,  3,  5,  5,  2,  2,  6,  5,  3,  5,  1,  3,  4,  6,\n",
      "        8, 10, 10, 12, 11, 15,  1,  5,  2,  6,  2, 15, 12,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 12, 15, -2, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 15,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 10 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 12]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t -2]\n",
      " [10\t u\t 12\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t u]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_2: 9\n",
      "training fct: {'observations': array([30,  1,  3,  1,  3,  5,  5,  2,  2,  6,  5,  3,  5,  2,  4,  4,  6,\n",
      "        9, 15, 10, 12, 11, 15,  1,  5,  2,  6,  2, 15, 12,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 12, 15, -2, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 10,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 12]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t -2]\n",
      " [10\t u\t 12\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([30,  1,  3,  1,  3,  5,  5,  2,  2,  6,  5,  3,  5,  1,  4,  4,  6,\n",
      "        8,  9, 10, 12, 11, 15,  1,  5,  2,  6,  2, 15, 12,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 12, 15, -2, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 10,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 9 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 6\t 12]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t -2]\n",
      " [10\t u\t 12\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_0: 7\n",
      "training fct: {'observations': array([30,  1,  3,  1,  3,  5,  5,  2,  2,  6,  5,  3,  5,  2,  4,  4,  6,\n",
      "        6, 15, 10, 12, 11, 15,  1,  5,  2,  9,  2, 15, 12,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 12, 15, -2, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 10,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 9\t 12]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t -2]\n",
      " [10\t u\t 12\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([30,  1,  3,  1,  3,  5,  5,  2,  2,  6,  4,  3,  5,  2,  4,  4,  6,\n",
      "        8,  6, 10, 12, 11, 15,  1,  5,  2,  9,  2, 15, 12,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 12, 15, -2, 11, 15, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 10,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 6 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 9\t 12]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t -2]\n",
      " [10\t u\t 12\t 11]\n",
      " [12\t 0\t u\t u]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_1: 23\n",
      "training fct: {'observations': array([30,  1,  3,  1,  3,  5,  5,  2,  2,  6,  5,  3,  5,  2,  5,  4,  6,\n",
      "        6, 15, 10, 12, 11, 15,  1,  5,  2,  9,  2, 15, 12,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 12, 15, -2, 11, 10, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 10,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 9\t 12]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t -2]\n",
      " [10\t u\t 12\t 11]\n",
      " [12\t 0\t u\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([30,  1,  3,  1,  3,  5,  5,  2,  2,  6,  5,  3,  5,  2,  5,  4,  6,\n",
      "        6, 10, 10, 12, 11, 15,  1,  5,  2,  9,  2, 15, 12,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 12, 15, -2, 11, 10, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "        1, 10,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 10 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 9\t 12]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t -2]\n",
      " [10\t u\t 12\t 11]\n",
      " [12\t 0\t u\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 1\t 3]]\n",
      "\n",
      "sampled action player_2: 8\n",
      "training fct: {'observations': array([39,  1,  3,  1,  3,  5,  5,  2,  2,  6,  5,  3,  5,  2,  6,  4,  6,\n",
      "        1, 15, 10, 12, 11, 15,  1,  5,  2,  9,  2, 15, 12,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 12, 15, -2, 11, 10, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 9\t 12]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t -2]\n",
      " [10\t u\t 12\t 11]\n",
      " [12\t 0\t u\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([39,  1,  3,  1,  3,  4,  5,  2,  2,  6,  5,  3,  5,  2,  6,  4,  6,\n",
      "        6,  1, 10, 12, 11, 15,  1,  5,  2,  9,  2, 15, 12,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 12, 15, -2, 11, 10, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 1 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[10\t u\t 2\t u]\n",
      " [12\t 1\t 9\t 12]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t -2]\n",
      " [10\t u\t 12\t 11]\n",
      " [12\t 0\t u\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_0: 3\n",
      "training fct: {'observations': array([39,  1,  3,  2,  3,  5,  5,  2,  2,  6,  5,  3,  5,  2,  6,  4,  6,\n",
      "       -1, 15, 10, 12, 11,  1,  1,  5,  2,  9,  2, 15, 12,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 12, 15, -2, 11, 10, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 2\t u]\n",
      " [12\t 1\t 9\t 12]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t -2]\n",
      " [10\t u\t 12\t 11]\n",
      " [12\t 0\t u\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_1: 24\n",
      "training fct: {'observations': array([39,  1,  3,  2,  3,  5,  5,  2,  2,  6,  5,  3,  5,  2,  6,  4,  6,\n",
      "       -1,  6, 10, 12, 11,  1,  1,  5,  2,  9,  2, 15, 12,  2,  8, 10, 12,\n",
      "        5, 15,  0, -2, 12, 15, -2, 11, 10, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 6 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 2\t u]\n",
      " [12\t 1\t 9\t 12]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 5\t -2\t -2]\n",
      " [10\t u\t 12\t 11]\n",
      " [12\t 0\t u\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_1: 3\n",
      "training fct: {'observations': array([39,  1,  3,  2,  3,  5,  5,  2,  2,  6,  6,  3,  5,  2,  6,  4,  6,\n",
      "        5, 15, 10, 12, 11,  1,  1,  5,  2,  9,  2, 15, 12,  2,  8, 10, 12,\n",
      "        6, 15,  0, -2, 12, 15, -2, 11, 10, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 2\t u]\n",
      " [12\t 1\t 9\t 12]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 6\t -2\t -2]\n",
      " [10\t u\t 12\t 11]\n",
      " [12\t 0\t u\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_2: 25\n",
      "training fct: {'observations': array([39,  1,  3,  2,  3,  5,  5,  2,  2,  5,  6,  3,  5,  2,  6,  4,  6,\n",
      "       -1,  5, 10, 12, 11,  1,  1,  5,  2,  9,  2, 15, 12,  2,  8, 10, 12,\n",
      "        6, 15,  0, -2, 12, 15, -2, 11, 10, -1, 15,  0,  6,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 5 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 2\t u]\n",
      " [12\t 1\t 9\t 12]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 6\t -2\t -2]\n",
      " [10\t u\t 12\t 11]\n",
      " [12\t 0\t u\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [0\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_2: 2\n",
      "training fct: {'observations': array([44,  1,  3,  2,  3,  5,  5,  2,  2,  6,  6,  3,  5,  2,  6,  4,  6,\n",
      "        0, 15, 10, 12, 11,  1,  1,  5,  2,  9,  2, 15, 12,  2,  8, 10, 12,\n",
      "        6, 15,  0, -2, 12, 15, -2, 11, 10, -1, 15,  5,  6,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 2\t u]\n",
      " [12\t 1\t 9\t 12]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 6\t -2\t -2]\n",
      " [10\t u\t 12\t 11]\n",
      " [12\t 0\t u\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [5\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_0: 25\n",
      "training fct: {'observations': array([44,  1,  3,  2,  2,  5,  5,  2,  2,  6,  6,  3,  5,  2,  6,  4,  6,\n",
      "       -1,  0, 10, 12, 11,  1,  1,  5,  2,  9,  2, 15, 12,  2,  8, 10, 12,\n",
      "        6, 15,  0, -2, 12, 15, -2, 11, 10, -1, 15,  5,  6,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 0 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 2\t u]\n",
      " [12\t 1\t 9\t 12]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 6\t -2\t -2]\n",
      " [10\t u\t 12\t 11]\n",
      " [12\t 0\t u\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [5\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_0: 10\n",
      "training fct: {'observations': array([44,  1,  3,  2,  3,  5,  5,  2,  2,  6,  6,  3,  5,  2,  6,  4,  6,\n",
      "       12, 15, 10, 12, 11,  1,  1,  5,  2,  9,  2, 15,  0,  2,  8, 10, 12,\n",
      "        6, 15,  0, -2, 12, 15, -2, 11, 10, -1, 15,  5,  6,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 2\t u]\n",
      " [12\t 1\t 9\t 0]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 6\t -2\t -2]\n",
      " [10\t u\t 12\t 11]\n",
      " [12\t 0\t u\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [5\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([44,  1,  3,  2,  3,  5,  5,  2,  2,  6,  6,  3,  5,  2,  6,  4,  5,\n",
      "       -1, 12, 10, 12, 11,  1,  1,  5,  2,  9,  2, 15,  0,  2,  8, 10, 12,\n",
      "        6, 15,  0, -2, 12, 15, -2, 11, 10, -1, 15,  5,  6,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 12 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 2\t u]\n",
      " [12\t 1\t 9\t 0]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 6\t -2\t -2]\n",
      " [10\t u\t 12\t 11]\n",
      " [12\t 0\t u\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [5\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_1: 16\n",
      "training fct: {'observations': array([44,  1,  3,  3,  3,  5,  5,  2,  2,  6,  6,  3,  5,  2,  6,  4,  6,\n",
      "       12, 15, 10, 12, 11,  1,  1,  5,  2,  9,  2, 15,  0,  2,  8, 10, 12,\n",
      "        6, -1,  0, -2, 12, 15, -2, 11, 10, -1, 15,  5,  6,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 2\t u]\n",
      " [12\t 1\t 9\t 0]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 6\t -2\t -2]\n",
      " [10\t -1\t 12\t 11]\n",
      " [12\t 0\t u\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [5\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([44,  1,  3,  3,  3,  5,  5,  2,  2,  6,  6,  3,  5,  2,  6,  4,  6,\n",
      "       12, 10, 10, 12, 11,  1,  1,  5,  2,  9,  2, 15,  0,  2,  8, 10, 12,\n",
      "        6, -1,  0, -2, 12, 15, -2, 11, 10, -1, 15,  5,  6,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 10 \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 2\t u]\n",
      " [12\t 1\t 9\t 0]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 6\t -2\t -2]\n",
      " [10\t -1\t 12\t 11]\n",
      " [12\t 0\t u\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[-1\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [5\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_2: 0\n",
      "training fct: {'observations': array([55,  1,  3,  3,  3,  5,  5,  2,  2,  6,  6,  3,  5,  2,  7,  4,  6,\n",
      "       -1, 15, 10, 12, 11,  1,  1,  5,  2,  9,  2, 15,  0,  2,  8, 10, 12,\n",
      "        6, -1,  0, -2, 12, 15, -2, 11, 10, 10, 15,  5,  6,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 2\t u]\n",
      " [12\t 1\t 9\t 0]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 6\t -2\t -2]\n",
      " [10\t -1\t 12\t 11]\n",
      " [12\t 0\t u\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[10\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [5\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([55,  1,  3,  3,  3,  5,  5,  2,  2,  6,  6,  3,  5,  2,  7,  4,  6,\n",
      "       -1,  0, 10, 12, 11,  1,  1,  5,  2,  9,  2, 15,  0,  2,  8, 10, 12,\n",
      "        6, -1,  0, -2, 12, 15, -2, 11, 10, 10, 15,  5,  6,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 0 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 2\t u]\n",
      " [12\t 1\t 9\t 0]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 6\t -2\t -2]\n",
      " [10\t -1\t 12\t 11]\n",
      " [12\t 0\t u\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[10\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [5\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_0: 1\n",
      "training fct: {'observations': array([43,  1,  3,  3,  4,  5,  5,  2,  2,  6,  6,  3,  5,  2,  7,  4,  6,\n",
      "       12, 15, 10,  0, 11,  1,  1,  5,  2,  9,  2, 15,  0,  2,  8, 10, 12,\n",
      "        6, -1,  0, -2, 12, 15, -2, 11, 10, 10, 15,  5,  6,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 2\t u]\n",
      " [0\t 1\t 9\t 0]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 6\t -2\t -2]\n",
      " [10\t -1\t 12\t 11]\n",
      " [12\t 0\t u\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[10\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [5\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([43,  1,  3,  3,  4,  5,  5,  2,  2,  6,  6,  3,  5,  2,  7,  4,  5,\n",
      "       -1, 12, 10,  0, 11,  1,  1,  5,  2,  9,  2, 15,  0,  2,  8, 10, 12,\n",
      "        6, -1,  0, -2, 12, 15, -2, 11, 10, 10, 15,  5,  6,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 12 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 2\t u]\n",
      " [0\t 1\t 9\t 0]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 6\t -2\t -2]\n",
      " [10\t -1\t 12\t 11]\n",
      " [12\t 0\t u\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[10\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [5\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_1: 8\n",
      "training fct: {'observations': array([43,  0,  3,  3,  4,  5,  5,  2,  2,  6,  6,  3,  5,  2,  7,  4,  7,\n",
      "       12, 15, 10,  0, 11,  1,  1,  5,  2,  9,  2, 15,  0,  2,  8, 10, 12,\n",
      "        6, -1,  0, -2, 12, 12, -2, 11, 10, 10, 15,  5,  6,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 2\t u]\n",
      " [0\t 1\t 9\t 0]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 6\t -2\t -2]\n",
      " [10\t -1\t 12\t 11]\n",
      " [12\t 0\t 12\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[10\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [5\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_2: 24\n",
      "training fct: {'observations': array([43,  0,  3,  3,  4,  5,  5,  2,  2,  6,  6,  3,  5,  2,  7,  4,  7,\n",
      "       12, -1, 10,  0, 11,  1,  1,  5,  2,  9,  2, 15,  0,  2,  8, 10, 12,\n",
      "        6, -1,  0, -2, 12, 12, -2, 11, 10, 10, 15,  5,  6,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: -1 \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 2\t u]\n",
      " [0\t 1\t 9\t 0]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 6\t -2\t -2]\n",
      " [10\t -1\t 12\t 11]\n",
      " [12\t 0\t 12\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[10\t 6\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [5\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_2: 3\n",
      "training fct: {'observations': array([43,  0,  3,  4,  4,  5,  5,  2,  2,  6,  6,  3,  5,  2,  7,  4,  7,\n",
      "        6, 15, 10,  0, 11,  1,  1,  5,  2,  9,  2, 15,  0,  2,  8, 10, 12,\n",
      "        6, -1,  0, -2, 12, 12, -2, 11, 10, 10, 15,  5, -1,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 2\t u]\n",
      " [0\t 1\t 9\t 0]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 6\t -2\t -2]\n",
      " [10\t -1\t 12\t 11]\n",
      " [12\t 0\t 12\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[10\t -1\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [5\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_0: 24\n",
      "training fct: {'observations': array([43,  0,  3,  4,  4,  5,  5,  2,  2,  6,  6,  3,  5,  2,  7,  4,  7,\n",
      "        6,  9, 10,  0, 11,  1,  1,  5,  2,  9,  2, 15,  0,  2,  8, 10, 12,\n",
      "        6, -1,  0, -2, 12, 12, -2, 11, 10, 10, 15,  5, -1,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 9 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 2\t u]\n",
      " [0\t 1\t 9\t 0]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 6\t -2\t -2]\n",
      " [10\t -1\t 12\t 11]\n",
      " [12\t 0\t 12\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[10\t -1\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [5\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_0: 6\n",
      "training fct: {'observations': array([48,  0,  3,  4,  4,  5,  5,  2,  2,  6,  6,  3,  5,  3,  7,  4,  7,\n",
      "        2, 15, 10,  0, 11,  1,  1,  5,  9,  9,  2, 15,  0,  2,  8, 10, 12,\n",
      "        6, -1,  0, -2, 12, 12, -2, 11, 10, 10, 15,  5, -1,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 0 False {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 9\t u]\n",
      " [0\t 1\t 9\t 0]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 6\t -2\t -2]\n",
      " [10\t -1\t 12\t 11]\n",
      " [12\t 0\t 12\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[10\t -1\t 3\t 10]\n",
      " [u\t 5\t -2\t 1]\n",
      " [5\t 4\t 10\t 3]]\n",
      "\n",
      "sampled action player_1: 25\n",
      "training fct: {'observations': array([48,  0,  3,  4,  4,  5,  5,  2,  2,  6,  6,  3,  5,  3,  7,  4,  7,\n",
      "        2, 15, 10,  0, 11,  1,  1,  5,  9,  9,  2, 15,  0,  2,  8, 10, 12,\n",
      "        6, -1,  0, -2, 12, 12, -2, 11, 10, 10, 15,  5, -1,  5,  4,  3, -2,\n",
      "       10, 10,  1,  3], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)} 32.0 True {}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 2 \n",
      "======= GAME DONE ======== \n",
      "Results: {0: 58.0, 1: 152.0, 2: 57.0} \n",
      "======= Player 0 ========== \n",
      "[[10\t 1\t 9\t u8]\n",
      " [0\t 1\t 9\t 0]\n",
      " [11\t 5\t 2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[8\t 6\t -2\t -2]\n",
      " [10\t -1\t 12\t 11]\n",
      " [12\t 0\t 12\t 10]]\n",
      "======= Player 2 ========== \n",
      "[[10\t -1\t 3\t 10]\n",
      " [u9\t 5\t -2\t 1]\n",
      " [5\t 4\t 10\t 3]]\n",
      "\n",
      "done 32.0\n",
      "{'player_1': -62.0, 'player_2': 33.0}\n"
     ]
    }
   ],
   "source": [
    "i_episode = 1\n",
    "while i_episode <= 1:\n",
    "    i_episode += 1\n",
    "    env_pettingzoo.reset()\n",
    "    for agent in env_pettingzoo.agent_iter(max_iter=600):\n",
    "        # get observation (state) for current agent:\n",
    "        obs, reward, done, info = env_pettingzoo.last()\n",
    "\n",
    "        print(\"training fct:\", obs, reward, done, info)\n",
    "        # perform q-learning with update_Q_value()\n",
    "        # your code here\n",
    "\n",
    "        env_pettingzoo.render()\n",
    "\n",
    "        # store current state\n",
    "        if not done:\n",
    "            # choose action using epsilon_greedy_policy()\n",
    "            # your code here\n",
    "            observation = obs[\"observations\"]\n",
    "            action_mask = obs[\"action_mask\"]\n",
    "            action = random_admissible_policy(observation, action_mask)\n",
    "\n",
    "            print(f\"sampled action {agent}: {action}\")\n",
    "            env_pettingzoo.step(action)\n",
    "        else:\n",
    "            # agent is done\n",
    "            env_pettingzoo.step(None)\n",
    "            print(\"done\", reward)\n",
    "            break\n",
    "\n",
    "\n",
    "else:\n",
    "    print(env_pettingzoo._cumulative_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more envs test with rllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.22.125.215',\n",
       " 'raylet_ip_address': '172.22.125.215',\n",
       " 'redis_address': '172.22.125.215:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2022-01-30_00-11-54_673538_18806/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-01-30_00-11-54_673538_18806/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-01-30_00-11-54_673538_18806',\n",
       " 'metrics_export_port': 60361,\n",
       " 'node_id': '34cffa258ec88ac704e62ccedb5526415bf6d1452b8ff4614d35e31a'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import init\n",
    "\n",
    "init(num_cpus=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "from rlskyjo.environment import skyjo_env\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "import ray.rllib.agents.dqn as dqn\n",
    "from ray.rllib.agents import ppo\n",
    "from copy import deepcopy\n",
    "from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from rlskyjo.models.action_mask_model import TorchMaskedActions, TorchActionMaskModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.agents.dqn import DQNTrainer, DQNTorchPolicy\n",
    "from ray.tune.logger import pretty_print\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "  warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "/home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "  warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n"
     ]
    }
   ],
   "source": [
    "env_name = \"pettingzoo_skyjo\"\n",
    "\n",
    "\n",
    "def env_creator():\n",
    "    env = skyjo_env.env(**skyjo_env_cfg)\n",
    "    return env\n",
    "\n",
    "\n",
    "register_env(env_name, lambda config: PettingZooEnv(env_creator()))\n",
    "ModelCatalog.register_custom_model(\"pa_model2\", TorchActionMaskModel)\n",
    "env = PettingZooEnv(env_creator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observations': array([ 4, 10,  0,  1,  0,  1,  0,  1,  0,  2,  0,  1,  0,  1,  0,  0,  0,\n",
       "         7, 15, 15, 15, 15, -1, 15, 15,  5, 15, 15, 15, 15, 15, 15, 15,  9,\n",
       "        15, 15, 15,  1, 15, 15, 15, 15, 15, 15, 15, 15, 15,  5,  3, 15, 15,\n",
       "        15, 15, 15, 15], dtype=int8),\n",
       " 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1], dtype=int8)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.observe(env.env.agent_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with multiagent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 00:11:59,495\tWARNING ppo.py:143 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=15 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 266.\n",
      "2022-01-30 00:11:59,496\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-01-30 00:11:59,496\tINFO trainer.py:743 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18989)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18989)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18989)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18989)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18978)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18978)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18978)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18978)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18980)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18980)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18980)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18980)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18992)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18992)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18992)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18992)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18984)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18984)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18984)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18984)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18985)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18985)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18985)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18985)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18983)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18983)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18983)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18983)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18987)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18987)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18987)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18987)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18982)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18982)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18982)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18982)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18991)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18991)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18991)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18991)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18988)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18988)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18988)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18988)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18990)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18990)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18990)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18990)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18986)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18986)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18986)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18986)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18981)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18981)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18981)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18981)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18979)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18979)\u001b[0m   warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18979)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18979)\u001b[0m   warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n",
      "2022-01-30 00:12:19,506\tINFO trainable.py:124 -- Trainable.setup took 20.012 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    }
   ],
   "source": [
    "custom_config = {\n",
    "    \"env\": env_name,\n",
    "    \"model\": {\n",
    "        \"custom_model\": \"pa_model2\",\n",
    "    },\n",
    "    \"framework\": \"torch\",\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "    \"num_gpus\": int(torch.cuda.device_count()),\n",
    "    \"num_workers\": os.cpu_count() - 1,\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            name: (None, env.observation_space, env.action_space, {})\n",
    "            for name in env.agents\n",
    "        },\n",
    "        \"policy_mapping_fn\": lambda agent_id: agent_id,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "ppo_config = ppo.DEFAULT_CONFIG.copy()\n",
    "ppo_config.update(custom_config)\n",
    "\n",
    "trainer = ppo.PPOTrainer(config=ppo_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=18989)\u001b[0m 2022-01-30 00:12:19,619\tWARNING deprecation.py:45 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18989)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/numpy/core/_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18989)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18989)\u001b[0m /home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/numpy/core/_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18989)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 7950\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-12-42\n",
      "done: false\n",
      "episode_len_mean: 106.72727272727273\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000036\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 66\n",
      "episodes_total: 66\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.7204936081171036\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011820796451732007\n",
      "        policy_loss: -0.06959378906525672\n",
      "        total_loss: 614.5029327678681\n",
      "        vf_explained_var: 0.008369981348514556\n",
      "        vf_loss: 614.5701631991069\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.7231340459982554\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010285974530027574\n",
      "        policy_loss: -0.03197940264518062\n",
      "        total_loss: 635.7101377868653\n",
      "        vf_explained_var: 0.003624355097611745\n",
      "        vf_loss: 635.7400609079997\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.724741859038671\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008718138033950848\n",
      "        policy_loss: -0.024698198360080522\n",
      "        total_loss: 601.8177190462749\n",
      "        vf_explained_var: 0.010214567482471466\n",
      "        vf_loss: 601.8406718826294\n",
      "  num_agent_steps_sampled: 7950\n",
      "  num_agent_steps_trained: 7950\n",
      "  num_steps_sampled: 7980\n",
      "  num_steps_trained: 7980\n",
      "iterations_since_restore: 1\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 31.415384615384617\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 67.86538461538463\n",
      "  vram_util_percent0: 0.42578751001602566\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 64.0\n",
      "  player_1: 47.66666666666667\n",
      "  player_2: 52.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: -0.5757575757575742\n",
      "  player_1: 0.7272727272727292\n",
      "  player_2: 2.8484848484848495\n",
      "policy_reward_min:\n",
      "  player_0: -73.33333333333333\n",
      "  player_1: -96.33333333333333\n",
      "  player_2: -80.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12997387009799105\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 7.314105953986692\n",
      "  mean_inference_ms: 2.4290225539580765\n",
      "  mean_raw_obs_processing_ms: 0.3274392021665444\n",
      "time_since_restore: 22.76088857650757\n",
      "time_this_iter_s: 22.76088857650757\n",
      "time_total_s: 22.76088857650757\n",
      "timers:\n",
      "  learn_throughput: 468.728\n",
      "  learn_time_ms: 17024.804\n",
      "  load_throughput: 756721.438\n",
      "  load_time_ms: 10.545\n",
      "  sample_throughput: 1381.909\n",
      "  sample_time_ms: 5774.62\n",
      "  update_time_ms: 4.564\n",
      "timestamp: 1643497962\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7980\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 23910\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-13-19\n",
      "done: false\n",
      "episode_len_mean: 109.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 69\n",
      "episodes_total: 211\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6903561021884281\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00913672460976419\n",
      "        policy_loss: -0.060356911675383644\n",
      "        total_loss: 486.64632511138916\n",
      "        vf_explained_var: 0.02642344097296397\n",
      "        vf_loss: 486.7048527654012\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6913296073675155\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00863532760572275\n",
      "        policy_loss: -0.06781968410437306\n",
      "        total_loss: 605.1195311609904\n",
      "        vf_explained_var: 0.005633373161156972\n",
      "        vf_loss: 605.1856208292643\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6973744849363963\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009413439409807248\n",
      "        policy_loss: -0.05978739356466879\n",
      "        total_loss: 650.0285737800598\n",
      "        vf_explained_var: 0.008190508782863617\n",
      "        vf_loss: 650.0864798863729\n",
      "  num_agent_steps_sampled: 23910\n",
      "  num_agent_steps_trained: 23910\n",
      "  num_steps_sampled: 23940\n",
      "  num_steps_trained: 23940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 3\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.59583333333333\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 70.84583333333332\n",
      "  vram_util_percent0: 0.42788357204861116\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 48.66666666666667\n",
      "  player_1: 52.0\n",
      "  player_2: 52.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: -1.2500000000000009\n",
      "  player_1: -1.1400000000000008\n",
      "  player_2: 5.389999999999998\n",
      "policy_reward_min:\n",
      "  player_0: -78.66666666666667\n",
      "  player_1: -78.66666666666667\n",
      "  player_2: -92.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10880698124757611\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 3.0025102965590182\n",
      "  mean_inference_ms: 2.003824104036841\n",
      "  mean_raw_obs_processing_ms: 0.2717722198495322\n",
      "time_since_restore: 59.66110801696777\n",
      "time_this_iter_s: 20.0410635471344\n",
      "time_total_s: 59.66110801696777\n",
      "timers:\n",
      "  learn_throughput: 467.424\n",
      "  learn_time_ms: 17072.303\n",
      "  load_throughput: 962055.319\n",
      "  load_time_ms: 8.295\n",
      "  sample_throughput: 582.962\n",
      "  sample_time_ms: 13688.715\n",
      "  update_time_ms: 5.732\n",
      "timestamp: 1643497999\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 23940\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 39872\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-13-55\n",
      "done: false\n",
      "episode_len_mean: 112.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 65\n",
      "episodes_total: 352\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.655437270005544\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010821794481491852\n",
      "        policy_loss: -0.0706142747340103\n",
      "        total_loss: 325.8505024147034\n",
      "        vf_explained_var: 0.0402158784866333\n",
      "        vf_loss: 325.9189535586039\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6540913218259812\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011797501317714705\n",
      "        policy_loss: -0.06909812696898977\n",
      "        total_loss: 486.07720465342203\n",
      "        vf_explained_var: 0.05051946808894475\n",
      "        vf_loss: 486.14394437154135\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6443536337216695\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010813206360650725\n",
      "        policy_loss: -0.08375998528363804\n",
      "        total_loss: 490.1066540686289\n",
      "        vf_explained_var: 0.030617413421471912\n",
      "        vf_loss: 490.1882523059845\n",
      "  num_agent_steps_sampled: 39872\n",
      "  num_agent_steps_trained: 39872\n",
      "  num_steps_sampled: 39900\n",
      "  num_steps_trained: 39900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 5\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.227272727272727\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 67.6181818181818\n",
      "  vram_util_percent0: 0.42789713541666663\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 45.33333333333333\n",
      "  player_1: 44.33333333333333\n",
      "  player_2: 60.0\n",
      "policy_reward_mean:\n",
      "  player_0: -0.7033333333333331\n",
      "  player_1: -1.8233333333333328\n",
      "  player_2: 5.526666666666666\n",
      "policy_reward_min:\n",
      "  player_0: -78.66666666666667\n",
      "  player_1: -84.66666666666667\n",
      "  player_2: -78.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11068892440322156\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 1.8525589229256363\n",
      "  mean_inference_ms: 2.0198619617686298\n",
      "  mean_raw_obs_processing_ms: 0.27105607440152896\n",
      "time_since_restore: 96.2592830657959\n",
      "time_this_iter_s: 18.08060359954834\n",
      "time_total_s: 96.2592830657959\n",
      "timers:\n",
      "  learn_throughput: 472.243\n",
      "  learn_time_ms: 16898.088\n",
      "  load_throughput: 973654.619\n",
      "  load_time_ms: 8.196\n",
      "  sample_throughput: 498.651\n",
      "  sample_time_ms: 16003.173\n",
      "  update_time_ms: 5.471\n",
      "timestamp: 1643498035\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 39900\n",
      "training_iteration: 5\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 55830\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-14-32\n",
      "done: false\n",
      "episode_len_mean: 120.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 68\n",
      "episodes_total: 489\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6082327119509379\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013352861410788504\n",
      "        policy_loss: -0.09474918677471578\n",
      "        total_loss: 569.8931136639912\n",
      "        vf_explained_var: 0.00702485203742981\n",
      "        vf_loss: 569.9851935895284\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6099271219968796\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013007432026206515\n",
      "        policy_loss: -0.07920308330406746\n",
      "        total_loss: 506.03838088989255\n",
      "        vf_explained_var: 0.021583457291126252\n",
      "        vf_loss: 506.11498333613076\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.6053582235177357\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014755114546067413\n",
      "        policy_loss: -0.07620883074744295\n",
      "        total_loss: 590.6894476064047\n",
      "        vf_explained_var: 0.06043377051750819\n",
      "        vf_loss: 590.7627031453451\n",
      "  num_agent_steps_sampled: 55830\n",
      "  num_agent_steps_trained: 55830\n",
      "  num_steps_sampled: 55860\n",
      "  num_steps_trained: 55860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 7\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.214285714285715\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 67.92857142857143\n",
      "  vram_util_percent0: 0.4278971354166667\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 61.0\n",
      "  player_1: 59.66666666666667\n",
      "  player_2: 52.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.4000000000000015\n",
      "  player_1: -0.5999999999999991\n",
      "  player_2: 2.200000000000001\n",
      "policy_reward_min:\n",
      "  player_0: -79.66666666666667\n",
      "  player_1: -83.0\n",
      "  player_2: -86.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10987464334485283\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 1.3942251307509537\n",
      "  mean_inference_ms: 1.9964622553727835\n",
      "  mean_raw_obs_processing_ms: 0.2685985599212627\n",
      "time_since_restore: 132.87008547782898\n",
      "time_this_iter_s: 17.103546857833862\n",
      "time_total_s: 132.87008547782898\n",
      "timers:\n",
      "  learn_throughput: 472.971\n",
      "  learn_time_ms: 16872.078\n",
      "  load_throughput: 1027190.0\n",
      "  load_time_ms: 7.769\n",
      "  sample_throughput: 475.451\n",
      "  sample_time_ms: 16784.06\n",
      "  update_time_ms: 5.654\n",
      "timestamp: 1643498072\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 55860\n",
      "training_iteration: 7\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 71791\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-15-08\n",
      "done: false\n",
      "episode_len_mean: 119.54\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 66\n",
      "episodes_total: 623\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5577853467067082\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011594797356149987\n",
      "        policy_loss: -0.07307864991327127\n",
      "        total_loss: 503.70005938212074\n",
      "        vf_explained_var: 0.049558320542176564\n",
      "        vf_loss: 503.77081968307493\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.592821248571078\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01303635314519094\n",
      "        policy_loss: -0.08113610057160259\n",
      "        total_loss: 558.943468354543\n",
      "        vf_explained_var: 0.050576917231082916\n",
      "        vf_loss: 559.0219980748494\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5678957867622376\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013643383897233434\n",
      "        policy_loss: -0.09190657654156288\n",
      "        total_loss: 534.2901742490133\n",
      "        vf_explained_var: 0.03540877292553584\n",
      "        vf_loss: 534.3793500804901\n",
      "  num_agent_steps_sampled: 71791\n",
      "  num_agent_steps_trained: 71791\n",
      "  num_steps_sampled: 71820\n",
      "  num_steps_trained: 71820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 9\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.10454545454546\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.1409090909091\n",
      "  vram_util_percent0: 0.42789713541666663\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 58.0\n",
      "  player_1: 56.0\n",
      "  player_2: 46.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: -1.7033333333333334\n",
      "  player_1: 3.286666666666667\n",
      "  player_2: 1.4166666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -89.0\n",
      "  player_1: -97.0\n",
      "  player_2: -79.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10903189158761332\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 1.153878598796996\n",
      "  mean_inference_ms: 1.9754121486419263\n",
      "  mean_raw_obs_processing_ms: 0.2647760048849081\n",
      "time_since_restore: 168.76790142059326\n",
      "time_this_iter_s: 18.540443897247314\n",
      "time_total_s: 168.76790142059326\n",
      "timers:\n",
      "  learn_throughput: 475.785\n",
      "  learn_time_ms: 16772.273\n",
      "  load_throughput: 1015585.726\n",
      "  load_time_ms: 7.858\n",
      "  sample_throughput: 471.818\n",
      "  sample_time_ms: 16913.314\n",
      "  update_time_ms: 5.63\n",
      "timestamp: 1643498108\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 71820\n",
      "training_iteration: 9\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 87753\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-15-44\n",
      "done: false\n",
      "episode_len_mean: 124.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 65\n",
      "episodes_total: 753\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5232174569368362\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015677857395935652\n",
      "        policy_loss: -0.09173529137857259\n",
      "        total_loss: 438.93213216145836\n",
      "        vf_explained_var: -0.06147236416737239\n",
      "        vf_loss: 439.02073059082034\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5613664086659749\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013609265919110053\n",
      "        policy_loss: -0.07193065840750933\n",
      "        total_loss: 474.50722976366677\n",
      "        vf_explained_var: 0.05955805430809657\n",
      "        vf_loss: 474.57643789927164\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.545366072456042\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01467613735143503\n",
      "        policy_loss: -0.10986056639502446\n",
      "        total_loss: 482.28883136749266\n",
      "        vf_explained_var: 0.009641129473845165\n",
      "        vf_loss: 482.39575871149697\n",
      "  num_agent_steps_sampled: 87753\n",
      "  num_agent_steps_trained: 87753\n",
      "  num_steps_sampled: 87780\n",
      "  num_steps_trained: 87780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 11\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.90952380952381\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 67.98095238095237\n",
      "  vram_util_percent0: 0.4278971354166667\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 44.0\n",
      "  player_1: 55.66666666666667\n",
      "  player_2: 51.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 2.423333333333334\n",
      "  player_1: -1.2966666666666662\n",
      "  player_2: 1.8733333333333326\n",
      "policy_reward_min:\n",
      "  player_0: -65.33333333333333\n",
      "  player_1: -82.66666666666667\n",
      "  player_2: -74.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11116488074355957\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 1.0099026106243278\n",
      "  mean_inference_ms: 2.0063172251846244\n",
      "  mean_raw_obs_processing_ms: 0.2677082231189033\n",
      "time_since_restore: 204.43376660346985\n",
      "time_this_iter_s: 17.636125326156616\n",
      "time_total_s: 204.43376660346985\n",
      "timers:\n",
      "  learn_throughput: 479.812\n",
      "  learn_time_ms: 16631.525\n",
      "  load_throughput: 892424.142\n",
      "  load_time_ms: 8.942\n",
      "  sample_throughput: 436.335\n",
      "  sample_time_ms: 18288.72\n",
      "  update_time_ms: 5.911\n",
      "timestamp: 1643498144\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 87780\n",
      "training_iteration: 11\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 103710\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-16-19\n",
      "done: false\n",
      "episode_len_mean: 132.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 58\n",
      "episodes_total: 873\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.481907992164294\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015144145731055948\n",
      "        policy_loss: -0.05704772149523099\n",
      "        total_loss: 531.2152475484212\n",
      "        vf_explained_var: 0.0392143044869105\n",
      "        vf_loss: 531.2692651112874\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5164742126067479\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015454143205225061\n",
      "        policy_loss: -0.06882368545358379\n",
      "        total_loss: 533.0437933858236\n",
      "        vf_explained_var: -0.05244310567776362\n",
      "        vf_loss: 533.1095263417562\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.5048232595125834\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013418254889743745\n",
      "        policy_loss: -0.13587649409659208\n",
      "        total_loss: 477.2709238688151\n",
      "        vf_explained_var: -0.00855154275894165\n",
      "        vf_loss: 477.4041167704264\n",
      "  num_agent_steps_sampled: 103710\n",
      "  num_agent_steps_trained: 103710\n",
      "  num_steps_sampled: 103740\n",
      "  num_steps_trained: 103740\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 13\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.280952380952376\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 67.95238095238095\n",
      "  vram_util_percent0: 0.4278971354166667\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 54.66666666666667\n",
      "  player_1: 59.0\n",
      "  player_2: 67.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 6.593333333333334\n",
      "  player_1: -9.296666666666667\n",
      "  player_2: 5.703333333333334\n",
      "policy_reward_min:\n",
      "  player_0: -67.0\n",
      "  player_1: -74.0\n",
      "  player_2: -95.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11043245516846904\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.909558592811031\n",
      "  mean_inference_ms: 1.9945364976503732\n",
      "  mean_raw_obs_processing_ms: 0.2652768469101448\n",
      "time_since_restore: 239.70041060447693\n",
      "time_this_iter_s: 17.582683086395264\n",
      "time_total_s: 239.70041060447693\n",
      "timers:\n",
      "  learn_throughput: 485.48\n",
      "  learn_time_ms: 16437.344\n",
      "  load_throughput: 898337.979\n",
      "  load_time_ms: 8.883\n",
      "  sample_throughput: 436.181\n",
      "  sample_time_ms: 18295.146\n",
      "  update_time_ms: 5.916\n",
      "timestamp: 1643498179\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 103740\n",
      "training_iteration: 13\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 119670\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-16-56\n",
      "done: false\n",
      "episode_len_mean: 136.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000018\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 58\n",
      "episodes_total: 990\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4483990554014843\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014600394133859898\n",
      "        policy_loss: -0.06857675371536365\n",
      "        total_loss: 212.33442971229553\n",
      "        vf_explained_var: -0.0610435434182485\n",
      "        vf_loss: 212.40008666674296\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4976609275738397\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016377375657264867\n",
      "        policy_loss: -0.06013544433284551\n",
      "        total_loss: 528.6721723556518\n",
      "        vf_explained_var: 0.021935873230298362\n",
      "        vf_loss: 528.729032611847\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4568741488456727\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01486234838990337\n",
      "        policy_loss: -0.13124850385511916\n",
      "        total_loss: 447.7480396493276\n",
      "        vf_explained_var: 0.02863229642311732\n",
      "        vf_loss: 447.87631759643557\n",
      "  num_agent_steps_sampled: 119670\n",
      "  num_agent_steps_trained: 119670\n",
      "  num_steps_sampled: 119700\n",
      "  num_steps_trained: 119700\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 15\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.804545454545455\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 67.90000000000003\n",
      "  vram_util_percent0: 0.42789713541666663\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 52.66666666666667\n",
      "  player_1: 54.0\n",
      "  player_2: 52.0\n",
      "policy_reward_mean:\n",
      "  player_0: 7.190000000000001\n",
      "  player_1: -3.7700000000000005\n",
      "  player_2: -0.41999999999999976\n",
      "policy_reward_min:\n",
      "  player_0: -73.0\n",
      "  player_1: -92.33333333333333\n",
      "  player_2: -68.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1104812371588887\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.83265679476598\n",
      "  mean_inference_ms: 1.9909052221920844\n",
      "  mean_raw_obs_processing_ms: 0.26458767239804337\n",
      "time_since_restore: 276.20893144607544\n",
      "time_this_iter_s: 18.502495527267456\n",
      "time_total_s: 276.20893144607544\n",
      "timers:\n",
      "  learn_throughput: 485.055\n",
      "  learn_time_ms: 16451.743\n",
      "  load_throughput: 791329.467\n",
      "  load_time_ms: 10.084\n",
      "  sample_throughput: 444.077\n",
      "  sample_time_ms: 17969.856\n",
      "  update_time_ms: 6.151\n",
      "timestamp: 1643498216\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 119700\n",
      "training_iteration: 15\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 135630\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-17-31\n",
      "done: false\n",
      "episode_len_mean: 151.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 53\n",
      "episodes_total: 1094\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4057070420185724\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015369281496228477\n",
      "        policy_loss: -0.06441466206374268\n",
      "        total_loss: 344.92069831848147\n",
      "        vf_explained_var: 0.05130397429068883\n",
      "        vf_loss: 344.98203950246176\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4420833464463552\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014505032854309926\n",
      "        policy_loss: -0.14708961552940308\n",
      "        total_loss: 341.2096804968516\n",
      "        vf_explained_var: 0.1373729994893074\n",
      "        vf_loss: 341.35386978467307\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.4321560343106587\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01661395159281511\n",
      "        policy_loss: -0.04548046329990029\n",
      "        total_loss: 300.8746794128418\n",
      "        vf_explained_var: 0.12071178138256072\n",
      "        vf_loss: 300.9168364461263\n",
      "  num_agent_steps_sampled: 135630\n",
      "  num_agent_steps_trained: 135630\n",
      "  num_steps_sampled: 135660\n",
      "  num_steps_trained: 135660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 17\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.142857142857142\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.11428571428573\n",
      "  vram_util_percent0: 0.4278971354166667\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 60.33333333333333\n",
      "  player_1: 55.0\n",
      "  player_2: 49.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.173333333333333\n",
      "  player_1: -1.6466666666666674\n",
      "  player_2: 3.473333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -65.0\n",
      "  player_1: -82.66666666666667\n",
      "  player_2: -80.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11054457566223437\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7761914704632014\n",
      "  mean_inference_ms: 1.9926794459581794\n",
      "  mean_raw_obs_processing_ms: 0.26421702602706953\n",
      "time_since_restore: 311.65264558792114\n",
      "time_this_iter_s: 17.521949291229248\n",
      "time_total_s: 311.65264558792114\n",
      "timers:\n",
      "  learn_throughput: 488.786\n",
      "  learn_time_ms: 16326.15\n",
      "  load_throughput: 790122.705\n",
      "  load_time_ms: 10.1\n",
      "  sample_throughput: 446.848\n",
      "  sample_time_ms: 17858.426\n",
      "  update_time_ms: 6.022\n",
      "timestamp: 1643498251\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 135660\n",
      "training_iteration: 17\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 151590\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-18-07\n",
      "done: false\n",
      "episode_len_mean: 152.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 54\n",
      "episodes_total: 1200\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3620615257819493\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01579188953503376\n",
      "        policy_loss: -0.08864648551369707\n",
      "        total_loss: 218.83502634684245\n",
      "        vf_explained_var: -0.08678530921538671\n",
      "        vf_loss: 218.92051371892293\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.384095494945844\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017000491525510832\n",
      "        policy_loss: -0.1090446932738026\n",
      "        total_loss: 291.93702173868814\n",
      "        vf_explained_var: 0.012356901963551839\n",
      "        vf_loss: 292.0426657295227\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3960666978359222\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015311308485088375\n",
      "        policy_loss: -0.07630375017722447\n",
      "        total_loss: 360.5072573407491\n",
      "        vf_explained_var: 0.15371389190355936\n",
      "        vf_loss: 360.58049756368\n",
      "  num_agent_steps_sampled: 151590\n",
      "  num_agent_steps_trained: 151590\n",
      "  num_steps_sampled: 151620\n",
      "  num_steps_trained: 151620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 19\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 18.686363636363637\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.61363636363639\n",
      "  vram_util_percent0: 0.42789713541666663\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 46.33333333333333\n",
      "  player_1: 44.33333333333333\n",
      "  player_2: 48.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 5.000000000000001\n",
      "  player_1: -3.1000000000000005\n",
      "  player_2: 1.0999999999999985\n",
      "policy_reward_min:\n",
      "  player_0: -67.66666666666667\n",
      "  player_1: -71.66666666666667\n",
      "  player_2: -64.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10932947039318704\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7276307758844012\n",
      "  mean_inference_ms: 1.9713732178541585\n",
      "  mean_raw_obs_processing_ms: 0.2611369288186615\n",
      "time_since_restore: 347.39973521232605\n",
      "time_this_iter_s: 18.196481466293335\n",
      "time_total_s: 347.39973521232605\n",
      "timers:\n",
      "  learn_throughput: 488.485\n",
      "  learn_time_ms: 16336.234\n",
      "  load_throughput: 671208.415\n",
      "  load_time_ms: 11.889\n",
      "  sample_throughput: 445.946\n",
      "  sample_time_ms: 17894.553\n",
      "  update_time_ms: 6.091\n",
      "timestamp: 1643498287\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 151620\n",
      "training_iteration: 19\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 167550\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-18-46\n",
      "done: false\n",
      "episode_len_mean: 151.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 49\n",
      "episodes_total: 1306\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3050419586896895\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017087750678362424\n",
      "        policy_loss: -0.06895290491481622\n",
      "        total_loss: 414.5112043507894\n",
      "        vf_explained_var: -0.03906338691711426\n",
      "        vf_loss: 414.5767374293009\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3498970562219619\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017198291684334822\n",
      "        policy_loss: -0.10784777202953895\n",
      "        total_loss: 301.70221599260964\n",
      "        vf_explained_var: 0.07309333185354869\n",
      "        vf_loss: 301.8066251436869\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3924597948789597\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01843517811792213\n",
      "        policy_loss: -0.10211709859780967\n",
      "        total_loss: 374.23589164733886\n",
      "        vf_explained_var: 0.07962765256563822\n",
      "        vf_loss: 374.3343231709798\n",
      "  num_agent_steps_sampled: 167550\n",
      "  num_agent_steps_trained: 167550\n",
      "  num_steps_sampled: 167580\n",
      "  num_steps_trained: 167580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 21\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.283333333333331\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.2625\n",
      "  vram_util_percent0: 0.4278971354166667\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 50.66666666666667\n",
      "  player_1: 48.33333333333333\n",
      "  player_2: 66.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 1.5399999999999991\n",
      "  player_1: 1.0400000000000005\n",
      "  player_2: 0.4199999999999994\n",
      "policy_reward_min:\n",
      "  player_0: -86.33333333333333\n",
      "  player_1: -74.33333333333333\n",
      "  player_2: -86.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11025392263656707\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.6932347342713399\n",
      "  mean_inference_ms: 1.983184327116725\n",
      "  mean_raw_obs_processing_ms: 0.2621200099256011\n",
      "time_since_restore: 386.41452980041504\n",
      "time_this_iter_s: 19.916096925735474\n",
      "time_total_s: 386.41452980041504\n",
      "timers:\n",
      "  learn_throughput: 479.394\n",
      "  learn_time_ms: 16646.019\n",
      "  load_throughput: 788466.154\n",
      "  load_time_ms: 10.121\n",
      "  sample_throughput: 442.31\n",
      "  sample_time_ms: 18041.639\n",
      "  update_time_ms: 5.894\n",
      "timestamp: 1643498326\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 167580\n",
      "training_iteration: 21\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 183510\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-19-23\n",
      "done: false\n",
      "episode_len_mean: 163.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 42\n",
      "episodes_total: 1398\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2703170314431191\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017222073387377558\n",
      "        policy_loss: -0.03207569710289439\n",
      "        total_loss: 211.06482925097148\n",
      "        vf_explained_var: 0.0025016897916793823\n",
      "        vf_loss: 211.09346094449361\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3270320522785186\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017738493437718716\n",
      "        policy_loss: -0.08551221161149442\n",
      "        total_loss: 213.28135019779205\n",
      "        vf_explained_var: 0.02879932959874471\n",
      "        vf_loss: 213.36331480026246\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.3441049039363862\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016161207908799657\n",
      "        policy_loss: -0.12594094909106693\n",
      "        total_loss: 254.52420918623605\n",
      "        vf_explained_var: -0.10875047286351522\n",
      "        vf_loss: 254.64691793759664\n",
      "  num_agent_steps_sampled: 183510\n",
      "  num_agent_steps_trained: 183510\n",
      "  num_steps_sampled: 183540\n",
      "  num_steps_trained: 183540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 23\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.247826086956525\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.43478260869566\n",
      "  vram_util_percent0: 0.42789713541666663\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 56.0\n",
      "  player_1: 54.0\n",
      "  player_2: 54.0\n",
      "policy_reward_mean:\n",
      "  player_0: 9.23\n",
      "  player_1: -0.8700000000000001\n",
      "  player_2: -5.360000000000001\n",
      "policy_reward_min:\n",
      "  player_0: -74.33333333333333\n",
      "  player_1: -107.0\n",
      "  player_2: -80.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11100070408219523\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.6688854691674551\n",
      "  mean_inference_ms: 2.001614341604107\n",
      "  mean_raw_obs_processing_ms: 0.26405473403466273\n",
      "time_since_restore: 423.2224419116974\n",
      "time_this_iter_s: 18.63118839263916\n",
      "time_total_s: 423.2224419116974\n",
      "timers:\n",
      "  learn_throughput: 475.533\n",
      "  learn_time_ms: 16781.178\n",
      "  load_throughput: 782248.734\n",
      "  load_time_ms: 10.201\n",
      "  sample_throughput: 436.789\n",
      "  sample_time_ms: 18269.706\n",
      "  update_time_ms: 6.481\n",
      "timestamp: 1643498363\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 183540\n",
      "training_iteration: 23\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 199470\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-19-57\n",
      "done: false\n",
      "episode_len_mean: 173.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 44\n",
      "episodes_total: 1492\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2668623942136765\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01663767173161735\n",
      "        policy_loss: -0.09664572372566908\n",
      "        total_loss: 373.4611269124349\n",
      "        vf_explained_var: -0.06986913472414016\n",
      "        vf_loss: 373.55444699287415\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.285013989607493\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01619832539861818\n",
      "        policy_loss: -0.09209032590190569\n",
      "        total_loss: 409.64359100341795\n",
      "        vf_explained_var: 0.15538573414087295\n",
      "        vf_loss: 409.7324415588379\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.323826120297114\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01782417186330489\n",
      "        policy_loss: -0.08708439557813108\n",
      "        total_loss: 286.7395893160502\n",
      "        vf_explained_var: 0.008955335319042206\n",
      "        vf_loss: 286.82310872395834\n",
      "  num_agent_steps_sampled: 199470\n",
      "  num_agent_steps_trained: 199470\n",
      "  num_steps_sampled: 199500\n",
      "  num_steps_trained: 199500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 25\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.814285714285713\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.28571428571426\n",
      "  vram_util_percent0: 0.4278971354166667\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 66.0\n",
      "  player_1: 47.66666666666667\n",
      "  player_2: 55.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 1.0566666666666669\n",
      "  player_1: -5.653333333333332\n",
      "  player_2: 7.596666666666667\n",
      "policy_reward_min:\n",
      "  player_0: -74.0\n",
      "  player_1: -96.33333333333333\n",
      "  player_2: -70.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1109571933026558\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.6438514697952449\n",
      "  mean_inference_ms: 2.000334368505215\n",
      "  mean_raw_obs_processing_ms: 0.2634059474075712\n",
      "time_since_restore: 457.00265192985535\n",
      "time_this_iter_s: 17.029091835021973\n",
      "time_total_s: 457.00265192985535\n",
      "timers:\n",
      "  learn_throughput: 483.447\n",
      "  learn_time_ms: 16506.476\n",
      "  load_throughput: 840651.662\n",
      "  load_time_ms: 9.493\n",
      "  sample_throughput: 437.013\n",
      "  sample_time_ms: 18260.34\n",
      "  update_time_ms: 6.275\n",
      "timestamp: 1643498397\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 199500\n",
      "training_iteration: 25\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 215430\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-20-34\n",
      "done: false\n",
      "episode_len_mean: 165.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.999999999999998\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 51\n",
      "episodes_total: 1592\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2118823354442914\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01648095897657683\n",
      "        policy_loss: -0.10091173687949777\n",
      "        total_loss: 274.0981518681844\n",
      "        vf_explained_var: -0.09313640316327412\n",
      "        vf_loss: 274.1957684421539\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2964454106489818\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020859369728122797\n",
      "        policy_loss: -0.1045278103199477\n",
      "        total_loss: 310.81392536799115\n",
      "        vf_explained_var: 0.13203291048606236\n",
      "        vf_loss: 310.91428246815997\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2909077201286951\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019218432651095914\n",
      "        policy_loss: -0.08697121224366128\n",
      "        total_loss: 386.89930833816527\n",
      "        vf_explained_var: -0.019878543814023337\n",
      "        vf_loss: 386.98243640263877\n",
      "  num_agent_steps_sampled: 215430\n",
      "  num_agent_steps_trained: 215430\n",
      "  num_steps_sampled: 215460\n",
      "  num_steps_trained: 215460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 27\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.916666666666666\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.16666666666667\n",
      "  vram_util_percent0: 0.4280259874131944\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 57.0\n",
      "  player_1: 61.33333333333333\n",
      "  player_2: 45.0\n",
      "policy_reward_mean:\n",
      "  player_0: 6.843333333333331\n",
      "  player_1: 3.6933333333333325\n",
      "  player_2: -7.536666666666669\n",
      "policy_reward_min:\n",
      "  player_0: -77.66666666666666\n",
      "  player_1: -73.0\n",
      "  player_2: -81.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11057094096949523\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.6189521657223803\n",
      "  mean_inference_ms: 1.9926059206661142\n",
      "  mean_raw_obs_processing_ms: 0.26251954220610774\n",
      "time_since_restore: 494.2407765388489\n",
      "time_this_iter_s: 19.691646575927734\n",
      "time_total_s: 494.2407765388489\n",
      "timers:\n",
      "  learn_throughput: 477.656\n",
      "  learn_time_ms: 16706.567\n",
      "  load_throughput: 805015.836\n",
      "  load_time_ms: 9.913\n",
      "  sample_throughput: 441.586\n",
      "  sample_time_ms: 18071.206\n",
      "  update_time_ms: 6.284\n",
      "timestamp: 1643498434\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 215460\n",
      "training_iteration: 27\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 231390\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-21-11\n",
      "done: false\n",
      "episode_len_mean: 186.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 42\n",
      "episodes_total: 1673\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.218518189986547\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01762165020290619\n",
      "        policy_loss: -0.11506934598709147\n",
      "        total_loss: 204.7238342444102\n",
      "        vf_explained_var: 0.01870091656843821\n",
      "        vf_loss: 204.83537906010946\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2321088729302088\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015073021469864898\n",
      "        policy_loss: -0.061755021127561725\n",
      "        total_loss: 216.60740290006\n",
      "        vf_explained_var: -0.004616455733776092\n",
      "        vf_loss: 216.66463647206623\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2544396259387334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019174827194026564\n",
      "        policy_loss: -0.08910129209669927\n",
      "        total_loss: 228.60762453079224\n",
      "        vf_explained_var: -0.05095230966806412\n",
      "        vf_loss: 228.6928901354472\n",
      "  num_agent_steps_sampled: 231390\n",
      "  num_agent_steps_trained: 231390\n",
      "  num_steps_sampled: 231420\n",
      "  num_steps_trained: 231420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 29\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 18.89130434782609\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.59565217391302\n",
      "  vram_util_percent0: 0.42805989583333337\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 39.66666666666667\n",
      "  player_1: 51.33333333333333\n",
      "  player_2: 60.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 2.5733333333333337\n",
      "  player_1: -0.9666666666666661\n",
      "  player_2: 1.393333333333333\n",
      "policy_reward_min:\n",
      "  player_0: -77.66666666666666\n",
      "  player_1: -87.33333333333333\n",
      "  player_2: -69.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10996815972953806\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.6013309252809962\n",
      "  mean_inference_ms: 1.983214572847696\n",
      "  mean_raw_obs_processing_ms: 0.26078297633018765\n",
      "time_since_restore: 530.826696395874\n",
      "time_this_iter_s: 19.545502185821533\n",
      "time_total_s: 530.826696395874\n",
      "timers:\n",
      "  learn_throughput: 475.677\n",
      "  learn_time_ms: 16776.1\n",
      "  load_throughput: 823679.499\n",
      "  load_time_ms: 9.688\n",
      "  sample_throughput: 437.037\n",
      "  sample_time_ms: 18259.337\n",
      "  update_time_ms: 6.498\n",
      "timestamp: 1643498471\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 231420\n",
      "training_iteration: 29\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 247350\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-21-49\n",
      "done: false\n",
      "episode_len_mean: 199.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999982\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 40\n",
      "episodes_total: 1752\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1607004889845849\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01813971531743005\n",
      "        policy_loss: -0.08328236291805903\n",
      "        total_loss: 188.55466705640157\n",
      "        vf_explained_var: -0.03323864797751109\n",
      "        vf_loss: 188.63432132403057\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2064592184623082\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0151848381033453\n",
      "        policy_loss: -0.10601152843640496\n",
      "        total_loss: 235.8493475373586\n",
      "        vf_explained_var: 0.02358671247959137\n",
      "        vf_loss: 235.9508049249649\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.2229135513305665\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018019637777648777\n",
      "        policy_loss: -0.08162561537697911\n",
      "        total_loss: 260.50119841893513\n",
      "        vf_explained_var: -0.09912453601757686\n",
      "        vf_loss: 260.57741728464765\n",
      "  num_agent_steps_sampled: 247350\n",
      "  num_agent_steps_trained: 247350\n",
      "  num_steps_sampled: 247380\n",
      "  num_steps_trained: 247380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 31\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.066666666666666\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.04166666666667\n",
      "  vram_util_percent0: 0.4280598958333333\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 53.0\n",
      "  player_1: 61.0\n",
      "  player_2: 58.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: -0.06333333333333385\n",
      "  player_1: -0.48333333333333384\n",
      "  player_2: 3.5466666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -77.0\n",
      "  player_1: -105.33333333333333\n",
      "  player_2: -68.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1097668728295464\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5852972844772718\n",
      "  mean_inference_ms: 1.97931576297294\n",
      "  mean_raw_obs_processing_ms: 0.26066824293981133\n",
      "time_since_restore: 568.5345120429993\n",
      "time_this_iter_s: 19.45830535888672\n",
      "time_total_s: 568.5345120429993\n",
      "timers:\n",
      "  learn_throughput: 478.015\n",
      "  learn_time_ms: 16694.049\n",
      "  load_throughput: 820881.64\n",
      "  load_time_ms: 9.721\n",
      "  sample_throughput: 437.639\n",
      "  sample_time_ms: 18234.224\n",
      "  update_time_ms: 9.404\n",
      "timestamp: 1643498509\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 247380\n",
      "training_iteration: 31\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 263312\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-22-27\n",
      "done: false\n",
      "episode_len_mean: 203.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 38\n",
      "episodes_total: 1828\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0762649423877397\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019329700877636212\n",
      "        policy_loss: -0.08789795162777106\n",
      "        total_loss: 220.79030238469443\n",
      "        vf_explained_var: -0.07502291371424993\n",
      "        vf_loss: 220.8743333832423\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1529276658097902\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01671968468508794\n",
      "        policy_loss: -0.05125375409610569\n",
      "        total_loss: 187.78469010988871\n",
      "        vf_explained_var: 0.09862420390049617\n",
      "        vf_loss: 187.8309278011322\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.179066029191017\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014428292970639328\n",
      "        policy_loss: -0.11558465048981209\n",
      "        total_loss: 160.2794003756841\n",
      "        vf_explained_var: 0.04074347863594691\n",
      "        vf_loss: 160.39065654118855\n",
      "  num_agent_steps_sampled: 263312\n",
      "  num_agent_steps_trained: 263312\n",
      "  num_steps_sampled: 263340\n",
      "  num_steps_trained: 263340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 33\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 19.39090909090909\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.57727272727273\n",
      "  vram_util_percent0: 0.42805989583333337\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 53.0\n",
      "  player_1: 56.66666666666667\n",
      "  player_2: 50.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 0.5200000000000008\n",
      "  player_1: -3.9399999999999995\n",
      "  player_2: 6.42\n",
      "policy_reward_min:\n",
      "  player_0: -71.33333333333333\n",
      "  player_1: -54.0\n",
      "  player_2: -68.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10971595461828204\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5708453102999137\n",
      "  mean_inference_ms: 1.9784146788467638\n",
      "  mean_raw_obs_processing_ms: 0.2606384686808608\n",
      "time_since_restore: 607.1315352916718\n",
      "time_this_iter_s: 18.693085193634033\n",
      "time_total_s: 607.1315352916718\n",
      "timers:\n",
      "  learn_throughput: 473.364\n",
      "  learn_time_ms: 16858.06\n",
      "  load_throughput: 830151.491\n",
      "  load_time_ms: 9.613\n",
      "  sample_throughput: 432.555\n",
      "  sample_time_ms: 18448.535\n",
      "  update_time_ms: 8.665\n",
      "timestamp: 1643498547\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 263340\n",
      "training_iteration: 33\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 279270\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-23-03\n",
      "done: false\n",
      "episode_len_mean: 202.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 39\n",
      "episodes_total: 1905\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0646184135476748\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01371622882587232\n",
      "        policy_loss: -0.10137091264439126\n",
      "        total_loss: 139.87472299098968\n",
      "        vf_explained_var: -0.07520783285299937\n",
      "        vf_loss: 139.97335074742634\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1516817226012548\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01587265332825737\n",
      "        policy_loss: -0.07971226768568158\n",
      "        total_loss: 183.67006074905396\n",
      "        vf_explained_var: -0.14903145432472228\n",
      "        vf_loss: 183.74501195907592\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1728580991427104\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01565465152475696\n",
      "        policy_loss: -0.06345654840581119\n",
      "        total_loss: 133.5274517472585\n",
      "        vf_explained_var: 0.10712698489427566\n",
      "        vf_loss: 133.58621225357055\n",
      "  num_agent_steps_sampled: 279270\n",
      "  num_agent_steps_trained: 279270\n",
      "  num_steps_sampled: 279300\n",
      "  num_steps_trained: 279300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 35\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.704347826086956\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.09999999999998\n",
      "  vram_util_percent0: 0.42805989583333337\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 53.0\n",
      "  player_1: 57.0\n",
      "  player_2: 54.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 2.373333333333333\n",
      "  player_1: -1.906666666666667\n",
      "  player_2: 2.5333333333333328\n",
      "policy_reward_min:\n",
      "  player_0: -69.33333333333333\n",
      "  player_1: -82.0\n",
      "  player_2: -80.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10974658779299702\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5590127935649466\n",
      "  mean_inference_ms: 1.9798143600836735\n",
      "  mean_raw_obs_processing_ms: 0.260021655823999\n",
      "time_since_restore: 642.9018821716309\n",
      "time_this_iter_s: 18.539117574691772\n",
      "time_total_s: 642.9018821716309\n",
      "timers:\n",
      "  learn_throughput: 467.593\n",
      "  learn_time_ms: 17066.117\n",
      "  load_throughput: 699069.647\n",
      "  load_time_ms: 11.415\n",
      "  sample_throughput: 432.022\n",
      "  sample_time_ms: 18471.29\n",
      "  update_time_ms: 9.009\n",
      "timestamp: 1643498583\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 279300\n",
      "training_iteration: 35\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 295232\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-23-38\n",
      "done: false\n",
      "episode_len_mean: 208.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 34\n",
      "episodes_total: 1980\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0108048524459203\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018428599889644962\n",
      "        policy_loss: -0.10602750650296609\n",
      "        total_loss: 171.644049838384\n",
      "        vf_explained_var: -0.058621611992518106\n",
      "        vf_loss: 171.74639286677044\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.082850265900294\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01701698120421502\n",
      "        policy_loss: -0.0593526620681708\n",
      "        total_loss: 130.59220063289007\n",
      "        vf_explained_var: -0.05310974945624669\n",
      "        vf_loss: 130.64644964853923\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1292816510796546\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017203311843058202\n",
      "        policy_loss: -0.08819166247034445\n",
      "        total_loss: 194.83137884775797\n",
      "        vf_explained_var: 0.02941164900859197\n",
      "        vf_loss: 194.91440982818602\n",
      "  num_agent_steps_sampled: 295232\n",
      "  num_agent_steps_trained: 295232\n",
      "  num_steps_sampled: 295260\n",
      "  num_steps_trained: 295260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 37\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.930000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.69000000000001\n",
      "  vram_util_percent0: 0.4280598958333333\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 52.0\n",
      "  player_1: 45.666666666666664\n",
      "  player_2: 51.0\n",
      "policy_reward_mean:\n",
      "  player_0: 3.733333333333332\n",
      "  player_1: -4.736666666666668\n",
      "  player_2: 4.003333333333332\n",
      "policy_reward_min:\n",
      "  player_0: -67.66666666666667\n",
      "  player_1: -63.66666666666667\n",
      "  player_2: -81.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10947023786228349\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5477958504369461\n",
      "  mean_inference_ms: 1.977341706739524\n",
      "  mean_raw_obs_processing_ms: 0.259280295303058\n",
      "time_since_restore: 677.8993730545044\n",
      "time_this_iter_s: 16.668480157852173\n",
      "time_total_s: 677.8993730545044\n",
      "timers:\n",
      "  learn_throughput: 474.518\n",
      "  learn_time_ms: 16817.082\n",
      "  load_throughput: 737347.134\n",
      "  load_time_ms: 10.823\n",
      "  sample_throughput: 426.001\n",
      "  sample_time_ms: 18732.334\n",
      "  update_time_ms: 9.32\n",
      "timestamp: 1643498618\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 295260\n",
      "training_iteration: 37\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 311190\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-24-17\n",
      "done: false\n",
      "episode_len_mean: 218.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 36\n",
      "episodes_total: 2053\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9992938501636187\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018033198286508803\n",
      "        policy_loss: -0.07048914788135638\n",
      "        total_loss: 112.23069485028584\n",
      "        vf_explained_var: 0.3717025955518087\n",
      "        vf_loss: 112.29577444712321\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.085199786623319\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01873693008419347\n",
      "        policy_loss: -0.0864523216833671\n",
      "        total_loss: 226.10153415997823\n",
      "        vf_explained_var: 0.03322352806727091\n",
      "        vf_loss: 226.18236584504444\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.1157453821102779\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017034214908160077\n",
      "        policy_loss: -0.09693182197088997\n",
      "        total_loss: 225.8491376845042\n",
      "        vf_explained_var: -0.01506414532661438\n",
      "        vf_loss: 225.94095845540366\n",
      "  num_agent_steps_sampled: 311190\n",
      "  num_agent_steps_trained: 311190\n",
      "  num_steps_sampled: 311220\n",
      "  num_steps_trained: 311220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 39\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 19.929166666666667\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.31666666666666\n",
      "  vram_util_percent0: 0.4280598958333333\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 50.66666666666667\n",
      "  player_1: 45.666666666666664\n",
      "  player_2: 53.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 6.2799999999999985\n",
      "  player_1: -2.429999999999999\n",
      "  player_2: -0.85\n",
      "policy_reward_min:\n",
      "  player_0: -76.66666666666667\n",
      "  player_1: -64.33333333333333\n",
      "  player_2: -69.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10963696163102521\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5372384247178783\n",
      "  mean_inference_ms: 1.976839044339614\n",
      "  mean_raw_obs_processing_ms: 0.25887059057795847\n",
      "time_since_restore: 716.209525346756\n",
      "time_this_iter_s: 20.51382851600647\n",
      "time_total_s: 716.209525346756\n",
      "timers:\n",
      "  learn_throughput: 469.886\n",
      "  learn_time_ms: 16982.85\n",
      "  load_throughput: 851728.518\n",
      "  load_time_ms: 9.369\n",
      "  sample_throughput: 431.748\n",
      "  sample_time_ms: 18483.007\n",
      "  update_time_ms: 11.376\n",
      "timestamp: 1643498657\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 311220\n",
      "training_iteration: 39\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 327150\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-24-55\n",
      "done: false\n",
      "episode_len_mean: 233.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 32\n",
      "episodes_total: 2117\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9200937452912331\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014817447848715044\n",
      "        policy_loss: -0.08632091193304707\n",
      "        total_loss: 161.3214894469579\n",
      "        vf_explained_var: 0.08106232871611913\n",
      "        vf_loss: 161.40114329814912\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0592757207155228\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01772173498068658\n",
      "        policy_loss: -0.11426761331347128\n",
      "        total_loss: 122.44548890272776\n",
      "        vf_explained_var: 0.15509822060664494\n",
      "        vf_loss: 122.55444027582804\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.089553313056628\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018924071774826908\n",
      "        policy_loss: -0.06882726235625644\n",
      "        total_loss: 186.72862689654033\n",
      "        vf_explained_var: 0.10693195203940074\n",
      "        vf_loss: 186.7917763376236\n",
      "  num_agent_steps_sampled: 327150\n",
      "  num_agent_steps_trained: 327150\n",
      "  num_steps_sampled: 327180\n",
      "  num_steps_trained: 327180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 41\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.54782608695652\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.5782608695652\n",
      "  vram_util_percent0: 0.42805989583333337\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 44.333333333333336\n",
      "  player_1: 43.0\n",
      "  player_2: 53.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 6.036666666666666\n",
      "  player_1: -3.143333333333333\n",
      "  player_2: 0.10666666666666665\n",
      "policy_reward_min:\n",
      "  player_0: -53.66666666666667\n",
      "  player_1: -64.33333333333333\n",
      "  player_2: -76.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10958809522296989\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5284487506432686\n",
      "  mean_inference_ms: 1.9768927008708181\n",
      "  mean_raw_obs_processing_ms: 0.258359640851347\n",
      "time_since_restore: 754.6486852169037\n",
      "time_this_iter_s: 18.9141583442688\n",
      "time_total_s: 754.6486852169037\n",
      "timers:\n",
      "  learn_throughput: 468.013\n",
      "  learn_time_ms: 17050.817\n",
      "  load_throughput: 663659.644\n",
      "  load_time_ms: 12.024\n",
      "  sample_throughput: 426.325\n",
      "  sample_time_ms: 18718.116\n",
      "  update_time_ms: 8.596\n",
      "timestamp: 1643498695\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 327180\n",
      "training_iteration: 41\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 343112\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-25-33\n",
      "done: false\n",
      "episode_len_mean: 249.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 33\n",
      "episodes_total: 2184\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8678500883777936\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014413311142946745\n",
      "        policy_loss: -0.10552734112987916\n",
      "        total_loss: 123.4606375392278\n",
      "        vf_explained_var: 0.057009276946385704\n",
      "        vf_loss: 123.55967935403189\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9778960417707762\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01696421903258283\n",
      "        policy_loss: -0.0778969810072643\n",
      "        total_loss: 165.95863442818325\n",
      "        vf_explained_var: 0.05497660994529724\n",
      "        vf_loss: 166.0314421526591\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0472627003987631\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015792537361339783\n",
      "        policy_loss: -0.07874362523512293\n",
      "        total_loss: 174.78704426606495\n",
      "        vf_explained_var: -0.18529836575190226\n",
      "        vf_loss: 174.86104938348134\n",
      "  num_agent_steps_sampled: 343112\n",
      "  num_agent_steps_trained: 343112\n",
      "  num_steps_sampled: 343140\n",
      "  num_steps_trained: 343140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 43\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.06521739130435\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.72608695652173\n",
      "  vram_util_percent0: 0.42805989583333337\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 41.666666666666664\n",
      "  player_1: 46.0\n",
      "  player_2: 49.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 2.876666666666667\n",
      "  player_1: -1.3733333333333337\n",
      "  player_2: 1.4966666666666668\n",
      "policy_reward_min:\n",
      "  player_0: -74.66666666666667\n",
      "  player_1: -79.0\n",
      "  player_2: -76.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10927749143649908\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.52014129972538\n",
      "  mean_inference_ms: 1.9728524460283814\n",
      "  mean_raw_obs_processing_ms: 0.25785033087972403\n",
      "time_since_restore: 792.7415790557861\n",
      "time_this_iter_s: 18.746604442596436\n",
      "time_total_s: 792.7415790557861\n",
      "timers:\n",
      "  learn_throughput: 468.425\n",
      "  learn_time_ms: 17035.795\n",
      "  load_throughput: 664270.791\n",
      "  load_time_ms: 12.013\n",
      "  sample_throughput: 429.754\n",
      "  sample_time_ms: 18568.774\n",
      "  update_time_ms: 8.583\n",
      "timestamp: 1643498733\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 343140\n",
      "training_iteration: 43\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 359072\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-26-10\n",
      "done: false\n",
      "episode_len_mean: 247.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 34\n",
      "episodes_total: 2249\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.891565334200859\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014851447235338735\n",
      "        policy_loss: -0.07532503097628554\n",
      "        total_loss: 180.21893364429474\n",
      "        vf_explained_var: 0.13718296070893607\n",
      "        vf_loss: 180.2875757487615\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0089959073066712\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01842933027570074\n",
      "        policy_loss: -0.0991943084821105\n",
      "        total_loss: 128.09103317578635\n",
      "        vf_explained_var: 0.03467026482025782\n",
      "        vf_loss: 128.18469858805338\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0984004352490107\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01936070881895527\n",
      "        policy_loss: -0.09619914261624217\n",
      "        total_loss: 234.34165830930075\n",
      "        vf_explained_var: 0.13012735515832902\n",
      "        vf_loss: 234.43204961299895\n",
      "  num_agent_steps_sampled: 359072\n",
      "  num_agent_steps_trained: 359072\n",
      "  num_steps_sampled: 359100\n",
      "  num_steps_trained: 359100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 45\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.695454545454549\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.94545454545455\n",
      "  vram_util_percent0: 0.42805989583333337\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 44.666666666666664\n",
      "  player_1: 54.0\n",
      "  player_2: 51.33333333333333\n",
      "policy_reward_mean:\n",
      "  player_0: 4.703333333333335\n",
      "  player_1: 0.633333333333333\n",
      "  player_2: -2.3366666666666664\n",
      "policy_reward_min:\n",
      "  player_0: -52.66666666666667\n",
      "  player_1: -88.66666666666667\n",
      "  player_2: -70.33333333333333\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10943131631408981\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.513365576851481\n",
      "  mean_inference_ms: 1.9756168564256558\n",
      "  mean_raw_obs_processing_ms: 0.2574441005884872\n",
      "time_since_restore: 829.6407060623169\n",
      "time_this_iter_s: 18.356245040893555\n",
      "time_total_s: 829.6407060623169\n",
      "timers:\n",
      "  learn_throughput: 466.388\n",
      "  learn_time_ms: 17110.224\n",
      "  load_throughput: 685012.769\n",
      "  load_time_ms: 11.649\n",
      "  sample_throughput: 425.646\n",
      "  sample_time_ms: 18747.953\n",
      "  update_time_ms: 8.415\n",
      "timestamp: 1643498770\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 359100\n",
      "training_iteration: 45\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 375030\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-26-50\n",
      "done: false\n",
      "episode_len_mean: 232.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 32\n",
      "episodes_total: 2317\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8861040666699409\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01403128008986073\n",
      "        policy_loss: -0.08009484568921228\n",
      "        total_loss: 158.60568046569824\n",
      "        vf_explained_var: -0.0467265052596728\n",
      "        vf_loss: 158.67946129163107\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9897605693340301\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019496150271976754\n",
      "        policy_loss: -0.08988002348070344\n",
      "        total_loss: 217.05520215034485\n",
      "        vf_explained_var: 0.14050844689210257\n",
      "        vf_loss: 217.13923396746318\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0287288006146749\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018448182595427364\n",
      "        policy_loss: -0.08079418741477032\n",
      "        total_loss: 222.2220920976003\n",
      "        vf_explained_var: 0.0976171495517095\n",
      "        vf_loss: 222.2973510456085\n",
      "  num_agent_steps_sampled: 375030\n",
      "  num_agent_steps_trained: 375030\n",
      "  num_steps_sampled: 375060\n",
      "  num_steps_trained: 375060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 47\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.433333333333332\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.77083333333333\n",
      "  vram_util_percent0: 0.4280598958333333\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 44.0\n",
      "  player_1: 47.0\n",
      "  player_2: 51.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 2.97\n",
      "  player_1: 0.4599999999999996\n",
      "  player_2: -0.4299999999999999\n",
      "policy_reward_min:\n",
      "  player_0: -67.66666666666667\n",
      "  player_1: -79.33333333333333\n",
      "  player_2: -65.66666666666666\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10965130403273929\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5060522163220735\n",
      "  mean_inference_ms: 1.9810558973172387\n",
      "  mean_raw_obs_processing_ms: 0.2578059436344338\n",
      "time_since_restore: 869.165744304657\n",
      "time_this_iter_s: 19.64555311203003\n",
      "time_total_s: 869.165744304657\n",
      "timers:\n",
      "  learn_throughput: 454.762\n",
      "  learn_time_ms: 17547.656\n",
      "  load_throughput: 668636.637\n",
      "  load_time_ms: 11.935\n",
      "  sample_throughput: 422.735\n",
      "  sample_time_ms: 18877.078\n",
      "  update_time_ms: 8.263\n",
      "timestamp: 1643498810\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 375060\n",
      "training_iteration: 47\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 390990\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-27-29\n",
      "done: false\n",
      "episode_len_mean: 238.28\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999996\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 2381\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8793315181136131\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013307222214360384\n",
      "        policy_loss: -0.0924615704609702\n",
      "        total_loss: 93.43446860472362\n",
      "        vf_explained_var: 0.032935812572638196\n",
      "        vf_loss: 93.52094173272451\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9858268974224726\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018699479923495802\n",
      "        policy_loss: -0.08950781929306686\n",
      "        total_loss: 91.05119281133015\n",
      "        vf_explained_var: 0.0488740466038386\n",
      "        vf_loss: 91.13509080886841\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 1.0327334602673848\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017149142817861833\n",
      "        policy_loss: -0.07867005235825976\n",
      "        total_loss: 153.528086104393\n",
      "        vf_explained_var: 0.1618047688404719\n",
      "        vf_loss: 153.6016111834844\n",
      "  num_agent_steps_sampled: 390990\n",
      "  num_agent_steps_trained: 390990\n",
      "  num_steps_sampled: 391020\n",
      "  num_steps_trained: 391020\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 49\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.531818181818187\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.00000000000001\n",
      "  vram_util_percent0: 0.42805989583333337\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 65.33333333333333\n",
      "  player_1: 41.33333333333333\n",
      "  player_2: 54.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 4.016666666666667\n",
      "  player_1: 0.4366666666666665\n",
      "  player_2: -1.4533333333333331\n",
      "policy_reward_min:\n",
      "  player_0: -67.66666666666667\n",
      "  player_1: -73.66666666666667\n",
      "  player_2: -58.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11001205240729117\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.5028618262172487\n",
      "  mean_inference_ms: 1.990551460608582\n",
      "  mean_raw_obs_processing_ms: 0.2586505100719937\n",
      "time_since_restore: 907.9277288913727\n",
      "time_this_iter_s: 18.459940671920776\n",
      "time_total_s: 907.9277288913727\n",
      "timers:\n",
      "  learn_throughput: 455.318\n",
      "  learn_time_ms: 17526.23\n",
      "  load_throughput: 502466.443\n",
      "  load_time_ms: 15.882\n",
      "  sample_throughput: 410.748\n",
      "  sample_time_ms: 19427.991\n",
      "  update_time_ms: 6.128\n",
      "timestamp: 1643498849\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 391020\n",
      "training_iteration: 49\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 406950\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-28-06\n",
      "done: false\n",
      "episode_len_mean: 270.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000004\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 2432\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.890407572388649\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01580103890809672\n",
      "        policy_loss: -0.10355438562342897\n",
      "        total_loss: 47.84762918631236\n",
      "        vf_explained_var: 0.24166712790727615\n",
      "        vf_loss: 47.94407292922338\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8637430021166801\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01868092638945427\n",
      "        policy_loss: -0.0751331665335844\n",
      "        total_loss: 76.71794919570287\n",
      "        vf_explained_var: -0.0025165735681851706\n",
      "        vf_loss: 76.78747832775116\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9748691977063815\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01654842830107408\n",
      "        policy_loss: -0.0983371883087481\n",
      "        total_loss: 95.53205896774928\n",
      "        vf_explained_var: -0.13063483347495397\n",
      "        vf_loss: 95.62294907490413\n",
      "  num_agent_steps_sampled: 406950\n",
      "  num_agent_steps_trained: 406950\n",
      "  num_steps_sampled: 406980\n",
      "  num_steps_trained: 406980\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 51\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.776190476190475\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 67.95714285714287\n",
      "  vram_util_percent0: 0.4280598958333333\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 65.33333333333333\n",
      "  player_1: 39.0\n",
      "  player_2: 54.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 2.996666666666667\n",
      "  player_1: -1.6933333333333331\n",
      "  player_2: 1.6966666666666663\n",
      "policy_reward_min:\n",
      "  player_0: -66.33333333333333\n",
      "  player_1: -70.66666666666667\n",
      "  player_2: -59.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11079003767219023\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4998951646263839\n",
      "  mean_inference_ms: 2.0017866226333867\n",
      "  mean_raw_obs_processing_ms: 0.25967985252241366\n",
      "time_since_restore: 945.1223521232605\n",
      "time_this_iter_s: 18.042829751968384\n",
      "time_total_s: 945.1223521232605\n",
      "timers:\n",
      "  learn_throughput: 460.529\n",
      "  learn_time_ms: 17327.909\n",
      "  load_throughput: 548531.116\n",
      "  load_time_ms: 14.548\n",
      "  sample_throughput: 414.727\n",
      "  sample_time_ms: 19241.588\n",
      "  update_time_ms: 6.253\n",
      "timestamp: 1643498886\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 406980\n",
      "training_iteration: 51\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 422910\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-28-42\n",
      "done: false\n",
      "episode_len_mean: 309.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 2483\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.813937120338281\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01609964930214692\n",
      "        policy_loss: -0.046297359003995855\n",
      "        total_loss: 154.92117095629374\n",
      "        vf_explained_var: 0.011616352597872416\n",
      "        vf_loss: 154.96022251764933\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8381394176185131\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01751713996990181\n",
      "        policy_loss: -0.1166162143383796\n",
      "        total_loss: 144.03200889905293\n",
      "        vf_explained_var: 0.07977721035480499\n",
      "        vf_loss: 144.1433692852656\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9687775664528211\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01485268545448586\n",
      "        policy_loss: -0.06853694356667499\n",
      "        total_loss: 162.04971869309745\n",
      "        vf_explained_var: -0.03594498078028361\n",
      "        vf_loss: 162.1115725851059\n",
      "  num_agent_steps_sampled: 422910\n",
      "  num_agent_steps_trained: 422910\n",
      "  num_steps_sampled: 422940\n",
      "  num_steps_trained: 422940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 53\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.90454545454545\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.00454545454545\n",
      "  vram_util_percent0: 0.42805989583333337\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 54.33333333333333\n",
      "  player_1: 45.33333333333333\n",
      "  player_2: 50.0\n",
      "policy_reward_mean:\n",
      "  player_0: 1.25\n",
      "  player_1: -2.4600000000000004\n",
      "  player_2: 4.210000000000001\n",
      "policy_reward_min:\n",
      "  player_0: -61.666666666666664\n",
      "  player_1: -66.33333333333333\n",
      "  player_2: -71.66666666666667\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11096509231263604\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4950821503774953\n",
      "  mean_inference_ms: 2.007467563711951\n",
      "  mean_raw_obs_processing_ms: 0.2601744209818365\n",
      "time_since_restore: 981.2398507595062\n",
      "time_this_iter_s: 18.197331190109253\n",
      "time_total_s: 981.2398507595062\n",
      "timers:\n",
      "  learn_throughput: 465.881\n",
      "  learn_time_ms: 17128.844\n",
      "  load_throughput: 555155.479\n",
      "  load_time_ms: 14.374\n",
      "  sample_throughput: 420.716\n",
      "  sample_time_ms: 18967.668\n",
      "  update_time_ms: 6.412\n",
      "timestamp: 1643498922\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 422940\n",
      "training_iteration: 53\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 438870\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-29-19\n",
      "done: false\n",
      "episode_len_mean: 322.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0000000000000013\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 29\n",
      "episodes_total: 2533\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8221568946540355\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01475705220096747\n",
      "        policy_loss: -0.09494012163331111\n",
      "        total_loss: 69.35144858996074\n",
      "        vf_explained_var: -0.07113509545723597\n",
      "        vf_loss: 69.43974800427755\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8497394624352456\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017194577513952633\n",
      "        policy_loss: -0.014833589767416317\n",
      "        total_loss: 100.99145708084106\n",
      "        vf_explained_var: -0.04040900260210037\n",
      "        vf_loss: 101.00113268375397\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9415079815189044\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013411522438183283\n",
      "        policy_loss: -0.11430532884473601\n",
      "        total_loss: 172.51094595909117\n",
      "        vf_explained_var: 0.08157073398431142\n",
      "        vf_loss: 172.61921559333803\n",
      "  num_agent_steps_sampled: 438870\n",
      "  num_agent_steps_trained: 438870\n",
      "  num_steps_sampled: 438900\n",
      "  num_steps_trained: 438900\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 55\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.934782608695652\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.36086956521741\n",
      "  vram_util_percent0: 0.42805989583333337\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 54.33333333333333\n",
      "  player_1: 59.0\n",
      "  player_2: 46.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 3.5000000000000013\n",
      "  player_1: -0.1499999999999998\n",
      "  player_2: -0.34999999999999987\n",
      "policy_reward_min:\n",
      "  player_0: -65.66666666666667\n",
      "  player_1: -65.66666666666667\n",
      "  player_2: -84.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11108375397183334\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4908187766459484\n",
      "  mean_inference_ms: 2.0118084131837204\n",
      "  mean_raw_obs_processing_ms: 0.2606831767586314\n",
      "time_since_restore: 1018.2857010364532\n",
      "time_this_iter_s: 18.983421802520752\n",
      "time_total_s: 1018.2857010364532\n",
      "timers:\n",
      "  learn_throughput: 465.451\n",
      "  learn_time_ms: 17144.652\n",
      "  load_throughput: 642280.97\n",
      "  load_time_ms: 12.424\n",
      "  sample_throughput: 423.79\n",
      "  sample_time_ms: 18830.099\n",
      "  update_time_ms: 6.611\n",
      "timestamp: 1643498959\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 438900\n",
      "training_iteration: 55\n",
      "trial_id: default\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0130 00:29:40.956252107   30179 backup_poller.cc:134]       Run client channel backup poller: {\"created\":\"@1643498980.956227260\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1643498980.956223583\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 454830\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-29-56\n",
      "done: false\n",
      "episode_len_mean: 313.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 2585\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8341989959279696\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015439362842436709\n",
      "        policy_loss: -0.06870315346090744\n",
      "        total_loss: 101.0763548374176\n",
      "        vf_explained_var: 0.14213007052739463\n",
      "        vf_loss: 101.138110871315\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.840692231853803\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017356043629588383\n",
      "        policy_loss: -0.08558359454075495\n",
      "        total_loss: 115.641189549764\n",
      "        vf_explained_var: 0.07998764952023824\n",
      "        vf_loss: 115.72156665166219\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.9414480262001356\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017081171352274396\n",
      "        policy_loss: -0.08100238447388013\n",
      "        total_loss: 137.11275079886119\n",
      "        vf_explained_var: 0.05097639958063761\n",
      "        vf_loss: 137.1860666735967\n",
      "  num_agent_steps_sampled: 454830\n",
      "  num_agent_steps_trained: 454830\n",
      "  num_steps_sampled: 454860\n",
      "  num_steps_trained: 454860\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 57\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.930434782608698\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.39565217391308\n",
      "  vram_util_percent0: 0.42805989583333337\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 42.33333333333333\n",
      "  player_1: 59.0\n",
      "  player_2: 46.333333333333336\n",
      "policy_reward_mean:\n",
      "  player_0: 2.3333333333333335\n",
      "  player_1: 1.9133333333333329\n",
      "  player_2: -1.2466666666666668\n",
      "policy_reward_min:\n",
      "  player_0: -65.66666666666667\n",
      "  player_1: -66.66666666666667\n",
      "  player_2: -84.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11103453466797589\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4859202895728727\n",
      "  mean_inference_ms: 2.0111087632491054\n",
      "  mean_raw_obs_processing_ms: 0.2605363832839482\n",
      "time_since_restore: 1055.3149387836456\n",
      "time_this_iter_s: 19.021422147750854\n",
      "time_total_s: 1055.3149387836456\n",
      "timers:\n",
      "  learn_throughput: 471.42\n",
      "  learn_time_ms: 16927.563\n",
      "  load_throughput: 624729.047\n",
      "  load_time_ms: 12.774\n",
      "  sample_throughput: 427.157\n",
      "  sample_time_ms: 18681.667\n",
      "  update_time_ms: 6.537\n",
      "timestamp: 1643498996\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 454860\n",
      "training_iteration: 57\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 470790\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-30-33\n",
      "done: false\n",
      "episode_len_mean: 309.84\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 2.9999999999999987\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 30\n",
      "episodes_total: 2641\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8288185840348402\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01611535901067024\n",
      "        policy_loss: -0.03814939691219479\n",
      "        total_loss: 159.09323300838471\n",
      "        vf_explained_var: 0.17003277460734048\n",
      "        vf_loss: 159.12413096268972\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8070621519287428\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017531143665081194\n",
      "        policy_loss: -0.0593133615081509\n",
      "        total_loss: 131.26040059725443\n",
      "        vf_explained_var: 0.0684946937362353\n",
      "        vf_loss: 131.31445485432943\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8941133639216423\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013835196074290745\n",
      "        policy_loss: -0.12119832762827476\n",
      "        total_loss: 131.2428291686376\n",
      "        vf_explained_var: 0.1505091185371081\n",
      "        vf_loss: 131.35780165831247\n",
      "  num_agent_steps_sampled: 470790\n",
      "  num_agent_steps_trained: 470790\n",
      "  num_steps_sampled: 470820\n",
      "  num_steps_trained: 470820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 59\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.933333333333328\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.75714285714287\n",
      "  vram_util_percent0: 0.4280598958333333\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 52.0\n",
      "  player_1: 50.0\n",
      "  player_2: 54.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 2.66\n",
      "  player_1: 0.029999999999999645\n",
      "  player_2: 0.3099999999999997\n",
      "policy_reward_min:\n",
      "  player_0: -83.33333333333333\n",
      "  player_1: -102.33333333333333\n",
      "  player_2: -82.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11065721244676506\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4808796612934016\n",
      "  mean_inference_ms: 2.006473394052679\n",
      "  mean_raw_obs_processing_ms: 0.2592860159730316\n",
      "time_since_restore: 1092.041674375534\n",
      "time_this_iter_s: 17.012645483016968\n",
      "time_total_s: 1092.041674375534\n",
      "timers:\n",
      "  learn_throughput: 474.714\n",
      "  learn_time_ms: 16810.117\n",
      "  load_throughput: 954798.187\n",
      "  load_time_ms: 8.358\n",
      "  sample_throughput: 430.209\n",
      "  sample_time_ms: 18549.107\n",
      "  update_time_ms: 6.186\n",
      "timestamp: 1643499033\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 470820\n",
      "training_iteration: 59\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 486750\n",
      "custom_metrics: {}\n",
      "date: 2022-01-30_00-31-07\n",
      "done: false\n",
      "episode_len_mean: 296.84\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.000000000000014\n",
      "episode_reward_mean: 3.0\n",
      "episode_reward_min: 2.999999999999986\n",
      "episodes_this_iter: 18\n",
      "episodes_total: 2683\n",
      "experiment_id: 736c238500184c1faf884c2b6581ce08\n",
      "hostname: lenovo-michi\n",
      "info:\n",
      "  learner:\n",
      "    player_0:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7160547390083472\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015088919515473549\n",
      "        policy_loss: -0.01759973424176375\n",
      "        total_loss: 53.70209211111069\n",
      "        vf_explained_var: -0.17541035920381545\n",
      "        vf_loss: 53.71290177583695\n",
      "    player_1:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.7318891656895479\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0160362039814936\n",
      "        policy_loss: 0.001794651256253322\n",
      "        total_loss: 34.66765187740326\n",
      "        vf_explained_var: -0.14416483124097187\n",
      "        vf_loss: 34.65864082256953\n",
      "    player_2:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.45\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 0.8863636556267739\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015061561267912112\n",
      "        policy_loss: -0.14570397860681017\n",
      "        total_loss: 85.7206895128886\n",
      "        vf_explained_var: -0.08109597623348236\n",
      "        vf_loss: 85.85961580673853\n",
      "  num_agent_steps_sampled: 486750\n",
      "  num_agent_steps_trained: 486750\n",
      "  num_steps_sampled: 486780\n",
      "  num_steps_trained: 486780\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 61\n",
      "node_ip: 172.22.125.215\n",
      "num_healthy_workers: 15\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.680000000000001\n",
      "  gpu_util_percent0: .nan\n",
      "  ram_util_percent: 68.60999999999999\n",
      "  vram_util_percent0: 0.4280598958333333\n",
      "pid: 18806\n",
      "policy_reward_max:\n",
      "  player_0: 55.0\n",
      "  player_1: 50.0\n",
      "  player_2: 54.66666666666667\n",
      "policy_reward_mean:\n",
      "  player_0: 4.286666666666667\n",
      "  player_1: 2.5466666666666664\n",
      "  player_2: -3.8333333333333326\n",
      "policy_reward_min:\n",
      "  player_0: -83.33333333333333\n",
      "  player_1: -102.33333333333333\n",
      "  player_2: -82.0\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.11029318250241911\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.4768451492488215\n",
      "  mean_inference_ms: 1.9985326892195263\n",
      "  mean_raw_obs_processing_ms: 0.25814695576852886\n",
      "time_since_restore: 1126.1854329109192\n",
      "time_this_iter_s: 16.800477743148804\n",
      "time_total_s: 1126.1854329109192\n",
      "timers:\n",
      "  learn_throughput: 480.806\n",
      "  learn_time_ms: 16597.132\n",
      "  load_throughput: 1122905.144\n",
      "  load_time_ms: 7.107\n",
      "  sample_throughput: 438.657\n",
      "  sample_time_ms: 18191.885\n",
      "  update_time_ms: 5.927\n",
      "timestamp: 1643499067\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 486780\n",
      "training_iteration: 61\n",
      "trial_id: default\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run manual training loop and print results after each iteration\n",
    "max_steps = 2e6\n",
    "max_iters = 100\n",
    "for iters in range(max_iters):\n",
    "    result = trainer.train()\n",
    "    if iters % 2 == 0:\n",
    "        print(pretty_print(result))\n",
    "    # stop training if the target train steps or reward are reached\n",
    "    if result[\"timesteps_total\"] >= max_steps:\n",
    "        print(\n",
    "            f\"training done, because max_steps {max_steps} {result['timesteps_total']} reached\"\n",
    "        )\n",
    "        break\n",
    "else:\n",
    "    print(f\"training done, because max_iters {max_iters} reached\")\n",
    "# manual test loop\n",
    "print(\"Finished training. Running manual test/inference loop.\")\n",
    "# prepare environment with max 10 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:44: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "  warnings.warn(\"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\")\n",
      "/home/michi/anaconda3/envs/skybo/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base.py:52: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "  warnings.warn(\"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'player_0': {'observations': array([ 3, 10,  1,  0,  2,  0,  1,  1,  0,  1,  0,  0,  0,  1,  0,  0,  0,\n",
      "        0, 15, 15,  9, 15, 15, 15, 15, 15, 15, 15, 15, 15,  2, 15,  9, 15,\n",
      "       15, 15, 15, 15, 15, 15, 15, 15,  2, 15,  9, 15, 15, 15, 15, 15, 15,\n",
      "       15, 15, 15,  2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t u\t u\t -2]\n",
      " [5\t u\t u\t u]]\n",
      "\n",
      "agent  player_0  action  draw from drawpile\n",
      "{'player_0': {'observations': array([ 3, 10,  1,  0,  2,  0,  1,  1,  0,  1,  0,  0,  0,  1,  0,  0,  0,\n",
      "        0,  9, 15,  9, 15, 15, 15, 15, 15, 15, 15, 15, 15,  2, 15,  9, 15,\n",
      "       15, 15, 15, 15, 15, 15, 15, 15,  2, 15,  9, 15, 15, 15, 15, 15, 15,\n",
      "       15, 15, 15,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 9 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t u\t u\t -2]\n",
      " [5\t u\t u\t u]]\n",
      "\n",
      "agent  player_0  action  handcard discard & reveal card (21) - col:1 row:3\n",
      "{'player_1': {'observations': array([ 3,  9,  1,  0,  2,  0,  1,  1,  1,  1,  0,  0,  0,  2,  0,  0,  0,\n",
      "        9, 15, 15, 15, 15,  3, 15, 15, 15, 15, 15, 15,  0, 15, 15, 15, 15,\n",
      "        3, 15, 15, 15, 15, 15, 15,  0, 15, 15, 15, 15,  3, 15, 15, 15, 15,\n",
      "       15, 15,  0, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t u\t u\t -2]\n",
      " [5\t u\t u\t u]]\n",
      "\n",
      "agent  player_1  action  draw from discard pile\n",
      "{'player_1': {'observations': array([ 3,  9,  1,  0,  2,  0,  1,  1,  1,  1,  0,  0,  0,  1,  0,  0,  0,\n",
      "        0,  9, 15, 15, 15,  3, 15, 15, 15, 15, 15, 15,  0, 15, 15, 15, 15,\n",
      "        3, 15, 15, 15, 15, 15, 15,  0, 15, 15, 15, 15,  3, 15, 15, 15, 15,\n",
      "       15, 15,  0, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "       0, 1, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 9 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [u\t u\t u\t u]\n",
      " [u\t u\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t u\t u\t -2]\n",
      " [5\t u\t u\t u]]\n",
      "\n",
      "agent  player_1  action  handcard discard & reveal card (21) - col:1 row:3\n",
      "{'player_2': {'observations': array([ 3,  9,  1,  0,  2,  0,  1,  2,  1,  1,  0,  0,  0,  2,  0,  0,  0,\n",
      "        9, 15, 15, 15, 15, 15, 15, 15, 15, -2,  5, 15, 15, 15, 15, 15, 15,\n",
      "       15, 15, 15, 15, -2,  5, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, -2,\n",
      "        5, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t u\t u\t -2]\n",
      " [5\t u\t u\t u]]\n",
      "\n",
      "agent  player_2  action  draw from discard pile\n",
      "{'player_2': {'observations': array([ 3,  9,  1,  0,  2,  0,  1,  2,  1,  1,  0,  0,  0,  1,  0,  0,  0,\n",
      "        0,  9, 15, 15, 15, 15, 15, 15, 15, -2,  5, 15, 15, 15, 15, 15, 15,\n",
      "       15, 15, 15, 15, -2,  5, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, -2,\n",
      "        5, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 9 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t u\t u\t -2]\n",
      " [5\t u\t u\t u]]\n",
      "\n",
      "agent  player_2  action  handcard discard & reveal card (17) - col:1 row:1\n",
      "{'player_0': {'observations': array([ 6,  9,  1,  0,  2,  0,  1,  2,  1,  1,  0,  0,  1,  2,  0,  0,  0,\n",
      "        9, 15, 15,  9, 15, 15, 15, 15, 15, 15, 15,  4, 15,  2, 15,  9, 15,\n",
      "       15, 15, 15, 15, 15, 15,  4, 15,  2, 15,  9, 15, 15, 15, 15, 15, 15,\n",
      "       15,  4, 15,  2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t u\t -2]\n",
      " [5\t u\t u\t u]]\n",
      "\n",
      "agent  player_0  action  draw from drawpile\n",
      "{'player_0': {'observations': array([ 6,  9,  1,  0,  2,  0,  1,  2,  1,  1,  0,  0,  1,  2,  0,  0,  0,\n",
      "        9,  1, 15,  9, 15, 15, 15, 15, 15, 15, 15,  4, 15,  2, 15,  9, 15,\n",
      "       15, 15, 15, 15, 15, 15,  4, 15,  2, 15,  9, 15, 15, 15, 15, 15, 15,\n",
      "       15,  4, 15,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       1, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 1 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t u\t -2]\n",
      " [5\t u\t u\t u]]\n",
      "\n",
      "agent  player_0  action  handcard discard & reveal card (20) - col:0 row:2\n",
      "{'player_1': {'observations': array([ 6,  8,  1,  0,  2,  1,  1,  2,  1,  1,  0,  0,  1,  3,  0,  0,  0,\n",
      "        1, 15, 15, 15, 15,  3, 15, 15, 15, 15, 15,  3,  0, 15, 15, 15, 15,\n",
      "        3, 15, 15, 15, 15, 15,  3,  0, 15, 15, 15, 15,  3, 15, 15, 15, 15,\n",
      "       15,  3,  0, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t u\t -2]\n",
      " [5\t u\t u\t u]]\n",
      "\n",
      "agent  player_1  action  draw from drawpile\n",
      "{'player_1': {'observations': array([ 6,  8,  1,  0,  2,  1,  1,  2,  1,  1,  0,  0,  1,  3,  0,  0,  0,\n",
      "        1, -2, 15, 15, 15,  3, 15, 15, 15, 15, 15,  3,  0, 15, 15, 15, 15,\n",
      "        3, 15, 15, 15, 15, 15,  3,  0, 15, 15, 15, 15,  3, 15, 15, 15, 15,\n",
      "       15,  3,  0, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: -2 \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [u\t u\t u\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t u\t -2]\n",
      " [5\t u\t u\t u]]\n",
      "\n",
      "agent  player_1  action  place card (4) - col:0 row:1\n",
      "{'player_2': {'observations': array([ 4,  8,  2,  1,  2,  1,  1,  2,  1,  1,  0,  0,  1,  3,  0,  0,  0,\n",
      "       -1, 15, 15, 15, 15, 15, 15,  8, 15, -2,  5, 15, 15, 15, 15, 15, 15,\n",
      "       15, 15,  8, 15, -2,  5, 15, 15, 15, 15, 15, 15, 15, 15,  8, 15, -2,\n",
      "        5, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [-2\t u\t u\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t u\t -2]\n",
      " [5\t u\t u\t u]]\n",
      "\n",
      "agent  player_2  action  draw from discard pile\n",
      "{'player_2': {'observations': array([ 4,  8,  2,  0,  2,  1,  1,  2,  1,  1,  0,  0,  1,  3,  0,  0,  0,\n",
      "        1, -1, 15, 15, 15, 15, 15,  8, 15, -2,  5, 15, 15, 15, 15, 15, 15,\n",
      "       15, 15,  8, 15, -2,  5, 15, 15, 15, 15, 15, 15, 15, 15,  8, 15, -2,\n",
      "        5, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: -1 \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [-2\t u\t u\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t u\t -2]\n",
      " [5\t u\t u\t u]]\n",
      "\n",
      "agent  player_2  action  place card (6) - col:2 row:2\n",
      "{'player_0': {'observations': array([ 4,  8,  2,  1,  2,  1,  1,  2,  1,  1,  1,  0,  1,  3,  0,  0,  0,\n",
      "        6, 15, 15,  9, 15, 15, 15, 15, 15, 15,  9,  4, 15,  2, 15,  9, 15,\n",
      "       15, 15, 15, 15, 15,  9,  4, 15,  2, 15,  9, 15, 15, 15, 15, 15, 15,\n",
      "        9,  4, 15,  2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [-2\t u\t u\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t u\t u\t u]]\n",
      "\n",
      "agent  player_0  action  draw from discard pile\n",
      "{'player_0': {'observations': array([ 4,  8,  2,  1,  2,  1,  1,  2,  1,  1,  0,  0,  1,  3,  0,  0,  0,\n",
      "        1,  6, 15,  9, 15, 15, 15, 15, 15, 15,  9,  4, 15,  2, 15,  9, 15,\n",
      "       15, 15, 15, 15, 15,  9,  4, 15,  2, 15,  9, 15, 15, 15, 15, 15, 15,\n",
      "        9,  4, 15,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 6 \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t u]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [-2\t u\t u\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t u\t u\t u]]\n",
      "\n",
      "agent  player_0  action  handcard discard & reveal card (15) - col:3 row:1\n",
      "{'player_1': {'observations': array([ 4,  7,  2,  1,  2,  1,  2,  2,  1,  1,  1,  0,  1,  3,  0,  0,  0,\n",
      "        6, 15, 15, 15, 15,  3, -2, 15, 15, 15, 15,  3,  0, 15, 15, 15, 15,\n",
      "        3, -2, 15, 15, 15, 15,  3,  0, 15, 15, 15, 15,  3, -2, 15, 15, 15,\n",
      "       15,  3,  0, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t 2]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [-2\t u\t u\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t u\t u\t u]]\n",
      "\n",
      "agent  player_1  action  draw from discard pile\n",
      "{'player_1': {'observations': array([ 4,  7,  2,  1,  2,  1,  2,  2,  1,  1,  0,  0,  1,  3,  0,  0,  0,\n",
      "        1,  6, 15, 15, 15,  3, -2, 15, 15, 15, 15,  3,  0, 15, 15, 15, 15,\n",
      "        3, -2, 15, 15, 15, 15,  3,  0, 15, 15, 15, 15,  3, -2, 15, 15, 15,\n",
      "       15,  3,  0, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 6 \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t 2]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [-2\t u\t u\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t u\t u\t u]]\n",
      "\n",
      "agent  player_1  action  place card (6) - col:2 row:2\n",
      "{'player_2': {'observations': array([10,  7,  2,  1,  3,  1,  2,  2,  1,  1,  1,  0,  1,  3,  0,  0,  0,\n",
      "        0, 15, 15, 15, 15, 15, 15,  8, -1, -2,  5, 15, 15, 15, 15, 15, 15,\n",
      "       15, 15,  8, -1, -2,  5, 15, 15, 15, 15, 15, 15, 15, 15,  8, -1, -2,\n",
      "        5, 15, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t 2]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [-2\t u\t 6\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t u\t u\t u]]\n",
      "\n",
      "agent  player_2  action  draw from discard pile\n",
      "{'player_2': {'observations': array([10,  7,  2,  1,  2,  1,  2,  2,  1,  1,  1,  0,  1,  3,  0,  0,  0,\n",
      "        1,  0, 15, 15, 15, 15, 15,  8, -1, -2,  5, 15, 15, 15, 15, 15, 15,\n",
      "       15, 15,  8, -1, -2,  5, 15, 15, 15, 15, 15, 15, 15, 15,  8, -1, -2,\n",
      "        5, 15, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
      "       1, 1, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 0 \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t 2]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [-2\t u\t 6\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t u\t u\t u]]\n",
      "\n",
      "agent  player_2  action  place card (9) - col:1 row:3\n",
      "{'player_0': {'observations': array([10,  7,  2,  1,  3,  1,  2,  2,  1,  1,  1,  0,  1,  3,  0,  1,  0,\n",
      "       11, 15, 15,  9, 15,  2, 15, 15, 15, 15,  9,  4, 15,  2, 15,  9, 15,\n",
      "        2, 15, 15, 15, 15,  9,  4, 15,  2, 15,  9, 15,  2, 15, 15, 15, 15,\n",
      "        9,  4, 15,  2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t 2]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [-2\t u\t 6\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t 0\t u\t u]]\n",
      "\n",
      "agent  player_0  action  draw from drawpile\n",
      "{'player_0': {'observations': array([10,  7,  2,  1,  3,  1,  2,  2,  1,  1,  1,  0,  1,  3,  0,  1,  0,\n",
      "       11, -2, 15,  9, 15,  2, 15, 15, 15, 15,  9,  4, 15,  2, 15,  9, 15,\n",
      "        2, 15, 15, 15, 15,  9,  4, 15,  2, 15,  9, 15,  2, 15, 15, 15, 15,\n",
      "        9,  4, 15,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: -2 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[u\t 9\t u\t 2]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [-2\t u\t 6\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t 0\t u\t u]]\n",
      "\n",
      "agent  player_0  action  handcard discard & reveal card (12) - col:0 row:0\n",
      "{'player_1': {'observations': array([10,  6,  3,  2,  3,  1,  2,  2,  1,  1,  1,  0,  1,  3,  0,  1,  0,\n",
      "       -2, 15, 15, 15, 15,  3, -2, 15,  6, 15, 15,  3,  0, 15, 15, 15, 15,\n",
      "        3, -2, 15,  6, 15, 15,  3,  0, 15, 15, 15, 15,  3, -2, 15,  6, 15,\n",
      "       15,  3,  0, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [-2\t u\t 6\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t 0\t u\t u]]\n",
      "\n",
      "agent  player_1  action  draw from discard pile\n",
      "{'player_1': {'observations': array([10,  6,  2,  2,  3,  1,  2,  2,  1,  1,  1,  0,  1,  3,  0,  1,  0,\n",
      "       11, -2, 15, 15, 15,  3, -2, 15,  6, 15, 15,  3,  0, 15, 15, 15, 15,\n",
      "        3, -2, 15,  6, 15, 15,  3,  0, 15, 15, 15, 15,  3, -2, 15,  6, 15,\n",
      "       15,  3,  0, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: -2 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t u\t u\t 3]\n",
      " [-2\t u\t 6\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t 0\t u\t u]]\n",
      "\n",
      "agent  player_1  action  handcard discard & reveal card (13) - col:1 row:0\n",
      "{'player_2': {'observations': array([ 9,  6,  3,  3,  3,  1,  2,  2,  1,  1,  1,  0,  1,  3,  0,  1,  0,\n",
      "       -2, 15, 15, 15, 15, 15, 15,  8, -1, -2,  5,  0, 15, 15, 15, 15, 15,\n",
      "       15, 15,  8, -1, -2,  5,  0, 15, 15, 15, 15, 15, 15, 15,  8, -1, -2,\n",
      "        5,  0, 15, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t 0\t u\t u]]\n",
      "\n",
      "agent  player_2  action  draw from discard pile\n",
      "{'player_2': {'observations': array([ 9,  6,  2,  3,  3,  1,  2,  2,  1,  1,  1,  0,  1,  3,  0,  1,  0,\n",
      "       11, -2, 15, 15, 15, 15, 15,  8, -1, -2,  5,  0, 15, 15, 15, 15, 15,\n",
      "       15, 15,  8, -1, -2,  5,  0, 15, 15, 15, 15, 15, 15, 15,  8, -1, -2,\n",
      "        5,  0, 15, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "       1, 1, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: -2 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t 0\t u\t u]]\n",
      "\n",
      "agent  player_2  action  place card (11) - col:3 row:3\n",
      "{'player_0': {'observations': array([ 8,  6,  3,  3,  3,  1,  2,  2,  2,  1,  1,  0,  1,  3,  0,  1,  0,\n",
      "        4, 15, -1,  9, 15,  2, 15, 15, 15, 15,  9,  4, 15,  2, -1,  9, 15,\n",
      "        2, 15, 15, 15, 15,  9,  4, 15,  2, -1,  9, 15,  2, 15, 15, 15, 15,\n",
      "        9,  4, 15,  2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t 0\t u\t -2]]\n",
      "\n",
      "agent  player_0  action  draw from drawpile\n",
      "{'player_0': {'observations': array([ 8,  6,  3,  3,  3,  1,  2,  2,  2,  1,  1,  0,  1,  3,  0,  1,  0,\n",
      "        4,  7, -1,  9, 15,  2, 15, 15, 15, 15,  9,  4, 15,  2, -1,  9, 15,\n",
      "        2, 15, 15, 15, 15,  9,  4, 15,  2, -1,  9, 15,  2, 15, 15, 15, 15,\n",
      "        9,  4, 15,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 7 \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t u\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t 0\t u\t -2]]\n",
      "\n",
      "agent  player_0  action  place card (10) - col:2 row:3\n",
      "{'player_1': {'observations': array([ 8,  5,  4,  3,  3,  1,  2,  2,  2,  1,  1,  1,  1,  3,  0,  1,  0,\n",
      "       -2, 15, 15, -1, 15,  3, -2, 15,  6, 15, 15,  3,  0, 15, 15, -1, 15,\n",
      "        3, -2, 15,  6, 15, 15,  3,  0, 15, 15, -1, 15,  3, -2, 15,  6, 15,\n",
      "       15,  3,  0, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: -2 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t 7\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t 0\t u\t -2]]\n",
      "\n",
      "agent  player_1  action  draw from discard pile\n",
      "{'player_1': {'observations': array([ 8,  5,  3,  3,  3,  1,  2,  2,  2,  1,  1,  1,  1,  3,  0,  1,  0,\n",
      "        4, -2, 15, -1, 15,  3, -2, 15,  6, 15, 15,  3,  0, 15, 15, -1, 15,\n",
      "        3, -2, 15,  6, 15, 15,  3,  0, 15, 15, -1, 15,  3, -2, 15,  6, 15,\n",
      "       15,  3,  0, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: -2 \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t 7\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[u\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t 0\t u\t -2]]\n",
      "\n",
      "agent  player_1  action  place card (0) - col:0 row:0\n",
      "{'player_2': {'observations': array([ 7,  5,  4,  3,  3,  1,  3,  2,  2,  1,  1,  1,  1,  3,  0,  1,  0,\n",
      "        2, 15, 15, 15, 15, 15, 15,  8, -1, -2,  5,  0, 15, -2, 15, 15, 15,\n",
      "       15, 15,  8, -1, -2,  5,  0, 15, -2, 15, 15, 15, 15, 15,  8, -1, -2,\n",
      "        5,  0, 15, -2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t 7\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t 0\t u\t -2]]\n",
      "\n",
      "agent  player_2  action  draw from drawpile\n",
      "{'player_2': {'observations': array([ 7,  5,  4,  3,  3,  1,  3,  2,  2,  1,  1,  1,  1,  3,  0,  1,  0,\n",
      "        2,  6, 15, 15, 15, 15, 15,  8, -1, -2,  5,  0, 15, -2, 15, 15, 15,\n",
      "       15, 15,  8, -1, -2,  5,  0, 15, -2, 15, 15, 15, 15, 15,  8, -1, -2,\n",
      "        5,  0, 15, -2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 6 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t 7\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t u\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t 0\t u\t -2]]\n",
      "\n",
      "agent  player_2  action  place card (2) - col:2 row:0\n",
      "{'player_0': {'observations': array([ 7,  5,  4,  3,  3,  1,  3,  2,  2,  1,  2,  1,  2,  3,  0,  1,  0,\n",
      "        8, 15, -1,  9, 15,  2, 15, 15, 15, 15,  9,  4,  7,  2, -1,  9, 15,\n",
      "        2, 15, 15, 15, 15,  9,  4,  7,  2, -1,  9, 15,  2, 15, 15, 15, 15,\n",
      "        9,  4,  7,  2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t 7\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t 6\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t 0\t u\t -2]]\n",
      "\n",
      "agent  player_0  action  draw from drawpile\n",
      "{'player_0': {'observations': array([ 7,  5,  4,  3,  3,  1,  3,  2,  2,  1,  2,  1,  2,  3,  0,  1,  0,\n",
      "        8,  6, -1,  9, 15,  2, 15, 15, 15, 15,  9,  4,  7,  2, -1,  9, 15,\n",
      "        2, 15, 15, 15, 15,  9,  4,  7,  2, -1,  9, 15,  2, 15, 15, 15, 15,\n",
      "        9,  4,  7,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 6 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t u\t u\t u]\n",
      " [9\t 4\t 7\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t 6\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t 0\t u\t -2]]\n",
      "\n",
      "agent  player_0  action  place card (5) - col:1 row:1\n",
      "{'player_1': {'observations': array([ 7,  4,  4,  3,  3,  1,  4,  2,  2,  1,  3,  1,  2,  3,  0,  1,  0,\n",
      "        2, 15, -2, -1, 15,  3, -2, 15,  6, 15, 15,  3,  0, 15, -2, -1, 15,\n",
      "        3, -2, 15,  6, 15, 15,  3,  0, 15, -2, -1, 15,  3, -2, 15,  6, 15,\n",
      "       15,  3,  0, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t 6\t u\t u]\n",
      " [9\t 4\t 7\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t 6\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t 0\t u\t -2]]\n",
      "\n",
      "agent  player_1  action  draw from discard pile\n",
      "{'player_1': {'observations': array([ 7,  4,  4,  3,  3,  1,  3,  2,  2,  1,  3,  1,  2,  3,  0,  1,  0,\n",
      "        8,  2, -2, -1, 15,  3, -2, 15,  6, 15, 15,  3,  0, 15, -2, -1, 15,\n",
      "        3, -2, 15,  6, 15, 15,  3,  0, 15, -2, -1, 15,  3, -2, 15,  6, 15,\n",
      "       15,  3,  0, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 2 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t 6\t u\t u]\n",
      " [9\t 4\t 7\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t u]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t 6\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t 0\t u\t -2]]\n",
      "\n",
      "agent  player_1  action  place card (7) - col:3 row:2\n",
      "{'player_2': {'observations': array([ 9,  4,  4,  3,  3,  1,  4,  2,  2,  1,  3,  1,  2,  3,  1,  1,  0,\n",
      "       10, 15, 15, 15,  6, 15, 15,  8, -1, -2,  5,  0, 15, -2, 15, 15,  6,\n",
      "       15, 15,  8, -1, -2,  5,  0, 15, -2, 15, 15,  6, 15, 15,  8, -1, -2,\n",
      "        5,  0, 15, -2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t 6\t u\t u]\n",
      " [9\t 4\t 7\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t 6\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t 0\t u\t -2]]\n",
      "\n",
      "agent  player_2  action  draw from drawpile\n",
      "{'player_2': {'observations': array([ 9,  4,  4,  3,  3,  1,  4,  2,  2,  1,  3,  1,  2,  3,  1,  1,  0,\n",
      "       10, -2, 15, 15,  6, 15, 15,  8, -1, -2,  5,  0, 15, -2, 15, 15,  6,\n",
      "       15, 15,  8, -1, -2,  5,  0, 15, -2, 15, 15,  6, 15, 15,  8, -1, -2,\n",
      "        5,  0, 15, -2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: -2 \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t 6\t u\t u]\n",
      " [9\t 4\t 7\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t 6\t u]\n",
      " [u\t 8\t -1\t -2]\n",
      " [5\t 0\t u\t -2]]\n",
      "\n",
      "agent  player_2  action  place card (3) - col:3 row:1\n",
      "{'player_0': {'observations': array([ 9,  4,  2,  3,  6,  1,  4,  2,  2,  1,  3,  1,  3,  3,  1,  1,  0,\n",
      "        0, 15, -1,  9, 15,  2, 15,  6, 15, 15,  9,  4,  7,  2, -1,  9, 15,\n",
      "        2, 15,  6, 15, 15,  9,  4,  7,  2, -1,  9, 15,  2, 15,  6, 15, 15,\n",
      "        9,  4,  7,  2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t 6\t u\t u]\n",
      " [9\t 4\t 7\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t 6\t d]\n",
      " [u\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  draw from drawpile\n",
      "{'player_0': {'observations': array([ 9,  4,  2,  3,  6,  1,  4,  2,  2,  1,  3,  1,  3,  3,  1,  1,  0,\n",
      "        0, 10, -1,  9, 15,  2, 15,  6, 15, 15,  9,  4,  7,  2, -1,  9, 15,\n",
      "        2, 15,  6, 15, 15,  9,  4,  7,  2, -1,  9, 15,  2, 15,  6, 15, 15,\n",
      "        9,  4,  7,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 10 \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t 6\t u\t u]\n",
      " [9\t 4\t 7\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t 6\t d]\n",
      " [u\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  place card (8) - col:0 row:2\n",
      "{'player_1': {'observations': array([ 9,  4,  2,  3,  6,  1,  4,  2,  2,  1,  3,  1,  3,  3,  2,  1,  0,\n",
      "        9, 15, -2, -1, 15,  3, -2, 15,  6,  2, 15,  3,  0, 15, -2, -1, 15,\n",
      "        3, -2, 15,  6,  2, 15,  3,  0, 15, -2, -1, 15,  3, -2, 15,  6,  2,\n",
      "       15,  3,  0, 15], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t 6\t u\t u]\n",
      " [10\t 4\t 7\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t 6\t d]\n",
      " [u\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  draw from drawpile\n",
      "{'player_1': {'observations': array([ 9,  4,  2,  3,  6,  1,  4,  2,  2,  1,  3,  1,  3,  3,  2,  1,  0,\n",
      "        9,  0, -2, -1, 15,  3, -2, 15,  6,  2, 15,  3,  0, 15, -2, -1, 15,\n",
      "        3, -2, 15,  6,  2, 15,  3,  0, 15, -2, -1, 15,  3, -2, 15,  6,  2,\n",
      "       15,  3,  0, 15], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "       0, 1, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 0 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t 6\t u\t u]\n",
      " [10\t 4\t 7\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [u\t 3\t 0\t u]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t 6\t d]\n",
      " [u\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  place card (11) - col:3 row:3\n",
      "{'player_2': {'observations': array([ 9,  3,  2,  3,  7,  1,  4,  2,  2,  1,  3,  1,  3,  3,  3,  1,  0,\n",
      "       10, 15, 15, 15,  6, 14, 15,  8, -1, 14,  5,  0, 15, 14, 15, 15,  6,\n",
      "       14, 15,  8, -1, 14,  5,  0, 15, 14, 15, 15,  6, 14, 15,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t 6\t u\t u]\n",
      " [10\t 4\t 7\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [u\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t 6\t d]\n",
      " [u\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  draw from drawpile\n",
      "{'player_2': {'observations': array([ 9,  3,  2,  3,  7,  1,  4,  2,  2,  1,  3,  1,  3,  3,  3,  1,  0,\n",
      "       10,  7, 15, 15,  6, 14, 15,  8, -1, 14,  5,  0, 15, 14, 15, 15,  6,\n",
      "       14, 15,  8, -1, 14,  5,  0, 15, 14, 15, 15,  6, 14, 15,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 7 \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t 6\t u\t u]\n",
      " [10\t 4\t 7\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [u\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t 6\t d]\n",
      " [u\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  place card (4) - col:0 row:1\n",
      "{'player_0': {'observations': array([ 9,  3,  2,  3,  7,  1,  4,  2,  2,  1,  4,  2,  3,  3,  3,  1,  0,\n",
      "        6, 15, -1,  9, 15,  2, 15,  6, 15, 15, 10,  4,  7,  2, -1,  9, 15,\n",
      "        2, 15,  6, 15, 15, 10,  4,  7,  2, -1,  9, 15,  2, 15,  6, 15, 15,\n",
      "       10,  4,  7,  2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t 6\t u\t u]\n",
      " [10\t 4\t 7\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [u\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t 6\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  draw from drawpile\n",
      "{'player_0': {'observations': array([ 9,  3,  2,  3,  7,  1,  4,  2,  2,  1,  4,  2,  3,  3,  3,  1,  0,\n",
      "        6, -2, -1,  9, 15,  2, 15,  6, 15, 15, 10,  4,  7,  2, -1,  9, 15,\n",
      "        2, 15,  6, 15, 15, 10,  4,  7,  2, -1,  9, 15,  2, 15,  6, 15, 15,\n",
      "       10,  4,  7,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: -2 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t 6\t u\t u]\n",
      " [10\t 4\t 7\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [u\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t 6\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  place card (10) - col:2 row:3\n",
      "{'player_1': {'observations': array([ 9,  3,  3,  3,  7,  1,  4,  2,  2,  1,  4,  2,  3,  3,  3,  1,  0,\n",
      "        7, 15, -2, -1, 15,  3, -2, 15,  6,  2, 15,  3,  0,  0, -2, -1, 15,\n",
      "        3, -2, 15,  6,  2, 15,  3,  0,  0, -2, -1, 15,  3, -2, 15,  6,  2,\n",
      "       15,  3,  0,  0], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 7 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t 6\t u\t u]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [u\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t 6\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  draw from drawpile\n",
      "{'player_1': {'observations': array([ 9,  3,  3,  3,  7,  1,  4,  2,  2,  1,  4,  2,  3,  3,  3,  1,  0,\n",
      "        7,  5, -2, -1, 15,  3, -2, 15,  6,  2, 15,  3,  0,  0, -2, -1, 15,\n",
      "        3, -2, 15,  6,  2, 15,  3,  0,  0, -2, -1, 15,  3, -2, 15,  6,  2,\n",
      "       15,  3,  0,  0], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 5 \n",
      "discard pile top: 7 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t 6\t u\t u]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t u\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [u\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t 6\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  place card (2) - col:2 row:0\n",
      "{'player_2': {'observations': array([14,  2,  3,  4,  7,  1,  4,  2,  2,  2,  4,  2,  3,  3,  3,  1,  0,\n",
      "       -1, 15, 15, 15,  6, 14,  7,  8, -1, 14,  5,  0, 15, 14, 15, 15,  6,\n",
      "       14,  7,  8, -1, 14,  5,  0, 15, 14, 15, 15,  6, 14,  7,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t 6\t u\t u]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [u\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t 6\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  draw from discard pile\n",
      "{'player_2': {'observations': array([14,  2,  3,  3,  7,  1,  4,  2,  2,  2,  4,  2,  3,  3,  3,  1,  0,\n",
      "        7, -1, 15, 15,  6, 14,  7,  8, -1, 14,  5,  0, 15, 14, 15, 15,  6,\n",
      "       14,  7,  8, -1, 14,  5,  0, 15, 14, 15, 15,  6, 14,  7,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: -1 \n",
      "discard pile top: 7 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t 6\t u\t u]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [u\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t u\t 6\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  place card (1) - col:1 row:0\n",
      "{'player_0': {'observations': array([14,  2,  3,  4,  7,  1,  4,  2,  2,  2,  4,  2,  4,  3,  3,  1,  0,\n",
      "        8, 15, -1,  9, 15,  2, 15,  6, 15, 15, 10,  4, -2,  2, -1,  9, 15,\n",
      "        2, 15,  6, 15, 15, 10,  4, -2,  2, -1,  9, 15,  2, 15,  6, 15, 15,\n",
      "       10,  4, -2,  2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t 6\t u\t u]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [u\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 6\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  draw from drawpile\n",
      "{'player_0': {'observations': array([14,  2,  3,  4,  7,  1,  4,  2,  2,  2,  4,  2,  4,  3,  3,  1,  0,\n",
      "        8, 11, -1,  9, 15,  2, 15,  6, 15, 15, 10,  4, -2,  2, -1,  9, 15,\n",
      "        2, 15,  6, 15, 15, 10,  4, -2,  2, -1,  9, 15,  2, 15,  6, 15, 15,\n",
      "       10,  4, -2,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 11 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [u\t 6\t u\t u]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [u\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 6\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  handcard discard & reveal card (16) - col:0 row:1\n",
      "{'player_1': {'observations': array([14,  2,  3,  4,  7,  1,  4,  2,  3,  2,  4,  2,  4,  3,  3,  2,  0,\n",
      "       11, 15, -2, -1,  5,  3, -2, 15,  6,  2, 15,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  6,  2, 15,  3,  0,  0, -2, -1,  5,  3, -2, 15,  6,  2,\n",
      "       15,  3,  0,  0], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [4\t 6\t u\t u]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [u\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 6\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  draw from drawpile\n",
      "{'player_1': {'observations': array([14,  2,  3,  4,  7,  1,  4,  2,  3,  2,  4,  2,  4,  3,  3,  2,  0,\n",
      "       11,  5, -2, -1,  5,  3, -2, 15,  6,  2, 15,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  6,  2, 15,  3,  0,  0, -2, -1,  5,  3, -2, 15,  6,  2,\n",
      "       15,  3,  0,  0], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 5 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [4\t 6\t u\t u]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [u\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 6\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  place card (8) - col:0 row:2\n",
      "{'player_2': {'observations': array([19,  1,  3,  5,  7,  1,  4,  2,  3,  3,  4,  2,  4,  3,  3,  2,  0,\n",
      "       -1, 15, 15, -1,  6, 14,  7,  8, -1, 14,  5,  0, 15, 14, 15, -1,  6,\n",
      "       14,  7,  8, -1, 14,  5,  0, 15, 14, 15, -1,  6, 14,  7,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [4\t 6\t u\t u]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [5\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 6\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  draw from drawpile\n",
      "{'player_2': {'observations': array([19,  1,  3,  5,  7,  1,  4,  2,  3,  3,  4,  2,  4,  3,  3,  2,  0,\n",
      "       -1, 10, 15, -1,  6, 14,  7,  8, -1, 14,  5,  0, 15, 14, 15, -1,  6,\n",
      "       14,  7,  8, -1, 14,  5,  0, 15, 14, 15, -1,  6, 14,  7,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 10 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [4\t 6\t u\t u]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [5\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 6\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  place card (2) - col:2 row:0\n",
      "{'player_0': {'observations': array([19,  1,  3,  5,  7,  1,  4,  2,  3,  3,  4,  2,  4,  3,  4,  2,  0,\n",
      "        6, 15, -1,  9, 15,  2,  4,  6, 15, 15, 10,  4, -2,  2, -1,  9, 15,\n",
      "        2,  4,  6, 15, 15, 10,  4, -2,  2, -1,  9, 15,  2,  4,  6, 15, 15,\n",
      "       10,  4, -2,  2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [4\t 6\t u\t u]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [5\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 10\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  draw from discard pile\n",
      "{'player_0': {'observations': array([19,  1,  3,  5,  7,  1,  4,  2,  3,  3,  3,  2,  4,  3,  4,  2,  0,\n",
      "       -1,  6, -1,  9, 15,  2,  4,  6, 15, 15, 10,  4, -2,  2, -1,  9, 15,\n",
      "        2,  4,  6, 15, 15, 10,  4, -2,  2, -1,  9, 15,  2,  4,  6, 15, 15,\n",
      "       10,  4, -2,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 6 \n",
      "discard pile top: -1 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [4\t 6\t u\t u]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [5\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 10\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  place card (4) - col:0 row:1\n",
      "{'player_1': {'observations': array([19,  1,  3,  5,  7,  1,  4,  2,  3,  3,  4,  2,  4,  3,  4,  2,  0,\n",
      "        4, 15, -2, -1,  5,  3, -2, 15,  6,  2,  5,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  6,  2,  5,  3,  0,  0, -2, -1,  5,  3, -2, 15,  6,  2,\n",
      "        5,  3,  0,  0], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t u]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [5\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 10\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  draw from drawpile\n",
      "{'player_1': {'observations': array([19,  1,  3,  5,  7,  1,  4,  2,  3,  3,  4,  2,  4,  3,  4,  2,  0,\n",
      "        4,  1, -2, -1,  5,  3, -2, 15,  6,  2,  5,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  6,  2,  5,  3,  0,  0, -2, -1,  5,  3, -2, 15,  6,  2,\n",
      "        5,  3,  0,  0], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 1 \n",
      "discard pile top: 4 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t u]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 6\t 2]\n",
      " [5\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 10\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  place card (6) - col:2 row:2\n",
      "{'player_2': {'observations': array([14,  1,  3,  5,  7,  2,  4,  2,  3,  3,  4,  2,  4,  3,  4,  2,  0,\n",
      "        6, 15, 15, -1, 10, 14,  7,  8, -1, 14,  5,  0, 15, 14, 15, -1, 10,\n",
      "       14,  7,  8, -1, 14,  5,  0, 15, 14, 15, -1, 10, 14,  7,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t u]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 2]\n",
      " [5\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 10\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  draw from drawpile\n",
      "{'player_2': {'observations': array([14,  1,  3,  5,  7,  2,  4,  2,  3,  3,  4,  2,  4,  3,  4,  2,  0,\n",
      "        6,  0, 15, -1, 10, 14,  7,  8, -1, 14,  5,  0, 15, 14, 15, -1, 10,\n",
      "       14,  7,  8, -1, 14,  5,  0, 15, 14, 15, -1, 10, 14,  7,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 0 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t u]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 2]\n",
      " [5\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 10\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  place card (9) - col:1 row:3\n",
      "{'player_0': {'observations': array([14,  1,  3,  5,  8,  2,  4,  2,  3,  3,  4,  2,  4,  3,  4,  2,  0,\n",
      "        0, 15, -1,  9, 15,  2,  6,  6, 15, 15, 10,  4, -2,  2, -1,  9, 15,\n",
      "        2,  6,  6, 15, 15, 10,  4, -2,  2, -1,  9, 15,  2,  6,  6, 15, 15,\n",
      "       10,  4, -2,  2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t u]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 2]\n",
      " [5\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 10\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  draw from discard pile\n",
      "{'player_0': {'observations': array([14,  1,  3,  5,  7,  2,  4,  2,  3,  3,  4,  2,  4,  3,  4,  2,  0,\n",
      "        6,  0, -1,  9, 15,  2,  6,  6, 15, 15, 10,  4, -2,  2, -1,  9, 15,\n",
      "        2,  6,  6, 15, 15, 10,  4, -2,  2, -1,  9, 15,  2,  6,  6, 15, 15,\n",
      "       10,  4, -2,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 0 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t u]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 2]\n",
      " [5\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 10\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  place card (7) - col:3 row:2\n",
      "{'player_1': {'observations': array([14,  1,  3,  5,  8,  2,  4,  2,  3,  3,  4,  2,  4,  3,  4,  3,  0,\n",
      "       11, 15, -2, -1,  5,  3, -2, 15,  1,  2,  5,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  1,  2,  5,  3,  0,  0, -2, -1,  5,  3, -2, 15,  1,  2,\n",
      "        5,  3,  0,  0], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t 0]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 2]\n",
      " [5\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 10\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  draw from drawpile\n",
      "{'player_1': {'observations': array([14,  1,  3,  5,  8,  2,  4,  2,  3,  3,  4,  2,  4,  3,  4,  3,  0,\n",
      "       11, 12, -2, -1,  5,  3, -2, 15,  1,  2,  5,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  1,  2,  5,  3,  0,  0, -2, -1,  5,  3, -2, 15,  1,  2,\n",
      "        5,  3,  0,  0], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 12 \n",
      "discard pile top: 11 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t 0]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 2]\n",
      " [5\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 10\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  place card (7) - col:3 row:2\n",
      "{'player_2': {'observations': array([24,  1,  3,  5,  8,  2,  4,  2,  3,  3,  4,  2,  4,  3,  4,  3,  1,\n",
      "        2, 15, 15, -1, 10, 14,  7,  8, -1, 14,  5,  0, 15, 14, 15, -1, 10,\n",
      "       14,  7,  8, -1, 14,  5,  0, 15, 14, 15, -1, 10, 14,  7,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t 0]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 12]\n",
      " [5\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 10\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  draw from drawpile\n",
      "{'player_2': {'observations': array([24,  1,  3,  5,  8,  2,  4,  2,  3,  3,  4,  2,  4,  3,  4,  3,  1,\n",
      "        2,  5, 15, -1, 10, 14,  7,  8, -1, 14,  5,  0, 15, 14, 15, -1, 10,\n",
      "       14,  7,  8, -1, 14,  5,  0, 15, 14, 15, -1, 10, 14,  7,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 5 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t 0]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 12]\n",
      " [5\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 10\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  place card (2) - col:2 row:0\n",
      "{'player_0': {'observations': array([23,  1,  3,  5,  8,  2,  4,  2,  3,  4,  4,  2,  4,  3,  4,  3,  1,\n",
      "       10, 15, -1,  9, 15,  2,  6,  6, 15,  0, 10,  4, -2,  2, -1,  9, 15,\n",
      "        2,  6,  6, 15,  0, 10,  4, -2,  2, -1,  9, 15,  2,  6,  6, 15,  0,\n",
      "       10,  4, -2,  2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t 0]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 12]\n",
      " [5\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 5\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  draw from discard pile\n",
      "{'player_0': {'observations': array([23,  1,  3,  5,  8,  2,  4,  2,  3,  4,  4,  2,  4,  3,  3,  3,  1,\n",
      "        2, 10, -1,  9, 15,  2,  6,  6, 15,  0, 10,  4, -2,  2, -1,  9, 15,\n",
      "        2,  6,  6, 15,  0, 10,  4, -2,  2, -1,  9, 15,  2,  6,  6, 15,  0,\n",
      "       10,  4, -2,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 10 \n",
      "discard pile top: 2 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t 0]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 12]\n",
      " [5\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 5\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  place card (8) - col:0 row:2\n",
      "{'player_1': {'observations': array([23,  1,  3,  5,  8,  2,  4,  2,  3,  4,  4,  2,  4,  3,  4,  3,  1,\n",
      "       10, 15, -2, -1,  5,  3, -2, 15,  1, 12,  5,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  1, 12,  5,  3,  0,  0, -2, -1,  5,  3, -2, 15,  1, 12,\n",
      "        5,  3,  0,  0], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t 0]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 12]\n",
      " [5\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 5\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  draw from drawpile\n",
      "{'player_1': {'observations': array([23,  1,  3,  5,  8,  2,  4,  2,  3,  4,  4,  2,  4,  3,  4,  3,  1,\n",
      "       10,  6, -2, -1,  5,  3, -2, 15,  1, 12,  5,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  1, 12,  5,  3,  0,  0, -2, -1,  5,  3, -2, 15,  1, 12,\n",
      "        5,  3,  0,  0], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 6 \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t 0]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 12]\n",
      " [5\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 5\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  place card (8) - col:0 row:2\n",
      "{'player_2': {'observations': array([23,  1,  3,  5,  8,  2,  4,  2,  3,  4,  5,  2,  4,  3,  4,  3,  1,\n",
      "        5, 15, 15, -1,  5, 14,  7,  8, -1, 14,  5,  0, 15, 14, 15, -1,  5,\n",
      "       14,  7,  8, -1, 14,  5,  0, 15, 14, 15, -1,  5, 14,  7,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t 0]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 12]\n",
      " [6\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 5\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  draw from drawpile\n",
      "{'player_2': {'observations': array([23,  1,  3,  5,  8,  2,  4,  2,  3,  4,  5,  2,  4,  3,  4,  3,  1,\n",
      "        5,  2, 15, -1,  5, 14,  7,  8, -1, 14,  5,  0, 15, 14, 15, -1,  5,\n",
      "       14,  7,  8, -1, 14,  5,  0, 15, 14, 15, -1,  5, 14,  7,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 2 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t 0]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 12]\n",
      " [6\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 5\t d]\n",
      " [7\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  place card (4) - col:0 row:1\n",
      "{'player_0': {'observations': array([18,  1,  3,  5,  8,  2,  5,  2,  3,  4,  5,  2,  4,  3,  4,  3,  1,\n",
      "        7, 15, -1,  9, 15,  2,  6,  6, 15,  0, 10,  4, -2,  2, -1,  9, 15,\n",
      "        2,  6,  6, 15,  0, 10,  4, -2,  2, -1,  9, 15,  2,  6,  6, 15,  0,\n",
      "       10,  4, -2,  2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 7 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t 0]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 12]\n",
      " [6\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 5\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  draw from discard pile\n",
      "{'player_0': {'observations': array([18,  1,  3,  5,  8,  2,  5,  2,  3,  4,  5,  1,  4,  3,  4,  3,  1,\n",
      "        5,  7, -1,  9, 15,  2,  6,  6, 15,  0, 10,  4, -2,  2, -1,  9, 15,\n",
      "        2,  6,  6, 15,  0, 10,  4, -2,  2, -1,  9, 15,  2,  6,  6, 15,  0,\n",
      "       10,  4, -2,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 7 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t 0]\n",
      " [10\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 12]\n",
      " [6\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 5\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  place card (8) - col:0 row:2\n",
      "{'player_1': {'observations': array([18,  1,  3,  5,  8,  2,  5,  2,  3,  4,  5,  2,  4,  3,  4,  3,  1,\n",
      "       10, 15, -2, -1,  5,  3, -2, 15,  1, 12,  6,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  1, 12,  6,  3,  0,  0, -2, -1,  5,  3, -2, 15,  1, 12,\n",
      "        6,  3,  0,  0], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t 0]\n",
      " [7\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 12]\n",
      " [6\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 5\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  draw from drawpile\n",
      "{'player_1': {'observations': array([18,  1,  3,  5,  8,  2,  5,  2,  3,  4,  5,  2,  4,  3,  4,  3,  1,\n",
      "       10,  1, -2, -1,  5,  3, -2, 15,  1, 12,  6,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  1, 12,  6,  3,  0,  0, -2, -1,  5,  3, -2, 15,  1, 12,\n",
      "        6,  3,  0,  0], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 1 \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t 0]\n",
      " [7\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 12]\n",
      " [6\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 5\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  place card (8) - col:0 row:2\n",
      "{'player_2': {'observations': array([18,  1,  3,  5,  8,  3,  5,  2,  3,  4,  5,  2,  4,  3,  4,  3,  1,\n",
      "        6, 15, 15, -1,  5, 14,  2,  8, -1, 14,  5,  0, 15, 14, 15, -1,  5,\n",
      "       14,  2,  8, -1, 14,  5,  0, 15, 14, 15, -1,  5, 14,  2,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t 0]\n",
      " [7\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 12]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 5\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  draw from drawpile\n",
      "{'player_2': {'observations': array([18,  1,  3,  5,  8,  3,  5,  2,  3,  4,  5,  2,  4,  3,  4,  3,  1,\n",
      "        6,  0, 15, -1,  5, 14,  2,  8, -1, 14,  5,  0, 15, 14, 15, -1,  5,\n",
      "       14,  2,  8, -1, 14,  5,  0, 15, 14, 15, -1,  5, 14,  2,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 0 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t 0]\n",
      " [7\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 12]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 5\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  place card (9) - col:1 row:3\n",
      "{'player_0': {'observations': array([18,  1,  3,  5,  9,  3,  5,  2,  3,  4,  5,  2,  4,  3,  4,  3,  1,\n",
      "        0, 15, -1,  9, 15,  2,  6,  6, 15,  0,  7,  4, -2,  2, -1,  9, 15,\n",
      "        2,  6,  6, 15,  0,  7,  4, -2,  2, -1,  9, 15,  2,  6,  6, 15,  0,\n",
      "        7,  4, -2,  2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t 0]\n",
      " [7\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 12]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 5\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  draw from discard pile\n",
      "{'player_0': {'observations': array([18,  1,  3,  5,  8,  3,  5,  2,  3,  4,  5,  2,  4,  3,  4,  3,  1,\n",
      "        6,  0, -1,  9, 15,  2,  6,  6, 15,  0,  7,  4, -2,  2, -1,  9, 15,\n",
      "        2,  6,  6, 15,  0,  7,  4, -2,  2, -1,  9, 15,  2,  6,  6, 15,  0,\n",
      "        7,  4, -2,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 0 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 6\t u\t 0]\n",
      " [7\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 12]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 5\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  place card (5) - col:1 row:1\n",
      "{'player_1': {'observations': array([18,  1,  3,  5,  9,  3,  5,  2,  3,  4,  5,  2,  4,  3,  4,  3,  1,\n",
      "        6, 15, -2, -1,  5,  3, -2, 15,  1, 12,  1,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  1, 12,  1,  3,  0,  0, -2, -1,  5,  3, -2, 15,  1, 12,\n",
      "        1,  3,  0,  0], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 0\t u\t 0]\n",
      " [7\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 12]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 5\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  draw from drawpile\n",
      "{'player_1': {'observations': array([18,  1,  3,  5,  9,  3,  5,  2,  3,  4,  5,  2,  4,  3,  4,  3,  1,\n",
      "        6, 10, -2, -1,  5,  3, -2, 15,  1, 12,  1,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  1, 12,  1,  3,  0,  0, -2, -1,  5,  3, -2, 15,  1, 12,\n",
      "        1,  3,  0,  0], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 10 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 0\t u\t 0]\n",
      " [7\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 12]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 5\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  place card (7) - col:3 row:2\n",
      "{'player_2': {'observations': array([18,  1,  3,  5,  9,  3,  5,  2,  3,  4,  5,  2,  4,  3,  5,  3,  1,\n",
      "       12, 15, 15, -1,  5, 14,  2,  8, -1, 14,  5,  0, 15, 14, 15, -1,  5,\n",
      "       14,  2,  8, -1, 14,  5,  0, 15, 14, 15, -1,  5, 14,  2,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 0\t u\t 0]\n",
      " [7\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 10]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 5\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  draw from drawpile\n",
      "{'player_2': {'observations': array([18,  1,  3,  5,  9,  3,  5,  2,  3,  4,  5,  2,  4,  3,  5,  3,  1,\n",
      "       12,  7, 15, -1,  5, 14,  2,  8, -1, 14,  5,  0, 15, 14, 15, -1,  5,\n",
      "       14,  2,  8, -1, 14,  5,  0, 15, 14, 15, -1,  5, 14,  2,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 7 \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 0\t u\t 0]\n",
      " [7\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 10]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 5\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  place card (2) - col:2 row:0\n",
      "{'player_0': {'observations': array([18,  1,  3,  5,  9,  3,  5,  2,  3,  4,  5,  3,  4,  3,  5,  3,  1,\n",
      "        5, 15, -1,  9, 15,  2,  6,  0, 15,  0,  7,  4, -2,  2, -1,  9, 15,\n",
      "        2,  6,  0, 15,  0,  7,  4, -2,  2, -1,  9, 15,  2,  6,  0, 15,  0,\n",
      "        7,  4, -2,  2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 0\t u\t 0]\n",
      " [7\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 10]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  draw from discard pile\n",
      "{'player_0': {'observations': array([18,  1,  3,  5,  9,  3,  5,  2,  3,  3,  5,  3,  4,  3,  5,  3,  1,\n",
      "       12,  5, -1,  9, 15,  2,  6,  0, 15,  0,  7,  4, -2,  2, -1,  9, 15,\n",
      "        2,  6,  0, 15,  0,  7,  4, -2,  2, -1,  9, 15,  2,  6,  0, 15,  0,\n",
      "        7,  4, -2,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 5 \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 0\t u\t 0]\n",
      " [7\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 10]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  place card (8) - col:0 row:2\n",
      "{'player_1': {'observations': array([18,  1,  3,  5,  9,  3,  5,  2,  3,  4,  5,  3,  4,  3,  5,  3,  1,\n",
      "        7, 15, -2, -1,  5,  3, -2, 15,  1, 10,  1,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  1, 10,  1,  3,  0,  0, -2, -1,  5,  3, -2, 15,  1, 10,\n",
      "        1,  3,  0,  0], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 7 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 10]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  draw from drawpile\n",
      "{'player_1': {'observations': array([18,  1,  3,  5,  9,  3,  5,  2,  3,  4,  5,  3,  4,  3,  5,  3,  1,\n",
      "        7,  8, -2, -1,  5,  3, -2, 15,  1, 10,  1,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  1, 10,  1,  3,  0,  0, -2, -1,  5,  3, -2, 15,  1, 10,\n",
      "        1,  3,  0,  0], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 8 \n",
      "discard pile top: 7 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 10]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  place card (7) - col:3 row:2\n",
      "{'player_2': {'observations': array([16,  1,  3,  5,  9,  3,  5,  2,  3,  4,  5,  3,  5,  3,  5,  3,  1,\n",
      "       10, 15, 15, -1,  7, 14,  2,  8, -1, 14,  5,  0, 15, 14, 15, -1,  7,\n",
      "       14,  2,  8, -1, 14,  5,  0, 15, 14, 15, -1,  7, 14,  2,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 8]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  draw from drawpile\n",
      "{'player_2': {'observations': array([16,  1,  3,  5,  9,  3,  5,  2,  3,  4,  5,  3,  5,  3,  5,  3,  1,\n",
      "       10,  0, 15, -1,  7, 14,  2,  8, -1, 14,  5,  0, 15, 14, 15, -1,  7,\n",
      "       14,  2,  8, -1, 14,  5,  0, 15, 14, 15, -1,  7, 14,  2,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 0 \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 8]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  place card (9) - col:1 row:3\n",
      "{'player_0': {'observations': array([16,  1,  3,  5, 10,  3,  5,  2,  3,  4,  5,  3,  5,  3,  5,  3,  1,\n",
      "        0, 15, -1,  9, 15,  2,  6,  0, 15,  0,  5,  4, -2,  2, -1,  9, 15,\n",
      "        2,  6,  0, 15,  0,  5,  4, -2,  2, -1,  9, 15,  2,  6,  0, 15,  0,\n",
      "        5,  4, -2,  2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 8]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  draw from discard pile\n",
      "{'player_0': {'observations': array([16,  1,  3,  5,  9,  3,  5,  2,  3,  4,  5,  3,  5,  3,  5,  3,  1,\n",
      "       10,  0, -1,  9, 15,  2,  6,  0, 15,  0,  5,  4, -2,  2, -1,  9, 15,\n",
      "        2,  6,  0, 15,  0,  5,  4, -2,  2, -1,  9, 15,  2,  6,  0, 15,  0,\n",
      "        5,  4, -2,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 0 \n",
      "discard pile top: 10 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [6\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 8]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  place card (4) - col:0 row:1\n",
      "{'player_1': {'observations': array([16,  1,  3,  5, 10,  3,  5,  2,  3,  4,  5,  3,  5,  3,  5,  3,  1,\n",
      "        6, 15, -2, -1,  5,  3, -2, 15,  1,  8,  1,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  1,  8,  1,  3,  0,  0, -2, -1,  5,  3, -2, 15,  1,  8,\n",
      "        1,  3,  0,  0], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [0\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 8]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  draw from drawpile\n",
      "{'player_1': {'observations': array([16,  1,  3,  5, 10,  3,  5,  2,  3,  4,  5,  3,  5,  3,  5,  3,  1,\n",
      "        6,  9, -2, -1,  5,  3, -2, 15,  1,  8,  1,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  1,  8,  1,  3,  0,  0, -2, -1,  5,  3, -2, 15,  1,  8,\n",
      "        1,  3,  0,  0], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 9 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [0\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 8]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  place card (7) - col:3 row:2\n",
      "{'player_2': {'observations': array([17,  1,  3,  5, 10,  3,  5,  2,  3,  4,  5,  3,  5,  4,  5,  3,  1,\n",
      "        8, 15, 15, -1,  7, 14,  2,  8, -1, 14,  5,  0, 15, 14, 15, -1,  7,\n",
      "       14,  2,  8, -1, 14,  5,  0, 15, 14, 15, -1,  7, 14,  2,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [0\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 9]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  draw from drawpile\n",
      "{'player_2': {'observations': array([17,  1,  3,  5, 10,  3,  5,  2,  3,  4,  5,  3,  5,  4,  5,  3,  1,\n",
      "        8,  1, 15, -1,  7, 14,  2,  8, -1, 14,  5,  0, 15, 14, 15, -1,  7,\n",
      "       14,  2,  8, -1, 14,  5,  0, 15, 14, 15, -1,  7, 14,  2,  8, -1, 14,\n",
      "        5,  0, 15, 14], dtype=int8), 'action_mask': array([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 1 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [0\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 9]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_2  action  place card (9) - col:1 row:3\n",
      "{'player_0': {'observations': array([17,  1,  3,  5, 10,  4,  5,  2,  3,  4,  5,  3,  5,  4,  5,  3,  1,\n",
      "        0, 15, -1,  9, 15,  2,  0,  0, 15,  0,  5,  4, -2,  2, -1,  9, 15,\n",
      "        2,  0,  0, 15,  0,  5,  4, -2,  2, -1,  9, 15,  2,  0,  0, 15,  0,\n",
      "        5,  4, -2,  2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 0 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [0\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 9]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 1\t u\t d]]\n",
      "\n",
      "agent  player_0  action  draw from discard pile\n",
      "{'player_0': {'observations': array([17,  1,  3,  5,  9,  4,  5,  2,  3,  4,  5,  3,  5,  4,  5,  3,  1,\n",
      "        8,  0, -1,  9, 15,  2,  0,  0, 15,  0,  5,  4, -2,  2, -1,  9, 15,\n",
      "        2,  0,  0, 15,  0,  5,  4, -2,  2, -1,  9, 15,  2,  0,  0, 15,  0,\n",
      "        5,  4, -2,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 0 \n",
      "discard pile top: 8 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t u\t 2]\n",
      " [0\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 9]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 1\t u\t d]]\n",
      "\n",
      "agent  player_0  action  place card (2) - col:2 row:0\n",
      "{'player_1': {'observations': array([17,  1,  3,  5, 10,  4,  5,  2,  3,  4,  6,  3,  5,  4,  5,  3,  1,\n",
      "        6, 15, -2, -1,  5,  3, -2, 15,  1,  9,  1,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  1,  9,  1,  3,  0,  0, -2, -1,  5,  3, -2, 15,  1,  9,\n",
      "        1,  3,  0,  0], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t 0\t 2]\n",
      " [0\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 9]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 1\t u\t d]]\n",
      "\n",
      "agent  player_1  action  draw from drawpile\n",
      "{'player_1': {'observations': array([17,  1,  3,  5, 10,  4,  5,  2,  3,  4,  6,  3,  5,  4,  5,  3,  1,\n",
      "        6,  3, -2, -1,  5,  3, -2, 15,  1,  9,  1,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  1,  9,  1,  3,  0,  0, -2, -1,  5,  3, -2, 15,  1,  9,\n",
      "        1,  3,  0,  0], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 3 \n",
      "discard pile top: 6 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t 0\t 2]\n",
      " [0\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 9]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 1\t u\t d]]\n",
      "\n",
      "agent  player_1  action  place card (7) - col:3 row:2\n",
      "{'player_2': {'observations': array([11,  1,  3,  5, 10,  4,  5,  3,  3,  4,  6,  3,  5,  4,  5,  3,  1,\n",
      "        9, 15, 15, -1,  7, 14,  2,  8, -1, 14,  5,  1, 15, 14, 15, -1,  7,\n",
      "       14,  2,  8, -1, 14,  5,  1, 15, 14, 15, -1,  7, 14,  2,  8, -1, 14,\n",
      "        5,  1, 15, 14], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t 0\t 2]\n",
      " [0\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 3]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 1\t u\t d]]\n",
      "\n",
      "agent  player_2  action  draw from drawpile\n",
      "{'player_2': {'observations': array([11,  1,  3,  5, 10,  4,  5,  3,  3,  4,  6,  3,  5,  4,  5,  3,  1,\n",
      "        9,  5, 15, -1,  7, 14,  2,  8, -1, 14,  5,  1, 15, 14, 15, -1,  7,\n",
      "       14,  2,  8, -1, 14,  5,  1, 15, 14, 15, -1,  7, 14,  2,  8, -1, 14,\n",
      "        5,  1, 15, 14], dtype=int8), 'action_mask': array([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 5 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t 0\t 2]\n",
      " [0\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 3]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 1\t u\t d]]\n",
      "\n",
      "agent  player_2  action  place card (9) - col:1 row:3\n",
      "{'player_0': {'observations': array([11,  1,  3,  5, 10,  4,  5,  3,  3,  5,  6,  3,  5,  4,  5,  3,  1,\n",
      "        1, 15, -1,  9,  0,  2,  0,  0, 15,  0,  5,  4, -2,  2, -1,  9,  0,\n",
      "        2,  0,  0, 15,  0,  5,  4, -2,  2, -1,  9,  0,  2,  0,  0, 15,  0,\n",
      "        5,  4, -2,  2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t 0\t 2]\n",
      " [0\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 3]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 5\t u\t d]]\n",
      "\n",
      "agent  player_0  action  draw from drawpile\n",
      "{'player_0': {'observations': array([11,  1,  3,  5, 10,  4,  5,  3,  3,  5,  6,  3,  5,  4,  5,  3,  1,\n",
      "        1, -1, -1,  9,  0,  2,  0,  0, 15,  0,  5,  4, -2,  2, -1,  9,  0,\n",
      "        2,  0,  0, 15,  0,  5,  4, -2,  2, -1,  9,  0,  2,  0,  0, 15,  0,\n",
      "        5,  4, -2,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: -1 \n",
      "discard pile top: 1 \n",
      "======= Player 0 ========== \n",
      "[[-1\t 9\t 0\t 2]\n",
      " [0\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 3]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 5\t u\t d]]\n",
      "\n",
      "agent  player_0  action  place card (1) - col:1 row:0\n",
      "{'player_1': {'observations': array([ 9,  1,  3,  6, 10,  4,  5,  3,  3,  5,  6,  3,  5,  4,  5,  3,  1,\n",
      "        9, 15, -2, -1,  5,  3, -2, 15,  1,  3,  1,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  1,  3,  1,  3,  0,  0, -2, -1,  5,  3, -2, 15,  1,  3,\n",
      "        1,  3,  0,  0], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[-1\t -1\t 0\t 2]\n",
      " [0\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 3]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 5\t u\t d]]\n",
      "\n",
      "agent  player_1  action  draw from drawpile\n",
      "{'player_1': {'observations': array([ 9,  1,  3,  6, 10,  4,  5,  3,  3,  5,  6,  3,  5,  4,  5,  3,  1,\n",
      "        9,  1, -2, -1,  5,  3, -2, 15,  1,  3,  1,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2, 15,  1,  3,  1,  3,  0,  0, -2, -1,  5,  3, -2, 15,  1,  3,\n",
      "        1,  3,  0,  0], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 1 \n",
      "holding card player 1: 1 \n",
      "discard pile top: 9 \n",
      "======= Player 0 ========== \n",
      "[[-1\t -1\t 0\t 2]\n",
      " [0\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t u\t 1\t 3]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 5\t u\t d]]\n",
      "\n",
      "agent  player_1  action  place card (5) - col:1 row:1\n",
      "{'player_2': {'observations': array([ 9,  0,  3,  6, 10,  5,  5,  3,  3,  5,  6,  3,  5,  4,  5,  3,  2,\n",
      "       12, 15, 15, -1,  7, 14,  2,  8, -1, 14,  5,  5, 15, 14, 15, -1,  7,\n",
      "       14,  2,  8, -1, 14,  5,  5, 15, 14, 15, -1,  7, 14,  2,  8, -1, 14,\n",
      "        5,  5, 15, 14], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 2 \n",
      "holding card player 2: empty \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[-1\t -1\t 0\t 2]\n",
      " [0\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t 1\t 1\t 3]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 5\t u\t d]]\n",
      "\n",
      "agent  player_2  action  draw from drawpile\n",
      "{'player_2': {'observations': array([ 9,  0,  3,  6, 10,  5,  5,  3,  3,  5,  6,  3,  5,  4,  5,  3,  2,\n",
      "       12,  0, 15, -1,  7, 14,  2,  8, -1, 14,  5,  5, 15, 14, 15, -1,  7,\n",
      "       14,  2,  8, -1, 14,  5,  5, 15, 14, 15, -1,  7, 14,  2,  8, -1, 14,\n",
      "        5,  5, 15, 14], dtype=int8), 'action_mask': array([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 2 \n",
      "holding card player 2: 0 \n",
      "discard pile top: 12 \n",
      "======= Player 0 ========== \n",
      "[[-1\t -1\t 0\t 2]\n",
      " [0\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t 1\t 1\t 3]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 5\t u\t d]]\n",
      "\n",
      "agent  player_2  action  place card (9) - col:1 row:3\n",
      "{'player_0': {'observations': array([ 9,  0,  3,  6, 11,  5,  5,  3,  3,  5,  6,  3,  5,  4,  5,  3,  2,\n",
      "        5, 15, -1, -1,  0,  2,  0,  0, 15,  0,  5,  4, -2,  2, -1, -1,  0,\n",
      "        2,  0,  0, 15,  0,  5,  4, -2,  2, -1, -1,  0,  2,  0,  0, 15,  0,\n",
      "        5,  4, -2,  2], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 0 \n",
      "holding card player 0: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[-1\t -1\t 0\t 2]\n",
      " [0\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t 1\t 1\t 3]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  draw from drawpile\n",
      "{'player_0': {'observations': array([ 9,  0,  3,  6, 11,  5,  5,  3,  3,  5,  6,  3,  5,  4,  5,  3,  2,\n",
      "        5,  6, -1, -1,  0,  2,  0,  0, 15,  0,  5,  4, -2,  2, -1, -1,  0,\n",
      "        2,  0,  0, 15,  0,  5,  4, -2,  2, -1, -1,  0,  2,  0,  0, 15,  0,\n",
      "        5,  4, -2,  2], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: place by Player 0 \n",
      "holding card player 0: 6 \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[-1\t -1\t 0\t 2]\n",
      " [0\t 0\t u\t 0]\n",
      " [5\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t 1\t 1\t 3]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_0  action  place card (8) - col:0 row:2\n",
      "{'player_1': {'observations': array([10,  0,  3,  6, 11,  5,  5,  3,  3,  5,  7,  3,  5,  4,  5,  3,  2,\n",
      "        5, 15, -2, -1,  5,  3, -2,  1,  1,  3,  1,  3,  0,  0, -2, -1,  5,\n",
      "        3, -2,  1,  1,  3,  1,  3,  0,  0, -2, -1,  5,  3, -2,  1,  1,  3,\n",
      "        1,  3,  0,  0], dtype=int8), 'action_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1], dtype=int8)}}\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 5 \n",
      "======= Player 0 ========== \n",
      "[[-1\t -1\t 0\t 2]\n",
      " [0\t 0\t u\t 0]\n",
      " [6\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t 1\t 1\t 3]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u\t d]]\n",
      "\n",
      "agent  player_1  action  draw from drawpile\n",
      "game done\n",
      "======= render board: ===== \n",
      "======= stats ============ \n",
      "next turn: draw by Player 1 \n",
      "holding card player 1: empty \n",
      "discard pile top: 5 \n",
      "======= GAME DONE ======== \n",
      "Results: {0: 14.0, 1: 12.0, 2: 28.0} \n",
      "======= Player 0 ========== \n",
      "[[-1\t -1\t 0\t 2]\n",
      " [0\t 0\t u4\t 0]\n",
      " [6\t 4\t -2\t 2]]\n",
      "======= Player 1 ========== \n",
      "[[-2\t -1\t 5\t 3]\n",
      " [-2\t 1\t 1\t 3]\n",
      " [1\t 3\t 0\t 0]]\n",
      "======= Player 2 ========== \n",
      "[[u0\t -1\t 7\t d]\n",
      " [2\t 8\t -1\t d]\n",
      " [5\t 0\t u8\t d]]\n",
      "\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0129 22:12:53.991486286   28715 backup_poller.cc:134]       Run client channel backup poller: {\"created\":\"@1643490773.991444826\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1643490773.991441540\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n",
      "E0129 22:41:44.046908525   14112 backup_poller.cc:134]       Run client channel backup poller: {\"created\":\"@1643492504.046883176\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1643492504.046880992\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n",
      "E0129 22:50:04.063877137   19203 backup_poller.cc:134]       Run client channel backup poller: {\"created\":\"@1643493004.063847080\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1643493004.063844465\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n",
      "E0129 22:58:04.063978082   23733 backup_poller.cc:134]       Run client channel backup poller: {\"created\":\"@1643493484.063950649\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1643493484.063947884\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n"
     ]
    }
   ],
   "source": [
    "from rlskyjo.game.skyjo import SkyjoGame\n",
    "\n",
    "env = PettingZooEnv(env_creator())\n",
    "obs = env.reset()\n",
    "done = {\"__all__\": False}\n",
    "# run one iteration until done\n",
    "\n",
    "for i in range(1000):\n",
    "    if done[\"__all__\"]:\n",
    "        print(\"game done\")\n",
    "        break\n",
    "    # get agent from current observation\n",
    "    agent = list(obs.keys())[0]\n",
    "\n",
    "    # format observation dict\n",
    "    print(obs)\n",
    "    obs = obs[agent]\n",
    "    env.render()\n",
    "\n",
    "    # get deterministic action\n",
    "    # trainer.compute_single_action(obs, policy_id=agent)\n",
    "    policy = trainer.get_policy(policy_id=agent)\n",
    "    action_exploration_policy, _, action_info = policy.compute_single_action(obs)\n",
    "    logits = action_info[\"action_dist_inputs\"]\n",
    "    action = logits.argmax()\n",
    "    #\n",
    "    print(\"agent \", agent, \" action \", SkyjoGame.render_action_explainer(action))\n",
    "    obs, reward, done, _ = env.step({agent: action})\n",
    "    # observations contain original observations and the action mask\n",
    "    # print(f\"Obs: {obs}, Action: {action}, done: {done}\")\n",
    "\n",
    "env.render()\n",
    "print(env.env.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Observation ({} dtype={}) outside given space ({})!', {'observation': array([15, 15,  7,  4, 15, -2, 15,  6, 12, 15, 15,  4,  9,  6,  3,  0,  1,\n        0,  1,  0,  2,  1,  1,  2,  0,  1,  1,  0,  1, -2,  4], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)}, None, Dict(action_mask:Box([0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], (26,), int8), observations:Box([-24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24\n -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24], [127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127\n 127 127 127 127 127 127 127 127 127 127 127 127 127], (31,), int8)))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m obs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m15\u001b[39m,  \u001b[38;5;241m7\u001b[39m,  \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m15\u001b[39m,  \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m15\u001b[39m,  \u001b[38;5;241m4\u001b[39m,  \u001b[38;5;241m9\u001b[39m,  \u001b[38;5;241m6\u001b[39m,  \u001b[38;5;241m3\u001b[39m,  \u001b[38;5;241m0\u001b[39m,  \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;241m0\u001b[39m,  \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;241m0\u001b[39m,  \u001b[38;5;241m2\u001b[39m,  \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;241m2\u001b[39m,  \u001b[38;5;241m0\u001b[39m,  \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;241m0\u001b[39m,  \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,  \u001b[38;5;241m4\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint8), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint8)}\n\u001b[0;32m----> 3\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_single_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/skybo/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:1234\u001b[0m, in \u001b[0;36mTrainer.compute_single_action\u001b[0;34m(self, observation, state, prev_action, prev_reward, info, input_dict, policy_id, full_fetch, explore, timestep, episode, unsquash_action, clip_action, unsquash_actions, clip_actions, **kwargs)\u001b[0m\n\u001b[1;32m   1232\u001b[0m pp \u001b[38;5;241m=\u001b[39m local_worker\u001b[38;5;241m.\u001b[39mpreprocessors[policy_id]\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pp \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(pp)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNoPreprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1234\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1235\u001b[0m observation \u001b[38;5;241m=\u001b[39m local_worker\u001b[38;5;241m.\u001b[39mfilters[policy_id](\n\u001b[1;32m   1236\u001b[0m     observation, update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;66;03m# Input-dict.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/skybo/lib/python3.9/site-packages/ray/rllib/models/preprocessors.py:275\u001b[0m, in \u001b[0;36mDictFlatteningPreprocessor.transform\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;129m@override\u001b[39m(Preprocessor)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: TensorType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(observation, array, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/skybo/lib/python3.9/site-packages/ray/rllib/models/preprocessors.py:72\u001b[0m, in \u001b[0;36mPreprocessor.check_shape\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_space\u001b[38;5;241m.\u001b[39mcontains(observation):\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     73\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservation (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dtype=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) outside given space (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)!\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     74\u001b[0m             observation, observation\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m     75\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_space,\n\u001b[1;32m     76\u001b[0m                 gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mBox) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_space)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservation for a Box/MultiBinary/MultiDiscrete space \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould be an np.array, not a Python list.\u001b[39m\u001b[38;5;124m\"\u001b[39m, observation)\n",
      "\u001b[0;31mValueError\u001b[0m: ('Observation ({} dtype={}) outside given space ({})!', {'observation': array([15, 15,  7,  4, 15, -2, 15,  6, 12, 15, 15,  4,  9,  6,  3,  0,  1,\n        0,  1,  0,  2,  1,  1,  2,  0,  1,  1,  0,  1, -2,  4], dtype=int8), 'action_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int8)}, None, Dict(action_mask:Box([0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], (26,), int8), observations:Box([-24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24\n -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24 -24], [127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127 127\n 127 127 127 127 127 127 127 127 127 127 127 127 127], (31,), int8)))"
     ]
    }
   ],
   "source": [
    "obs = {\n",
    "    \"observation\": np.array(\n",
    "        [\n",
    "            15,\n",
    "            15,\n",
    "            7,\n",
    "            4,\n",
    "            15,\n",
    "            -2,\n",
    "            15,\n",
    "            6,\n",
    "            12,\n",
    "            15,\n",
    "            15,\n",
    "            4,\n",
    "            9,\n",
    "            6,\n",
    "            3,\n",
    "            0,\n",
    "            1,\n",
    "            0,\n",
    "            1,\n",
    "            0,\n",
    "            2,\n",
    "            1,\n",
    "            1,\n",
    "            2,\n",
    "            0,\n",
    "            1,\n",
    "            1,\n",
    "            0,\n",
    "            1,\n",
    "            -2,\n",
    "            4,\n",
    "        ],\n",
    "        dtype=np.int8,\n",
    "    ),\n",
    "    \"action_mask\": np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=np.int8),\n",
    "}\n",
    "action = trainer.compute_single_action(obs, policy_id=agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try creating with rllib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
