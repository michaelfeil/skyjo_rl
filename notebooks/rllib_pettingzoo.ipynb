{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import os\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "from rlskyjo.environment import simple_skyjo_env\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.tune.registry import register_env\n",
    "from gym.spaces import Box\n",
    "from ray.rllib.agents.dqn.dqn_torch_model import DQNTorchModel\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.torch_utils import FLOAT_MAX\n",
    "from supersuit.multiagent_wrappers import pad_action_space_v0\n",
    "\n",
    "torch, nn = try_import_torch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchMaskedActions(DQNTorchModel):\n",
    "    \"\"\"PyTorch version of above ParametricActionsModel.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 obs_space,\n",
    "                 action_space,\n",
    "                 num_outputs,\n",
    "                 model_config,\n",
    "                 name,\n",
    "                 **kw):\n",
    "        DQNTorchModel.__init__(self, obs_space, action_space, num_outputs,\n",
    "                               model_config, name, **kw)\n",
    "\n",
    "        obs_len = obs_space.shape[0]-action_space.n\n",
    "\n",
    "        orig_obs_space = Box(shape=(obs_len,), low=obs_space.low[:obs_len], high=obs_space.high[:obs_len])\n",
    "        self.action_embed_model = TorchFC(orig_obs_space, action_space, action_space.n, model_config, name + \"_action_embed\")\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        # Extract the available actions tensor from the observation.\n",
    "        print(\"input_dict\",input_dict)\n",
    "        action_mask = input_dict[\"obs\"][\"action_mask\"]\n",
    "\n",
    "        # Compute the predicted action embedding\n",
    "        action_logits, _ = self.action_embed_model({\n",
    "            \"obs\": input_dict[\"obs\"]['observation']\n",
    "        })\n",
    "        # turns probit action mask into logit action mask\n",
    "        inf_mask = torch.clamp(torch.log(action_mask), -1e10, FLOAT_MAX)\n",
    "\n",
    "        return action_logits + inf_mask, state\n",
    "\n",
    "    def value_function(self):\n",
    "        return self.action_embed_model.value_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    alg_name = \"DQN\"\n",
    "    env_name  = \"pettingzoo_skyjo\"\n",
    "    ModelCatalog.register_custom_model(\n",
    "        \"pa_model\", TorchMaskedActions\n",
    "    )\n",
    "    # function that outputs the environment you wish to register.\n",
    "\n",
    "    def env_creator():\n",
    "        env = simple_skyjo_env.env(**{\"name\":env_name, \"num_players\": 2})\n",
    "        return env\n",
    "\n",
    "\n",
    "    config = deepcopy(get_agent_class(alg_name)._default_config)\n",
    "\n",
    "    register_env(env_name,\n",
    "                 lambda config: PettingZooEnv(env_creator()))\n",
    "\n",
    "    test_env = PettingZooEnv(env_creator())\n",
    "    obs_space = test_env.observation_space\n",
    "    print(\"obs_space\", obs_space)\n",
    "    act_space = test_env.action_space\n",
    "    print(\"act_space\", act_space)\n",
    "\n",
    "    config[\"multiagent\"] = {\n",
    "        \"policies\": {\n",
    "            \"draw\": (None, obs_space, act_space, {}),\n",
    "            \"place\": (None, obs_space, act_space, {}),\n",
    "        },\n",
    "        \"policy_mapping_fn\": lambda agent_id: agent_id.split(\"_\")[0]\n",
    "    }\n",
    "\n",
    "    config[\"num_gpus\"] = int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\"))\n",
    "    config[\"log_level\"] = \"DEBUG\"\n",
    "    config[\"num_workers\"] = 1\n",
    "    config[\"rollout_fragment_length\"] = 30\n",
    "    config[\"train_batch_size\"] = 200\n",
    "    config[\"horizon\"] = 200\n",
    "    config[\"no_done_at_end\"] = False\n",
    "    config[\"framework\"] = \"torch\"\n",
    "    config[\"model\"] = {\n",
    "        \"custom_model\": \"pa_model\",\n",
    "    }\n",
    "    config['n_step'] = 1\n",
    "\n",
    "    config[\"exploration_config\"] = {\n",
    "        # The Exploration class to use.\n",
    "        \"type\": \"EpsilonGreedy\",\n",
    "        # Config for the Exploration class' constructor:\n",
    "        \"initial_epsilon\": 0.1,\n",
    "        \"final_epsilon\": 0.0,\n",
    "        \"epsilon_timesteps\": 100000,  # Timesteps over which to anneal epsilon.\n",
    "    }\n",
    "    config['hiddens'] = []\n",
    "    config['dueling'] = False\n",
    "    config['env'] = env_name\n",
    "\n",
    "    \n",
    "\n",
    "    tune.run(\n",
    "        alg_name,\n",
    "        name=\"DQN\",\n",
    "        stop={\"timesteps_total\": 10000000},\n",
    "        checkpoint_freq=10,\n",
    "        config=config\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
